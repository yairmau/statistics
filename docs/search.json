[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Machine Learning",
    "section": "",
    "text": "home\nI’m teaching myself statistics and machine learning, and the best way to truly understand is to use the new tools I’ve acquired. This is what this website is for. It is mainly a reference guide for my future self.",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Statistics and Machine Learning",
    "section": "books",
    "text": "books\nThese are the books that I’ve read and recommend.\n\n\nModern Statistics: Intuition, Math, Python, R\nby Mike X Cohen\nGithub\n\n\nData-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control\nby Steven L. Brunton, J. Nathan Kutz The whole book is available in this website.",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "data/height.html",
    "href": "data/height.html",
    "title": "1  height data",
    "section": "",
    "text": "I found growth curves for girls and boys in Israel:\n\nurl girls, pdf girls\nurl boys, pdf boys\nurl both, png boys, png girls.\n\nFor example, see this:\n\nI used the great online resource Web Plot Digitizer v4 to extract the data from the images files. I captured all the growth curves as best as I could. The first step now is to get interpolated versions of the digitized data. For instance, see below the 50th percentile for boys:\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.optimize import curve_fit\nfrom scipy.special import erf\nfrom scipy.interpolate import UnivariateSpline\nimport matplotlib.animation as animation\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n# %matplotlib widget\n\n\n\n\ndefine useful arrays\nage_list = np.round(np.arange(2.0, 20.1, 0.1), 1)\nheight_list = np.round(np.arange(70, 220, 0.1), 1)\n\n\n\n\nimport sample data, boys 50th percentile\ndf_temp_boys_50th = pd.read_csv('../archive/data/height/boys-p50.csv', names=['age','height'])\nspline = UnivariateSpline(df_temp_boys_50th['age'], df_temp_boys_50th['height'], s=0.5)\ninterpolated = spline(age_list)\n\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(df_temp_boys_50th['age'], df_temp_boys_50th['height'], label='digitized data',\n        marker='o', markerfacecolor='None', markeredgecolor=\"black\", markersize=6, linestyle='None')\nax.plot(age_list, interpolated, label='interpolated', color=\"black\", linewidth=2)\nax.set(xlabel='age (years)',\n       ylabel='height (cm)',\n       xticks=np.arange(2, 21, 2),\n       title=\"boys, 50th percentile\"\n       )\nax.legend(frameon=False);\n\n\n\n\n\n\n\n\n\nLet’s do the same for all the other curves, and then save them to a file.\n\n\ninterpolate all growth curves\ncol_names = ['p05', 'p10', 'p25', 'p50', 'p75', 'p90', 'p95']\nfile_names_boys = ['boys-p05.csv', 'boys-p10.csv', 'boys-p25.csv', 'boys-p50.csv',\n                   'boys-p75.csv', 'boys-p90.csv', 'boys-p95.csv',]\nfile_names_girls = ['girls-p05.csv', 'girls-p10.csv', 'girls-p25.csv', 'girls-p50.csv',\n                   'girls-p75.csv', 'girls-p90.csv', 'girls-p95.csv',]\n\n# create dataframe with age column\ndf_boys = pd.DataFrame({'age': age_list})\ndf_girls = pd.DataFrame({'age': age_list})\n# loop over file names and read in data\nfor i, file_name in enumerate(file_names_boys):\n    # read in data\n    df_temp = pd.read_csv('../archive/data/height/' + file_name, names=['age','height'])\n    spline = UnivariateSpline(df_temp['age'], df_temp['height'], s=0.5)\n    df_boys[col_names[i]] = spline(age_list)\nfor i, file_name in enumerate(file_names_girls):\n    # read in data\n    df_temp = pd.read_csv('../archive/data/height/' + file_name, names=['age','height'])\n    spline = UnivariateSpline(df_temp['age'], df_temp['height'], s=0.5)\n    df_girls[col_names[i]] = spline(age_list)\n\n# make age index\ndf_boys.set_index('age', inplace=True)\ndf_boys.index = df_boys.index.round(1)\ndf_boys.to_csv('../archive/data/height/boys_height_vs_age_combined.csv', index=True)\ndf_girls.set_index('age', inplace=True)\ndf_girls.index = df_girls.index.round(1)\ndf_girls.to_csv('../archive/data/height/girls_height_vs_age_combined.csv', index=True)\n\n\nLet’s take a look at what we just did.\n\ndf_girls\n\n\n\n\n\n\n\n\np05\np10\np25\np50\np75\np90\np95\n\n\nage\n\n\n\n\n\n\n\n\n\n\n\n2.0\n79.269087\n80.794167\n83.049251\n85.155597\n87.475854\n89.779822\n90.882059\n\n\n2.1\n80.202106\n81.772053\n84.052858\n86.207778\n88.713405\n90.883740\n92.409913\n\n\n2.2\n81.130687\n82.706754\n85.011591\n87.211543\n89.856186\n91.940642\n93.416959\n\n\n2.3\n82.048325\n83.601023\n85.928399\n88.170313\n90.914093\n92.953965\n94.270653\n\n\n2.4\n82.948516\n84.457612\n86.806234\n89.087509\n91.897022\n93.927147\n95.226089\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19.6\n152.520938\n154.812286\n158.775277\n163.337149\n167.699533\n171.531349\n173.969235\n\n\n19.7\n152.534223\n154.814440\n158.791925\n163.310864\n167.704618\n171.519600\n173.980150\n\n\n19.8\n152.548001\n154.827666\n158.815071\n163.275852\n167.708562\n171.504730\n173.990964\n\n\n19.9\n152.562338\n154.853760\n158.845506\n163.231563\n167.711342\n171.486629\n174.001704\n\n\n20.0\n152.577300\n154.894521\n158.884019\n163.177444\n167.712936\n171.465189\n174.012396\n\n\n\n\n181 rows × 7 columns\n\n\n\n\n\nshow all interpolated curves for girls\nfig, ax = plt.subplots(figsize=(8, 6))\n# loop over col_names and plot each column\ncolors = sns.color_palette(\"Oranges\", len(col_names))\nfor col, color in zip(col_names, colors):\n    ax.plot(df_girls.index, df_girls[col], label=col, color=color)\nax.set(xlabel='age (years)',\n       ylabel='height (cm)',\n       xticks=np.arange(2, 21, 2),\n       title=\"growth curves for girls\\npercentile curves: 5, 10, 25, 50, 75, 90, 95\",\n       );\n\n\n\n\n\n\n\n\n\nLet’s now see the percentiles for girls age 20.\n\n\nplot cdf for girls, age 20\nfig, ax = plt.subplots(figsize=(8, 6))\npercentile_list = np.array([5, 10, 25, 50, 75, 90, 95])\ndata = df_girls.loc[20.0]\nax.plot(data, percentile_list, ls='', marker='o', markersize=6, color=\"black\")\nax.set(xlabel='height (cm)',\n         ylabel='percentile',\n         yticks=percentile_list,\n         title=\"cdf for girls, age 20\"\n         );\n\n\n\n\n\n\n\n\n\nI suspect that the heights in the population are normally distributed. Let’s check that. I’ll fit the data to the integral of a gaussian, because the percentiles correspond to a cdf. If a pdf is a gaussian, its cumulative is given by\n\n\\Phi(x) = \\frac{1}{2} \\left( 1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma \\sqrt{2}}\\right) \\right)\n\nwhere \\mu is the mean and \\sigma is the standard deviation of the distribution. The error function \\text{erf} is a sigmoid function, which is a good approximation for the cdf of the normal distribution.\n\n\ndefine functions\ndef erf_model(x, mu, sigma):\n    return 50 * (1 + erf((x - mu) / (sigma * np.sqrt(2))) )\n# initial guess for parameters: [mu, sigma]\np0 = [150, 6]\n# Calculate R-squared\ndef calculate_r2(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - (ss_res / ss_tot)\n\n\n\n\nfit model to data\ndata = df_girls.loc[20.0]\nparams, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                        bounds=([100, 3],   # lower bounds for mu and sigma\n                                [200, 10])  # upper bounds for mu and sigma\n                        )\n# store the parameters in the dataframe\npercentile_predicted = erf_model(data, *params)\n# R-squared value\nr2 = calculate_r2(percentile_list, percentile_predicted)\n\n\n\n\nshow results\nfig, ax = plt.subplots(figsize=(8, 6))\npercentile_list = np.array([5, 10, 25, 50, 75, 90, 95])\ndata = df_girls.loc[20.0]\nax.plot(data, percentile_list, ls='', marker='o', markersize=6, color=\"black\", label='data')\nfit = erf_model(height_list, *params)\nax.plot(height_list, fit, label='fit', color=\"red\", linewidth=2)\nax.text(150, 75, f'$\\mu$ = {params[0]:.1f} cm\\n$\\sigma$ = {params[1]:.1f} cm\\nR$^2$ = {r2:.6f}',\n        fontsize=14, bbox=dict(facecolor='white', alpha=0.5))\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       xlim=(140, 190),\n         ylabel='percentile',\n         yticks=percentile_list,\n         title=\"the data is very well fitted by a normal distribution\"\n         );\n\n\n\n\n\n\n\n\n\nAnother way of making sure that the model fits the data is to make a QQ plot. In this plot, the quantiles of the data are plotted against the quantiles of the normal distribution. If the data is normally distributed, the points should fall on a straight line.\n\n\nshow QQ plot\nfitted_quantiles = norm.cdf(data, loc=params[0], scale=params[1])\nexperimental_quantiles = percentile_list / 100\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_aspect('equal', adjustable='box')\nax.plot(experimental_quantiles, fitted_quantiles,\n        ls='', marker='o', markersize=6, color=\"black\",\n        label='qq points')\nax.plot([0, 1], [0, 1], color='red', linewidth=2, label=\"1:1 line\")\nax.set(xlabel='empirical quantiles',\n       ylabel='fitted quantiles',\n       xlim=(0, 1),\n       ylim=(0, 1),\n       title=\"QQ plot\")\nax.legend(frameon=False)\n\n\n\n\n\n\n\n\n\nGreat, now we just need to do exactly the same for both sexes, and all the ages. I chose to divide age from 2 to 20 into 0.1 intervals.\n\n\ncreate dataframes to store the parameters mu, sigma, r2\ndf_stats_boys = pd.DataFrame(index=age_list, columns=['mu', 'sigma', 'r2'])\ndf_stats_boys['mu'] = 0.0\ndf_stats_boys['sigma'] = 0.0\ndf_stats_boys['r2'] = 0.0\ndf_stats_girls = pd.DataFrame(index=age_list, columns=['mu', 'sigma', 'r2'])\ndf_stats_girls['mu'] = 0.0\ndf_stats_girls['sigma'] = 0.0\ndf_stats_girls['r2'] = 0.0\n\n\n\n\nfit model to all the data\np0 = [80, 3]\n# loop over ages in the index, calculate mu and sigma\nfor i in df_boys.index:\n    # fit the model to the data\n    data = df_boys.loc[i]\n    params, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                          bounds=([70, 2],   # lower bounds for mu and sigma\n                                  [200, 10])  # upper bounds for mu and sigma\n                         )\n    # store the parameters in the dataframe\n    df_stats_boys.at[i, 'mu'] = params[0]\n    df_stats_boys.at[i, 'sigma'] = params[1]\n    percentile_predicted = erf_model(data, *params)\n    # R-squared value\n    r2 = calculate_r2(percentile_list, percentile_predicted)\n    df_stats_boys.at[i, 'r2'] = r2\n    p0 = params\n# same for girls\np0 = [80, 3]\nfor i in df_girls.index:\n    # fit the model to the data\n    data = df_girls.loc[i]\n    params, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                          bounds=([70, 3],   # lower bounds for mu and sigma\n                                  [200, 10])  # upper bounds for mu and sigma\n                         )\n    # store the parameters in the dataframe\n    df_stats_girls.at[i, 'mu'] = params[0]\n    df_stats_girls.at[i, 'sigma'] = params[1]\n    percentile_predicted = erf_model(data, *params)\n    # R-squared value\n    r2 = calculate_r2(percentile_list, percentile_predicted)\n    df_stats_girls.at[i, 'r2'] = r2\n    p0 = params\n\n# save the dataframes to csv files\ndf_stats_boys.to_csv('../archive/data/height/boys_height_stats.csv', index=True)\ndf_stats_girls.to_csv('../archive/data/height/girls_height_stats.csv', index=True)\n\n\nLet’s see what we got. The top panel in the graph shows the average height for boys and girls, the middle panel shows the coefficient of variation (\\sigma/\\mu), and the bottom panel shows the R2 of the fit (note that the range is very close to 1).\n\ndf_stats_boys\n\n\n\n\n\n\n\n\nmu\nsigma\nr2\n\n\n\n\n2.0\n86.463069\n3.563785\n0.999511\n\n\n2.1\n87.374895\n3.596583\n0.999676\n\n\n2.2\n88.269676\n3.627433\n0.999742\n\n\n2.3\n89.148086\n3.657263\n0.999752\n\n\n2.4\n90.010783\n3.686764\n0.999733\n\n\n...\n...\n...\n...\n\n\n19.6\n176.802810\n7.134561\n0.999991\n\n\n19.7\n176.845789\n7.135786\n0.999994\n\n\n19.8\n176.892196\n7.137430\n0.999995\n\n\n19.9\n176.942521\n7.139466\n0.999990\n\n\n20.0\n176.997255\n7.141858\n0.999976\n\n\n\n\n181 rows × 3 columns\n\n\n\n\n\nplot results\nfig, ax = plt.subplots(3,1, figsize=(8, 10), sharex=True)\nfig.subplots_adjust(left=0.15)\nax[0].plot(df_stats_boys['mu'], label='boys', lw=2)\nax[0].plot(df_stats_girls['mu'], label='girls', lw=2)\nax[0].legend(frameon=False)\n\nax[1].plot(df_stats_boys['sigma'] / df_stats_boys['mu'], lw=2)\nax[1].plot(df_stats_girls['sigma'] / df_stats_girls['mu'], lw=2)\n\nax[2].plot(df_stats_boys.index, df_stats_boys['r2'], label=r'$r2$ boys', lw=2)\nax[2].plot(df_stats_girls.index, df_stats_girls['r2'], label=r'$r2$ girls', lw=2)\n\nax[0].set(ylabel='average height (cm)',)\nax[1].set(ylabel='CV',\n          ylim=[0,0.055])\nax[2].set(xlabel='age (years)',\n            ylabel=r'$R^2$',\n            xticks=np.arange(2, 21, 2),\n          );\n\n\n\n\n\n\n\n\n\nLet’s see how the pdfs for boys and girls move and morph as age increases.\n\n\nproduce dataframes for pre-calculated pdfs\nage_list_string = age_list.astype(str).tolist()\ndf_pdf_boys = pd.DataFrame(index=height_list, columns=age_list_string)\ndf_pdf_girls = pd.DataFrame(index=height_list, columns=age_list_string)\n\nfor age in df_pdf_boys.columns:\n    age_float = round(float(age), 1)\n    df_pdf_boys[age] = norm.pdf(height_list,\n                                loc=df_stats_boys.loc[age_float]['mu'],\n                                scale=df_stats_boys.loc[age_float]['sigma'])\nfor age in df_pdf_girls.columns:\n    age_float = round(float(age), 1)\n    df_pdf_girls[age] = norm.pdf(height_list,\n                                loc=df_stats_girls.loc[age_float]['mu'],\n                                scale=df_stats_girls.loc[age_float]['sigma'])\n\n\n\ndf_pdf_girls\n\n\n\n\n\n\n\n\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n...\n19.1\n19.2\n19.3\n19.4\n19.5\n19.6\n19.7\n19.8\n19.9\n20.0\n\n\n\n\n70.0\n0.000006\n2.962419e-06\n1.229580e-06\n4.740717e-07\n1.893495e-07\n7.928033e-08\n3.395629e-08\n1.454961e-08\n6.214658e-09\n2.698367e-09\n...\n3.876760e-46\n4.998212e-46\n6.108274e-46\n6.965756e-46\n7.300518e-46\n6.928073e-46\n5.866310e-46\n4.367574e-46\n2.817087e-46\n1.550490e-46\n\n\n70.1\n0.000007\n3.369929e-06\n1.401926e-06\n5.423176e-07\n2.172465e-07\n9.118694e-08\n3.914667e-08\n1.681357e-08\n7.199311e-09\n3.133161e-09\n...\n4.821662e-46\n6.212999e-46\n7.589544e-46\n8.652519e-46\n9.067461e-46\n8.605908e-46\n7.289698e-46\n5.430839e-46\n3.506265e-46\n1.932327e-46\n\n\n70.2\n0.000008\n3.830459e-06\n1.597215e-06\n6.199308e-07\n2.490751e-07\n1.048086e-07\n4.509972e-08\n1.941687e-08\n8.334521e-09\n3.635676e-09\n...\n5.995467e-46\n7.721230e-46\n9.427830e-46\n1.074523e-45\n1.125944e-45\n1.068759e-45\n9.056344e-46\n6.751373e-46\n4.363019e-46\n2.407630e-46\n\n\n70.3\n0.000009\n4.350475e-06\n1.818328e-06\n7.081296e-07\n2.853621e-07\n1.203810e-07\n5.192270e-08\n2.240831e-08\n9.642428e-09\n4.216078e-09\n...\n7.453283e-46\n9.593350e-46\n1.170864e-45\n1.334099e-45\n1.397806e-45\n1.326973e-45\n1.124851e-45\n8.391039e-46\n5.427845e-46\n2.999137e-46\n\n\n70.4\n0.000010\n4.937172e-06\n2.068480e-06\n8.082806e-07\n3.267014e-07\n1.381707e-07\n5.973725e-08\n2.584341e-08\n1.114829e-08\n4.885994e-09\n...\n9.263403e-46\n1.191661e-45\n1.453785e-45\n1.655996e-45\n1.734906e-45\n1.647188e-45\n1.396806e-45\n1.042648e-45\n6.750965e-46\n3.735083e-46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n219.5\n0.000000\n5.214425e-307\n1.377605e-289\n3.568527e-277\n6.457994e-266\n2.232144e-255\n6.340272e-246\n9.969867e-238\n1.389324e-230\n5.441854e-224\n...\n5.200570e-18\n5.741874e-18\n6.194642e-18\n6.495054e-18\n6.583545e-18\n6.417949e-18\n5.986319e-18\n5.315101e-18\n4.468701e-18\n3.538724e-18\n\n\n219.6\n0.000000\n1.813597e-307\n5.050074e-290\n1.356408e-277\n2.537010e-266\n9.046507e-256\n2.642444e-246\n4.256155e-238\n6.055129e-231\n2.417510e-224\n...\n4.558798e-18\n5.035058e-18\n5.433557e-18\n5.698034e-18\n5.775970e-18\n5.630212e-18\n5.250299e-18\n4.659675e-18\n3.915265e-18\n3.097919e-18\n\n\n219.7\n0.000000\n6.302763e-308\n1.849870e-290\n5.151948e-278\n9.959447e-267\n3.663840e-256\n1.100546e-246\n1.815751e-238\n2.637298e-231\n1.073274e-224\n...\n3.995288e-18\n4.414220e-18\n4.764871e-18\n4.997654e-18\n5.066279e-18\n4.938013e-18\n4.603699e-18\n4.084117e-18\n3.429566e-18\n2.711382e-18\n\n\n219.8\n0.000000\n2.188653e-308\n6.771033e-291\n1.955386e-278\n3.906942e-267\n1.482823e-256\n4.580523e-247\n7.741154e-239\n1.147918e-231\n4.761829e-225\n...\n3.500614e-18\n3.869030e-18\n4.177503e-18\n4.382343e-18\n4.442754e-18\n4.329907e-18\n4.035791e-18\n3.578814e-18\n3.003413e-18\n2.372514e-18\n\n\n219.9\n0.000000\n7.594139e-309\n2.476504e-291\n7.416066e-279\n1.531537e-267\n5.997065e-257\n1.905138e-247\n3.298116e-239\n4.993198e-232\n2.111339e-225\n...\n3.066470e-18\n3.390384e-18\n3.661688e-18\n3.841895e-18\n3.895062e-18\n3.795805e-18\n3.537115e-18\n3.135297e-18\n2.629596e-18\n2.075507e-18\n\n\n\n\n1500 rows × 181 columns\n\n\n\n\n\nplotly widget\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'notebook'\n\n# create figure\nfig = go.Figure()\n\n# assume both dataframes have the same columns (ages) and index (height)\nages = df_pdf_boys.columns\nx_vals = df_pdf_boys.index\n\n# add traces: 2 per age (boys and girls), all hidden except the first pair\nfor i, age in enumerate(ages):\n    fig.add_trace(go.Scatter(x=x_vals, y=df_pdf_boys[age], name=f'Boys {age}', \n                             line=dict(color='#1f77b4'), visible=(i == 0)))\n    fig.add_trace(go.Scatter(x=x_vals, y=df_pdf_girls[age], name=f'Girls {age}', \n                             line=dict(color='#ff7f0e'), visible=(i == 0)))\n\n# create slider steps\nsteps = []\nfor i, age in enumerate(ages):\n    vis = [False] * (2 * len(ages))\n    vis[2*i] = True      # boys trace\n    vis[2*i + 1] = True  # girls trace\n\n    steps.append(dict(\n        method='update',\n        args=[{'visible': vis},\n              {'title': f'Height Distribution - Age: {age}'}],\n        label=str(age)\n    ))\n\n# define slider\nsliders = [dict(\n    active=0,\n    currentvalue={\"prefix\": \"Age: \"},\n    pad={\"t\": 50},\n    steps=steps\n)]\n\n# update layout\nfig.update_layout(\n    sliders=sliders,\n    title='Height Distribution by Age',\n    xaxis_title='Height (cm)',\n    yaxis_title='Density',\n    yaxis=dict(range=[0, 0.12]),\n    showlegend=True,\n    height=600,\n    width=800\n)\n\nfig.show()\n\n\n                                                \n\n\nA few notes about what we can learn from the analysis above.\n\nMy impression that 12-year-old girls are taller than boys is indeed true.\nBoys and girls have very similar distributions up to age 11.\nFrom age 11 to 13 girls are on average taller than boys.\nFrom age 13 boys become taller than girls, on average.\nThe graph showing the coefficient of variation is interesting. CV for girls peaks roughtly at age 12, and for boys it peaks around age 14. These local maxima may be explained by the wide variability in the age ofpuberty onset.\nThe height distribution for each sex, across all ages, is indeed extremely well described by the normal distribution. What biological factors may account for such a fact?\n\nI’ll plot one last graph from now, let’s see what we can learn from it. Let’s see the pdf for boys and girls across three age groups: 8, 12, and 15 year olds.\n\n\ncomparison across three ages\nfig, ax = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\nfig.subplots_adjust(hspace=0.1)\nages_to_plot = [8.0, 12.0, 15.0]\n\nfor i, age in enumerate(ages_to_plot):\n    pdf_boys = norm.pdf(height_list, loc=df_stats_boys.loc[age]['mu'], scale=df_stats_boys.loc[age]['sigma'])\n    pdf_girls = norm.pdf(height_list, loc=df_stats_girls.loc[age]['mu'], scale=df_stats_girls.loc[age]['sigma'])\n    ax[i].plot(height_list, pdf_boys, label='boys', color='tab:blue')\n    ax[i].plot(height_list, pdf_girls, label='girls', color='tab:orange')\n    ax[i].text(0.98, 0.98, f'age: {age} years', transform=ax[i].transAxes, verticalalignment='top', horizontalalignment='right')\n    ax[i].set(ylabel='pdf',\n              ylim=(0, 0.07),\n            )\nax[2].legend(frameon=False)\nax[2].set(xlabel='height (cm)',\n          xlim=(100, 200),);\n\n\n\n\n\n\n\n\n\n\nIndeed, boys and girls age 8 have the exact same height distribution.\n12-year-old girls are indeed taller than boys, on average. This difference is relatiely small, though.\nBy age 15 boys have long surpassed girls in height, and the difference is quite large. Boys still have some growing to do, but girls are mostly done growing.",
    "crumbs": [
      "data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>height data</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html",
    "href": "t_test/t_test_one_sample.html",
    "title": "2  one-sample t-test",
    "section": "",
    "text": "2.1 Question\nI measured the height of 10 adult men. Were they sampled from the general population of men?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#hypotheses",
    "href": "t_test/t_test_one_sample.html#hypotheses",
    "title": "2  one-sample t-test",
    "section": "2.2 Hypotheses",
    "text": "2.2 Hypotheses\n\nNull hypothesis: The sample mean is equal to the population mean. In this case, the answer would be “yes”\nAlternative hypothesis: The sample mean is not equal to the population mean. Answer would be “no”.\nSignificance level: 0.05\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_1samp, t\n%matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[20.0, 'mu']\nsigma_boys = df_boys.loc[20.0, 'sigma']\n\n\nLet’s start with a sample of 10.\n\n\ngenerate data\nN = 10\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample10 = norm.rvs(size=N, loc=mu_boys+2, scale=sigma_boys)\n\n\n\n\nplot sample against population pdf\nheight_list = np.arange(140, 220, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nax.eventplot(sample10, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='gray', label='sample')\n\nax.text(190, 0.04, \n       f\"sample mean: {sample10.mean():.2f} cm\\nsample std: {sample10.std(ddof=1):.2f} cm\", \n       ha='left', va='top', color='gray')\n\nax.text(190, 0.02, \n       f\"pop. mean: {mu_boys:.2f} cm\\npop. std: {sigma_boys:.2f} cm\", \n       ha='left', va='top', color='tab:blue')\n\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n       title=\"men (age 20)\",\n       xlim=(140, 220),\n       );\n\n\n\n\n\nThe t value is calculated as follows: \nt = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}\n\nwhere\n\n\\bar{x}: sample mean\n\\mu: population mean\ns: sample standard deviation\nn: sample size\n\nLet’s try the formula above and compare it with scipy’s ttest_1samp function.\n\n\ncalculate t-value\nt_value_formula = (sample10.mean() - mu_boys) / (sample10.std(ddof=1) / np.sqrt(N))\nt_value_scipy = ttest_1samp(sample10, popmean=mu_boys)\nprint(f\"t-value (formula): {t_value_formula:.3f}\")\nprint(f\"t-value (scipy): {t_value_scipy.statistic:.3f}\")\n\n\nt-value (formula): 1.759\nt-value (scipy): 1.759\n\n\nLet’s convert this t value to a p value. It is easy to visualize the p value by ploting the pdf for the t distribution. The p value is the area under the curve for t greater than the t value and smaller than the negative t value.\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.10),\n                        xytext=(t_value_scipy.statistic, 0.30),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\nax.annotate(f\"-t value = -{t_value_scipy.statistic:.3f}\",\n                        xy=(-t_value_scipy.statistic, 0.10),\n                        xytext=(-t_value_scipy.statistic, 0.30),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(np.abs(t_array) &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=10)\",\n       );\n\n\n\n\n\n\n\n\n\nThe p value is the fraction of the t distribution that is more extreme than the observed t value. If the p value is less than the significance level, we reject the null hypothesis. In this case, the p value is larger than the significance level, so we fail to reject the null hypothesis. This means that we do not have enough evidence to say that the sample mean is different from the population mean. In other words, we cannot conclude that the 10 men samples were drawn from a distribution different than the general population.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#increase-the-sample-size",
    "href": "t_test/t_test_one_sample.html#increase-the-sample-size",
    "title": "2  one-sample t-test",
    "section": "2.3 increase the sample size",
    "text": "2.3 increase the sample size\nLet’s see what happens when we increase the sample size to 100.\n\n\ngenerate data\nN = 100\n# set scipy seed for reproducibility\nnp.random.seed(628)\nsample100 = norm.rvs(size=N, loc=mu_boys+2, scale=sigma_boys)\n\n\n\n\nplot sample against population pdf\nheight_list = np.arange(140, 220, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nax.eventplot(sample100, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='gray', label='sample')\n\nax.text(190, 0.04, \n       f\"sample mean: {sample100.mean():.2f} cm\\nsample std: {sample100.std(ddof=1):.2f} cm\", \n       ha='left', va='top', color='gray')\n\nax.text(190, 0.02, \n       f\"pop. mean: {mu_boys:.2f} cm\\npop. std: {sigma_boys:.2f} cm\", \n       ha='left', va='top', color='tab:blue')\n\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n       title=\"men (age 20)\",\n       xlim=(140, 220),\n       );\n\n\n\n\n\n\n\n\n\n\n\ncalculate t-value\nt_value_scipy = ttest_1samp(sample100, popmean=mu_boys)\nprint(f\"t-value: {t_value_scipy.statistic:.3f}\")\nprint(f\"p-value: {t_value_scipy.pvalue:.3f}\")\n\n\nt-value: 2.675\np-value: 0.009\n\n\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.03),\n                        xytext=(t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\nax.annotate(f\"-t value = -{t_value_scipy.statistic:.3f}\",\n                        xy=(-t_value_scipy.statistic, 0.03),\n                        xytext=(-t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(np.abs(t_array) &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=100)\",\n       );",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#question-2",
    "href": "t_test/t_test_one_sample.html#question-2",
    "title": "2  one-sample t-test",
    "section": "2.4 Question 2",
    "text": "2.4 Question 2\nCan we say that the sampled men are taller than the general population?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#hypotheses-1",
    "href": "t_test/t_test_one_sample.html#hypotheses-1",
    "title": "2  one-sample t-test",
    "section": "2.5 Hypotheses",
    "text": "2.5 Hypotheses\n\nNull hypothesis: The sample mean is equal to the population mean.\nAlternative hypothesis: The sample mean is higher the population mean.\nSignificance level: 0.05\n\nThe analysis is the same as before, but we will use a one-tailed test. The t statistic is the same, but the p value is smaller, since we account for a smaller portion of the total area of the pdf.\n\n\ncalculate t-value and p-value\nt_value_scipy = ttest_1samp(sample100, popmean=mu_boys, alternative='greater')\nprint(f\"t-value: {t_value_scipy.statistic:.3f}\")\nprint(f\"p-value: {t_value_scipy.pvalue:.3f}\")\n\n\nt-value: 2.675\np-value: 0.004\n\n\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.03),\n                        xytext=(t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(t_array &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=100)\",\n       );\n\n\n\n\n\n\n\n\n\nThe answer is yes: the sampled men are significantly taller than the general population, since the p value is smaller than the significance level.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_independent_samples.html",
    "href": "t_test/t_test_independent_samples.html",
    "title": "3  independent samples t-test",
    "section": "",
    "text": "3.1 Question\nAre 12-year old girls significantly taller than 12-year old boys?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_independent_samples.html#hypotheses",
    "href": "t_test/t_test_independent_samples.html#hypotheses",
    "title": "3  independent samples t-test",
    "section": "3.2 Hypotheses",
    "text": "3.2 Hypotheses\n\nNull hypothesis: Girls and boys have the same mean height.\nAlternative hypothesis: Girls are significantly taller.\nSignificance level: 0.05\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\n\nIn this example, we sampled 10 boys and 14 girls. See below the samples data and their underlying distributions.\n\n\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nShow the code\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    )\n\n\n\n\n\n\n\n\n\nTo answer the question, we will use an independent samples t-test.\n\\begin{align}\nt &= \\frac{\\bar{X}_1 - \\bar{X}_2}{\\Theta} \\\\\n\\Theta &= \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\end{align}\nThis is a generalization of the one-sample t-test. If we take one of the samples to be infinite, we get the one-sample t-test.\nWe can compute the t-statistic by ourselves, and compare the results with those of scipy.stats.ttest_ind. Because we are interested in the difference between the means, we will use the equal_var=False option to compute Welch’s t-test. Also, because we are testing the alternative hypothesis that girls are taller, we will use the one sided test.\n\n\nShow the code\nTheta = np.sqrt(sample_boys.std(ddof=1)**2/sample_boys.size + \\\n                sample_girls.std(ddof=1)**2/sample_girls.size)\nt_stat = (sample_boys.mean() - sample_girls.mean()) / Theta\ndof = N_boys + N_girls - 2\np_val = t.cdf(t_stat, dof)\n\n# the option alternative=\"less\" is used because we are testing whether the first sample (boys) is less than the second sample (girls)\nt_value_scipy = ttest_ind(sample_boys, sample_girls, equal_var=False, alternative=\"less\")\n\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\nprint(f\"t-statistic (scipy): {t_value_scipy.statistic:.3f}, p-value (scipy): {t_value_scipy.pvalue:.3f}\")\n\n\nt-statistic: -0.999, p-value: 0.164\nt-statistic (scipy): -0.999, p-value (scipy): 0.165\n\n\nWe got the exact same results :)\nNow let’s visualize what the p-value means.\n\n\nvisualize t-distribution\n# degrees of freedom\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.25),\n                        xytext=(t_value_scipy.statistic, 0.35),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(t_array &lt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (dof=22)\",\n       );\n\n\n\n\n\n\n\n\n\nBecause the p-value is higher than the significance level, we fail to reject the null hypothesis. This means that, based on the data, we cannot conclude that girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_independent_samples.html#increasing-sample-size",
    "href": "t_test/t_test_independent_samples.html#increasing-sample-size",
    "title": "3  independent samples t-test",
    "section": "3.3 increasing sample size",
    "text": "3.3 increasing sample size\nLet’s increase the sample size to see how it affects the p-value. We’ll sample 250 boys and 200 girls now.\n\n\ngenerate data\nN_boys = 250\nN_girls = 200\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nShow the code\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nTheta = np.sqrt(sample_boys.std(ddof=1)**2/sample_boys.size + \\\n                sample_girls.std(ddof=1)**2/sample_girls.size)\nt_stat = (sample_boys.mean() - sample_girls.mean()) / Theta\ndof = N_boys + N_girls - 2\np_val = t.cdf(t_stat, dof)\n\n# the option alternative=\"less\" is used because we are testing whether the first sample (boys) is less than the second sample (girls)\nt_value_scipy = ttest_ind(sample_boys, sample_girls, equal_var=False, alternative=\"less\")\n\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\nprint(f\"t-statistic (scipy): {t_value_scipy.statistic:.3f}, p-value (scipy): {t_value_scipy.pvalue:.3f}\")\n\n\nt-statistic: -2.639, p-value: 0.004\nt-statistic (scipy): -2.639, p-value (scipy): 0.004\n\n\nWe found now a p-value lower than the significance level, so we reject the null hypothesis. This means that, based on the data, we can conclude that girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "confidence_interval/basic_concepts.html",
    "href": "confidence_interval/basic_concepts.html",
    "title": "4  basic concepts",
    "section": "",
    "text": "Suppose we randomly select 30 seven-year-old boys from schools around the country and measure their heights (this is our sample). We’d like to use their average height to estimate the true average height of all seven-year-old boys nationwide (the population). Because different samples of 30 boys would yield slightly different averages, we need a way to quantify that uncertainty. A confidence interval gives us a range—based on our sample data—that expresses what we would expect to find if we were to repeat this sampling process many times.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\nimport scipy\nfrom matplotlib.lines import Line2D\nimport matplotlib.gridspec as gridspec\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nage = 7.0\nmu_boys = df_boys.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\n\n\nSee the height distribution for seven-year-old boys. Below it we see the means for 20 samples of groups of 30 boys. The 95% confidence interval is the range of values that, on average, 95% of the samples CI contain the true population mean. In this case, this amounts to one out of the 20 samples.\n\n\nplot samples against population pdf\nnp.random.seed(628)\nheight_list = np.arange(90, 150, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig = plt.figure(figsize=(8, 6))\ngs = gridspec.GridSpec(2, 1, height_ratios=[0.1, 0.9])\ngs.update(left=0.09, right=0.86,top=0.98, bottom=0.06, hspace=0.30, wspace=0.05)\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[1, 0])\n\nax0.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nN_samples = 20\nN = 30\n\nfor i in range(N_samples):\n    sample = norm.rvs(loc=mu_boys, scale=sigma_boys, size=N)\n    sample_mean = sample.mean()\n    # confidence interval\n    alpha = 0.05\n    z_crit = scipy.stats.t.isf(alpha/2, N-1)\n    CI = z_crit * sample.std(ddof=1) / np.sqrt(N)\n    ax1.errorbar(sample_mean, i, xerr=CI, fmt='o', color='tab:blue', \n                 label=f'sample {i+1}' if i == 0 else \"\", capsize=0)\n\n\nfrom matplotlib.patches import ConnectionPatch\nline = ConnectionPatch(xyA=(mu_boys, pdf_boys.max()), xyB=(mu_boys, -1), coordsA=\"data\", coordsB=\"data\",\n                      axesA=ax0, axesB=ax1, color=\"gray\", linestyle='--', linewidth=1.5, alpha=0.5)\nax1.add_artist(line)\n\nax1.annotate(\n        '',\n        xy=(mu_boys + 5, 13),  # tip of the arrow (first error bar, y=0)\n        xytext=(mu_boys + 5 + 13, 13),  # text location\n        arrowprops=dict(arrowstyle='-&gt;', lw=2, color='black'),\n        fontsize=13,\n        color='tab:blue',\n        ha='left',\n        va='center'\n)\n\nax1.text(mu_boys + 5 + 2, 12, \"on average, the CI\\nof 1 out of 20 samples\\n\"\n         r\"($\\alpha=5$% significance level)\"\n          \"\\nwill not contain\\nthe population mean\",\n          va=\"top\", fontsize=12)\n\n# write \"sample i\" for each error bar\nfor i in range(N_samples):\n    ax1.text(mu_boys -10, i, f'sample {N_samples-i:02d}', \n             fontsize=13, color='black',\n             ha='right', va='center')\n\n# ax.legend(frameon=False)\nax0.spines['top'].set_visible(False)\nax0.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\n\nax0.set(xticks=np.arange(90, 151, 10),\n        xlim=(90, 150),\n        xlabel='height (cm)',\n        # xticklabels=[],\n        yticks=[],\n        ylabel='pdf',\n        )\nax1.set(xticks=[],\n        xlim=(90, 150),\n        ylim=(-1, N_samples),\n        yticks=[],\n       );",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>basic concepts</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html",
    "href": "confidence_interval/analytical_confidence_interval.html",
    "title": "5  analytical confidence interval",
    "section": "",
    "text": "5.1 CLT\nWe wish to compute the confidence interval for the mean height of 7-year-old boys, for a sample of size N.\nWe will start our journey with a refresher of the Central Limit Theorem (CLT).\nThe Central Limit Theorem states that the sampling distribution of the sample mean\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i\napproaches a normal distribution as the sample size N increases, regardless of the shape of the population distribution. This normal distribution can be expressed as:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{N}\\right),\nwhere \\mu and \\sigma^2 are the population mean and variance, respectively. When talking about samples, we use \\bar{x} and s^2 to denote the sample mean and variance.\nLet’s visualize this. The graph below shows how the sample size N affects the sampling distribution of the sample mean \\bar{X}. The higher the sample size, the more concentrated the distribution becomes around the population mean \\mu. If we take N to be infinity, the sampling distribution of the sample mean becomes a delta function at \\mu, and we will know the exact value of the population mean.\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[7.0, 'mu']\nsigma_boys = df_boys.loc[7.0, 'sigma']\nplot pdfs as function of sample size\nfig, ax = plt.subplots(1,2, figsize=(10, 6), sharex=True, sharey=True)\n\nheight_list = np.arange(mu_boys-12, mu_boys+12, 0.01)\nN_list = [10, 30, 100]\nalpha_list = [0.4, 0.6, 1.0]\n\ncolors = plt.cm.hot([0.6, 0.3, 0.1])\n\nN_samples = 1000\nnp.random.seed(628)\nmean_list_10 = []\nmean_list_30 = []\nmean_list_100 = []\nfor i in range(N_samples):\n    mean_list_10.append(np.mean(norm.rvs(size=10, loc=mu_boys, scale=sigma_boys)))\n    mean_list_30.append(np.mean(norm.rvs(size=30, loc=mu_boys, scale=sigma_boys)))\n    mean_list_100.append(np.mean(norm.rvs(size=100, loc=mu_boys, scale=sigma_boys)))\n\nalpha = 0.05\n\n# z_alpha_over_two = norm(loc=mu_boys, scale=SE).ppf(1 - alpha / 2)\n# z_alpha_over_two = np.round(z_alpha_over_two, 2)\n\nfor i,N in enumerate(N_list):\n    SE = sigma_boys / np.sqrt(N)\n    ax[0].plot(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list),\n            color=colors[i], label=f\"N={N}\")\n    \nax[1].hist(mean_list_10, bins=30, density=True, color=colors[0], label=\"N=10\", align='mid', histtype='step')\nax[1].hist(mean_list_30, bins=30, density=True, color=colors[1], label=\"N=10\", align='mid', histtype='step')\nax[1].hist(mean_list_100, bins=30, density=True, color=colors[2], label=\"N=10\", align='mid', histtype='step')\n\nax[1].text(0.99, 0.98, \"number of samples\\n1000\", ha='right', va='top', transform=ax[1].transAxes, fontsize=14)\n\nax[0].legend(frameon=False)\nax[0].set(xlabel=\"height (cm)\",\n       ylabel=\"pdf\",\n       title=\"analytical\"\n       )\nax[1].set(xlabel=\"height (cm)\",\n          title=\"numerical\"\n          )\n# title that hovers over both subplots\nfig.suptitle(f\"Distribution of the sample means for 3 different sample sizes\");",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#confidence-interval-1",
    "href": "confidence_interval/analytical_confidence_interval.html#confidence-interval-1",
    "title": "5  analytical confidence interval",
    "section": "5.2 confidence interval 1",
    "text": "5.2 confidence interval 1\nLet’s use now the sample size N=30. The confidence interval for a significance level \\alpha=0.05 is the interval that leaves \\alpha/2 of the pdf area in each tail of the distribution.\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.0, hspace=0.1)\nN = 30\nSE = sigma_boys / np.sqrt(N)\n\nh_min = np.round(norm(loc=mu_boys, scale=SE).ppf(0.001), 2)\nh_max = np.round(norm(loc=mu_boys, scale=SE).ppf(0.999), 2)\nheight_list = np.arange(h_min, h_max, 0.01)\n\nalpha = 0.05\nz_alpha_over_two_hi = np.round(norm(loc=mu_boys, scale=SE).ppf(1 - alpha / 2), 2)\nz_alpha_over_two_lo = np.round(norm(loc=mu_boys, scale=SE).ppf(alpha / 2), 2)\n\n\nax[0].plot(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list))\nax[1].plot(height_list, norm(loc=mu_boys, scale=SE).cdf(height_list))\n\nax[0].fill_between(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list),\n                   where=((height_list &gt; z_alpha_over_two_hi) | (height_list &lt; z_alpha_over_two_lo)),\n                   color='tab:blue', alpha=0.5,\n                   label='rejection region')\n\nax[0].annotate(f\"\",\n               xy=(z_alpha_over_two_hi, 0.02),\n               xytext=(z_alpha_over_two_lo, 0.02),\n               arrowprops=dict(arrowstyle=\"&lt;-&gt;\", lw=1.5, color='black', shrinkA=0.0, shrinkB=0.0),\n               )\nax[1].text(h_max+0.15, norm(loc=mu_boys, scale=SE).cdf(z_alpha_over_two_lo), r\"$\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].text(h_max+0.15, norm(loc=mu_boys, scale=SE).cdf(z_alpha_over_two_hi), r\"$1-\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].axhline(alpha/2, color='gray', linestyle=':')\nax[1].axhline(1-alpha/2, color='gray', linestyle=':')\nax[0].text(mu_boys, 0.03, \"95% confidence interval\", ha=\"center\")\nax[0].set(ylim=(0, 0.42),\n          ylabel=\"pdf\",\n          title=r\"significance level $\\alpha$ = 0.05\",\n          )\nax[1].set(ylim=(-0.1, 1.1),\n          xlim=(h_min, h_max),\n          ylabel=\"cdf\",\n          xlabel=\"height (cm)\",\n          );\n\n\n\n\n\n\n\n\n\nThat’s it. That’s the whole story.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#confidence-interval-2",
    "href": "confidence_interval/analytical_confidence_interval.html#confidence-interval-2",
    "title": "5  analytical confidence interval",
    "section": "5.3 confidence interval 2",
    "text": "5.3 confidence interval 2\nThe rest is repackaging the above in a slightly different way. Instead of finding the top and bottom of the confidence interval according to the cdf of a normal distribution of mean \\mu and variance \\sigma^2/N, we first standardize this distribution to a standard normal distribution Z \\sim N(0,1), compute the confidence interval for Z, and then transform it back to the original distribution.\nIf the distribution of the sample mean \\bar{X}\n\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{N}\\right),\n\nthen the standardized variable Z is defined as:\n\nZ = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{N}} \\sim N(0,1).\n\nWhy is this useful? Because we usually use the same significance level \\alpha for all confidence intervals, and we can compute the confidence interval for Z once and use it for all confidence intervals. For Z \\sim N(0,1) and \\alpha=0.05, the top and bottom of the confidence interval are Z_{\\alpha/2}=\\pm 1.96. Now we only have to invert the expression above to get the confidence interval for \\bar{X}:\n\nX_{1,2} = \\mu \\pm Z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{N}}.\n\nThe very last thing we have to account for is the fact that we don’t know the population statistics \\mu and \\sigma^2. Instead, we have to use the sample statistics \\bar{x} and s^2. Furthermore, we have to use the t-distribution instead of the normal distribution, because we are estimating the population variance from the sample variance. The t-distribution has a shape similar to the normal distribution, but it has heavier tails, which accounts for the additional uncertainty introduced by estimating the population variance. Thus, we replace \\mu with \\bar{x} and \\sigma^2 with s^2, and we use the t-distribution with N-1 degrees of freedom. This gives us the final expression for the confidence interval:\n\nX_{1,2} = \\bar{x} \\pm t^*_{N-1} \\cdot \\frac{s}{\\sqrt{N}},\n\nwhere t^*_{N-1} is the critical value from the t-distribution with N-1 degrees of freedom.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#the-solution",
    "href": "confidence_interval/analytical_confidence_interval.html#the-solution",
    "title": "5  analytical confidence interval",
    "section": "5.4 the solution",
    "text": "5.4 the solution\nLet’s say I measured the heights of 30 7-year-old boys, and this is the data I got:\n\n\nShow the code\nN = 30\nnp.random.seed(271)\nsample = norm.rvs(size=N, loc=mu_boys, scale=sigma_boys)\nprint(f\"Sample mean: {np.mean(sample):.2f} cm\")\nprint(sample)\n\n\nSample mean: 122.60 cm\n[114.15972134 128.21581493 122.9864136  117.94247325 132.11013925\n 118.69131645 123.67695468 112.03152008 121.59853424 114.8629358\n 121.90458112 115.68839748 127.18043069 118.33193499 125.28525617\n 124.5287395  120.72706375 113.10575734 132.229147   129.16820684\n 125.94682095 126.08299475 125.95056303 125.6858065  115.07854075\n 124.93539918 125.12886271 126.91366971 120.88030405 127.04777082]\n\n\nUsing the formula for the confidence interval we get:\n\n\nShow the code\nalpha = 0.05\nz_crit = scipy.stats.t.isf(alpha/2, N-1)\nCI = z_crit * sample.std(ddof=1) / np.sqrt(N)\nCI_low = np.round(sample.mean() - CI, 2)\nCI_high = np.round(sample.mean() + CI, 2)\nprint(f\"Sample mean: {np.mean(sample):.2f} cm\")\nprint(\"The 95% confidence interval is [{}, {}] cm\".format(CI_low, CI_high))\nprint(f\"The true population mean is {mu_boys:.2f} cm\")\n\n\nSample mean: 122.60 cm\nThe 95% confidence interval is [120.54, 124.67] cm\nThe true population mean is 121.74 cm",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#a-few-points-to-stress",
    "href": "confidence_interval/analytical_confidence_interval.html#a-few-points-to-stress",
    "title": "5  analytical confidence interval",
    "section": "5.5 a few points to stress",
    "text": "5.5 a few points to stress\nIt is worth commenting on a few points:\n\nIf we were to sample a great many number of samples of size N=30, and compute the confidence interval for each sample, then approximately 95% of these intervals would contain the true population mean \\mu.\nIt is not true that the probability that the true population mean \\mu is in the confidence interval is 95%. The true population mean is either in the interval or not, and it does not have a probability associated with it. The 95% confidence level refers to the long-run frequency of intervals containing the true population mean if we were to repeat the sampling process many times. This is the common frequentist interpretation of confidence intervals.\nIf you want to talk about confidence interval in the Bayesian framework, then first we would have to assign a prior distribution to the population mean \\mu, and then we would compute the posterior distribution of \\mu given the data. The credible interval is then the interval that contains 95% of the posterior distribution of \\mu.\nTo sum up the difference between the frequentist and Bayesian interpretations of confidence intervals:\n\nFrequentist CI: “I am 95% confident in the method” (long-run frequency).\nBayesian credible interval: “There is a 95% probability that μ lies in this interval” (degree of belief).",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html",
    "href": "confidence_interval/empirical_confidence_interval.html",
    "title": "6  empirical confidence interval",
    "section": "",
    "text": "6.1 bootstrap confidence interval\nNot always we want to compute the confidence interval of the mean. Sometimes we are interested in a different statistic, such as the median, the standard deviation, or the maximum. The equations we saw before for the confidence interval of the mean do not apply to these statistics. However, we can still compute a confidence interval for them using the empirical bootstrap method.\nThat’s it. Now let’s do it in code.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html#bootstrap-confidence-interval",
    "href": "confidence_interval/empirical_confidence_interval.html#bootstrap-confidence-interval",
    "title": "6  empirical confidence interval",
    "section": "",
    "text": "Draw a sample of size N from the population. Let’s assume you made an experiment and you could only afford to collect N samples. You will not have the opportunity to collect more samples, and that’s all you have available.\nAssume that the sample is representative of the population. This is a strong assumption, but we will use it to compute the confidence interval.\nFrom this original sample, draw B bootstrap samples of size N with replacement. This means that you will randomly select N samples from the original sample, allowing for duplicates. This is like drawing pieces of paper from a hat, where you can put the paper back after drawing it.\nFor each bootstrap sample, compute the statistic of interest (e.g., median, standard deviation, maximum).\nCompute the cdf of the bootstrap statistics. This will give you the empirical distribution of the statistic.\nCompute the confidence interval using the empirical distribution. For a 95% confidence interval, you can take the 2.5th and 97.5th percentiles of the bootstrap statistics.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html#question",
    "href": "confidence_interval/empirical_confidence_interval.html#question",
    "title": "6  empirical confidence interval",
    "section": "6.2 question",
    "text": "6.2 question\nWe have a sample of 30 7-year-old boys. What can we say about the maximum height of 7-year-olds in the general population?\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\nimport scipy\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[7.0, 'mu']\nsigma_boys = df_boys.loc[7.0, 'sigma']\n\n\n\nN = 100\nB = 10000\nsample = norm.rvs(size=N, loc=mu_boys, scale=sigma_boys)\nmedian_list = []\nfor i in range(B):\n    sample_bootstrap = np.random.choice(sample, size=N, replace=True)\n    median_list.append(np.median(sample_bootstrap))\nmedian_list = np.array(median_list)\n\nalpha = 0.05\nci_bottom = np.quantile(median_list,alpha/2)\nci_top = np.quantile(median_list, 1-alpha/2)\nprint(f\"Bootstrap CI for median: {ci_bottom:.2f} - {ci_top:.2f} cm\")\n\nBootstrap CI for median: 121.19 - 123.63 cm\n\n\n\n\nShow the code\nfig, ax = plt.subplots(2,1, figsize=(8, 6), sharex=True)\nax[0].hist(median_list, bins=30, density=True, align='mid')\nax[1].hist(median_list, bins=30, density=True, cumulative=True, align='mid')\n\nax[1].axhline(alpha/2, color='gray', linestyle=':')\nax[1].axhline(1-alpha/2, color='gray', linestyle=':')\n\nxlim = ax[1].get_xlim()\nax[1].text(xlim[1]+0.15, alpha/2, r\"$\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].text(xlim[1]+0.15, 1-alpha/2, r\"$1-\\alpha/2$\",\n           ha=\"left\", va=\"center\")\n\nax[1].annotate(\n     'bottom CI',\n     xy=(ci_bottom, alpha/2), xycoords='data',\n     xytext=(-100, 30), textcoords='offset points',\n     color='black',\n     arrowprops=dict(arrowstyle=\"-&gt;\", color='black',\n                     connectionstyle=\"angle,angleA=0,angleB=90,rad=10\"))\nax[1].annotate(\n     'top CI',\n     xy=(ci_top, 1-alpha/2), xycoords='data',\n     xytext=(-100, 15), textcoords='offset points',\n     color='black',\n     arrowprops=dict(arrowstyle=\"-&gt;\", color='black',\n                     connectionstyle=\"angle,angleA=0,angleB=90,rad=10\"))\n\nax[0].set(ylabel=\"pdf\",\n          title=\"empirical distribution of median heights from bootstrap samples\")\nax[1].set(ylabel=\"cdf\",\n          xlabel=\"height (cm)\")\n\n\n\n\n\n\n\n\n\nClearly, the distribution of median height is not normal. The bootstrap method gives us a way to compute the confidence interval of the median height (or any other statistic of your choosing) without assuming normality.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "permutation/problem-with-t-test.html",
    "href": "permutation/problem-with-t-test.html",
    "title": "7  the problem with t-test",
    "section": "",
    "text": "7.1 the normality assumption\nLet’s go back to the example of the independent samples t-test.\nWe sampled 10 boys and 14 girls, age 12, and asked:\nWe then went about answering this question by talking about the means of each sample, and if the differences between the means were large enough to be considered significant.\nThe whole machinery behind the t-test is based on the normality assumption.\nTwo possible interpretations come to mind.\nIn the context of the t-test, the above is a distinction without a difference. Even if the population is not normally distributed, the means of the samples will be normally distributed as long as the sample size is large enough. We then use the t-test and go on with our lives.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "permutation/problem-with-t-test.html#the-normality-assumption",
    "href": "permutation/problem-with-t-test.html#the-normality-assumption",
    "title": "7  the problem with t-test",
    "section": "",
    "text": "The assumption is that the height of men and women in the population is normally distributed. From these idealized populations we draw samples.\nThe t-test effectively compares the difference between the means of the two samples, and the variability within each sample. Because of the Central Limit Theorem, the means of the samples will approach a normal distribution as the sample size increases. In this interpretation, the normality assumption is about the distribution of the means of the samples, and not the distribution of the population.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "permutation/problem-with-t-test.html#other-statistical-tests",
    "href": "permutation/problem-with-t-test.html#other-statistical-tests",
    "title": "7  the problem with t-test",
    "section": "7.2 other statistical tests",
    "text": "7.2 other statistical tests\nThe Central Limit Theorem dictates that the means will be normally distributed, but it does not apply to other statistics, such as:\n\nthe median\nthe variance\nthe skewness\nthe maximum\nthe Interquartile Range (IQR)\netc.\n\nIn this case, the t-test can’t be relied upon, and we need another solution.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html",
    "href": "permutation/permutation.html",
    "title": "8  permutation test",
    "section": "",
    "text": "8.1 hypotheses\nWe wish to compare two samples, and see if they significantly differ regarding some statistic of interest (median, mean, etc.). To make things concrete, let’s talk about the heights of 12-year-old boys and girls. Are girls significantly taller than boys?\nThe basic idea behind the permutation test is that, if the null hypothesis is correct, then it wouldn’t matter if we relabelled the samples. If we randomly permute the labels “girls” and “boys” of the two samples, the statistic of interest should not change significantly. However, if by permuting the labels we get a significantly different statistic, then we can reject the null hypothesis.\nThat’s beautiful, right?",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#hypotheses",
    "href": "permutation/permutation.html#hypotheses",
    "title": "8  permutation test",
    "section": "",
    "text": "Null hypothesis (H0): The two samples come from the same distribution.\nAlternative hypothesis (H1): Girls are taller than boys.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#steps",
    "href": "permutation/permutation.html#steps",
    "title": "8  permutation test",
    "section": "8.2 steps",
    "text": "8.2 steps\n\nCompute the statistic of interest (e.g., the difference in medians) for the original samples.\nRandomly permute the labels of the two samples.\nCompute the statistic of interest for the permuted samples.\nRepeat steps 2 and 3 many times (e.g., 1000 times) to create a distribution of the statistic under the null hypothesis.\nCompare the original statistic to the distribution of permuted statistics to see if it is significantly different (e.g., by checking if it falls in the top 5% of the distribution). From this, we can numerically compute a p-value.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#example",
    "href": "permutation/permutation.html#example",
    "title": "8  permutation test",
    "section": "8.3 example",
    "text": "8.3 example\nLet’s use the very same example as in the independent samples t-test.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\n\n\n\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nplot population and sample data\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    );\n\n\n\n\n\n\n\n\n\nThe statistic of interest now is the difference in medians between the two samples.\n\n\ncompute the observed difference in medians\n# define the desired statistic.\n# in can be anything you want, you can even write your own function.\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\nprint(f\"median height for girls: {median_girls:.2f} cm\")\nprint(f\"median height for boys: {median_boys:.2f} cm\")\nprint(f\"median difference (girls minus boys): {observed_diff:.2f} cm\")\n\n\nmedian height for girls: 150.88 cm\nmedian height for boys: 151.19 cm\nmedian difference (girls minus boys): -0.31 cm\n\n\n\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\nNow let’s see the empirical cdf of the permuted statistics, and where the original statistic falls in that distribution.\n\n\nplot empirical distribution of differences\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# compute the empirical CDF\nrank = np.arange(len(diffs)) + 1\ncdf = rank / (len(diffs) + 1)\nsorted_diffs = np.sort(diffs)\n\nax.plot(sorted_diffs, cdf, lw=2, color='tab:blue', label='empirical CDF')\nax.axvline(observed_diff, color='tab:orange', lw=2, ls='--',\n           label=f'obs. median diff. = {observed_diff:.2f} cm')\nax.text(observed_diff + 0.5, 0.3, f'observed difference in\\nmedian height: {observed_diff:.2f} cm',\n        color='tab:orange', fontsize=14, ha='left', va='bottom')\n\nalpha = 0.05\n# for a one-tailed test\nax.axhline(1-alpha, color='k', lw=1, ls='--')\nax.annotate(r\"$1-\\alpha$\", xy=(1.01, 1-alpha), xycoords=('axes fraction', 'data'),\n            ha=\"left\", va=\"center\")\n# for a two-tailed test\n# ax.axhline(alpha/2, color='k', lw=1, ls='--')\n# ax.axhline(1-alpha/2, color='k', lw=1, ls='--')\n# ax.annotate(r\"$\\alpha/2$\", xy=(1.01, alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n# ax.annotate(r\"$1-\\alpha/2$\", xy=(1.01, 1-alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\nax.set(xlabel='difference in median height (cm)',\n         ylabel='empirical CDF',\n         title=f'empirical distribution of differences in median height');\n\n\n\n\n\n\n\n\n\nThe observed statistic is well within the boundaries set by the significance level of 5%. Therefore, we cannot reject the null hypothesis. We conclude that, based on this data, the most probable interpretation is that girls and boys have the same underlying distribution.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#increase-sample-size",
    "href": "permutation/permutation.html#increase-sample-size",
    "title": "8  permutation test",
    "section": "8.4 increase sample size",
    "text": "8.4 increase sample size\nLet’s increase the sample size to 200 girls and 300 boys.\n\n\nthe whole process in one cell\n# take samples\nN_boys = 300\nN_girls = 200\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n# compute the observed difference in medians\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\n\n# permutation algorithm\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 200 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\n\n\n\nplot empirical distribution of differences\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# compute the empirical CDF\nrank = np.arange(len(diffs)) + 1\ncdf = rank / (len(diffs) + 1)\nsorted_diffs = np.sort(diffs)\n\nax.plot(sorted_diffs, cdf, lw=2, color='tab:blue', label='empirical CDF')\nax.axvline(observed_diff, color='tab:orange', lw=2, ls='--',\n           label=f'obs. median diff. = {observed_diff:.2f} cm')\nax.text(observed_diff - 0.05, 0.3, f'observed difference in\\nmedian height: {observed_diff:.2f} cm',\n        color='tab:orange', fontsize=14, ha='right', va='bottom')\n\nalpha = 0.05\n# for a one-tailed test\nax.axhline(1-alpha, color='k', lw=1, ls='--')\nax.annotate(r\"$1-\\alpha$\", xy=(1.01, 1-alpha), xycoords=('axes fraction', 'data'),\n            ha=\"left\", va=\"center\")\n# for a two-tailed test\n# ax.axhline(alpha/2, color='k', lw=1, ls='--')\n# ax.axhline(1-alpha/2, color='k', lw=1, ls='--')\n# ax.annotate(r\"$\\alpha/2$\", xy=(1.01, alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n# ax.annotate(r\"$1-\\alpha/2$\", xy=(1.01, 1-alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n\nax.set(xlabel='difference in median height (cm)',\n         ylabel='empirical CDF',\n         title=f'empirical distribution of differences in median height');\n\n\n\n\n\n\n\n\n\nNow the observed statistic is well outside the right boundary set by the significance level of 5%. Therefore, we can reject the null hypothesis. We conclude that, based on this data, girls are significantly taller than boys.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#p-value",
    "href": "permutation/permutation.html#p-value",
    "title": "8  permutation test",
    "section": "8.5 p-value",
    "text": "8.5 p-value\nIt is quite easy to compute the p-value from the permutation test. It is simply the fraction of permuted statistics that are more extreme than the observed statistic. In this case, since we are testing whether girls are taller than boys, we have a one-tailed test, and we only consider the right tail of the distribution. If we were testing whether girls are significantly different from boys in their height, we would have a two-tailed test, and we would consider both tails of the distribution.\n\n# one-tailed p-value\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\nobserved difference: 2.004\np-value (one-tailed): 0.0050\n\n\nWe can now address the fact that we ran only 999 permutations, although I intended to run 1000. See in the code that after the permutation algorithm, I inserted the original statistic in the list of permuted statistics. This is because I want to compute the p-value as the fraction of permuted statistics that are more extreme than the original statistic, and I want to include the original statistic in the distribution. If I had not done this, for a truly extreme observed statistic, we would get that the p-value equals 0, that is, the fraction of permuted statistics that are more extreme than the observed statistic is zero. To avoid this behavior, we include the original statistic in the distribution of permuted statistics.\nA corollary of this is that the smallest p-value we can get is 0.001 (for our example with 1000 permutations).",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/numpy-vs-pandas.html",
    "href": "permutation/numpy-vs-pandas.html",
    "title": "9  numpy vs pandas",
    "section": "",
    "text": "9.1 numpy\nIn the previous chapter, we computed the permutation test using numpy. We had two samples of different sizes, and before the permutation test we concatenated the two samples into one array. Then we shuffled the concatenated array and split it back into two samples, according to the original sizes. See a sketch of the code below:\nStore the two samples in numpy arrays:\nDefine the statistic and compute the observed difference:\nRun the permutation test:\nAll this works great if this is how your data looks like. Sometimes, however, you have structured data with more information, such as a DataFrame with multiple columns. In this case, you can leverage the capabilities of pandas.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "permutation/numpy-vs-pandas.html#numpy",
    "href": "permutation/numpy-vs-pandas.html#numpy",
    "title": "9  numpy vs pandas",
    "section": "",
    "text": "boys = np.array([121, 123, 124, 125])\ngirls = np.array([120, 121, 121, 122, 123, 123, 128, 129])\nN_boys = len(boys)\nN_girls = len(girls)\n\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\n\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations - 1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first N_girls values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "permutation/numpy-vs-pandas.html#pandas",
    "href": "permutation/numpy-vs-pandas.html#pandas",
    "title": "9  numpy vs pandas",
    "section": "9.2 pandas",
    "text": "9.2 pandas\nLet’s give an example of structured data. Suppose we have a DataFrame with the following columns: sex, height, and weight.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\nN_total = 20\nnp.random.seed(3)\nheight_list = norm.rvs(size=N_total, loc=150, scale=7)\nweight_list = norm.rvs(size=N_total, loc=42, scale=5)\nsex_list = np.random.choice(['M', 'F'], size=N_total, replace=True)\ndf = pd.DataFrame({\n    'sex': sex_list,\n    'height (cm)': height_list,\n    'weight (kg)': weight_list\n})\ndf\n\n\n\n\n\n\n\n\nsex\nheight (cm)\nweight (kg)\n\n\n\n\n0\nM\n162.520399\n36.074767\n\n\n1\nM\n153.055569\n40.971751\n\n\n2\nM\n150.675482\n49.430742\n\n\n3\nF\n136.955551\n43.183581\n\n\n4\nF\n148.058283\n36.881074\n\n\n5\nM\n147.516687\n38.435034\n\n\n6\nF\n149.420810\n45.126225\n\n\n7\nF\n145.610995\n41.197433\n\n\n8\nF\n149.693273\n38.155818\n\n\n9\nM\n146.659474\n40.849846\n\n\n10\nM\n140.802947\n45.725281\n\n\n11\nF\n156.192357\n51.880554\n\n\n12\nF\n156.169226\n35.779383\n\n\n13\nM\n161.967011\n38.867915\n\n\n14\nF\n150.350235\n37.981170\n\n\n15\nM\n147.167258\n29.904584\n\n\n16\nM\n146.182480\n37.381040\n\n\n17\nM\n139.174659\n36.880621\n\n\n18\nM\n156.876572\n47.619890\n\n\n19\nM\n142.292527\n41.340429\n\n\n\n\n\n\n\nCalculate sample statistics using groupby:\n\nsample_stats = df.groupby('sex')['height (cm)'].median()\nobserved_diff = sample_stats['F'] - sample_stats['M']\n\nWe can now leverage the pandas.DataFrame.sample method to sample from the DataFrame. Here, we use the following options:\n\nfrac=1 means we want to sample 100% of rows, but shuffled.\nreplace=False means we want to sample without replacement, that is, no duplicate rows.\n\nWe will shuffle the sex column and store the result in a new column called sex_shuffled. Then we can use groupby to compute the median.\n\nN_permutations = 1000\ndiffs = np.empty(N_permutations - 1)\nfor i in range(N_permutations - 1):\n    # shuffle dataframe 'sex' colunn, store it in 'sex_shuffled'\n    df['sex_shuffled'] = df['sex'].sample(frac=1, replace=False).reset_index(drop=True)\n    shuffled_stats = df.groupby('sex_shuffled')['height (cm)'].median()\n    diffs[i] = shuffled_stats['F'] - shuffled_stats['M']  # median(F) - median(M)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "permutation/exact-vs-montecarlo.html",
    "href": "permutation/exact-vs-montecarlo.html",
    "title": "10  exact vs. Monte Carlo permutation tests",
    "section": "",
    "text": "10.1 Monte Carlo permutation tests\nThe permutation tests from before do not sample from the full distribution of the test statistic under the null hypothesis. This would be imppractical if the total number of permutations is large, as it would require computing the test statistic for every possible permutation of the data.\nFor example, if we have 10 boys and 14 girls, the total number of permutations is almost two million:\n\\binom{24}{14} = \\frac{24!}{14!\\cdot(24-14)!} = 1961256\nThe expression above is the binomial coefficient, which counts the number of ways to choose 14 samples from a total of 24, without regard to the order of selection. This is why we say “24 choose 14” to refer to the parenthesis above.\nThere is no preference in “24 choose 14” over “24 choose 10”, as both expressions yield the same result. You can verify this on your own.\nMonte Carlo methods are a class of computational algorithms that rely on repeated random sampling to obtain numerical results. In the context of permutation tests, Monte Carlo methods do not compute the test statistic for every possible permutation of the data. In the examples from before, we computed 1000 permutations only, and from that we estimated the p-value of the test statistic. If we had run the test more than once, we would have obtained a different p-value each time, as the test statistic is computed from a random sample of permutations.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\ncompute the observed difference in medians\n# define the desired statistic.\n# in can be anything you want, you can even write your own function.\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\nMonte Carlo permutation test 1\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(\"Monte Carlo permutation test 1\")\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\n\nMonte Carlo permutation test 1\nobserved difference: -0.314\np-value (one-tailed): 0.5450\nMonte Carlo permutation test 2\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(\"Monte Carlo permutation test 2\")\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\n\nMonte Carlo permutation test 2\nobserved difference: -0.314\np-value (one-tailed): 0.5340\nAs you can see, the p-value in not exactly the same, but the difference is negligible. This is because both times we sampled 1000 permutations that are representative of the full distribution of the test statistic under the null hypothesis.\nOne more thing. The example above with 10 boys and 14 girls is usually considered small. It is often the case that one has a lot more samples, and the number of permutations can be astronomically large, much much larger than two million.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>exact vs. Monte Carlo permutation tests</span>"
    ]
  },
  {
    "objectID": "permutation/exact-vs-montecarlo.html#exact-permutation-test",
    "href": "permutation/exact-vs-montecarlo.html#exact-permutation-test",
    "title": "10  exact vs. Monte Carlo permutation tests",
    "section": "10.2 exact permutation test",
    "text": "10.2 exact permutation test\nIf the total number of permutations is small, we can compute the exact p-value by sampling from the full distribution of the test statistic under the null hypothesis. That is to say, we compute the test statistic for every possible permutation of the data.\nIf we had height measurements of 7 boys and 6 girls, the total number of permutations is:\n\n\\binom{13}{7} = 1716\n\nAny computer can easily handle this number of permutations. How to do it in practice? We will use the itertools.combinations function.\n\n\nShow the code\nimport numpy as np\nfrom itertools import combinations\n\n#| code-summary: \"generate data\"\nN_girls = 6\nN_boys = 7\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\n\ncombined = np.concatenate([sample_girls, sample_boys])\nn_total = len(combined)\n\n# observed difference in means\nobserved_diff = np.median(sample_girls) - np.median(sample_boys)\n\n# generate all combinations of indices for group \"girls\"\nindices = np.arange(n_total)\nall_combos = list(combinations(indices, N_girls))\n\n# compute all permutations\ndiffs = []\nfor idx_a in all_combos:\n    mask = np.zeros(n_total, dtype=bool)\n    mask[list(idx_a)] = True\n    sample_g = combined[mask]\n    sample_b = combined[~mask]\n    diffs.append(np.median(sample_g) - np.median(sample_b))\n\ndiffs = np.array(diffs)\n\n# exact one-tailed p-value\np_value = np.mean(diffs &gt;= observed_diff)\nprint(f\"Observed difference: {observed_diff:.3f} cm\")\nprint(f\"Exact p-value (one-tailed): {p_value:.4f}\")\nprint(f\"Total permutations: {len(diffs)}\")\n\n\nObserved difference: 7.620 cm\nExact p-value (one-tailed): 0.0944\nTotal permutations: 1716\n\n\nAttention!\nIf you read the documentation of the itertools library, you might be tempted to use itertools.permutations instead of itertools.combinations.\nDon’t do that.\nAlthough we are conductiong a permutation test, we are not interested in the order of the samples, and that is what the permutations cares about. For instance, if we have 10 people called\n[Alice, Bob, Charlie, David, Eve, Frank, Grace, Heidi, Ivan, Judy]\nand we want to randomly assign the label “girl” to 4 of them, we do not care about the order in which we assign the labels. We just want to know which 4 people are assigned the label “girl”. The permutation function does care about the order, and that is why we should not use it. Instead, we use the combinations function, which return all possible combinations of the data, without regard to the order of selection.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>exact vs. Monte Carlo permutation tests</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html",
    "href": "regression/geometry-of-regression.html",
    "title": "11  the geometry of regression",
    "section": "",
    "text": "11.1 a very simple example\nIt’s almost always best to start with a simple and concrete example.\nGoal: We wish to find the best straight line that describes the following data points:\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nimport scipy\n\n# %matplotlib widget\ndefine and plot the simple problem\nx = np.array([1, 2, 3])\ny = np.array([2, 2, 6])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.scatter(x, y, label='data', facecolors='black', edgecolors='black')\n# linear regression\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\nx_domain = np.linspace(0, 4, 101)\nax.plot(x_domain, intercept + slope * x_domain, color='black', label='best line')\n\nax.legend(loc='upper left', fontsize=14, frameon=False)\nax.set(xlim=(0, 4),\n       ylim=(0, 7),\n       xticks=np.arange(0, 5, 1),\n       yticks=np.arange(0, 9, 1),\n       xlabel='X-axis',\n       ylabel='Y-axis');",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#formalizing-the-problem",
    "href": "regression/geometry-of-regression.html#formalizing-the-problem",
    "title": "11  the geometry of regression",
    "section": "11.2 formalizing the problem",
    "text": "11.2 formalizing the problem\nLet’s translate this problem into the language of linear algebra.\nThe independent variable x is the column vector\n\nx=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}\n\nand the dependent variable y is the column vector\n\ny=\n\\begin{pmatrix}\n2 \\\\ 2 \\\\ 6\n\\end{pmatrix}.\n\nBecause we are looking for a straight line, we can express the relationship between x and y as\n\n\\tilde{y} = \\beta_0 + \\beta_1 x.\n\nHere we introduced the notation \\tilde{y} to denote the predicted values of y based on the linear model. It is different from the actual values of y because the straight line usually does not pass exactly on top of y.\nThe parameter \\beta_0 is the intercept and \\beta_1 is the slope of the line.\nWhich values of \\beta_0,\\beta_1 will give us the very best line?",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#higher-dimensions",
    "href": "regression/geometry-of-regression.html#higher-dimensions",
    "title": "11  the geometry of regression",
    "section": "11.3 higher dimensions",
    "text": "11.3 higher dimensions\nIt is very informative to think about this problem not as a scatter plot in the X-Y plane, but as taking place in a higher-dimensional space. Because we have three data points, we can think of the problem in a three-dimensional space. We want to explain the vector y as a linear combination of the vector x and a constant vector (this is what our linear model states).\nIn three dimensions, our building blocks are the vectors c, the intercept, and x, the data points.\n\nc=\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\qquad\nx=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}.\n\nWe can combine these c and x as column vectors in a matrix called design matrix:\n\nX=\n\\begin{pmatrix}\n1 & x_0 \\\\\n| & | \\\\\n1 & x_i \\\\\n| & | \\\\\n1 & x_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n| & | \\\\\n1 & x \\\\\n| & |\n\\end{pmatrix}\n\nWhy is this convenient? Because now the linear combination of \\vec{1} and x can be expressed as a matrix multiplication:\n\n\\begin{pmatrix}\n\\hat{y}_0 \\\\\n\\hat{y}_1 \\\\\n\\hat{y}_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & x_0 \\\\\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\cdot\\beta_0 + x_0\\cdot\\beta_1 \\\\\n1\\cdot\\beta_0 + x_1\\cdot\\beta_1 \\\\\n1\\cdot\\beta_0 + x_2\\cdot\\beta_1\n\\end{pmatrix}\n\nIn short, the linear combination of our two building blocks yields a prediction vector \\hat{y}:\n\n\\hat{y} = X \\beta,\n\nwhere \\beta is the column vector (\\beta_0, \\beta_1)^T.\nThis prediction vector \\hat{y} lies on a plane in the 3d space, it cannot be anywhere in this 3d space. Mathematically, we say that the vector \\hat{y} is in the subspace spanned by the columns of the design matrix X.\nIt will be extremely improbable that the vector y will also lie on this plane, so we will have to find the best prediction \\hat{y} that lies on this plane. Geometrically, our goal is to find the point \\hat{y} on the plane that is closest to the point y in the 3d space.\n\nWhen the distance r=y-\\hat{y} is minimized, the vector r is orthogonal to the plane spanned by the columns of the design matrix X.\nWe call this vector r the residual vector.\nThe residual is orthogonal to each of the columns of X, that is, \\vec{1}\\cdot r=0 and x\\cdot r=0.\n\nI tried to summarize all the above in the 3d image below. This is, for me, the geometry of regression. If you have that in your head, you’ll never forget it.\n\nAnother angle of the image above. This time, because the view direction is within the plane, we see that the residual vector r is orthogonal to the plane spanned by the columns of the design matrix X. \nFor a fully interactive version, see this Geogebra applet.\nTaking advantage of the matrix notation, we can express the orthogonality condition as follows:\n\n\\begin{pmatrix}\n- & 1 & - \\\\\n- & x & -\n\\end{pmatrix} r =\nX^T r =\n0\n\nLet’s substitute r = y - \\hat{y} = y - X\\beta into the equation above.\n\nX^T(y - X\\beta) = 0\n\nDistributing yields\n\nX^Ty - X^TX\\beta = 0,\n\nand then\n\nX^TX\\beta = X^Ty.\n\nWe need to solve this equation for \\beta, so we left-multiply both sides by the inverse of X^TX,\n\n\\beta = (X^TX)^{-1}X^Ty.\n\nThat’s it. We did it. Given the data points x and y, we can compute the parameters \\beta_0 and \\beta_1 that bring \\hat{y} as close as possible to y. These parameters are the best fit of the straight line to the data points.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#overdetermined-system",
    "href": "regression/geometry-of-regression.html#overdetermined-system",
    "title": "11  the geometry of regression",
    "section": "11.4 overdetermined system",
    "text": "11.4 overdetermined system\nThe design matrix X is a tall and skinny matrix, meaning that it has more rows (n) than columns (m). This is called an overdetermined system, because we have more equations (rows) than unknowns (columns), so we have no hope in finding an exact solution \\beta.\nThis is to say that, almost certainly, the vector y does not lie on the plane spanned by the columns of the design matrix X. No combination of the parameters \\beta will yield a vector \\hat{y} that is exactly equal to y.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#least-squares",
    "href": "regression/geometry-of-regression.html#least-squares",
    "title": "11  the geometry of regression",
    "section": "11.5 least squares",
    "text": "11.5 least squares\nThe method above for finding the best parameters \\beta is called least squares. The name comes from the fact that we are trying to minimize the length of the residual vector\n\nr = y - \\hat{y}.\n\nThe length of the residual is given by the Euclidean norm (or L^2 norm), which is a direct generalization of the Pythagorean theorem for many dimensions.\n\\begin{align}\n\\Vert r\\Vert^2 &= \\Vert y - \\hat{y}\\Vert^2  \\\\\n&= (y_0 - \\hat{y}_0)^2 + (y_1 - \\hat{y}_1)^2 + \\cdots +  (y_{n-1} - \\hat{y}_{n-1})^2 \\\\\n&= r_0^2 + r_1^2 + \\cdots + r_{n-1}^2\n\\end{align}\nThe length (squared) of the residual vector is the sum of the squares of all residuals. The best parameters \\beta are those that yield the least squares, thus the name.\n\n\ndefine and plot the simple problem\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.scatter(x, y, label='data', facecolors='black', edgecolors='black')\nx_domain = np.linspace(0, 4, 101)\nax.plot(x_domain, intercept + slope * x_domain, color='black', label='best line')\n\ndef linear(x, slope, intercept):\n    return intercept + slope * x\n\nfor i, xi in enumerate(x):\n    ax.plot([xi, xi],\n            [y[i], linear(xi, slope, intercept)],\n            color='black', linestyle='--', linewidth=0.5,\n            label='residuals' if i == 0 else None)\n    \nax.legend(loc='upper left', fontsize=14, frameon=False) \nax.set(xlim=(0, 4),\n       ylim=(0, 7),\n       xticks=np.arange(0, 5, 1),\n       yticks=np.arange(0, 9, 1),\n       xlabel='X-axis',\n       ylabel='Y-axis');",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#many-more-dimensions",
    "href": "regression/geometry-of-regression.html#many-more-dimensions",
    "title": "11  the geometry of regression",
    "section": "11.6 many more dimensions",
    "text": "11.6 many more dimensions\nThe concrete example here dealt with only three data points, therefore we could visualize the problem in a three-dimensional space. However, the same reasoning applies to any number of data points and any number of independent variables.\n\nany number of data points: we call the number of data points n, and that makes y be a vector in an n-dimensional space.\nany number of independent variables: we calculated a regression for a straight line, and thus we had only two building blocks, the intercept \\vec{1} and the independent variable x. However, we can have any number of independent variables, say m of them. For example, we might want to predict the data using a polynomial of degree m, or we might have any arbitrary m functions that we wish to use: \\exp(x), \\tanh(x^2), whatever we want. All this will work as long as the parameters \\beta multiply these building blocks. That’s the topic of the next chapter.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html",
    "href": "regression/least-squares.html",
    "title": "12  least squares",
    "section": "",
    "text": "12.1 ordinary least squares (OLS) regression\nLet’s go over a few things that appear in this notebook, statsmodels, Ordinary Least Squares\nimport libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nnp.random.seed(9876789)",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#polynomial-regression",
    "href": "regression/least-squares.html#polynomial-regression",
    "title": "12  least squares",
    "section": "12.2 polynomial regression",
    "text": "12.2 polynomial regression\nLet’s start with a simple polynomial regression example. We will start by generating synthetic data for a quadratic equation plus some noise.\n\n# number of points\nnsample = 100\n# create independent variable x\nx = np.linspace(0, 10, 100)\n# create design matrix with linear and quadratic terms\nX = np.column_stack((x, x ** 2))\n# create coefficients array\nbeta = np.array([5, -2, 0.5])\n# create random error term\ne = np.random.normal(size=nsample)\n\nx and e can be understood as column vectors of length n, while X and \\beta are:\n\nX =\n\\begin{pmatrix}\nx_0 & x_0^2 \\\\\n| & | \\\\\nx_i & x_i^2 \\\\\n| & | \\\\\nx_n & x_n^2 \\\\\n\\end{pmatrix}, \\qquad\n\\beta =\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{pmatrix}.\n\nOops, there is no intercept column \\vec{1} in the design matrix X. Let’s add it:\n\nX = sm.add_constant(X)\nprint(X[:5, :])  # print first 5 rows of design matrix\n\n[[1.         0.         0.        ]\n [1.         0.1010101  0.01020304]\n [1.         0.2020202  0.04081216]\n [1.         0.3030303  0.09182736]\n [1.         0.4040404  0.16324865]]\n\n\nThis add_constant function is smart, it has as default a prepend=True argument, meaning that the intercept is added as the first column, and a has_constant='skip' argument, meaning that it will not add a constant if one is already present in the matrix.\nThe matrix X is now a design matrix for a polynomial regression of degree 2. \nX =\n\\begin{pmatrix}\n1 & x_0 & x_0^2 \\\\\n| & | & | \\\\\n1 & x_i & x_i^2 \\\\\n| & | & | \\\\\n1 & x_n & x_n^2 \\\\\n\\end{pmatrix}\n\nWe now put everything together in the following equation:\n\ny = X \\beta + e\n\nThis creates the dependend variable y as a linear combination of the independent variables in X and the coefficients in \\beta, plus an error term e.\n\ny = np.dot(X, beta) + e\n\nLet’s visualize this:\n\n\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.set(xlabel='x',\n       ylabel='y',\n       title='Simulated Data with Linear and Quadratic Terms'\n       );",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#solving-the-hard-way",
    "href": "regression/least-squares.html#solving-the-hard-way",
    "title": "12  least squares",
    "section": "12.3 solving the “hard way”",
    "text": "12.3 solving the “hard way”\nI’m going to do something that nobody does. I will use the formula we derived in the previous chapter to find the coefficients \\beta of the polynomial regression.\n\n\\beta = (X^TX)^{-1}X^Ty.\n\nTranslating this into code, and keeping in mind that matrix multiplication in Python is done with the @ operator, we get:\n\nbeta_opt = np.linalg.inv(X.T@X)@X.T@y\nprint(f\"beta = {beta_opt}\")\n\nbeta = [ 5.34233516 -2.14024948  0.51025357]\n\n\nThat’s it. We did it (again).\nLet’s take a look at the matrix X^TX. Because X is a tall and skinny matrix of shape (n, 3), the matrix X^T is a wide and short matrix of shape (3, n). This is because we have many more data points n than the number of predictors (\\vec{1},x,x^2), which is of course equal to the number of coefficients (\\beta_0,\\beta_1,\\beta_2).\n\nprint(X.T@X)\n\n[[1.00000000e+02 5.00000000e+02 3.35016835e+03]\n [5.00000000e+02 3.35016835e+03 2.52525253e+04]\n [3.35016835e+03 2.52525253e+04 2.03033670e+05]]\n\n\nWhen we multiply the matrices X^T_{3\\times n} and X_{n\\times 3}, we get a square matrix of shape (3, 3), because the inner dimensions match (the number of columns in X^T is equal to the number of rows in X). The product X^TX is a square matrix of shape (3, 3), which is quite easy to invert. If it were the other way around (X\\,X^T), we would have a matrix of shape (n, n), which is much harder to invert, especially if n is large. Lucky us.\nNow let’s see if the parameters we found are any good.\n\nprint(\"beta parameters used to generate data:\")\nprint(beta)\nprint(\"beta parameters estimated from data:\")\nprint(beta_opt)\n\nbeta parameters used to generate data:\n[ 5.  -2.   0.5]\nbeta parameters estimated from data:\n[ 5.34233516 -2.14024948  0.51025357]\n\n\nPretty good, right? Now let’s see the best fit polynomial on the graph.\n\n\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.plot(x, np.dot(X, beta_opt), color='red', label='fitted line')\nax.legend(frameon=False)\nax.set(xlabel='x',\n       ylabel='y',\n       title='Simulated Data with Linear and Quadratic Terms'\n       );\n\n\n\n\n\n\n\n\n\nWhy did I call it the “hard way”? Because these operations are so common that of course there are libraries that do this for us. We don’t need to remember the equation, we can just use, for example, statsmodels library’s OLS function, which does exactly this. Let’s see how it works.\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nNow we can compare the results of our manual calculation with the results from statsmodels. We should get the same coefficients, and we do.\n\nprint(\"beta parameters used to generate data:\")\nprint(beta)\nprint(\"beta parameters from our calculation:\")\nprint(beta_opt)\nprint(\"beta parameters from statsmodels:\")\nprint(results.params)\n\nbeta parameters used to generate data:\n[ 5.  -2.   0.5]\nbeta parameters from our calculation:\n[ 5.34233516 -2.14024948  0.51025357]\nbeta parameters from statsmodels:\n[ 5.34233516 -2.14024948  0.51025357]",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#statmodels.ols-and-the-summary",
    "href": "regression/least-squares.html#statmodels.ols-and-the-summary",
    "title": "12  least squares",
    "section": "12.4 statmodels.OLS and the summary",
    "text": "12.4 statmodels.OLS and the summary\nStatmodels provides us a lot more information than just the coefficients. Let’s take a look at the summary of the OLS regression.\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.988\nModel:                            OLS   Adj. R-squared:                  0.988\nMethod:                 Least Squares   F-statistic:                     3965.\nDate:                Mon, 23 Jun 2025   Prob (F-statistic):           9.77e-94\nTime:                        12:50:31   Log-Likelihood:                -146.51\nNo. Observations:                 100   AIC:                             299.0\nDf Residuals:                      97   BIC:                             306.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.3423      0.313     17.083      0.000       4.722       5.963\nx1            -2.1402      0.145    -14.808      0.000      -2.427      -1.853\nx2             0.5103      0.014     36.484      0.000       0.482       0.538\n==============================================================================\nOmnibus:                        2.042   Durbin-Watson:                   2.274\nProb(Omnibus):                  0.360   Jarque-Bera (JB):                1.875\nSkew:                           0.234   Prob(JB):                        0.392\nKurtosis:                       2.519   Cond. No.                         144.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nI won’t go into the details of the summary, but I encourage you to take a look at it and see if you can make sense of it.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#r-squared",
    "href": "regression/least-squares.html#r-squared",
    "title": "12  least squares",
    "section": "12.5 R-squared",
    "text": "12.5 R-squared\nR-squared is a measure of how well the model fits the data. It is defined as the proportion of the variance in the dependent variable that is predictable from the independent variables. It can be computed as follows:\n\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n\nwhere SS_{res} and SS_{tot} are defined as follows:\n\\begin{align*}\nSS_{res} &= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\nSS_{tot} &= \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\end{align*}\nThe letters SS mean “sum of squares”, and \\bar{y} is the mean of the dependent variable. Let’s compute it manually, and then compare it with the value from the statsmodels summary.\n\ny_hat = np.dot(X, beta_opt)\nSS_res = np.sum((y - y_hat) ** 2)\nSS_tot = np.sum((y - np.mean(y)) ** 2)\nR2 = 1 - (SS_res / SS_tot)\nprint(\"R-squared (manual calculation): \", R2)\nprint(\"R-squared (from statsmodels): \", results.rsquared)\n\nR-squared (manual calculation):  0.9879144521349076\nR-squared (from statsmodels):  0.9879144521349076\n\n\nThis high R^2 value tells us that the model explains a very large proportion of the variance in the dependent variable.\nHow can we know that the variance has anything to do with the R^2? If we divide both the SS_{res} and SS_{tot} by n-1, we get the sample variances of the residuals and the dependent variable, respectively.\n\\begin{align*}\ns^2_{res} = \\frac{SS_{res}}{n-1} &= \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-1} \\\\\ns^2_{tot} = \\frac{SS_{tot}}{n-1} &= \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n-1}\n\\end{align*}\nThen the R^2 can then be expressed as: \nR^2 = 1 - \\frac{s^2_{res}}{s^2_{tot}}.\n\nI prefer this equation over the first, because it makes it clear that R^2 is the ratio of the variances, which is a more intuitive way to think about it.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#any-function-will-do",
    "href": "regression/least-squares.html#any-function-will-do",
    "title": "12  least squares",
    "section": "12.6 any function will do",
    "text": "12.6 any function will do\nThe formula we derived the the previous chapter works for predictors (independent variables) of any kind, not only polynomials. The formula will work as long as the parameters \\beta are linear in the predictors. For exammple, we could have a nonlinear function like this:\n\ny = \\beta_0 + \\beta_1 e^x + \\beta_2 \\sin(x^2)\n\nbecause each beta multiplies a predictor. On the other hand, the following function would not work, because the parameters are not linear in the predictors:\n\ny = \\beta_0 + e^{\\beta_1 x} + \\sin(\\beta_2 x^2)\n\nLet’s this this in action, I’ll use the same example provided by statsmodels documentation, which is a nonlinear function of the form:\n\ny = \\beta_0 x + \\beta_1 \\sin(x) + \\beta_2(x - 5)^2 + \\beta_3\n\n\nnsample = 50\nsig = 0.5\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((\n    x,\n    np.sin(x),\n    (x - 5) ** 2,\n    np.ones(nsample)\n    ))\nbeta = [0.5, 0.5, -0.02, 5.0]\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.set(xlabel='x',\n       ylabel='y',\n       )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresult = sm.OLS(y, X).fit()\nprint(result.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.933\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     211.8\nDate:                Mon, 23 Jun 2025   Prob (F-statistic):           6.30e-27\nTime:                        12:51:08   Log-Likelihood:                -34.438\nNo. Observations:                  50   AIC:                             76.88\nDf Residuals:                      46   BIC:                             84.52\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.4687      0.026     17.751      0.000       0.416       0.522\nx2             0.4836      0.104      4.659      0.000       0.275       0.693\nx3            -0.0174      0.002     -7.507      0.000      -0.022      -0.013\nconst          5.2058      0.171     30.405      0.000       4.861       5.550\n==============================================================================\nOmnibus:                        0.655   Durbin-Watson:                   2.896\nProb(Omnibus):                  0.721   Jarque-Bera (JB):                0.360\nSkew:                           0.207   Prob(JB):                        0.835\nKurtosis:                       3.026   Cond. No.                         221.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote something interesting: in our design matrix X, we encoded the intercept column as the last column, there is no reason why it should be the first column (although first column is a common choice). The function ‘statsmodels.OLS’ sees this, and when we print the summary, it will show the intercept as the last coefficient. Nice!\nLet’s see a graph of the data and the fitted model.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.plot(x, np.dot(X, result.params), color='red', label='fitted line')\nax.legend(frameon=False)\nax.set(xlabel='x',\n       ylabel='y',\n       )",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html",
    "href": "regression/equivalence.html",
    "title": "13  equivalence",
    "section": "",
    "text": "13.1 orthogonality\nIn the context of linear regression, the orthogonality condition states that the residual vector r is orthogonal to the column space of the design matrix X:\nX^T r = 0\nLet’s substitute r = y - \\hat{y} = y - X\\beta into the equation above.\nX^T(y - X\\beta) = 0\nDistributing yields\nX^Ty - X^TX\\beta = 0,\nand then\nX^TX\\beta = X^Ty.\nWe need to solve this equation for \\beta, so we left-multiply both sides by the inverse of X^TX,\n\\beta = (X^TX)^{-1}X^Ty.\nWe already did that in a previous chapter. Now let’s get exactly the same result using another approach.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#optimization",
    "href": "regression/equivalence.html#optimization",
    "title": "13  equivalence",
    "section": "13.2 optimization",
    "text": "13.2 optimization\nWe wish to minimize the sum of squared errors, which is given by\n\nL = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - X_{ij}\\beta_j)^2.\n\nI’m not exactly sure, but I think we call this L because of the lagrangian function. Later on when we talk about regularization, we will see that the lagrangian function can be constrained by lagrange multipliers. In any case, let’s keep going with the optimization.\nIt is useful to express L in matrix notation. The sum of squared errors can be thought as the dot product of the residual vector with itself:\n\\begin{align*}\nL &= (y - X\\beta)^T(y - X\\beta) \\\\\n&= y^Ty - y^TX\\beta - \\beta^TX^Ty + \\beta^TX^TX\\beta,\n\\end{align*}\nwhere we used the following properties of matrix algebra: 1. The dot product of a vector with itself is the sum of the squares of its components, i.e., a^Ta = \\sum_{i=1}^n a_i^2. We used this to express the sum of squared errors in matrix notation. 2. The dot product is bilinear, i.e., (a - b)^T(c - d) = a^Tc - a^Td - b^Tc + b^Td. We used this to expand the expression for the sum of squared errors. 3. The transpose of a product of matrices is the product of their transposes in reverse order, i.e., (AB)^T = B^TA^T. We used this to compute (X\\beta)^T.\nLet’s use one more property to join the two middle terms, - y^TX\\beta - \\beta^TX^Ty:\n\nThe dot product is symmetric, i.e., a^Tb = b^Ta. This is evident we express the dot product as a summation:\n\n\n  a^Tb = \\sum_{i=1}^n a_i b_i = \\sum_{i=1}^n b_i a_i = b^Ta.\n  \nJoining the two middle terms results in the following L:\n\nL = y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta,\n\n\nThe set of parameters \\beta that minimizes L is that which satisfies the extreme condition of the function L (either maximum or minimum). This means that the gradient of L with respect to \\beta must be zero:\n\n\\frac{\\partial L}{\\partial \\beta} = 0.\n\nLet’s plug in the expression for L:\n\n\\frac{\\partial}{\\partial \\beta} \\left( y^Ty - 2\\beta^TX^Ty + \\beta^TX^TX\\beta \\right) = 0\n\nThe quantity L is a scalar, and also each of the three terms that we are differentiating is a scalar. Let’s differentiate them one by one.\n\nThe first term, y^Ty, is a constant with respect to \\beta, so its derivative is zero.\n\n\nThe second term\n\n\\frac{\\partial}{\\partial \\beta} \\left( - 2\\beta^TX^Ty \\right) = - 2\\frac{\\partial}{\\partial \\beta} \\left( \\beta^TX^Ty \\right).\n\nThe quantity being differentiated is a scalar, it’s the product of the row vector \\beta^T and the column vector X^Ty. Right now we don’t care much about X^Ty, it could be any column vector, so let’s call it c. The derivative of dot product \\beta^T c with respect to a specific element \\beta_k can be written explicitly as\n\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\beta_i c_i \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\beta_1 c_1 + \\beta_2 c_2 + \\ldots + \\beta_p c_p \\right) =  c_k.\n\nWhatever value for the index k we choose, the derivative will be zero for all indices except for k, and that explain the result above.\nSince the gradient \\nabla_{\\beta}(\\beta^T c) is a vector,\n\n\\nabla_{\\beta}(\\beta^T c) = \\frac{\\partial (\\beta^T c)}{\\partial \\beta} =\n\\begin{pmatrix}\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_1} \\\\\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_2} \\\\\n\\vdots \\\\\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_p}\n\\end{pmatrix}\n\nand we have just figured out what each component is, we can write the solution as\n\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta} =\n\\begin{pmatrix}\nc_1 \\\\ c_2 \\\\ \\vdots \\\\ c_p\n\\end{pmatrix}\n= c\n= X^Ty.\n\nSo the derivative of the second term is simply -2 X^Ty.\n\n\nTo solve the derivatie of the third term, \\beta^TX^TX\\beta, we use the following property:\n\n\\frac{\\partial}{\\partial \\beta} \\left( \\beta^T A \\beta \\right) = 2A\\beta,\n\nwhen A is a symmetric matrix. In our case, A = X^TX, which is symmetric because (X^TX)^T = X^T(X^T)^T = X^TX. Therefore we have:\n\n\\frac{\\partial}{\\partial \\beta} \\left( \\beta^T X^TX \\beta \\right) = 2X^TX\\beta,\n\nand that is the derivative of the third term.\nWe still have to prove why the derivative of \\beta^T A \\beta is 2A\\beta. First, let’s use the summation notation to express the term \\beta^T A \\beta:\n\n\\beta^T A \\beta = \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j.\n\nNow, let’s differentiate this expression with respect to \\beta_k, using the chain rule:\n\n\\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j \\right) = \\sum_{i=1}^p A_{ik} \\beta_i + \\sum_{j=1}^p \\beta_j A_{kj}.\n\nUsing the symmetry of A (A_{ij} = A_{ji}):\n\n\\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j \\right) = 2 \\sum_{i=1}^p A_{ik} \\beta_i = 2(A\\beta)_k.\n\nThis is the element k of the vector 2A\\beta. Since this is true for any index k, we can write the gradient as\n\n\\nabla_{\\beta}(\\beta^T A \\beta) = 2A\\beta.\n\n\nNow that we have the derivatives of all three terms, we can write the gradient of L:\n\n\\frac{\\partial L}{\\partial \\beta} = 0 - 2X^Ty + 2X^TX\\beta = 0.\n\nRearranging…\n\nX^TX\\beta = X^Ty\n\n…and solving for \\beta:\n\n\\beta = (X^TX)^{-1}X^Ty",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#discussion",
    "href": "regression/equivalence.html#discussion",
    "title": "13  equivalence",
    "section": "13.3 discussion",
    "text": "13.3 discussion\nUsing two completely different approaches, we arrived at the same result for the least squares solution:\n\n\\beta = (X^TX)^{-1}X^Ty\n.\n\nApproach 1: We used the orthogonality condition, which states that the residual vector is orthogonal to the column space of the design matrix.\nApproach 2: We applied the optimization method, minimizing the sum of squared errors—which corresponds to minimizing the squared length of the residual vector.\n\nThere is a deep connection here. The requirement that the residual vector is orthogonal to the column space of the design matrix is equivalent to minimizing the sum of squared errors. We can see this visually: if the projection of the response vector y onto the column space of X were anywhere else, the residual vector would be not only not orthogonal, but also longer!\n\n\n\\text{orthogonality} \\iff \\text{optimization}\n\n\nThis result even tranfers to other contexts, as long as there is a vector space with a well defined inner product (dot product) and an orthogonal basis. In these cases, the least squares solution can be interpreted as finding the projection of a vector onto a subspace spanned by an orthogonal basis. Some examples include:\n\nFourier series: the Fourier coefficients are the least squares solution to the problem of approximating a function by a sum of sines and cosines, where these functions are an orthogonal basis.\nSVD (Singular Value Decomposition): A matrix can be decomposed into orthogonal matrices, and the singular values can be interpreted as the least squares solution to the problem of approximating a matrix by a sum of outer products of orthogonal vectors.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html",
    "href": "regression/mixed-model.html",
    "title": "14  linear mixed effect model",
    "section": "",
    "text": "14.1 practical example\nA mixed effect model is an expansion of the ordinary linear regression model that includes both fixed effects and random effects. The fixed effects are the same as in a standard linear regression (could be with or without interactions), while the random effects account for variability across different groups or clusters in the data.\nWe are given a dataset of annual income (independent variable) and years of education (independent variable) for individuals that studied different majors in university (categorical variable). We want to predict the annual income based on years of education and the major studied, including an interaction term between years of education and major. One more thing: each individual appears more than once in the dataset, so we can assume that there is a random effect associated with each individual.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as smf\ngenerate synthetic data\n# set seed for reproducibility\nnp.random.seed(42)\n# define parameters\nmajors = ['Juggling', 'Magic', 'Dragon Taming']\nn_individuals = 90  # 30 per major\nyears_per_person = np.random.randint(1, 5, size=n_individuals)  # 1 to 4 time points\n\n# assign majors and person IDs\nperson_ids = [f'P{i+1:03d}' for i in range(n_individuals)]\nmajor_assignment = np.repeat(majors, n_individuals // len(majors))\n\n# simulate data\nrecords = []\nfor i, pid in enumerate(person_ids):\n    major = major_assignment[i]\n    n_years = years_per_person[i]\n    years = np.sort(np.random.choice(np.arange(1, 21), size=n_years, replace=False))\n    \n    # base intercept and slope by major\n    if major == 'Juggling':\n        base_income = 25_000\n        growth = 800\n    elif major == 'Magic':\n        base_income = 20_000\n        growth = 1500\n    elif major == 'Dragon Taming':\n        base_income = 30_000\n        growth = 400  # slower growth\n    \n    # add person-specific deviation\n    personal_offset = np.random.normal(0, 5000)\n    slope_offset = np.random.normal(0, 200)\n    \n    for y in years:\n        income = base_income + personal_offset + (growth + slope_offset) * y + np.random.normal(0, 3000)\n        records.append({\n            'person': pid,\n            'major': major,\n            'years_after_grad': y,\n            'income': income\n        })\n\ndf = pd.DataFrame(records)\nLet’s take a look at the dataset. There are many data points, so we will only see 15 points in three different places.\nShow the code\nprint(df[:5])\nprint(df[90:95])\nprint(df[190:195])\n\n\n  person     major  years_after_grad        income\n0   P001  Juggling                 3  37183.719609\n1   P001  Juggling                 5  35238.112407\n2   P001  Juggling                11  37905.435001\n3   P002  Juggling                 2  27432.186391\n4   P002  Juggling                 4  30617.926804\n   person  major  years_after_grad        income\n90   P034  Magic                 1  14151.072305\n91   P034  Magic                 7  19716.656861\n92   P035  Magic                12  41056.576643\n93   P035  Magic                14  46339.987229\n94   P036  Magic                16  41981.131518\n    person          major  years_after_grad        income\n190   P072  Dragon Taming                 7  36173.437735\n191   P073  Dragon Taming                 8  33450.564557\n192   P074  Dragon Taming                 9  35276.927416\n193   P074  Dragon Taming                17  37271.203018\n194   P075  Dragon Taming                 2  31819.051946\nNow let’s see the data in a plot.\nplot income by major\nfig, ax = plt.subplots(figsize=(8, 6))\n\ngb = df.groupby('major')\nfor major, group in gb:\n    ax.scatter(group['years_after_grad'], group['income'], label=major, alpha=0.6)\n\nax.legend(title='Major', frameon=False)\nax.set(xlabel='years after graduation',\n       ylabel='income',\n       xticks=np.arange(0, 21, 5)\n       );\nThe model we will use is\ny = \\underbrace{X \\beta}_{\\text{fixed effects}} + \\underbrace{Z b}_{\\text{random effects}} + \\underbrace{\\varepsilon}_{\\text{residuals}}\nThe only new term here is Zb, the random effects, where Z is the design matrix for the random effects and b is the vector of random effects coefficients. We will discuss that a bit later. Let’s start with the fixed effects part:\nX \\beta = \\beta_0 + \\beta_1 \\cdot \\text{years} + \\beta_2 \\cdot \\text{major} + \\beta_3 \\cdot (\\text{years} \\cdot \\text{major})\nThe “years” variable is continuous, while the “major” variable is categorical. How to include categorical variables in a linear regression model? We can use dummy coding, where we create binary variables for each category of the categorical variable (except one category, which serves as the reference group). In our case, we have three majors: Juggling, Magic, and Dragon Taming. Let’s use “Juggling” as the reference group. We can create two dummy variables that function as toggles.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#practical-example",
    "href": "regression/mixed-model.html#practical-example",
    "title": "14  linear mixed effect model",
    "section": "",
    "text": "major_Magic: 1 if the major is Magic, 0 otherwise\nmajor_DragonTaming: 1 if the major is Dragon Taming, 0 otherwise",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#visualizing-categories-as-toggles",
    "href": "regression/mixed-model.html#visualizing-categories-as-toggles",
    "title": "14  linear mixed effect model",
    "section": "14.2 visualizing categories as toggles",
    "text": "14.2 visualizing categories as toggles\nIn the equation above, we have only one parameter for “major” (\\beta_2), and only one parameter for the interaction terms (\\beta_3). In reality we have more, see:\n\\begin{align*}\n\\text{income} &= \\beta_0 + \\beta_1 \\cdot \\text{years} \\\\\n&+ \\beta_2 \\cdot \\text{major\\_Magic} + \\beta_3 \\cdot \\text{major\\_DragonTaming} \\\\\n&+ \\beta_4 \\cdot (\\text{years} \\cdot \\text{major\\_Magic}) + \\beta_5 \\cdot (\\text{years} \\cdot \\text{major\\_DragonTaming}) \\\\\n&+ \\epsilon\n\\end{align*}\nThe first line represents the linear relationship between income and education of the reference group (Juggling). The second line adds the effects on the intercept of having studied Magic or Dragon Taming instead, and the third line adds the the effects on the slope of these two majors.\nLet’s see for a few data points how this works. Below, dummy variables represent the pair (major_Magic, major_DragonTaming).\n\n\n\nyears_after_grad\nmajor\nDummy variables\nincome\n\n\n\n\n3\nJuggling\n(0, 0)\n37183.72\n\n\n5\nMagic\n(1, 0)\n35101.07\n\n\n7\nDragon Taming\n(0, 1)\n27179.77\n\n\n10\nJuggling\n(0, 0)\n26366.80\n\n\n12\nMagic\n(1, 0)\n26101.53\n\n\n16\nDragon Taming\n(0, 1)\n39252.76\n\n\n\nThe design matrix X would look like this:\n\nX =\n\\begin{array}{c}\n  \\begin{array}{cccccc}\n    \\beta_0 & \\beta_1 & \\beta_2 & \\beta_3 & \\beta_4 & \\beta_5\n  \\end{array} \\\\\n  \\begin{pmatrix}\n    1 & 3 & 0 & 0 & 0 & 0 \\\\\n    1 & 5 & 1 & 0 & 5 & 0 \\\\\n    1 & 7 & 0 & 1 & 0 & 7 \\\\\n    1 & 10 & 0 & 0 & 0 & 0 \\\\\n    1 & 12 & 1 & 0 & 12 & 0 \\\\\n    1 & 16 & 0 & 1 & 0 & 16\n  \\end{pmatrix}\n\\end{array}.\n\nThe betas above the matrix are there just to label the columns, they are not really part of the matrix. The 3rd and 4th columns are the dummy variables for the majors, and the 5th and 6th columns are the interaction terms between education and the majors.\nIf we were not interested in the random effects, we could stop here, and just use the ordinary least squares (OLS) method already discussed to estimate the coefficients \\beta.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#random-effects",
    "href": "regression/mixed-model.html#random-effects",
    "title": "14  linear mixed effect model",
    "section": "14.3 random effects",
    "text": "14.3 random effects\nConceptually, the random effects function in a very similar way to the fixed effects. Instead of a small number of categories, now each person in the dataset is a category. In our example we have 90 different people represented in the dataset, so the quantity Z in Zb is the design matrix for the random effects, which is a matrix with 90 columns, one for each person, and as many rows as there are data points in the dataset. Each row has a 1 in the column corresponding to the person, and 0s elsewhere. The vector b is a vector of random effects coefficients, one for each person.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#implementation",
    "href": "regression/mixed-model.html#implementation",
    "title": "14  linear mixed effect model",
    "section": "14.4 implementation",
    "text": "14.4 implementation\nWe can use statsmodels function smf.mixedlm to do everything for us. We just need to specify the formula, which includes the interaction term, and the data.\nIf you don’t mind which category is the reference group, you can skip the cell below. If you want to make sure a give one is the reference group (Juggling in our case), then you should run it.\n\n\nchoose Juggling as reference major\nfrom pandas.api.types import CategoricalDtype\n# define the desired order: Juggling as reference\nmajor_order = CategoricalDtype(categories=[\"Juggling\", \"Magic\", \"Dragon Taming\"], ordered=True)\ndf[\"major\"] = df[\"major\"].astype(major_order)\n\n\nThe syntax is fairly economic. The formula\nincome ~ years_after_grad * major\nspecifies a linear model where both the baseline income (intercept) and the effect of time since graduation (slope) can vary by major. The * operator includes both the main effects (years after graduation and major) and their interaction, allowing the model to fit a different intercept and slope for each major.\nIn the line\nmodel = smf.mixedlm(formula, data=df, groups=df[\"person\"])\nthe groups argument specifies that the random effects are associated with the “person” variable, meaning that each person can have their own random intercept.\n\n# formula with interaction\nformula = \"income ~ years_after_grad * major\"\n\n# fit mixed model with random intercept for person\nmodel = smf.mixedlm(formula, data=df, groups=df[\"person\"])\nresult = model.fit()\n\nLet’s see the results\n\nprint(result.summary())\n\n                            Mixed Linear Model Regression Results\n==============================================================================================\nModel:                         MixedLM            Dependent Variable:            income       \nNo. Observations:              239                Method:                        REML         \nNo. Groups:                    90                 Scale:                         10690821.7105\nMin. group size:               1                  Log-Likelihood:                -2327.5068   \nMax. group size:               4                  Converged:                     Yes          \nMean group size:               2.7                                                            \n----------------------------------------------------------------------------------------------\n                                           Coef.     Std.Err.   z    P&gt;|z|   [0.025    0.975] \n----------------------------------------------------------------------------------------------\nIntercept                                  25206.095 1349.760 18.675 0.000 22560.615 27851.575\nmajor[T.Magic]                             -2999.754 1995.748 -1.503 0.133 -6911.348   911.840\nmajor[T.Dragon Taming]                      5579.198 1954.661  2.854 0.004  1748.133  9410.263\nyears_after_grad                             723.745   72.028 10.048 0.000   582.573   864.917\nyears_after_grad:major[T.Magic]              635.180  109.599  5.795 0.000   420.370   849.989\nyears_after_grad:major[T.Dragon Taming]     -295.862  106.315 -2.783 0.005  -504.235   -87.488\nGroup Var                               33814137.626 2268.953                                 \n==============================================================================================",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#interpreting-the-results",
    "href": "regression/mixed-model.html#interpreting-the-results",
    "title": "14  linear mixed effect model",
    "section": "14.5 interpreting the results",
    "text": "14.5 interpreting the results\nTo interpret the coefficients, start with the reference group, which in this model is someone who studied Juggling. Their predicted income is:\n\n\\text{income} = 25206.10 + 723.75 \\times \\text{years}\n\nNow, for a person who studied Magic, the model adjusts both the intercept and the slope:\nIntercept shift: -2999.75 Slope shift: +635.18 So for Magic, the predicted income becomes:\n\\begin{align*}\n\\text{income} &= (25206.10 - 2999.75) + (723.75 + 635.18) \\times \\text{years} \\\\\n       &= 22206.35 + 1358.93 \\times \\text{years}\n\\end{align*}\nThis means that compared to Jugglers, Magicians start with a lower baseline salary, but their income grows much faster with each year after graduation.\nThe Coef. column shows the estimated value of each parameter (e.g., intercepts, slopes, interactions). The Std.Err. column reports the standard error of the estimate, reflecting its uncertainty. The z column is the test statistic (estimate divided by standard error), and P&gt;|z| gives the p-value, which helps assess whether the effect is statistically significant. The final two columns, [0.025 and 0.975], show the 95% confidence interval for the coefficient — if this interval does not include zero, the effect is likely meaningful.\nThe line labeled Group Var shows the estimated variance of the random intercepts — in this case, variation in baseline income between individuals. The second number reported is the standard error associated with this estimate, which indicates how much uncertainty there is in the estimate of the variance.\nIf you like, you can print out all the variances for the random effects. They are not explicity shown in the summary, but you can access them through the model’s random_effects attribute:\nresult.random_effects\nFinally, the model as is does not include random slopes, meaning that the effect of years after graduation is assumed to be the same for all individuals. If you want to allow for different slopes for each individual, you can modify the model to include random slopes as well. This would require changing the formula and the groups argument accordingly. Also, result.random_effects will then contain not only the random intercepts, but also the random slopes for each individual.\n\nmodel = smf.mixedlm(\n    \"income ~ years_after_grad * major\",\n    data=df,\n    groups=df[\"person\"],\n    re_formula=\"~years_after_grad\"\n)\nresult = model.fit()\nprint(result.summary())\n\n/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n  warnings.warn(\n\n\n                            Mixed Linear Model Regression Results\n==============================================================================================\nModel:                         MixedLM            Dependent Variable:            income       \nNo. Observations:              239                Method:                        REML         \nNo. Groups:                    90                 Scale:                         10125672.1682\nMin. group size:               1                  Log-Likelihood:                -2323.7559   \nMax. group size:               4                  Converged:                     Yes          \nMean group size:               2.7                                                            \n----------------------------------------------------------------------------------------------\n                                           Coef.     Std.Err.   z    P&gt;|z|   [0.025    0.975] \n----------------------------------------------------------------------------------------------\nIntercept                                  25133.841 1208.050 20.805 0.000 22766.106 27501.576\nmajor[T.Magic]                             -2805.540 1811.051 -1.549 0.121 -6355.135   744.055\nmajor[T.Dragon Taming]                      5980.367 1767.166  3.384 0.001  2516.786  9443.949\nyears_after_grad                             731.399   84.211  8.685 0.000   566.349   896.450\nyears_after_grad:major[T.Magic]              611.065  126.072  4.847 0.000   363.969   858.161\nyears_after_grad:major[T.Dragon Taming]     -329.530  122.977 -2.680 0.007  -570.561   -88.498\nGroup Var                               22392488.656 1835.422                                 \nGroup x years_after_grad Cov               90328.607   75.664                                 \nyears_after_grad Var                       39074.487    7.401                                 \n==============================================================================================",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#back-to-ols",
    "href": "regression/mixed-model.html#back-to-ols",
    "title": "14  linear mixed effect model",
    "section": "14.6 back to OLS",
    "text": "14.6 back to OLS\nIf you went this far, and now realized you don’t care about random effects, you can just use the statsmodels function smf.ols to fit an ordinary least squares regression model. The syntax is similar, but without the groups argument.\n\nimport statsmodels.formula.api as smf\n\n# formula with main effects and interaction\nformula = \"income ~ years_after_grad * major\"\n\n# fit the model with OLS (no random effects)\nols_model = smf.ols(formula, data=df)\nols_result = ols_model.fit()\n\n# print summary\nprint(ols_result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.455\nModel:                            OLS   Adj. R-squared:                  0.443\nMethod:                 Least Squares   F-statistic:                     38.85\nDate:                Tue, 24 Jun 2025   Prob (F-statistic):           6.27e-29\nTime:                        16:16:38   Log-Likelihood:                -2437.0\nNo. Observations:                 239   AIC:                             4886.\nDf Residuals:                     233   BIC:                             4907.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================================\n                                              coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------\nIntercept                                2.486e+04   1450.267     17.141      0.000     2.2e+04    2.77e+04\nmajor[T.Magic]                          -4402.0846   2281.475     -1.929      0.055   -8897.041      92.872\nmajor[T.Dragon Taming]                   7696.8705   2167.061      3.552      0.000    3427.332     1.2e+04\nyears_after_grad                          778.4674    123.280      6.315      0.000     535.582    1021.352\nyears_after_grad:major[T.Magic]           758.4393    185.397      4.091      0.000     393.170    1123.708\nyears_after_grad:major[T.Dragon Taming]  -510.1096    183.456     -2.781      0.006    -871.553    -148.666\n==============================================================================\nOmnibus:                        2.143   Durbin-Watson:                   1.088\nProb(Omnibus):                  0.343   Jarque-Bera (JB):                2.132\nSkew:                           0.176   Prob(JB):                        0.344\nKurtosis:                       2.699   Cond. No.                         93.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  }
]