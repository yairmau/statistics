[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Machine Learning",
    "section": "",
    "text": "home\nI’m teaching myself statistics and machine learning, and the best way to truly understand is to use the new tools I’ve acquired. This is what this website is for. It is mainly a reference guide for my future self.",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Statistics and Machine Learning",
    "section": "books",
    "text": "books\nThese are the books that I’ve read and recommend.\n\nModern Statistics: Intuition, Math, Python, R\nby Mike X Cohen\n\nGithub\nThis is a really approachable book, the author has a very nice conversational style, and I enjoyed it a lot. Highly recommended\n\n\n\nData-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control\nby Steven L. Brunton, J. Nathan Kutz\n\nThe whole book is available in this website.\nThis is the sort of books that is suitable for those who already know the subject. I would not recommend it as a first read. In any case, some chapters gave me new intuition on the subject. I do highly recommend Steve Brunton’s youtube channel, it’s fantastic.\n\n\n\nNeural Networks and Deep Learning\nby Michael Nielsen\n\nThis is an online book, freely available here. It can be tiring to read a whole book on a computer screen, so you can find Anton Vladyka’s LaTeX rendition of this book in his GitHub repository. I wanted to read the pdf in my tiny kindle reader, so I recompiled Anton’s LaTeX code to make it fit the screen, and on the way changed the font, and corrected typos here and there. Overleaf project. Download pdf.\nNielsen writes very well, I really enjoyed this book. The part on backprogation is a bit confusing, I would recommend watching 3b1b’s youtube video on that.\n\n\n\nIntroduction to Environmental Data Science\nby William W. Hsieh\n\nThis book’s best quality is that it covers a bunch of topics, methods, techniques. It is not a good book to learn concepts for the first time, it’s more useful as a menu of what exists, and maybe a brief reminder of topics you studied in the past but forgot. The “environmental” aspect is completely incidental, in my opinion. Hsieh brings examples from the Environment, but you don’t need to have a background in environmental science to be able to read it.",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#websites",
    "href": "index.html#websites",
    "title": "Statistics and Machine Learning",
    "section": "websites",
    "text": "websites\n\nDr. Roi Yehoshua’s tutorials\nReally good tutorials, you should check this out:\nhttps://towardsdatascience.com/author/roiyeho/\nIt seems the he wrote a book, I haven’t read it, but should be good:\nMachine Learning Foundations, Volume 1: Supervised Learning",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "data/height.html",
    "href": "data/height.html",
    "title": "1  height data",
    "section": "",
    "text": "I found growth curves for girls and boys in Israel:\n\nurl girls, pdf girls\nurl boys, pdf boys\nurl both, png boys, png girls.\n\nFor example, see this:\n\nI used the great online resource Web Plot Digitizer v4 to extract the data from the images files. I captured all the growth curves as best as I could. The first step now is to get interpolated versions of the digitized data. For instance, see below the 50th percentile for boys:\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.optimize import curve_fit\nfrom scipy.special import erf\nfrom scipy.interpolate import UnivariateSpline\nimport matplotlib.animation as animation\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n# %matplotlib widget\n\n\n\n\ndefine useful arrays\nage_list = np.round(np.arange(2.0, 20.1, 0.1), 1)\nheight_list = np.round(np.arange(70, 220, 0.1), 1)\n\n\n\n\nimport sample data, boys 50th percentile\ndf_temp_boys_50th = pd.read_csv('../archive/data/height/boys-p50.csv', names=['age','height'])\nspline = UnivariateSpline(df_temp_boys_50th['age'], df_temp_boys_50th['height'], s=0.5)\ninterpolated = spline(age_list)\n\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(df_temp_boys_50th['age'], df_temp_boys_50th['height'], label='digitized data',\n        marker='o', markerfacecolor='None', markeredgecolor=\"black\", markersize=6, linestyle='None')\nax.plot(age_list, interpolated, label='interpolated', color=\"black\", linewidth=2)\nax.set(xlabel='age (years)',\n       ylabel='height (cm)',\n       xticks=np.arange(2, 21, 2),\n       title=\"boys, 50th percentile\"\n       )\nax.legend(frameon=False);\n\n\n\n\n\n\n\n\n\nLet’s do the same for all the other curves, and then save them to a file.\n\n\ninterpolate all growth curves\ncol_names = ['p05', 'p10', 'p25', 'p50', 'p75', 'p90', 'p95']\nfile_names_boys = ['boys-p05.csv', 'boys-p10.csv', 'boys-p25.csv', 'boys-p50.csv',\n                   'boys-p75.csv', 'boys-p90.csv', 'boys-p95.csv',]\nfile_names_girls = ['girls-p05.csv', 'girls-p10.csv', 'girls-p25.csv', 'girls-p50.csv',\n                   'girls-p75.csv', 'girls-p90.csv', 'girls-p95.csv',]\n\n# create dataframe with age column\ndf_boys = pd.DataFrame({'age': age_list})\ndf_girls = pd.DataFrame({'age': age_list})\n# loop over file names and read in data\nfor i, file_name in enumerate(file_names_boys):\n    # read in data\n    df_temp = pd.read_csv('../archive/data/height/' + file_name, names=['age','height'])\n    spline = UnivariateSpline(df_temp['age'], df_temp['height'], s=0.5)\n    df_boys[col_names[i]] = spline(age_list)\nfor i, file_name in enumerate(file_names_girls):\n    # read in data\n    df_temp = pd.read_csv('../archive/data/height/' + file_name, names=['age','height'])\n    spline = UnivariateSpline(df_temp['age'], df_temp['height'], s=0.5)\n    df_girls[col_names[i]] = spline(age_list)\n\n# make age index\ndf_boys.set_index('age', inplace=True)\ndf_boys.index = df_boys.index.round(1)\ndf_boys.to_csv('../archive/data/height/boys_height_vs_age_combined.csv', index=True)\ndf_girls.set_index('age', inplace=True)\ndf_girls.index = df_girls.index.round(1)\ndf_girls.to_csv('../archive/data/height/girls_height_vs_age_combined.csv', index=True)\n\n\nLet’s take a look at what we just did.\n\ndf_girls\n\n\n\n\n\n\n\n\np05\np10\np25\np50\np75\np90\np95\n\n\nage\n\n\n\n\n\n\n\n\n\n\n\n2.0\n79.269087\n80.794167\n83.049251\n85.155597\n87.475854\n89.779822\n90.882059\n\n\n2.1\n80.202106\n81.772053\n84.052858\n86.207778\n88.713405\n90.883740\n92.409913\n\n\n2.2\n81.130687\n82.706754\n85.011591\n87.211543\n89.856186\n91.940642\n93.416959\n\n\n2.3\n82.048325\n83.601023\n85.928399\n88.170313\n90.914093\n92.953965\n94.270653\n\n\n2.4\n82.948516\n84.457612\n86.806234\n89.087509\n91.897022\n93.927147\n95.226089\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19.6\n152.520938\n154.812286\n158.775277\n163.337149\n167.699533\n171.531349\n173.969235\n\n\n19.7\n152.534223\n154.814440\n158.791925\n163.310864\n167.704618\n171.519600\n173.980150\n\n\n19.8\n152.548001\n154.827666\n158.815071\n163.275852\n167.708562\n171.504730\n173.990964\n\n\n19.9\n152.562338\n154.853760\n158.845506\n163.231563\n167.711342\n171.486629\n174.001704\n\n\n20.0\n152.577300\n154.894521\n158.884019\n163.177444\n167.712936\n171.465189\n174.012396\n\n\n\n\n181 rows × 7 columns\n\n\n\n\n\nshow all interpolated curves for girls\nfig, ax = plt.subplots(figsize=(8, 6))\n# loop over col_names and plot each column\ncolors = sns.color_palette(\"Oranges\", len(col_names))\nfor col, color in zip(col_names, colors):\n    ax.plot(df_girls.index, df_girls[col], label=col, color=color)\nax.set(xlabel='age (years)',\n       ylabel='height (cm)',\n       xticks=np.arange(2, 21, 2),\n       title=\"growth curves for girls\\npercentile curves: 5, 10, 25, 50, 75, 90, 95\",\n       );\n\n\n\n\n\n\n\n\n\nLet’s now see the percentiles for girls age 20.\n\n\nplot cdf for girls, age 20\nfig, ax = plt.subplots(figsize=(8, 6))\npercentile_list = np.array([5, 10, 25, 50, 75, 90, 95])\ndata = df_girls.loc[20.0]\nax.plot(data, percentile_list, ls='', marker='o', markersize=6, color=\"black\")\nax.set(xlabel='height (cm)',\n         ylabel='percentile',\n         yticks=percentile_list,\n         title=\"cdf for girls, age 20\"\n         );\n\n\n\n\n\n\n\n\n\nI suspect that the heights in the population are normally distributed. Let’s check that. I’ll fit the data to the integral of a gaussian, because the percentiles correspond to a cdf. If a pdf is a gaussian, its cumulative is given by\n\n\\Phi(x) = \\frac{1}{2} \\left( 1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma \\sqrt{2}}\\right) \\right)\n\nwhere \\mu is the mean and \\sigma is the standard deviation of the distribution. The error function \\text{erf} is a sigmoid function, which is a good approximation for the cdf of the normal distribution.\n\n\ndefine functions\ndef erf_model(x, mu, sigma):\n    return 50 * (1 + erf((x - mu) / (sigma * np.sqrt(2))) )\n# initial guess for parameters: [mu, sigma]\np0 = [150, 6]\n# Calculate R-squared\ndef calculate_r2(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - (ss_res / ss_tot)\n\n\n\n\nfit model to data\ndata = df_girls.loc[20.0]\nparams, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                        bounds=([100, 3],   # lower bounds for mu and sigma\n                                [200, 10])  # upper bounds for mu and sigma\n                        )\n# store the parameters in the dataframe\npercentile_predicted = erf_model(data, *params)\n# R-squared value\nr2 = calculate_r2(percentile_list, percentile_predicted)\n\n\n\n\nshow results\nfig, ax = plt.subplots(figsize=(8, 6))\npercentile_list = np.array([5, 10, 25, 50, 75, 90, 95])\ndata = df_girls.loc[20.0]\nax.plot(data, percentile_list, ls='', marker='o', markersize=6, color=\"black\", label='data')\nfit = erf_model(height_list, *params)\nax.plot(height_list, fit, label='fit', color=\"red\", linewidth=2)\nax.text(150, 75, f'$\\mu$ = {params[0]:.1f} cm\\n$\\sigma$ = {params[1]:.1f} cm\\nR$^2$ = {r2:.6f}',\n        fontsize=14, bbox=dict(facecolor='white', alpha=0.5))\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       xlim=(140, 190),\n         ylabel='percentile',\n         yticks=percentile_list,\n         title=\"the data is very well fitted by a normal distribution\"\n         );\n\n\n\n\n\n\n\n\n\nAnother way of making sure that the model fits the data is to make a QQ plot. In this plot, the quantiles of the data are plotted against the quantiles of the normal distribution. If the data is normally distributed, the points should fall on a straight line.\n\n\nshow QQ plot\nfitted_quantiles = norm.cdf(data, loc=params[0], scale=params[1])\nexperimental_quantiles = percentile_list / 100\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_aspect('equal', adjustable='box')\nax.plot(experimental_quantiles, fitted_quantiles,\n        ls='', marker='o', markersize=6, color=\"black\",\n        label='qq points')\nax.plot([0, 1], [0, 1], color='red', linewidth=2, label=\"1:1 line\")\nax.set(xlabel='empirical quantiles',\n       ylabel='fitted quantiles',\n       xlim=(0, 1),\n       ylim=(0, 1),\n       title=\"QQ plot\")\nax.legend(frameon=False)\n\n\n\n\n\n\n\n\n\nGreat, now we just need to do exactly the same for both sexes, and all the ages. I chose to divide age from 2 to 20 into 0.1 intervals.\n\n\ncreate dataframes to store the parameters mu, sigma, r2\ndf_stats_boys = pd.DataFrame(index=age_list, columns=['mu', 'sigma', 'r2'])\ndf_stats_boys['mu'] = 0.0\ndf_stats_boys['sigma'] = 0.0\ndf_stats_boys['r2'] = 0.0\ndf_stats_girls = pd.DataFrame(index=age_list, columns=['mu', 'sigma', 'r2'])\ndf_stats_girls['mu'] = 0.0\ndf_stats_girls['sigma'] = 0.0\ndf_stats_girls['r2'] = 0.0\n\n\n\n\nfit model to all the data\np0 = [80, 3]\n# loop over ages in the index, calculate mu and sigma\nfor i in df_boys.index:\n    # fit the model to the data\n    data = df_boys.loc[i]\n    params, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                          bounds=([70, 2],   # lower bounds for mu and sigma\n                                  [200, 10])  # upper bounds for mu and sigma\n                         )\n    # store the parameters in the dataframe\n    df_stats_boys.at[i, 'mu'] = params[0]\n    df_stats_boys.at[i, 'sigma'] = params[1]\n    percentile_predicted = erf_model(data, *params)\n    # R-squared value\n    r2 = calculate_r2(percentile_list, percentile_predicted)\n    df_stats_boys.at[i, 'r2'] = r2\n    p0 = params\n# same for girls\np0 = [80, 3]\nfor i in df_girls.index:\n    # fit the model to the data\n    data = df_girls.loc[i]\n    params, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                          bounds=([70, 3],   # lower bounds for mu and sigma\n                                  [200, 10])  # upper bounds for mu and sigma\n                         )\n    # store the parameters in the dataframe\n    df_stats_girls.at[i, 'mu'] = params[0]\n    df_stats_girls.at[i, 'sigma'] = params[1]\n    percentile_predicted = erf_model(data, *params)\n    # R-squared value\n    r2 = calculate_r2(percentile_list, percentile_predicted)\n    df_stats_girls.at[i, 'r2'] = r2\n    p0 = params\n\n# save the dataframes to csv files\ndf_stats_boys.to_csv('../archive/data/height/boys_height_stats.csv', index=True)\ndf_stats_girls.to_csv('../archive/data/height/girls_height_stats.csv', index=True)\n\n\nLet’s see what we got. The top panel in the graph shows the average height for boys and girls, the middle panel shows the coefficient of variation (\\sigma/\\mu), and the bottom panel shows the R2 of the fit (note that the range is very close to 1).\n\ndf_stats_boys\n\n\n\n\n\n\n\n\nmu\nsigma\nr2\n\n\n\n\n2.0\n86.463069\n3.563785\n0.999511\n\n\n2.1\n87.374895\n3.596583\n0.999676\n\n\n2.2\n88.269676\n3.627433\n0.999742\n\n\n2.3\n89.148086\n3.657263\n0.999752\n\n\n2.4\n90.010783\n3.686764\n0.999733\n\n\n...\n...\n...\n...\n\n\n19.6\n176.802810\n7.134561\n0.999991\n\n\n19.7\n176.845789\n7.135786\n0.999994\n\n\n19.8\n176.892196\n7.137430\n0.999995\n\n\n19.9\n176.942521\n7.139466\n0.999990\n\n\n20.0\n176.997255\n7.141858\n0.999976\n\n\n\n\n181 rows × 3 columns\n\n\n\n\n\nplot results\nfig, ax = plt.subplots(3,1, figsize=(8, 10), sharex=True)\nfig.subplots_adjust(left=0.15)\nax[0].plot(df_stats_boys['mu'], label='boys', lw=2)\nax[0].plot(df_stats_girls['mu'], label='girls', lw=2)\nax[0].legend(frameon=False)\n\nax[1].plot(df_stats_boys['sigma'] / df_stats_boys['mu'], lw=2)\nax[1].plot(df_stats_girls['sigma'] / df_stats_girls['mu'], lw=2)\n\nax[2].plot(df_stats_boys.index, df_stats_boys['r2'], label=r'$r2$ boys', lw=2)\nax[2].plot(df_stats_girls.index, df_stats_girls['r2'], label=r'$r2$ girls', lw=2)\n\nax[0].set(ylabel='average height (cm)',)\nax[1].set(ylabel='CV',\n          ylim=[0,0.055])\nax[2].set(xlabel='age (years)',\n            ylabel=r'$R^2$',\n            xticks=np.arange(2, 21, 2),\n          );\n\n\n\n\n\n\n\n\n\nLet’s see how the pdfs for boys and girls move and morph as age increases.\n\n\nproduce dataframes for pre-calculated pdfs\nage_list_string = age_list.astype(str).tolist()\ndf_pdf_boys = pd.DataFrame(index=height_list, columns=age_list_string)\ndf_pdf_girls = pd.DataFrame(index=height_list, columns=age_list_string)\n\nfor age in df_pdf_boys.columns:\n    age_float = round(float(age), 1)\n    df_pdf_boys[age] = norm.pdf(height_list,\n                                loc=df_stats_boys.loc[age_float]['mu'],\n                                scale=df_stats_boys.loc[age_float]['sigma'])\nfor age in df_pdf_girls.columns:\n    age_float = round(float(age), 1)\n    df_pdf_girls[age] = norm.pdf(height_list,\n                                loc=df_stats_girls.loc[age_float]['mu'],\n                                scale=df_stats_girls.loc[age_float]['sigma'])\n\n\n\ndf_pdf_girls\n\n\n\n\n\n\n\n\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n...\n19.1\n19.2\n19.3\n19.4\n19.5\n19.6\n19.7\n19.8\n19.9\n20.0\n\n\n\n\n70.0\n0.000006\n2.962419e-06\n1.229580e-06\n4.740717e-07\n1.893495e-07\n7.928033e-08\n3.395629e-08\n1.454961e-08\n6.214658e-09\n2.698367e-09\n...\n3.876760e-46\n4.998212e-46\n6.108274e-46\n6.965756e-46\n7.300518e-46\n6.928073e-46\n5.866310e-46\n4.367574e-46\n2.817087e-46\n1.550490e-46\n\n\n70.1\n0.000007\n3.369929e-06\n1.401926e-06\n5.423176e-07\n2.172465e-07\n9.118694e-08\n3.914667e-08\n1.681357e-08\n7.199311e-09\n3.133161e-09\n...\n4.821662e-46\n6.212999e-46\n7.589544e-46\n8.652519e-46\n9.067461e-46\n8.605908e-46\n7.289698e-46\n5.430839e-46\n3.506265e-46\n1.932327e-46\n\n\n70.2\n0.000008\n3.830459e-06\n1.597215e-06\n6.199308e-07\n2.490751e-07\n1.048086e-07\n4.509972e-08\n1.941687e-08\n8.334521e-09\n3.635676e-09\n...\n5.995467e-46\n7.721230e-46\n9.427830e-46\n1.074523e-45\n1.125944e-45\n1.068759e-45\n9.056344e-46\n6.751373e-46\n4.363019e-46\n2.407630e-46\n\n\n70.3\n0.000009\n4.350475e-06\n1.818328e-06\n7.081296e-07\n2.853621e-07\n1.203810e-07\n5.192270e-08\n2.240831e-08\n9.642428e-09\n4.216078e-09\n...\n7.453283e-46\n9.593350e-46\n1.170864e-45\n1.334099e-45\n1.397806e-45\n1.326973e-45\n1.124851e-45\n8.391039e-46\n5.427845e-46\n2.999137e-46\n\n\n70.4\n0.000010\n4.937172e-06\n2.068480e-06\n8.082806e-07\n3.267014e-07\n1.381707e-07\n5.973725e-08\n2.584341e-08\n1.114829e-08\n4.885994e-09\n...\n9.263403e-46\n1.191661e-45\n1.453785e-45\n1.655996e-45\n1.734906e-45\n1.647188e-45\n1.396806e-45\n1.042648e-45\n6.750965e-46\n3.735083e-46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n219.5\n0.000000\n5.214425e-307\n1.377605e-289\n3.568527e-277\n6.457994e-266\n2.232144e-255\n6.340272e-246\n9.969867e-238\n1.389324e-230\n5.441854e-224\n...\n5.200570e-18\n5.741874e-18\n6.194642e-18\n6.495054e-18\n6.583545e-18\n6.417949e-18\n5.986319e-18\n5.315101e-18\n4.468701e-18\n3.538724e-18\n\n\n219.6\n0.000000\n1.813597e-307\n5.050074e-290\n1.356408e-277\n2.537010e-266\n9.046507e-256\n2.642444e-246\n4.256155e-238\n6.055129e-231\n2.417510e-224\n...\n4.558798e-18\n5.035058e-18\n5.433557e-18\n5.698034e-18\n5.775970e-18\n5.630212e-18\n5.250299e-18\n4.659675e-18\n3.915265e-18\n3.097919e-18\n\n\n219.7\n0.000000\n6.302763e-308\n1.849870e-290\n5.151948e-278\n9.959447e-267\n3.663840e-256\n1.100546e-246\n1.815751e-238\n2.637298e-231\n1.073274e-224\n...\n3.995288e-18\n4.414220e-18\n4.764871e-18\n4.997654e-18\n5.066279e-18\n4.938013e-18\n4.603699e-18\n4.084117e-18\n3.429566e-18\n2.711382e-18\n\n\n219.8\n0.000000\n2.188653e-308\n6.771033e-291\n1.955386e-278\n3.906942e-267\n1.482823e-256\n4.580523e-247\n7.741154e-239\n1.147918e-231\n4.761829e-225\n...\n3.500614e-18\n3.869030e-18\n4.177503e-18\n4.382343e-18\n4.442754e-18\n4.329907e-18\n4.035791e-18\n3.578814e-18\n3.003413e-18\n2.372514e-18\n\n\n219.9\n0.000000\n7.594139e-309\n2.476504e-291\n7.416066e-279\n1.531537e-267\n5.997065e-257\n1.905138e-247\n3.298116e-239\n4.993198e-232\n2.111339e-225\n...\n3.066470e-18\n3.390384e-18\n3.661688e-18\n3.841895e-18\n3.895062e-18\n3.795805e-18\n3.537115e-18\n3.135297e-18\n2.629596e-18\n2.075507e-18\n\n\n\n\n1500 rows × 181 columns\n\n\n\n\n\nplotly widget\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'notebook'\n\n# create figure\nfig = go.Figure()\n\n# assume both dataframes have the same columns (ages) and index (height)\nages = df_pdf_boys.columns\nx_vals = df_pdf_boys.index\n\n# add traces: 2 per age (boys and girls), all hidden except the first pair\nfor i, age in enumerate(ages):\n    fig.add_trace(go.Scatter(x=x_vals, y=df_pdf_boys[age], name=f'Boys {age}', \n                             line=dict(color='#1f77b4'), visible=(i == 0)))\n    fig.add_trace(go.Scatter(x=x_vals, y=df_pdf_girls[age], name=f'Girls {age}', \n                             line=dict(color='#ff7f0e'), visible=(i == 0)))\n\n# create slider steps\nsteps = []\nfor i, age in enumerate(ages):\n    vis = [False] * (2 * len(ages))\n    vis[2*i] = True      # boys trace\n    vis[2*i + 1] = True  # girls trace\n\n    steps.append(dict(\n        method='update',\n        args=[{'visible': vis},\n              {'title': f'Height Distribution - Age: {age}'}],\n        label=str(age)\n    ))\n\n# define slider\nsliders = [dict(\n    active=0,\n    currentvalue={\"prefix\": \"Age: \"},\n    pad={\"t\": 50},\n    steps=steps\n)]\n\n# update layout\nfig.update_layout(\n    sliders=sliders,\n    title='Height Distribution by Age',\n    xaxis_title='Height (cm)',\n    yaxis_title='Density',\n    yaxis=dict(range=[0, 0.12]),\n    showlegend=True,\n    height=600,\n    width=800\n)\n\nfig.show()\n\n\n                                                \n\n\nA few notes about what we can learn from the analysis above.\n\nMy impression that 12-year-old girls are taller than boys is indeed true.\nBoys and girls have very similar distributions up to age 11.\nFrom age 11 to 13 girls are on average taller than boys.\nFrom age 13 boys become taller than girls, on average.\nThe graph showing the coefficient of variation is interesting. CV for girls peaks roughtly at age 12, and for boys it peaks around age 14. These local maxima may be explained by the wide variability in the age ofpuberty onset.\nThe height distribution for each sex, across all ages, is indeed extremely well described by the normal distribution. What biological factors may account for such a fact?\n\nI’ll plot one last graph from now, let’s see what we can learn from it. Let’s see the pdf for boys and girls across three age groups: 8, 12, and 15 year olds.\n\n\ncomparison across three ages\nfig, ax = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\nfig.subplots_adjust(hspace=0.1)\nages_to_plot = [8.0, 12.0, 15.0]\n\nfor i, age in enumerate(ages_to_plot):\n    pdf_boys = norm.pdf(height_list, loc=df_stats_boys.loc[age]['mu'], scale=df_stats_boys.loc[age]['sigma'])\n    pdf_girls = norm.pdf(height_list, loc=df_stats_girls.loc[age]['mu'], scale=df_stats_girls.loc[age]['sigma'])\n    ax[i].plot(height_list, pdf_boys, label='boys', color='tab:blue')\n    ax[i].plot(height_list, pdf_girls, label='girls', color='tab:orange')\n    ax[i].text(0.98, 0.98, f'age: {age} years', transform=ax[i].transAxes, verticalalignment='top', horizontalalignment='right')\n    ax[i].set(ylabel='pdf',\n              ylim=(0, 0.07),\n            )\nax[2].legend(frameon=False)\nax[2].set(xlabel='height (cm)',\n          xlim=(100, 200),);\n\n\n\n\n\n\n\n\n\n\nIndeed, boys and girls age 8 have the exact same height distribution.\n12-year-old girls are indeed taller than boys, on average. This difference is relatiely small, though.\nBy age 15 boys have long surpassed girls in height, and the difference is quite large. Boys still have some growing to do, but girls are mostly done growing.",
    "crumbs": [
      "data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>height data</span>"
    ]
  },
  {
    "objectID": "data/weight.html",
    "href": "data/weight.html",
    "title": "2  weight data",
    "section": "",
    "text": "2.1 example\nNow that we have height data covered, it’s time we deal with weight data.\nYes, I am VERY WELL AWARE that weight is a force, and it is not measured in kg. Nevertheless, I will use the word weight in the colloquial sense, and for all purposes it is a synonym for mass.\nThis analysis is based on the CDC Growth Charts Data Files. From there I downloaded a csv for the weight of boys and girls, from age 2 to 20.\nFor each sex and age, the csv contains three important columns for us:\nThese three variables can be combined to give the weight W at a given Z-score (number of standard deviations from the mean):\nW = M \\left(1 + L S Z\\right)^{1/L}\n\\tag{1}\nThe website contains a different formula for the case L=0, but in our data set L is never zero.\nIt will be useful in a little while to know the inverse of Eq. (1), which is\nZ = \\frac{(W/M)^L - 1}{L S}.\n\\tag{1b}\nThe formulas above indicate that we’re using the Box-Cox distribution (also called power-normal distribution), and they will help us compute the probability density function (pdf) for weight.\nGiven that the pdf of a z-scored variable is \nf_z(Z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-Z^2/2},\n\\tag{2}\nwe need to change variables from Z to W. To do that, we will use\nf_w(W)dW = f_z(Z)dZ.\n\\tag{3}\nThe rationale behind this is that the probability (area) of being in a small interval is the same, whether we measure it in terms of W or Z. See more here: Function of random variables and change of variables in the probability density function. Solving Eq. (3) for f_w(W), we get\nf_w(W) = f_z(Z) \\left|\\frac{dZ}{dW}\\right| = f_z(Z) \\left|\\frac{dW}{dZ}\\right|^{-1}.\n\\tag{4}\nUsing Eq. (1), the derivative of W with respect to Z is\n\\begin{align*}\n\\frac{dW}{dZ} &= MS\\left(1+LSZ\\right)^{\\frac{1}{L}-1}\\\\\n              &= \\underbrace{M\\left(1+LSZ\\right)^{\\frac{1}{L}}}_{=W \\text{ according to Eq. (1)}}S\\left(1+LSZ\\right)^{-1} \\\\\n              &= WS\\frac{1}{1+LSZ} \\\\\n              &= WS\\frac{1}{\\bcancel{1}+\\cancel{LS}\\frac{(W/M)^L - \\bcancel{1}}{\\cancel{L S}}} \\\\\n              &= WS\\frac{1}{(W/M)^L} \\\\\n              & = W^{1-L} M^L S\n\\tag{5}\n\\end{align*}\nPutting everything together [remember that we need the reciprocal of the result in Eq. (5)], we get\n\\begin{align*}\nf_w(W) &= f_z(Z) \\frac{W^{L-1}}{M^L S} \\\\\n       &= \\frac{1}{\\sqrt{2 \\pi}} e^{-Z^2/2} \\frac{W^{L-1}}{M^L S}\n\\tag{6}\n\\end{align*}\nOf course, we could have substituted Z from Eq. (1b) into Eq. (6) to get a formula that depends only on W, it would be too messy. Later on, we will compute f_w(W) numerically, so we will not need to do that.\nLet’s see the weight probability density for boys at age 10 and 15.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import powernorm\nload weight data and compute pdf\nweight_list = np.arange(10, 130, 0.1)\ndef power_normal_pdf(w, age, sex):\n    \"\"\"\n    Calculates the PDF of the Power Normal distribution from the derived formula.\n    This function correctly handles negative L values.\n    \"\"\"\n    # This function is only valid for w &gt; 0\n    w = np.asarray(w)\n    pdf = np.full(w.shape, np.nan)\n    positive_w = w[w &gt; 0]\n    df = pd.read_csv('../archive/data/weight/wtage.csv')\n    agemos = age * 12 + 0.5\n    df = df[(df['Agemos'] == agemos) & (df['Sex'] == sex)]\n    L = df['L'].values[0]\n    M = df['M'].values[0]\n    S = df['S'].values[0]\n    \n    # Calculate the z-score\n    z = ((positive_w / M)**L - 1) / (L * S)\n    \n    # Calculate the two main parts of the formula\n    pre_factor = positive_w**(L - 1) / (S * M**L * np.sqrt(2 * np.pi))\n    exp_term = np.exp(-0.5 * z**2)\n    \n    # Store the results only for the valid (positive w) indices\n    pdf[w &gt; 0] = pre_factor * exp_term\n    return pdf\n\npdf_boys10 = power_normal_pdf(weight_list, 10, 1)\npdf_boys15 = power_normal_pdf(weight_list, 15, 1)\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(weight_list, pdf_boys10, color='tab:blue', lw=2, alpha=0.5, label='10 years')\nax.plot(weight_list, pdf_boys15, color='tab:blue', lw=2, label='15 years')\nax.set(xlabel='weight (kg)',\n       ylabel='pdf',\n       title=\"weight distribution for boys\")\nax.legend(loc=\"upper right\", frameon=False)\nIn future chapters, when we want to talk about weight, it will be more convenient to leverage scipy’s capabilities, both to compute the pdf and to generate random samples.\nleveraging scipy.stats\ndef scipy_power_normal_pdf(w, age, sex):\n    \n    # Load LMS parameters from the CSV file\n    df = pd.read_csv('../archive/data/weight/wtage.csv')\n    agemos = age * 12 + 0.5\n    df = df[(df['Agemos'] == agemos) & (df['Sex'] == sex)]\n    L = df['L'].values[0]\n    M = df['M'].values[0]\n    S = df['S'].values[0]\n    \n    # 1. Transform weight (w) to the standard normal z-score\n    z = ((w / M)**L - 1) / (L * S)\n    \n    # 2. Calculate the derivative of the transformation (dz/dw)\n    # This is the Jacobian factor for the change of variables\n    dz_dw = (w**(L - 1)) / (S * M**L)\n    \n    # 3. Apply the change of variables formula: pdf(w) = pdf(z) * |dz/dw|\n    pdf = stats.norm.pdf(z) * dz_dw\n    \n    return pdf\n\ndef scipy_power_normal_draw_random(N, age, sex):   \n    # Load LMS parameters from the CSV file\n    df = pd.read_csv('../archive/data/weight/wtage.csv')\n    agemos = age * 12 + 0.5\n    df = df[(df['Agemos'] == agemos) & (df['Sex'] == sex)]\n    L = df['L'].values[0]\n    M = df['M'].values[0]\n    S = df['S'].values[0]\n\n    # draw random z from standard normal distribution\n    z = np.random.normal(0, 1, N)\n    # transform z to w\n    w = M * (1 + L * S * z)**(1 / L)\n    \n    return w\n\n# Calculate the PDFs for 10 and 15-year-old boys using the SciPy-based function\nscipy_pdf_boys10 = scipy_power_normal_pdf(weight_list, 10, 1)\ndraw10 = scipy_power_normal_draw_random(1000, 10, 1)\nscipy_pdf_boys15 = scipy_power_normal_pdf(weight_list, 15, 1)\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(weight_list, pdf_boys10, color='tab:blue', lw=2, alpha=0.5, label='10 years')\n# histogram of draw10\nax.hist(draw10, bins=30, density=True, color='tab:blue', alpha=0.3, label='random 10 years')\n# ax.plot(weight_list, scipy_pdf_boys10, color='tab:blue', ls=\":\", label='scipy 10')\nax.set(xlabel='weight (kg)',\n       ylabel='pdf',\n       title=\"using scipy's components to draw random samples\")\nax.legend(loc=\"upper right\", frameon=False)",
    "crumbs": [
      "data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>weight data</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_one_sample.html",
    "href": "hypothesis_testing/t_test_one_sample.html",
    "title": "3  one-sample t-test",
    "section": "",
    "text": "3.1 Question\nI measured the height of 10 adult men. Were they sampled from the general population of men?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_one_sample.html#hypotheses",
    "href": "hypothesis_testing/t_test_one_sample.html#hypotheses",
    "title": "3  one-sample t-test",
    "section": "3.2 Hypotheses",
    "text": "3.2 Hypotheses\n\nNull hypothesis: The sample mean is equal to the population mean. In this case, the answer would be “yes”\nAlternative hypothesis: The sample mean is not equal to the population mean. Answer would be “no”.\nSignificance level: 0.05\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_1samp, t\n%matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[20.0, 'mu']\nsigma_boys = df_boys.loc[20.0, 'sigma']\n\n\nLet’s start with a sample of 10.\n\n\ngenerate data\nN = 10\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample10 = norm.rvs(size=N, loc=mu_boys+2, scale=sigma_boys)\n\n\n\n\nplot sample against population pdf\nheight_list = np.arange(140, 220, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nax.eventplot(sample10, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='gray', label='sample')\n\nax.text(190, 0.04, \n       f\"sample mean: {sample10.mean():.2f} cm\\nsample std: {sample10.std(ddof=1):.2f} cm\", \n       ha='left', va='top', color='gray')\n\nax.text(190, 0.02, \n       f\"pop. mean: {mu_boys:.2f} cm\\npop. std: {sigma_boys:.2f} cm\", \n       ha='left', va='top', color='tab:blue')\n\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n       title=\"men (age 20)\",\n       xlim=(140, 220),\n       );\n\n\n\n\n\nThe t value is calculated as follows: \nt = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}\n\nwhere\n\n\\bar{x}: sample mean\n\\mu: population mean\ns: sample standard deviation\nn: sample size\n\nLet’s try the formula above and compare it with scipy’s ttest_1samp function.\n\n\ncalculate t-value\nt_value_formula = (sample10.mean() - mu_boys) / (sample10.std(ddof=1) / np.sqrt(N))\nt_value_scipy = ttest_1samp(sample10, popmean=mu_boys)\nprint(f\"t-value (formula): {t_value_formula:.3f}\")\nprint(f\"t-value (scipy): {t_value_scipy.statistic:.3f}\")\n\n\nt-value (formula): 1.759\nt-value (scipy): 1.759\n\n\nLet’s convert this t value to a p value. It is easy to visualize the p value by ploting the pdf for the t distribution. The p value is the area under the curve for t greater than the t value and smaller than the negative t value.\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.10),\n                        xytext=(t_value_scipy.statistic, 0.30),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\nax.annotate(f\"-t value = -{t_value_scipy.statistic:.3f}\",\n                        xy=(-t_value_scipy.statistic, 0.10),\n                        xytext=(-t_value_scipy.statistic, 0.30),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(np.abs(t_array) &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=10)\",\n       );\n\n\n\n\n\n\n\n\n\nThe p value is the fraction of the t distribution that is more extreme than the observed t value. If the p value is less than the significance level, we reject the null hypothesis. In this case, the p value is larger than the significance level, so we fail to reject the null hypothesis. This means that we do not have enough evidence to say that the sample mean is different from the population mean. In other words, we cannot conclude that the 10 men samples were drawn from a distribution different than the general population.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_one_sample.html#increase-the-sample-size",
    "href": "hypothesis_testing/t_test_one_sample.html#increase-the-sample-size",
    "title": "3  one-sample t-test",
    "section": "3.3 increase the sample size",
    "text": "3.3 increase the sample size\nLet’s see what happens when we increase the sample size to 100.\n\n\ngenerate data\nN = 100\n# set scipy seed for reproducibility\nnp.random.seed(628)\nsample100 = norm.rvs(size=N, loc=mu_boys+2, scale=sigma_boys)\n\n\n\n\nplot sample against population pdf\nheight_list = np.arange(140, 220, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nax.eventplot(sample100, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='gray', label='sample')\n\nax.text(190, 0.04, \n       f\"sample mean: {sample100.mean():.2f} cm\\nsample std: {sample100.std(ddof=1):.2f} cm\", \n       ha='left', va='top', color='gray')\n\nax.text(190, 0.02, \n       f\"pop. mean: {mu_boys:.2f} cm\\npop. std: {sigma_boys:.2f} cm\", \n       ha='left', va='top', color='tab:blue')\n\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n       title=\"men (age 20)\",\n       xlim=(140, 220),\n       );\n\n\n\n\n\n\n\n\n\n\n\ncalculate t-value\nt_value_scipy = ttest_1samp(sample100, popmean=mu_boys)\nprint(f\"t-value: {t_value_scipy.statistic:.3f}\")\nprint(f\"p-value: {t_value_scipy.pvalue:.3f}\")\n\n\nt-value: 2.675\np-value: 0.009\n\n\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.03),\n                        xytext=(t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\nax.annotate(f\"-t value = -{t_value_scipy.statistic:.3f}\",\n                        xy=(-t_value_scipy.statistic, 0.03),\n                        xytext=(-t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(np.abs(t_array) &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=100)\",\n       );",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_one_sample.html#question-2",
    "href": "hypothesis_testing/t_test_one_sample.html#question-2",
    "title": "3  one-sample t-test",
    "section": "3.4 Question 2",
    "text": "3.4 Question 2\nCan we say that the sampled men are taller than the general population?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_one_sample.html#hypotheses-1",
    "href": "hypothesis_testing/t_test_one_sample.html#hypotheses-1",
    "title": "3  one-sample t-test",
    "section": "3.5 Hypotheses",
    "text": "3.5 Hypotheses\n\nNull hypothesis: The sample mean is equal to the population mean.\nAlternative hypothesis: The sample mean is higher the population mean.\nSignificance level: 0.05\n\nThe analysis is the same as before, but we will use a one-tailed test. The t statistic is the same, but the p value is smaller, since we account for a smaller portion of the total area of the pdf.\n\n\ncalculate t-value and p-value\nt_value_scipy = ttest_1samp(sample100, popmean=mu_boys, alternative='greater')\nprint(f\"t-value: {t_value_scipy.statistic:.3f}\")\nprint(f\"p-value: {t_value_scipy.pvalue:.3f}\")\n\n\nt-value: 2.675\np-value: 0.004\n\n\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.03),\n                        xytext=(t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(t_array &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=100)\",\n       );\n\n\n\n\n\n\n\n\n\nThe answer is yes: the sampled men are significantly taller than the general population, since the p value is smaller than the significance level.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_independent_samples.html",
    "href": "hypothesis_testing/t_test_independent_samples.html",
    "title": "4  independent samples t-test",
    "section": "",
    "text": "4.1 Question\nAre 12-year old girls significantly taller than 12-year old boys?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_independent_samples.html#hypotheses",
    "href": "hypothesis_testing/t_test_independent_samples.html#hypotheses",
    "title": "4  independent samples t-test",
    "section": "4.2 Hypotheses",
    "text": "4.2 Hypotheses\n\nNull hypothesis: Girls and boys have the same mean height.\nAlternative hypothesis: Girls are significantly taller.\nSignificance level: 0.05\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\n\nIn this example, we sampled 10 boys and 14 girls. See below the samples data and their underlying distributions.\n\n\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nShow the code\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    )\n\n\n\n\n\n\n\n\n\nTo answer the question, we will use an independent samples t-test.\n\\begin{align}\nt &= \\frac{\\bar{X}_1 - \\bar{X}_2}{\\Theta} \\\\\n\\Theta &= \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\end{align}\nThis is a generalization of the one-sample t-test. If we take one of the samples to be infinite, we get the one-sample t-test.\nWe can compute the t-statistic by ourselves, and compare the results with those of scipy.stats.ttest_ind. Because we are interested in the difference between the means, we will use the equal_var=False option to compute Welch’s t-test. Also, because we are testing the alternative hypothesis that girls are taller, we will use the one sided test.\n\n\nShow the code\nTheta = np.sqrt(sample_boys.std(ddof=1)**2/sample_boys.size + \\\n                sample_girls.std(ddof=1)**2/sample_girls.size)\nt_stat = (sample_boys.mean() - sample_girls.mean()) / Theta\ndof = N_boys + N_girls - 2\np_val = t.cdf(t_stat, dof)\n\n# the option alternative=\"less\" is used because we are testing whether the first sample (boys) is less than the second sample (girls)\nt_value_scipy = ttest_ind(sample_boys, sample_girls, equal_var=False, alternative=\"less\")\n\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\nprint(f\"t-statistic (scipy): {t_value_scipy.statistic:.3f}, p-value (scipy): {t_value_scipy.pvalue:.3f}\")\n\n\nt-statistic: -0.999, p-value: 0.164\nt-statistic (scipy): -0.999, p-value (scipy): 0.165\n\n\nWe got the exact same results :)\nNow let’s visualize what the p-value means.\n\n\nvisualize t-distribution\n# degrees of freedom\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.25),\n                        xytext=(t_value_scipy.statistic, 0.35),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(t_array &lt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (dof=22)\",\n       );\n\n\n\n\n\n\n\n\n\nBecause the p-value is higher than the significance level, we fail to reject the null hypothesis. This means that, based on the data, we cannot conclude that girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/t_test_independent_samples.html#increasing-sample-size",
    "href": "hypothesis_testing/t_test_independent_samples.html#increasing-sample-size",
    "title": "4  independent samples t-test",
    "section": "4.3 increasing sample size",
    "text": "4.3 increasing sample size\nLet’s increase the sample size to see how it affects the p-value. We’ll sample 250 boys and 200 girls now.\n\n\ngenerate data\nN_boys = 250\nN_girls = 200\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nShow the code\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nTheta = np.sqrt(sample_boys.std(ddof=1)**2/sample_boys.size + \\\n                sample_girls.std(ddof=1)**2/sample_girls.size)\nt_stat = (sample_boys.mean() - sample_girls.mean()) / Theta\ndof = N_boys + N_girls - 2\np_val = t.cdf(t_stat, dof)\n\n# the option alternative=\"less\" is used because we are testing whether the first sample (boys) is less than the second sample (girls)\nt_value_scipy = ttest_ind(sample_boys, sample_girls, equal_var=False, alternative=\"less\")\n\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\nprint(f\"t-statistic (scipy): {t_value_scipy.statistic:.3f}, p-value (scipy): {t_value_scipy.pvalue:.3f}\")\n\n\nt-statistic: -2.639, p-value: 0.004\nt-statistic (scipy): -2.639, p-value (scipy): 0.004\n\n\nWe found now a p-value lower than the significance level, so we reject the null hypothesis. This means that, based on the data, we can conclude that girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/statistical_power.html",
    "href": "hypothesis_testing/statistical_power.html",
    "title": "5  statistical power",
    "section": "",
    "text": "5.1 motivation\nIn the chapter independent samples t-test, we asked the question:\nAre 12-year old girls significantly taller than 12-year old boys?\nWe showed that, when sampling 10 boys and 14 girls, we could not reject the null hypothesis that boys and girls have the same mean height. This was so because our p-value was higher that our significance level \\alpha=0.05. We then increased the sample sizes to 250 boys and 200 girls, and now the p-value dropped below \\alpha, so we could reject the null hypothesis, and accept the alternative hypothesis that girls are significantly taller than boys.\nStatistical power is the idea behind this relationship between sample size and our ability to discern (or not) significant effects in our data.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>statistical power</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/statistical_power.html#the-burning-house-mnemonic",
    "href": "hypothesis_testing/statistical_power.html#the-burning-house-mnemonic",
    "title": "5  statistical power",
    "section": "5.2 the burning house mnemonic",
    "text": "5.2 the burning house mnemonic\nIn my house I have a fire alarm. Sometimes it goes off without any apparent reason, or just because it sensed a little smoke from my toaster. Sure, this is annoying, but no sensor is perfect, and that’s the price I pay for securing my house. Online reviews of this fire alarm system say that this is a great choice, because the chance that there is a real fire and that the alarm doesn’t notice it is miniscule, but nonzero nonetheless.\nLet’s make this scenario more structured. For every hour that passes, there can either be a fire in the house or not. In that same time interval, the fire alarm might go off or not.\nIn trying to secure my house from fire, I have to consider two important probabilities.\n\nI really don’t want any misses, that is, events when there’s a real fire and the alarm doesn’t notice it. Let’s call \\beta the probability that, in the case of a real fire, the alarm misses a real fire. It will be easy to remember \\beta, it stands for burn.\nI can take precautions to decrease the chance that I don’t notice a real fire and my house burns: I can by a more sensitive sensor, or install many more sensors around the house. The downside of that is that the alarm will sound a lot more often, and almost always it will be a false alarm. Let’s call \\alpha the probability that there wasn’t any fire in the house, but the alarm went off. \\alpha stads for false alarm.\n\nNothing is perfect in the real world, and I have a tradeoff between \\alpha and \\beta:\n\nIf I am super risk-averse, I can decrease \\beta, the chance that my house will burn without the alarm going off, and at the same time increase \\alpha, the chance of false alarms.\nIf I can’t stand having the alarm going off all the time without reason, I can decrease \\alpha by decreasing the sensitivity of the sensor, but that will necessarily increase \\beta.\nThis is not a binary choice, it’s a tradeoff. The only way of getting the best of both worlds is by spending a lot of money. For example, I could buy 20 super-precise sensors and spread them around the house, and only have the alarm go off when 5 of them identify smoke, so my toaster alone will not trigger any false alarms.\n\nThe whole point of this “burning house” example is to help me (and maybe you) remember what \\alpha and \\beta mean. They are also given the horrible names of Type I and Type II errors, which baffles me: who would want to remember these names? Not all is lost, in the greek alphabet \\alpha is the 1st letter and \\beta is the 2nd, so we can rely on that to remember what are Type I and Type II errors. If I were king of statistics for one day, I would call the two types of error simply “false alarms” and “misses”.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>statistical power</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/statistical_power.html#statistical-power",
    "href": "hypothesis_testing/statistical_power.html#statistical-power",
    "title": "5  statistical power",
    "section": "5.3 statistical power",
    "text": "5.3 statistical power\nStatistical power the probability of correctly identifying the fire. Mathematically, it’s 1-\\beta. In the the language of hypothesis testing:\n\nH0, Null Hypothesis: there isn’t any fire.\nH1, Alternative Hypothesis: there is a fire.\nPower is the probability that I will correctly reject H0. That’s the whole point of an alarm system! That’s the whole point of any experiment designed to test a hypothesis! Sure, if H0 is indeed true, then I would want not to reject it.\n\nThe quantities \\alpha and \\beta are usually visualized in a matrix:\n\n\n\n\n\n\n\n\n\n\n\nH0 is TrueThere is no fire \nH0 is FalseThere is a fire \n\n\n\n\nReject H0Sound the alarm! \nFalse alarm =\\alphaType I Error\nCorrectly identify the firePower = 1-\\beta\n\n\nFail to Reject H0Alarm stays quiet \nNo fire and no alarm(1-\\alpha)\nHouse burned =\\betaType II Error\n\n\n\n\nIn this table, the probabilities are meant to be read vertically:\n\\begin{align*}\n\\underbrace{Pr(\\text{alarm}\\mid \\text{No fire} )}_{\\alpha} +\n\\underbrace{Pr(\\text{no alarm}\\mid \\text{No fire})}_{1-\\alpha} &= 1 \\\\\n\\underbrace{Pr(\\text{alarm}\\mid \\text{Fire} )}_{1-\\beta} +\n\\underbrace{Pr(\\text{no alarm}\\mid \\text{Fire})}_{\\beta} &= 1\n\\end{align*}",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>statistical power</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/statistical_power.html#power-analysis",
    "href": "hypothesis_testing/statistical_power.html#power-analysis",
    "title": "5  statistical power",
    "section": "5.4 power analysis",
    "text": "5.4 power analysis\nPower analysis is a puzzle made of four pieces:\n\nstatistical significance: This is \\alpha, the probability of false alarms.\nstatistical power: This is 1-\\beta, the probability of correctly identifying a real effect.\neffect size: What is the difference between my two hypothesis (The null H_0 and the alternative H_1).\nsample size: Self explanatory.\n\nIf I know three of the four, I can figure out the one missing. In general, \\alpha and \\beta are determined by the experimenter, and we are left with two common cases:\n\nGiven an expected effect size, I want to figure out the minimum sample size that gives me a high probability of rejecting H_0 if the effect is real.\nGiven a fixed sample size (e.g. limited resources), what is the greatest effect size I can hope to identify?\n\nLet’s go back to the question at the very top of this chapter:\nAre 12-year old girls significantly taller than 12-year old boys?\nWe will perform the simplest power analysis we can, just to demonstrate the logic involved in the calculation. We will assume the following:\n\n\\alpha=0.05, this is the standard, “vanilla” value for statistical significance.\n1-\\beta=0.80, this is the standard, “vanilla” value for statistical power.\nWe assume that the standard deviation in the population is known for each sex, and that it is equal. Here we assume it to be \\sigma=7.39 cm.\nAssume that we will sample the same number of boys and girls, call it n.\nAssume that n is large enough, such that, according to the Central Limit Theorem, the means will be normally distributed.\nOur question asks whether girls are taller than boys, so we will use a one-sided test. If instead we were asking if girls are significantly different from boys then it would be a two-sided test.\n\nLet’s call \\bar h_\\text{girls} and \\bar h_\\text{boys} the sample mean heights of girls and boys. Then, the estimator for the difference in mean height is\n\n\\hat{\\Delta} = \\bar h_\\text{girls} - \\bar h_\\text{boys}\n\nBecause we assume that n is large enough, the means for girls and boys are normally distributed, centered around the true population means, and with variance \\sigma^2/n (see Central Limit Theorem). This means that their difference is also normally distributed:\n\n\\hat{\\Delta} \\sim \\mathcal{N}\\left( \\Delta, \\frac{2\\sigma^2}{n}\\right),\n\nwhere \\Delta is the true difference in mean height between girls and boys. The factor of 2 arises because the difference in sample means contains uncertainty from both groups; since the two sample means are independent, their variances add.\nFor convenience, we can standardize \\hat{\\Delta} to get a Z-score:\n\nZ = \\frac{\\hat{\\Delta}}{\\sigma\\sqrt{2/n}}\n\nThis Z-score can be understood as a signal-to-noise ratio: the numerator \\hat{\\Delta} is the signal we are trying to detect (the difference in mean heights), while the denominator \\sigma\\sqrt{2/n} is the noise (the standard error of the difference in means).\nUnder the Null Hypothesis H_0, there is no difference between the population mean of girls and boys (\\Delta=0), so\n\nZ_{H_0} \\sim \\mathcal{N}\\left(0, 1\\right).\n\nThe Alternative Hypothesis H_1 states that girls are on average taller than boys (\\Delta&gt;0), so\n\nZ_{H_1} \\sim \\mathcal{N}\\left(\\frac{\\Delta}{\\sigma}\\sqrt{\\frac{n}{2}}, 1\\right).\n\nIn this example, the effect size will be quantified by the Cohen’s d, given by d=\\Delta/\\sigma, which is the true difference in units of standard deviation. Therefore, we can write another expression for Z_{H_1} as\n\nZ_{H_1} \\sim \\mathcal{N}\\left(d\\sqrt{\\frac{n}{2}}, 1\\right).\n\nLet’s visualize all this in a widget.\n\nPanel (a) shows the distribution of \\hat{\\Delta} for two sampling quantities, and you can play with the sliders to choose any values you want. It’s easy to see how increasing the sample size makes the standard error of the mean smaller (\\sigma/\\sqrt{n}), which causes the distribution to become more concentrated.\nPanel (b) shows the differences after the standardization, that is, their Z-scores. The black curve shows the H0 distribution. As the sample number increases, the H1 distributions move to the right, meaning that it should be easier for us to see the effect we’re looking for.\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport altair as alt\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\nsigma = (sigma_boys + sigma_girls)/2\nDelta = (mu_girls - mu_boys)\ncohens_d = Delta / sigma\n\n\n\n\nwidget\n# --- Parameters ---\nseed = 2  # change this to any int (or set to None for non-reproducible draws)\ndelta = mu_girls - mu_boys\nnum_samples_dist = 5000\nbin_step = 0.2\n\n# colors\nc_n1 = \"#dbb40c\"\nc_n2 = \"#86b4a9\"\nc_null = \"#000000\"\n\n# --- RNG (seedable) ---\nrng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n\n# --- Independent random draws ---\ndata_dist = pd.DataFrame({\"sample_id\": np.arange(num_samples_dist)})\ndata_dist[\"z0\"] = rng.normal(0, 1, num_samples_dist)  # null\ndata_dist[\"z1\"] = rng.normal(0, 1, num_samples_dist)  # N1\ndata_dist[\"z2\"] = rng.normal(0, 1, num_samples_dist)  # N2\n\n# --- Interactive sliders ---\nn1_slider = alt.param(name=\"N1\", value=50,  bind=alt.binding_range(min=5, max=400, step=1, name=\"n1 \"))\nn2_slider = alt.param(name=\"N2\", value=400, bind=alt.binding_range(min=5, max=400, step=1, name=\"n2 \"))\n\n# ------------------------------------------------------------------\n# helper: step histogram from a single computed value column\n# if label is not None, it creates a \"lbl\" field for legend/color\n# ------------------------------------------------------------------\ndef step_hist(calc_expr, x_domain, y_domain, x_title, color=None, label=None, show_legend=False):\n    base = (\n        alt.Chart(data_dist)\n        .transform_calculate(value=calc_expr)\n        .transform_joinaggregate(total=\"count()\")\n        .transform_bin(\n            [\"bin_start\", \"bin_end\"],\n            field=\"value\",\n            bin=alt.Bin(extent=x_domain, step=bin_step),\n        )\n        .transform_aggregate(count=\"count()\", groupby=[\"bin_start\", \"bin_end\", \"total\"])\n        .transform_calculate(density=f\"datum.count / (datum.total * {bin_step})\")\n        .mark_line(interpolate=\"step\", strokeWidth=2.5)\n        .encode(\n            x=alt.X(\"bin_start:Q\", scale=alt.Scale(domain=x_domain), title=x_title),\n            y=alt.Y(\"density:Q\", stack=None, scale=alt.Scale(domain=y_domain), title=\"Density\"),\n        )\n        .properties(width=550, height=230)\n    )\n\n    if label is None:\n        return base.encode(color=alt.value(color))\n\n    leg = (\n        alt.Legend(\n            title=None,\n            orient=\"right\",\n            labelFontSize=14,\n            symbolSize=180,\n            symbolStrokeWidth=3,\n        )\n        if show_legend\n        else None\n    )\n\n    return (\n        base\n        .transform_calculate(lbl=f\"'{label}'\")\n        .encode(\n            color=alt.Color(\n                \"lbl:N\",\n                scale=alt.Scale(domain=[\"N1\", \"N2\"], range=[c_n1, c_n2]),\n                legend=leg,\n            )\n        )\n    )\n\n# ---------------- TOP PANEL (only N1 and N2) ----------------\ntop_n1 = step_hist(\n    calc_expr=f\"{delta} + datum.z1 * sqrt(2 * {sigma**2} / N1)\",\n    x_domain=[-5, 10],\n    y_domain=[0, 1.0],\n    x_title=\"Observed Difference of Means (Δ̂)\",\n    label=\"N1\",\n    show_legend=True,\n)\n\ntop_n2 = step_hist(\n    calc_expr=f\"{delta} + datum.z2 * sqrt(2 * {sigma**2} / N2)\",\n    x_domain=[-5, 10],\n    y_domain=[0, 1.0],\n    x_title=\"Observed Difference of Means (Δ̂)\",\n    label=\"N2\",\n    show_legend=True,\n)\n\ntop_row = alt.layer(top_n1, top_n2).properties(\n    title=\"(a): Distribution of Sample Mean Differences\"\n).resolve_scale(color=\"shared\")\n\n# ---------------- BOTTOM PANEL (null + alts, no legend) ----------------\nbottom_null = step_hist(\n    calc_expr=\"datum.z0\",\n    x_domain=[-3, 7],\n    y_domain=[0, 0.6],\n    x_title=\"Standardized Value (Z)\",\n    color=c_null,\n)\n\nbottom_n1 = step_hist(\n    calc_expr=f\"datum.z1 + ({delta} / ({sigma} * sqrt(2 / N1)))\",\n    x_domain=[-3, 7],\n    y_domain=[0, 0.6],\n    x_title=\"Standardized Value (Z)\",\n    color=c_n1,\n)\n\nbottom_n2 = step_hist(\n    calc_expr=f\"datum.z2 + ({delta} / ({sigma} * sqrt(2 / N2)))\",\n    x_domain=[-3, 7],\n    y_domain=[0, 0.6],\n    x_title=\"Standardized Value (Z)\",\n    color=c_n2,\n)\n\nnull_text = alt.Chart(pd.DataFrame({\"x\": [-1.0], \"y\": [0.4], \"label\": [\"Null\"]})).mark_text(\n    align=\"left\",\n    baseline=\"middle\",\n    fontSize=18,\n    color=\"black\",\n).encode(\n    x=\"x:Q\",\n    y=\"y:Q\",\n    text=\"label:N\"\n)\n\nbottom_row = alt.layer(bottom_null, bottom_n1, bottom_n2, null_text).properties(\n    title=\"(b): Distributions of Standardized Differences\"\n)\n\n# ---------------- FINAL ----------------\nchart = (\n    alt.vconcat(top_row, bottom_row)\n    .add_params(n1_slider, n2_slider)\n    .configure_view(stroke=None)\n    .configure_axis(\n        grid=True,\n        titleFontSize=18,\n        labelFontSize=14,\n        titlePadding=10,\n    )\n    .configure_title(\n        fontSize=20,\n        anchor=\"start\",\n        offset=10,\n    )\n)\n\nchart\n\n\n\n\n\n\n\n\nNote that in this widget, and in the ones below, we are not really drawing random samples each time we move the sliders. Your are reading this on a static website, and I don’t have a python kernel to execute these calculations. I build this widget to give a feel of how the distributions would look like had we been able to run the code. The numbers N1 and N2 represent the sizes of the two samples, and we simulate the statistics of the means of 5000 such samples to build the distributions.\nThe two colored curves behave in the exact same way, I just thought it would be instructive to see how they compare. In the widget below, we show only one curve, and how it changes as we control either the sample size or the real difference \\Delta in the population between girls and boys. I find it really useful to get intuiton about the equation for Z_{H_1}. See how it responds a lot less in changes of n because of the square root.\n\n\nwidget\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n\n# --- Parameters ---\nseed = 2  # change this to any int (or set to None for non-reproducible draws)\nnum_samples_dist = 5000\nbin_step = 0.2\n\n# colors\nc_alt  = \"#dbb40c\"\nc_null = \"#000000\"\n\n# --- RNG (seedable) ---\nrng = np.random.default_rng(seed) if seed is not None else np.random.default_rng()\n\n# --- Independent random draws (fixed once) ---\ndata_dist = pd.DataFrame({\"sample_id\": np.arange(num_samples_dist)})\ndata_dist[\"z0\"] = rng.normal(0, 1, num_samples_dist)  # null\ndata_dist[\"z1\"] = rng.normal(0, 1, num_samples_dist)  # alt\n\n# --- Interactive sliders ---\nn_slider = alt.param(\n    name=\"n\", value=50,\n    bind=alt.binding_range(min=5, max=400, step=1, name=\"n \")\n)\ndelta_slider = alt.param(\n    name=\"Δ\", value=2.0,\n    bind=alt.binding_range(min=0.0, max=5.0, step=0.1, name=\"Δ \")\n)\n\n# ------------------------------------------------------------------\n# helper: step histogram from a single computed value column\n# if label is not None, it creates a \"lbl\" field for legend/color\n# ------------------------------------------------------------------\ndef step_hist(calc_expr, x_domain, y_domain, x_title, color=None):\n    return (\n        alt.Chart(data_dist)\n        .transform_calculate(value=calc_expr)\n        .transform_joinaggregate(total=\"count()\")\n        .transform_bin(\n            [\"bin_start\", \"bin_end\"],\n            field=\"value\",\n            bin=alt.Bin(extent=x_domain, step=bin_step),\n        )\n        .transform_aggregate(count=\"count()\", groupby=[\"bin_start\", \"bin_end\", \"total\"])\n        .transform_calculate(density=f\"datum.count / (datum.total * {bin_step})\")\n        .mark_line(interpolate=\"step\", strokeWidth=2.5)\n        .encode(\n            x=alt.X(\"bin_start:Q\", scale=alt.Scale(domain=x_domain), title=x_title),\n            y=alt.Y(\"density:Q\", stack=None, scale=alt.Scale(domain=y_domain), title=\"Density\"),\n            color=alt.value(color),\n        )\n        .properties(width=550, height=230)\n    )\n\n# ---------------- TOP PANEL (single curve: Δ-hat under H1) ----------------\n# uses delta_slider (Δ) and n_slider (n)\ntop_alt = step_hist(\n    calc_expr=f\"Δ + datum.z1 * sqrt(2 * {sigma**2} / n)\",\n    x_domain=[-5, 10],\n    y_domain=[0, 1.0],\n    x_title=\"Observed Difference of Means (Δ̂)\",\n    color=c_alt,\n)\n\ntop_row = top_alt.properties(title=\"(a): Distribution of Sample Mean Difference (H₁)\")\n\n# ---------------- BOTTOM PANEL (null + standardized H1) ----------------\nbottom_null = step_hist(\n    calc_expr=\"datum.z0\",\n    x_domain=[-3, 7],\n    y_domain=[0, 0.6],\n    x_title=\"Standardized Value (Z)\",\n    color=c_null,\n)\n\nbottom_alt = step_hist(\n    calc_expr=f\"datum.z1 + (Δ / ({sigma} * sqrt(2 / n)))\",\n    x_domain=[-3, 7],\n    y_domain=[0, 0.6],\n    x_title=\"Standardized Value (Z)\",\n    color=c_alt,\n)\n\nnull_text = alt.Chart(pd.DataFrame({\"x\": [-1.0], \"y\": [0.4], \"label\": [\"Null\"]})).mark_text(\n    align=\"left\",\n    baseline=\"middle\",\n    fontSize=18,\n    color=\"black\",\n).encode(\n    x=\"x:Q\",\n    y=\"y:Q\",\n    text=\"label:N\"\n)\n\nbottom_row = alt.layer(bottom_null, bottom_alt, null_text).properties(\n    title=\"(b): Standardized Distributions (H₀ vs H₁)\"\n)\n\n# ---------------- FINAL ----------------\nchart = (\n    alt.vconcat(top_row, bottom_row)\n    .add_params(n_slider, delta_slider)\n    .configure_view(stroke=None)\n    .configure_axis(\n        grid=True,\n        titleFontSize=18,\n        labelFontSize=14,\n        titlePadding=10,\n    )\n    .configure_title(\n        fontSize=20,\n        anchor=\"start\",\n        offset=10,\n    )\n)\n\nchart\n\n\n\n\n\n\n\n\nThe statistical significance \\alpha determines where we put the border between rejecting H0 or not. In this example, \\alpha=0.05, and because we are dealing with a one-sided test, this border is located at the value of Z that leaves 0.05 of the area below H0 to its right. This is the same as leaving 1-\\alpha=0.95 of the area to its left, so a common name in the literature is z_{1-\\alpha}. The cumulative distribution function (CDF) is the integral of the probability density function (PDF) from -\\infty to z, so it quantifies the area to the left of z. We need the value z for which the CDF is equal to 1-\\alpha. Luckily, we can use python’s scipy.stats.norm.ppf function to get this value:\n\nzcrit = norm.ppf(0.95, loc=0, scale=1)\nprint(f\"zcrit (for α=0.05) = {zcrit:.4f}\")\n\nzcrit (for α=0.05) = 1.6449\n\n\nThe probabilities \\alpha and \\beta are shown as shaded areas in the widget below. See what happens when you control the slider for n and \\Delta.\n\n\nwidget\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n\n# --- constants ---\nalpha = 0.05\nzcrit = 1.6448536269514722  # one-sided z_{1-alpha}\nsigma = 7.39\n\n# x-grid\nx = np.linspace(-4.0, 8.0, 1601)\ndf = pd.DataFrame({\"x\": x})\n\n# sliders\nn_slider = alt.param(\n    name=\"n\", value=50,\n    bind=alt.binding_range(min=5, max=400, step=1, name=\"n \")\n)\ndelta_slider = alt.param(\n    name=\"Delta\", value=2.0,\n    bind=alt.binding_range(min=0.0, max=5.0, step=0.1, name=\"Δ \")\n)\n\nbase = (\n    alt.Chart(df)\n    .add_params(n_slider, delta_slider)\n    .transform_calculate(\n        mu1=f\"(Delta / {sigma}) * sqrt(n / 2)\"\n    )\n    .transform_calculate(\n        pdf0=\"(1 / sqrt(2 * PI)) * exp(-0.5 * datum.x * datum.x)\",\n        pdf1=\"(1 / sqrt(2 * PI)) * exp(-0.5 * pow(datum.x - datum.mu1, 2))\"\n    )\n)\n\n# ---------------- curves ----------------\nh0_line = base.mark_line(strokeWidth=2.5, color=\"#000000\").encode(\n    x=alt.X(\"x:Q\", title=\"z\"),\n    y=alt.Y(\"pdf0:Q\", title=\"Density\"),\n)\n\nh1_line = base.mark_line(strokeWidth=2.5, color=\"#dbb40c\").encode(\n    x=\"x:Q\",\n    y=\"pdf1:Q\",\n)\n\n# ---------------- shaded regions ----------------\n# alpha: H0 right tail\nh0_shade = (\n    base\n    .transform_filter(f\"datum.x &gt;= {zcrit}\")\n    .mark_area(opacity=0.20, color=\"#808080\")\n    .encode(\n        x=\"x:Q\",\n        y=\"pdf0:Q\",\n    )\n)\n\n# beta: H1 left of cutoff\nh1_shade = (\n    base\n    .transform_filter(f\"datum.x &lt;= {zcrit}\")\n    .mark_area(opacity=0.20, color=\"#dbb40c\")\n    .encode(\n        x=\"x:Q\",\n        y=\"pdf1:Q\",\n    )\n)\n\n# ---------------- decision boundary ----------------\ncrit_line = (\n    alt.Chart(pd.DataFrame({\"z\": [zcrit]}))\n    .mark_rule(strokeWidth=2, strokeDash=[6, 6], color=\"#808080\")\n    .encode(x=\"z:Q\")\n)\n\ncrit_label = alt.Chart(\n    pd.DataFrame({\"x\": [zcrit], \"y\": [0.39], \"t\": [f\"z critical (α={alpha})\"]})\n).mark_text(\n    align=\"left\", baseline=\"top\", dx=6, fontSize=14, color=\"#808080\"\n).encode(\n    x=\"x:Q\",\n    y=\"y:Q\",\n    text=\"t:N\"\n)\n\n# ---------------- power label ----------------\npower_text_1 = (\n    alt.Chart(pd.DataFrame({\"x\": [5.0], \"y\": [0.38]}))\n    .mark_text(\n        align=\"left\",\n        baseline=\"middle\",\n        fontSize=16,\n        color=\"#dbb40c\",\n    )\n    .encode(\n        x=\"x:Q\",\n        y=\"y:Q\",\n        text=alt.value(\"Statistical Power\"),\n    )\n)\n\npower_text_2 = (\n    alt.Chart(pd.DataFrame({\"x\": [5.0], \"y\": [0.35]}))\n    .transform_calculate(\n        t=f\"1.702 * ({zcrit} - ((Delta / {sigma}) * sqrt(n / 2)))\",\n        t_clamped=\"min(max(datum.t, -60), 60)\",\n        power=\"1 / (1 + exp(datum.t_clamped))\",\n        label=\"'1 − β = ' + format(datum.power, '.3f')\"\n    )\n    .mark_text(\n        align=\"left\",\n        baseline=\"middle\",\n        fontSize=16,\n        color=\"#dbb40c\",\n    )\n    .encode(\n        x=\"x:Q\",\n        y=\"y:Q\",\n        text=\"label:N\",\n    )\n)\n\n\n# ---------------- legend for shaded areas ----------------\nlegend_df = pd.DataFrame({\n    \"region\": [\"α (False Alarms)\", \"β (Misses)\"]\n})\n\nshade_legend = (\n    alt.Chart(legend_df)\n    .mark_point(size=120, filled=True, opacity=0)  # hide the dummy points\n    .encode(\n        color=alt.Color(\n            \"region:N\",\n            scale=alt.Scale(\n                domain=[\"α (False Alarms)\", \"β (Misses)\"],\n                range=[\"#808080\", \"#dbb40c\"],\n            ),\n            legend=alt.Legend(\n                title=\"Shaded regions\",\n                orient=\"top-left\",\n                labelFontSize=14,\n                titleFontSize=14,\n                symbolSize=200,\n                symbolOpacity=1,          # keep legend symbols visible\n            ),\n        )\n    )\n)\n\n\n# ---------------- final chart ----------------\nchart = (\n    alt.layer(\n        h0_shade,\n        h1_shade,\n        h0_line,\n        h1_line,\n        crit_line,\n        crit_label,\n        power_text_1,\n        power_text_2,\n        shade_legend,\n    )\n    .properties(width=600, height=360, title=\"H0 vs H1 in Z-space (α = 0.05)\")\n    .configure_view(stroke=None)\n    .configure_axis(titleFontSize=18, labelFontSize=14, grid=True)\n    .configure_title(fontSize=20, anchor=\"start\", offset=10)\n)\n\nchart\n\n\n\n\n\n\n\n\nSo how can we compute the statistical power? If \\beta is the shaded area in yellow in the widget above, then the remaining area to the right is the statistical power, 1-\\beta. Remember that we already figured out how H_1 is distributed:\n\nZ_{H_1} \\sim \\mathcal{N}\\left(\\frac{\\Delta}{\\sigma}\\sqrt{\\frac{n}{2}}, 1\\right).\n\nTherefore, the statistical power is given by\n\n\\text{Power} = 1 - \\beta = \\int_{z_{1-\\alpha}}^{\\infty} f_{Z_{H_1}}(z) \\, dz.\n\nWe can combine the information in the two previous equations to get an explicit expression for the statistical power: \n\\text{Power} = 1 - \\beta = \\int_{z_{1-\\alpha}}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(z - d\\sqrt{n/2})^2}{2}\\right) \\, dz.\n\nWe started this chapter by saying that power analysis is a puzzle made of four pieces: statistical significance, \\alpha; statistical power, 1-\\beta; effect size, d; and sample size, n. By fixing any three of these quantities, we can solve for the fourth. When deriving these equations, we made several assumptions. A big one was that the sample size is large enough for the Central Limit Theorem to hold, and because of that we could use the normal distribution. A more rigorous treatment, valid for smaller sample sizes, would use the t-distribution instead of the normal distribution, but the fundamental logic would be the same. We will use python’s statsmodels package to perform power analysis and solve two common questions that arise when desigining an experiment.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>statistical power</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/statistical_power.html#in-action",
    "href": "hypothesis_testing/statistical_power.html#in-action",
    "title": "5  statistical power",
    "section": "5.5 in action",
    "text": "5.5 in action\n\n5.5.1 how many samples do I need?\nHow many boys and girls do I need to sample to have an 80% chance of detecting a real difference in mean height? We assume that:\n\nThe expected difference in mean height is not so large, \\Delta=\\mu_{\\text{girls}} - \\mu_{\\text{boys}}=2.05 cm.\nThe standard deviation in both populations is quite similar, about \\sigma=7.39 cm.\nWe want to use a significance level of \\alpha=0.05.\nWe want to have a statistical power of 1-\\beta=0.8.\n\n\nimport statsmodels.stats.power as smp\npower_analysis = smp.TTestIndPower()\ncohens_d = Delta / sigma\nsample_size = power_analysis.solve_power(effect_size=cohens_d, power=0.8, alpha=0.05)\nprint(f\"sample size: {sample_size:.0f}\")\n\nsample size: 206\n\n\nWe can easily plot power curves for various effect sizes using the plot_power method of statsmodels.stats.power.TTestIndPower.\n\n\npower curve\nnormalized_effect_sizes = np.array([0.2, 0.5, 0.8])\nsample_sizes = np.array(range(3, 500, 1))\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nfig = power_analysis.plot_power(\n    dep_var='nobs', nobs=sample_sizes,  \n    effect_size=normalized_effect_sizes, alpha=0.05, ax=ax, alternative=\"larger\",\n    title='Power of Independent Samples t-test\\n$\\\\alpha = 0.05$')\nax.axhline(0.8, color='gray', linestyle='--', label='80% Power')\nax.legend(['d = 0.2', 'd = 0.5', 'd = 0.8', '80% Power'])\nax.set(ylabel='Power', xlabel='Sample Size per Group');\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 what is the minimum effect size I can detect?\nAssume that I have limited resources, and I can only sample 50 boys and 50 girls. From the result above, I could not detect a difference of 2.05 cm with high probability. What difference in mean height can I hope to detect with high probability? We assume that:\n\nThe standard deviation in both populations is quite similar, about \\sigma=7.39 cm.\nWe want to use a significance level of \\alpha=0.05.\nWe want to have a statistical power of 1-\\beta=0.8.\nThe sample size is fixed to n=50.\n\n\nimport statsmodels.stats.power as smp\npower_analysis = smp.TTestIndPower()\nsample_size = 50\neffect_size = power_analysis.solve_power(nobs1=sample_size, power=0.8, alpha=0.05)\nprint(f\"minimum detectable effect size: Cohen's d = {effect_size:.2f}\")\nprint(f\"minimum detectable effect size: Delta = {effect_size * sigma:.2f} cm\")\n\nminimum detectable effect size: Cohen's d = 0.57\nminimum detectable effect size: Delta = 4.18 cm\n\n\n\n\npower curve\neffect_sizes = np.arange(0.01, 1.01, 0.01)\nsample_sizes = np.array([20, 50, 250])\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nfig = power_analysis.plot_power(\n    dep_var='effect_size', nobs=sample_sizes,  \n    effect_size=effect_sizes, alpha=0.05, ax=ax, alternative=\"larger\",\n    title='Power of Independent Samples t-test\\n$\\\\alpha = 0.05$')\n\nax.axhline(0.8, color='gray', linestyle='--', label='80% Power')\nax.legend(['n = 20', 'n = 50', 'n = 250', '80% Power'])\nax.set(ylabel='Power', xlabel='Effect Size (d)');",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>statistical power</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/problem-with-t-test.html",
    "href": "hypothesis_testing/problem-with-t-test.html",
    "title": "6  the problem with t-test",
    "section": "",
    "text": "6.1 the normality assumption\nLet’s go back to the example of the independent samples t-test.\nWe sampled 10 boys and 14 girls, age 12, and asked:\nWe then went about answering this question by talking about the means of each sample, and if the differences between the means were large enough to be considered significant.\nThe whole machinery behind the t-test is based on the normality assumption.\nTwo possible interpretations come to mind.\nIn the context of the t-test, the above is a distinction without a difference. Even if the population is not normally distributed, the means of the samples will be normally distributed as long as the sample size is large enough. We then use the t-test and go on with our lives.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/problem-with-t-test.html#the-normality-assumption",
    "href": "hypothesis_testing/problem-with-t-test.html#the-normality-assumption",
    "title": "6  the problem with t-test",
    "section": "",
    "text": "The assumption is that the height of men and women in the population is normally distributed. From these idealized populations we draw samples.\nThe t-test effectively compares the difference between the means of the two samples, and the variability within each sample. Because of the Central Limit Theorem, the means of the samples will approach a normal distribution as the sample size increases. In this interpretation, the normality assumption is about the distribution of the means of the samples, and not the distribution of the population.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/problem-with-t-test.html#other-statistical-tests",
    "href": "hypothesis_testing/problem-with-t-test.html#other-statistical-tests",
    "title": "6  the problem with t-test",
    "section": "6.2 other statistical tests",
    "text": "6.2 other statistical tests\nThe Central Limit Theorem dictates that the means will be normally distributed, but it does not apply to other statistics, such as:\n\nthe median\nthe variance\nthe skewness\nthe maximum\nthe Interquartile Range (IQR)\netc.\n\nIn this case, the t-test can’t be relied upon, and we need another solution.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/permutation.html",
    "href": "hypothesis_testing/permutation.html",
    "title": "7  permutation test",
    "section": "",
    "text": "7.1 hypotheses\nWe wish to compare two samples, and see if they significantly differ regarding some statistic of interest (median, mean, etc.). To make things concrete, let’s talk about the heights of 12-year-old boys and girls. Are girls significantly taller than boys?\nThe basic idea behind the permutation test is that, if the null hypothesis is correct, then it wouldn’t matter if we relabelled the samples. If we randomly permute the labels “girls” and “boys” of the two samples, the statistic of interest should not change significantly. However, if by permuting the labels we get a significantly different statistic, then we can reject the null hypothesis.\nThat’s beautiful, right?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/permutation.html#hypotheses",
    "href": "hypothesis_testing/permutation.html#hypotheses",
    "title": "7  permutation test",
    "section": "",
    "text": "Null hypothesis (H0): The two samples come from the same distribution.\nAlternative hypothesis (H1): Girls are taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/permutation.html#steps",
    "href": "hypothesis_testing/permutation.html#steps",
    "title": "7  permutation test",
    "section": "7.2 steps",
    "text": "7.2 steps\n\nCompute the statistic of interest (e.g., the difference in medians) for the original samples.\nRandomly permute the labels of the two samples.\nCompute the statistic of interest for the permuted samples.\nRepeat steps 2 and 3 many times (e.g., 1000 times) to create a distribution of the statistic under the null hypothesis.\nCompare the original statistic to the distribution of permuted statistics to see if it is significantly different (e.g., by checking if it falls in the top 5% of the distribution). From this, we can numerically compute a p-value.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/permutation.html#example",
    "href": "hypothesis_testing/permutation.html#example",
    "title": "7  permutation test",
    "section": "7.3 example",
    "text": "7.3 example\nLet’s use the very same example as in the independent samples t-test.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\n\n\n\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nplot population and sample data\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    );\n\n\n\n\n\n\n\n\n\nThe statistic of interest now is the difference in medians between the two samples.\n\n\ncompute the observed difference in medians\n# define the desired statistic.\n# in can be anything you want, you can even write your own function.\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\nprint(f\"median height for girls: {median_girls:.2f} cm\")\nprint(f\"median height for boys: {median_boys:.2f} cm\")\nprint(f\"median difference (girls minus boys): {observed_diff:.2f} cm\")\n\n\nmedian height for girls: 150.88 cm\nmedian height for boys: 151.19 cm\nmedian difference (girls minus boys): -0.31 cm\n\n\n\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\nNow let’s see the empirical cdf of the permuted statistics, and where the original statistic falls in that distribution.\n\n\nplot empirical distribution of differences\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# compute the empirical CDF\nrank = np.arange(len(diffs)) + 1\ncdf = rank / (len(diffs) + 1)\nsorted_diffs = np.sort(diffs)\n\nax.plot(sorted_diffs, cdf, lw=2, color='tab:blue', label='empirical CDF')\nax.axvline(observed_diff, color='tab:orange', lw=2, ls='--',\n           label=f'obs. median diff. = {observed_diff:.2f} cm')\nax.text(observed_diff + 0.5, 0.3, f'observed difference in\\nmedian height: {observed_diff:.2f} cm',\n        color='tab:orange', fontsize=14, ha='left', va='bottom')\n\nalpha = 0.05\n# for a one-tailed test\nax.axhline(1-alpha, color='k', lw=1, ls='--')\nax.annotate(r\"$1-\\alpha$\", xy=(1.01, 1-alpha), xycoords=('axes fraction', 'data'),\n            ha=\"left\", va=\"center\")\n# for a two-tailed test\n# ax.axhline(alpha/2, color='k', lw=1, ls='--')\n# ax.axhline(1-alpha/2, color='k', lw=1, ls='--')\n# ax.annotate(r\"$\\alpha/2$\", xy=(1.01, alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n# ax.annotate(r\"$1-\\alpha/2$\", xy=(1.01, 1-alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\nax.set(xlabel='difference in median height (cm)',\n         ylabel='empirical CDF',\n         title=f'empirical distribution of differences in median height');\n\n\n\n\n\n\n\n\n\nThe observed statistic is well within the boundaries set by the significance level of 5%. Therefore, we cannot reject the null hypothesis. We conclude that, based on this data, the most probable interpretation is that girls and boys have the same underlying distribution.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/permutation.html#increase-sample-size",
    "href": "hypothesis_testing/permutation.html#increase-sample-size",
    "title": "7  permutation test",
    "section": "7.4 increase sample size",
    "text": "7.4 increase sample size\nLet’s increase the sample size to 200 girls and 300 boys.\n\n\nthe whole process in one cell\n# take samples\nN_boys = 300\nN_girls = 200\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n# compute the observed difference in medians\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\n\n# permutation algorithm\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 200 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\n\n\n\nplot empirical distribution of differences\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# compute the empirical CDF\nrank = np.arange(len(diffs)) + 1\ncdf = rank / (len(diffs) + 1)\nsorted_diffs = np.sort(diffs)\n\nax.plot(sorted_diffs, cdf, lw=2, color='tab:blue', label='empirical CDF')\nax.axvline(observed_diff, color='tab:orange', lw=2, ls='--',\n           label=f'obs. median diff. = {observed_diff:.2f} cm')\nax.text(observed_diff - 0.05, 0.3, f'observed difference in\\nmedian height: {observed_diff:.2f} cm',\n        color='tab:orange', fontsize=14, ha='right', va='bottom')\n\nalpha = 0.05\n# for a one-tailed test\nax.axhline(1-alpha, color='k', lw=1, ls='--')\nax.annotate(r\"$1-\\alpha$\", xy=(1.01, 1-alpha), xycoords=('axes fraction', 'data'),\n            ha=\"left\", va=\"center\")\n# for a two-tailed test\n# ax.axhline(alpha/2, color='k', lw=1, ls='--')\n# ax.axhline(1-alpha/2, color='k', lw=1, ls='--')\n# ax.annotate(r\"$\\alpha/2$\", xy=(1.01, alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n# ax.annotate(r\"$1-\\alpha/2$\", xy=(1.01, 1-alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n\nax.set(xlabel='difference in median height (cm)',\n         ylabel='empirical CDF',\n         title=f'empirical distribution of differences in median height');\n\n\n\n\n\n\n\n\n\nNow the observed statistic is well outside the right boundary set by the significance level of 5%. Therefore, we can reject the null hypothesis. We conclude that, based on this data, girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/permutation.html#p-value",
    "href": "hypothesis_testing/permutation.html#p-value",
    "title": "7  permutation test",
    "section": "7.5 p-value",
    "text": "7.5 p-value\nIt is quite easy to compute the p-value from the permutation test. It is simply the fraction of permuted statistics that are more extreme than the observed statistic. In this case, since we are testing whether girls are taller than boys, we have a one-tailed test, and we only consider the right tail of the distribution. If we were testing whether girls are significantly different from boys in their height, we would have a two-tailed test, and we would consider both tails of the distribution.\n\n# one-tailed p-value\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\nobserved difference: 2.004\np-value (one-tailed): 0.0050\n\n\nWe can now address the fact that we ran only 999 permutations, although I intended to run 1000. See in the code that after the permutation algorithm, I inserted the original statistic in the list of permuted statistics. This is because I want to compute the p-value as the fraction of permuted statistics that are more extreme than the original statistic, and I want to include the original statistic in the distribution. If I had not done this, for a truly extreme observed statistic, we would get that the p-value equals 0, that is, the fraction of permuted statistics that are more extreme than the observed statistic is zero. To avoid this behavior, we include the original statistic in the distribution of permuted statistics.\nA corollary of this is that the smallest p-value we can get is 0.001 (for our example with 1000 permutations).",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/numpy-vs-pandas.html",
    "href": "hypothesis_testing/numpy-vs-pandas.html",
    "title": "8  numpy vs pandas",
    "section": "",
    "text": "8.1 numpy\nIn the previous chapter, we computed the permutation test using numpy. We had two samples of different sizes, and before the permutation test we concatenated the two samples into one array. Then we shuffled the concatenated array and split it back into two samples, according to the original sizes. See a sketch of the code below:\nStore the two samples in numpy arrays:\nDefine the statistic and compute the observed difference:\nRun the permutation test:\nAll this works great if this is how your data looks like. Sometimes, however, you have structured data with more information, such as a DataFrame with multiple columns. In this case, you can leverage the capabilities of pandas.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/numpy-vs-pandas.html#numpy",
    "href": "hypothesis_testing/numpy-vs-pandas.html#numpy",
    "title": "8  numpy vs pandas",
    "section": "",
    "text": "boys = np.array([121, 123, 124, 125])\ngirls = np.array([120, 121, 121, 122, 123, 123, 128, 129])\nN_boys = len(boys)\nN_girls = len(girls)\n\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\n\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations - 1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first N_girls values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/numpy-vs-pandas.html#pandas",
    "href": "hypothesis_testing/numpy-vs-pandas.html#pandas",
    "title": "8  numpy vs pandas",
    "section": "8.2 pandas",
    "text": "8.2 pandas\nLet’s give an example of structured data. Suppose we have a DataFrame with the following columns: sex, height, and weight.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\nN_total = 20\nnp.random.seed(3)\nheight_list = norm.rvs(size=N_total, loc=150, scale=7)\nweight_list = norm.rvs(size=N_total, loc=42, scale=5)\nsex_list = np.random.choice(['M', 'F'], size=N_total, replace=True)\ndf = pd.DataFrame({\n    'sex': sex_list,\n    'height (cm)': height_list,\n    'weight (kg)': weight_list\n})\ndf\n\n\n\n\n\n\n\n\nsex\nheight (cm)\nweight (kg)\n\n\n\n\n0\nM\n162.520399\n36.074767\n\n\n1\nM\n153.055569\n40.971751\n\n\n2\nM\n150.675482\n49.430742\n\n\n3\nF\n136.955551\n43.183581\n\n\n4\nF\n148.058283\n36.881074\n\n\n5\nM\n147.516687\n38.435034\n\n\n6\nF\n149.420810\n45.126225\n\n\n7\nF\n145.610995\n41.197433\n\n\n8\nF\n149.693273\n38.155818\n\n\n9\nM\n146.659474\n40.849846\n\n\n10\nM\n140.802947\n45.725281\n\n\n11\nF\n156.192357\n51.880554\n\n\n12\nF\n156.169226\n35.779383\n\n\n13\nM\n161.967011\n38.867915\n\n\n14\nF\n150.350235\n37.981170\n\n\n15\nM\n147.167258\n29.904584\n\n\n16\nM\n146.182480\n37.381040\n\n\n17\nM\n139.174659\n36.880621\n\n\n18\nM\n156.876572\n47.619890\n\n\n19\nM\n142.292527\n41.340429\n\n\n\n\n\n\n\nCalculate sample statistics using groupby:\n\nsample_stats = df.groupby('sex')['height (cm)'].median()\nobserved_diff = sample_stats['F'] - sample_stats['M']\n\nWe can now leverage the pandas.DataFrame.sample method to sample from the DataFrame. Here, we use the following options:\n\nfrac=1 means we want to sample 100% of rows, but shuffled.\nreplace=False means we want to sample without replacement, that is, no duplicate rows.\n\nWe will shuffle the sex column and store the result in a new column called sex_shuffled. Then we can use groupby to compute the median.\n\nN_permutations = 1000\ndiffs = np.empty(N_permutations - 1)\nfor i in range(N_permutations - 1):\n    # shuffle dataframe 'sex' colunn, store it in 'sex_shuffled'\n    df['sex_shuffled'] = df['sex'].sample(frac=1, replace=False).reset_index(drop=True)\n    shuffled_stats = df.groupby('sex_shuffled')['height (cm)'].median()\n    diffs[i] = shuffled_stats['F'] - shuffled_stats['M']  # median(F) - median(M)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/exact-vs-montecarlo.html",
    "href": "hypothesis_testing/exact-vs-montecarlo.html",
    "title": "9  exact vs. Monte Carlo permutation tests",
    "section": "",
    "text": "9.1 Monte Carlo permutation tests\nThe permutation tests from before do not sample from the full distribution of the test statistic under the null hypothesis. This would be imppractical if the total number of permutations is large, as it would require computing the test statistic for every possible permutation of the data.\nFor example, if we have 10 boys and 14 girls, the total number of permutations is almost two million:\n\\binom{24}{14} = \\frac{24!}{14!\\cdot(24-14)!} = 1961256\nThe expression above is the binomial coefficient, which counts the number of ways to choose 14 samples from a total of 24, without regard to the order of selection. This is why we say “24 choose 14” to refer to the parenthesis above.\nThere is no preference in “24 choose 14” over “24 choose 10”, as both expressions yield the same result. You can verify this on your own.\nMonte Carlo methods are a class of computational algorithms that rely on repeated random sampling to obtain numerical results. In the context of permutation tests, Monte Carlo methods do not compute the test statistic for every possible permutation of the data. In the examples from before, we computed 1000 permutations only, and from that we estimated the p-value of the test statistic. If we had run the test more than once, we would have obtained a different p-value each time, as the test statistic is computed from a random sample of permutations.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\ncompute the observed difference in medians\n# define the desired statistic.\n# in can be anything you want, you can even write your own function.\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\nMonte Carlo permutation test 1\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(\"Monte Carlo permutation test 1\")\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\n\nMonte Carlo permutation test 1\nobserved difference: -0.314\np-value (one-tailed): 0.5450\nMonte Carlo permutation test 2\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(\"Monte Carlo permutation test 2\")\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\n\nMonte Carlo permutation test 2\nobserved difference: -0.314\np-value (one-tailed): 0.5340\nAs you can see, the p-value in not exactly the same, but the difference is negligible. This is because both times we sampled 1000 permutations that are representative of the full distribution of the test statistic under the null hypothesis.\nOne more thing. The example above with 10 boys and 14 girls is usually considered small. It is often the case that one has a lot more samples, and the number of permutations can be astronomically large, much much larger than two million.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>exact vs. Monte Carlo permutation tests</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing/exact-vs-montecarlo.html#exact-permutation-test",
    "href": "hypothesis_testing/exact-vs-montecarlo.html#exact-permutation-test",
    "title": "9  exact vs. Monte Carlo permutation tests",
    "section": "9.2 exact permutation test",
    "text": "9.2 exact permutation test\nIf the total number of permutations is small, we can compute the exact p-value by sampling from the full distribution of the test statistic under the null hypothesis. That is to say, we compute the test statistic for every possible permutation of the data.\nIf we had height measurements of 7 boys and 6 girls, the total number of permutations is:\n\n\\binom{13}{7} = 1716\n\nAny computer can easily handle this number of permutations. How to do it in practice? We will use the itertools.combinations function.\n\n\nShow the code\nimport numpy as np\nfrom itertools import combinations\n\n#| code-summary: \"generate data\"\nN_girls = 6\nN_boys = 7\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\n\ncombined = np.concatenate([sample_girls, sample_boys])\nn_total = len(combined)\n\n# observed difference in means\nobserved_diff = np.median(sample_girls) - np.median(sample_boys)\n\n# generate all combinations of indices for group \"girls\"\nindices = np.arange(n_total)\nall_combos = list(combinations(indices, N_girls))\n\n# compute all permutations\ndiffs = []\nfor idx_a in all_combos:\n    mask = np.zeros(n_total, dtype=bool)\n    mask[list(idx_a)] = True\n    sample_g = combined[mask]\n    sample_b = combined[~mask]\n    diffs.append(np.median(sample_g) - np.median(sample_b))\n\ndiffs = np.array(diffs)\n\n# exact one-tailed p-value\np_value = np.mean(diffs &gt;= observed_diff)\nprint(f\"Observed difference: {observed_diff:.3f} cm\")\nprint(f\"Exact p-value (one-tailed): {p_value:.4f}\")\nprint(f\"Total permutations: {len(diffs)}\")\n\n\nObserved difference: 7.620 cm\nExact p-value (one-tailed): 0.0944\nTotal permutations: 1716\n\n\nAttention!\nIf you read the documentation of the itertools library, you might be tempted to use itertools.permutations instead of itertools.combinations.\nDon’t do that.\nAlthough we are conductiong a permutation test, we are not interested in the order of the samples, and that is what the permutations cares about. For instance, if we have 10 people called\n[Alice, Bob, Charlie, David, Eve, Frank, Grace, Heidi, Ivan, Judy]\nand we want to randomly assign the label “girl” to 4 of them, we do not care about the order in which we assign the labels. We just want to know which 4 people are assigned the label “girl”. The permutation function does care about the order, and that is why we should not use it. Instead, we use the combinations function, which return all possible combinations of the data, without regard to the order of selection.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>exact vs. Monte Carlo permutation tests</span>"
    ]
  },
  {
    "objectID": "confidence_interval/basic_concepts.html",
    "href": "confidence_interval/basic_concepts.html",
    "title": "10  basic concepts",
    "section": "",
    "text": "Suppose we randomly select 30 seven-year-old boys from schools around the country and measure their heights (this is our sample). We’d like to use their average height to estimate the true average height of all seven-year-old boys nationwide (the population). Because different samples of 30 boys would yield slightly different averages, we need a way to quantify that uncertainty. A confidence interval gives us a range—based on our sample data—that expresses what we would expect to find if we were to repeat this sampling process many times.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\nimport scipy\nfrom matplotlib.lines import Line2D\nimport matplotlib.gridspec as gridspec\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nage = 7.0\nmu_boys = df_boys.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\n\n\nSee the height distribution for seven-year-old boys. Below it we see the means for 20 samples of groups of 30 boys. The 95% confidence interval is the range of values that, on average, 95% of the samples CI contain the true population mean. In this case, this amounts to one out of the 20 samples.\n\n\nplot samples against population pdf\nnp.random.seed(628)\nheight_list = np.arange(90, 150, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig = plt.figure(figsize=(8, 6))\ngs = gridspec.GridSpec(2, 1, height_ratios=[0.1, 0.9])\ngs.update(left=0.09, right=0.86,top=0.98, bottom=0.06, hspace=0.30, wspace=0.05)\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[1, 0])\n\nax0.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nN_samples = 20\nN = 30\n\nfor i in range(N_samples):\n    sample = norm.rvs(loc=mu_boys, scale=sigma_boys, size=N)\n    sample_mean = sample.mean()\n    # confidence interval\n    alpha = 0.05\n    z_crit = scipy.stats.t.isf(alpha/2, N-1)\n    CI = z_crit * sample.std(ddof=1) / np.sqrt(N)\n    ax1.errorbar(sample_mean, i, xerr=CI, fmt='o', color='tab:blue', \n                 label=f'sample {i+1}' if i == 0 else \"\", capsize=0)\n\n\nfrom matplotlib.patches import ConnectionPatch\nline = ConnectionPatch(xyA=(mu_boys, pdf_boys.max()), xyB=(mu_boys, -1), coordsA=\"data\", coordsB=\"data\",\n                      axesA=ax0, axesB=ax1, color=\"gray\", linestyle='--', linewidth=1.5, alpha=0.5)\nax1.add_artist(line)\n\nax1.annotate(\n        '',\n        xy=(mu_boys + 5, 13),  # tip of the arrow (first error bar, y=0)\n        xytext=(mu_boys + 5 + 13, 13),  # text location\n        arrowprops=dict(arrowstyle='-&gt;', lw=2, color='black'),\n        fontsize=13,\n        color='tab:blue',\n        ha='left',\n        va='center'\n)\n\nax1.text(mu_boys + 5 + 2, 12, \"on average, the CI\\nof 1 out of 20 samples\\n\"\n         r\"($\\alpha=5$% significance level)\"\n          \"\\nwill not contain\\nthe population mean\",\n          va=\"top\", fontsize=12)\n\n# write \"sample i\" for each error bar\nfor i in range(N_samples):\n    ax1.text(mu_boys -10, i, f'sample {N_samples-i:02d}', \n             fontsize=13, color='black',\n             ha='right', va='center')\n\n# ax.legend(frameon=False)\nax0.spines['top'].set_visible(False)\nax0.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\n\nax0.set(xticks=np.arange(90, 151, 10),\n        xlim=(90, 150),\n        xlabel='height (cm)',\n        # xticklabels=[],\n        yticks=[],\n        ylabel='pdf',\n        )\nax1.set(xticks=[],\n        xlim=(90, 150),\n        ylim=(-1, N_samples),\n        yticks=[],\n       );",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>basic concepts</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html",
    "href": "confidence_interval/analytical_confidence_interval.html",
    "title": "11  analytical confidence interval",
    "section": "",
    "text": "11.1 CLT\nWe wish to compute the confidence interval for the mean height of 7-year-old boys, for a sample of size N.\nWe will start our journey with a refresher of the Central Limit Theorem (CLT).\nThe Central Limit Theorem states that the sampling distribution of the sample mean\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i\napproaches a normal distribution as the sample size N increases, regardless of the shape of the population distribution. This normal distribution can be expressed as:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{N}\\right),\nwhere \\mu and \\sigma^2 are the population mean and variance, respectively. When talking about samples, we use \\bar{x} and s^2 to denote the sample mean and variance.\nLet’s visualize this. The graph below shows how the sample size N affects the sampling distribution of the sample mean \\bar{X}. The higher the sample size, the more concentrated the distribution becomes around the population mean \\mu. If we take N to be infinity, the sampling distribution of the sample mean becomes a delta function at \\mu, and we will know the exact value of the population mean.\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[7.0, 'mu']\nsigma_boys = df_boys.loc[7.0, 'sigma']\nplot pdfs as function of sample size\nfig, ax = plt.subplots(1,2, figsize=(10, 6), sharex=True, sharey=True)\n\nheight_list = np.arange(mu_boys-12, mu_boys+12, 0.01)\nN_list = [10, 30, 100]\nalpha_list = [0.4, 0.6, 1.0]\n\ncolors = plt.cm.hot([0.6, 0.3, 0.1])\n\nN_samples = 1000\nnp.random.seed(628)\nmean_list_10 = []\nmean_list_30 = []\nmean_list_100 = []\nfor i in range(N_samples):\n    mean_list_10.append(np.mean(norm.rvs(size=10, loc=mu_boys, scale=sigma_boys)))\n    mean_list_30.append(np.mean(norm.rvs(size=30, loc=mu_boys, scale=sigma_boys)))\n    mean_list_100.append(np.mean(norm.rvs(size=100, loc=mu_boys, scale=sigma_boys)))\n\nalpha = 0.05\n\n# z_alpha_over_two = norm(loc=mu_boys, scale=SE).ppf(1 - alpha / 2)\n# z_alpha_over_two = np.round(z_alpha_over_two, 2)\n\nfor i,N in enumerate(N_list):\n    SE = sigma_boys / np.sqrt(N)\n    ax[0].plot(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list),\n            color=colors[i], label=f\"N={N}\")\n    \nax[1].hist(mean_list_10, bins=30, density=True, color=colors[0], label=\"N=10\", align='mid', histtype='step')\nax[1].hist(mean_list_30, bins=30, density=True, color=colors[1], label=\"N=10\", align='mid', histtype='step')\nax[1].hist(mean_list_100, bins=30, density=True, color=colors[2], label=\"N=10\", align='mid', histtype='step')\n\nax[1].text(0.99, 0.98, \"number of samples\\n1000\", ha='right', va='top', transform=ax[1].transAxes, fontsize=14)\n\nax[0].legend(frameon=False)\nax[0].set(xlabel=\"height (cm)\",\n       ylabel=\"pdf\",\n       title=\"analytical\"\n       )\nax[1].set(xlabel=\"height (cm)\",\n          title=\"numerical\"\n          )\n# title that hovers over both subplots\nfig.suptitle(f\"Distribution of the sample means for 3 different sample sizes\");",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#confidence-interval-1",
    "href": "confidence_interval/analytical_confidence_interval.html#confidence-interval-1",
    "title": "11  analytical confidence interval",
    "section": "11.2 confidence interval 1",
    "text": "11.2 confidence interval 1\nLet’s use now the sample size N=30. The confidence interval for a significance level \\alpha=0.05 is the interval that leaves \\alpha/2 of the pdf area in each tail of the distribution.\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.0, hspace=0.1)\nN = 30\nSE = sigma_boys / np.sqrt(N)\n\nh_min = np.round(norm(loc=mu_boys, scale=SE).ppf(0.001), 2)\nh_max = np.round(norm(loc=mu_boys, scale=SE).ppf(0.999), 2)\nheight_list = np.arange(h_min, h_max, 0.01)\n\nalpha = 0.05\nz_alpha_over_two_hi = np.round(norm(loc=mu_boys, scale=SE).ppf(1 - alpha / 2), 2)\nz_alpha_over_two_lo = np.round(norm(loc=mu_boys, scale=SE).ppf(alpha / 2), 2)\n\n\nax[0].plot(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list))\nax[1].plot(height_list, norm(loc=mu_boys, scale=SE).cdf(height_list))\n\nax[0].fill_between(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list),\n                   where=((height_list &gt; z_alpha_over_two_hi) | (height_list &lt; z_alpha_over_two_lo)),\n                   color='tab:blue', alpha=0.5,\n                   label='rejection region')\n\nax[0].annotate(f\"\",\n               xy=(z_alpha_over_two_hi, 0.02),\n               xytext=(z_alpha_over_two_lo, 0.02),\n               arrowprops=dict(arrowstyle=\"&lt;-&gt;\", lw=1.5, color='black', shrinkA=0.0, shrinkB=0.0),\n               )\nax[1].text(h_max+0.15, norm(loc=mu_boys, scale=SE).cdf(z_alpha_over_two_lo), r\"$\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].text(h_max+0.15, norm(loc=mu_boys, scale=SE).cdf(z_alpha_over_two_hi), r\"$1-\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].axhline(alpha/2, color='gray', linestyle=':')\nax[1].axhline(1-alpha/2, color='gray', linestyle=':')\nax[0].text(mu_boys, 0.03, \"95% confidence interval\", ha=\"center\")\nax[0].set(ylim=(0, 0.42),\n          ylabel=\"pdf\",\n          title=r\"significance level $\\alpha$ = 0.05\",\n          )\nax[1].set(ylim=(-0.1, 1.1),\n          xlim=(h_min, h_max),\n          ylabel=\"cdf\",\n          xlabel=\"height (cm)\",\n          );\n\n\n\n\n\n\n\n\n\nThat’s it. That’s the whole story.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#confidence-interval-2",
    "href": "confidence_interval/analytical_confidence_interval.html#confidence-interval-2",
    "title": "11  analytical confidence interval",
    "section": "11.3 confidence interval 2",
    "text": "11.3 confidence interval 2\nThe rest is repackaging the above in a slightly different way. Instead of finding the top and bottom of the confidence interval according to the cdf of a normal distribution of mean \\mu and variance \\sigma^2/N, we first standardize this distribution to a standard normal distribution Z \\sim N(0,1), compute the confidence interval for Z, and then transform it back to the original distribution.\nIf the distribution of the sample mean \\bar{X}\n\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{N}\\right),\n\nthen the standardized variable Z is defined as:\n\nZ = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{N}} \\sim N(0,1).\n\nWhy is this useful? Because we usually use the same significance level \\alpha for all confidence intervals, and we can compute the confidence interval for Z once and use it for all confidence intervals. For Z \\sim N(0,1) and \\alpha=0.05, the top and bottom of the confidence interval are Z_{\\alpha/2}=\\pm 1.96. Now we only have to invert the expression above to get the confidence interval for \\bar{X}:\n\nX_{1,2} = \\mu \\pm Z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{N}}.\n\nThe very last thing we have to account for is the fact that we don’t know the population statistics \\mu and \\sigma^2. Instead, we have to use the sample statistics \\bar{x} and s^2. Furthermore, we have to use the t-distribution instead of the normal distribution, because we are estimating the population variance from the sample variance. The t-distribution has a shape similar to the normal distribution, but it has heavier tails, which accounts for the additional uncertainty introduced by estimating the population variance. Thus, we replace \\mu with \\bar{x} and \\sigma^2 with s^2, and we use the t-distribution with N-1 degrees of freedom. This gives us the final expression for the confidence interval:\n\nX_{1,2} = \\bar{x} \\pm t^*_{N-1} \\cdot \\frac{s}{\\sqrt{N}},\n\nwhere t^*_{N-1} is the critical value from the t-distribution with N-1 degrees of freedom.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#the-solution",
    "href": "confidence_interval/analytical_confidence_interval.html#the-solution",
    "title": "11  analytical confidence interval",
    "section": "11.4 the solution",
    "text": "11.4 the solution\nLet’s say I measured the heights of 30 7-year-old boys, and this is the data I got:\n\n\nShow the code\nN = 30\nnp.random.seed(271)\nsample = norm.rvs(size=N, loc=mu_boys, scale=sigma_boys)\nprint(f\"Sample mean: {np.mean(sample):.2f} cm\")\nprint(sample)\n\n\nSample mean: 122.60 cm\n[114.15972134 128.21581493 122.9864136  117.94247325 132.11013925\n 118.69131645 123.67695468 112.03152008 121.59853424 114.8629358\n 121.90458112 115.68839748 127.18043069 118.33193499 125.28525617\n 124.5287395  120.72706375 113.10575734 132.229147   129.16820684\n 125.94682095 126.08299475 125.95056303 125.6858065  115.07854075\n 124.93539918 125.12886271 126.91366971 120.88030405 127.04777082]\n\n\nUsing the formula for the confidence interval we get:\n\n\nShow the code\nalpha = 0.05\nz_crit = scipy.stats.t.isf(alpha/2, N-1)\nCI = z_crit * sample.std(ddof=1) / np.sqrt(N)\nCI_low = np.round(sample.mean() - CI, 2)\nCI_high = np.round(sample.mean() + CI, 2)\nprint(f\"Sample mean: {np.mean(sample):.2f} cm\")\nprint(\"The 95% confidence interval is [{}, {}] cm\".format(CI_low, CI_high))\nprint(f\"The true population mean is {mu_boys:.2f} cm\")\n\n\nSample mean: 122.60 cm\nThe 95% confidence interval is [120.54, 124.67] cm\nThe true population mean is 121.74 cm",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#a-few-points-to-stress",
    "href": "confidence_interval/analytical_confidence_interval.html#a-few-points-to-stress",
    "title": "11  analytical confidence interval",
    "section": "11.5 a few points to stress",
    "text": "11.5 a few points to stress\nIt is worth commenting on a few points:\n\nIf we were to sample a great many number of samples of size N=30, and compute the confidence interval for each sample, then approximately 95% of these intervals would contain the true population mean \\mu.\nIt is not true that the probability that the true population mean \\mu is in the confidence interval is 95%. The true population mean is either in the interval or not, and it does not have a probability associated with it. The 95% confidence level refers to the long-run frequency of intervals containing the true population mean if we were to repeat the sampling process many times. This is the common frequentist interpretation of confidence intervals.\nIf you want to talk about confidence interval in the Bayesian framework, then first we would have to assign a prior distribution to the population mean \\mu, and then we would compute the posterior distribution of \\mu given the data. The credible interval is then the interval that contains 95% of the posterior distribution of \\mu.\nTo sum up the difference between the frequentist and Bayesian interpretations of confidence intervals:\n\nFrequentist CI: “I am 95% confident in the method” (long-run frequency).\nBayesian credible interval: “There is a 95% probability that μ lies in this interval” (degree of belief).",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html",
    "href": "confidence_interval/empirical_confidence_interval.html",
    "title": "12  empirical confidence interval",
    "section": "",
    "text": "12.1 bootstrap confidence interval\nNot always we want to compute the confidence interval of the mean. Sometimes we are interested in a different statistic, such as the median, the standard deviation, or the maximum. The equations we saw before for the confidence interval of the mean do not apply to these statistics. However, we can still compute a confidence interval for them using the empirical bootstrap method.\nThat’s it. Now let’s do it in code.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html#bootstrap-confidence-interval",
    "href": "confidence_interval/empirical_confidence_interval.html#bootstrap-confidence-interval",
    "title": "12  empirical confidence interval",
    "section": "",
    "text": "Draw a sample of size N from the population. Let’s assume you made an experiment and you could only afford to collect N samples. You will not have the opportunity to collect more samples, and that’s all you have available.\nAssume that the sample is representative of the population. This is a strong assumption, but we will use it to compute the confidence interval.\nFrom this original sample, draw B bootstrap samples of size N with replacement. This means that you will randomly select N samples from the original sample, allowing for duplicates. This is like drawing pieces of paper from a hat, where you can put the paper back after drawing it.\nFor each bootstrap sample, compute the statistic of interest (e.g., median, standard deviation, maximum).\nCompute the cdf of the bootstrap statistics. This will give you the empirical distribution of the statistic.\nCompute the confidence interval using the empirical distribution. For a 95% confidence interval, you can take the 2.5th and 97.5th percentiles of the bootstrap statistics.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html#question",
    "href": "confidence_interval/empirical_confidence_interval.html#question",
    "title": "12  empirical confidence interval",
    "section": "12.2 question",
    "text": "12.2 question\nWe have a sample of 30 7-year-old boys. What can we say about the maximum height of 7-year-olds in the general population?\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\nimport scipy\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[7.0, 'mu']\nsigma_boys = df_boys.loc[7.0, 'sigma']\n\n\n\nN = 30     # bootstrap sample size equal to original sample size\nB = 10000  # number of bootstrap samples\nnp.random.seed(1)\nsample = norm.rvs(size=N, loc=mu_boys, scale=sigma_boys)\nmedian_list = []\nfor i in range(B):\n    sample_bootstrap = np.random.choice(sample, size=N, replace=True)\n    median_list.append(np.median(sample_bootstrap))\nmedian_list = np.array(median_list)\n\nalpha = 0.05\nci_bottom = np.quantile(median_list,alpha/2)\nci_top = np.quantile(median_list, 1-alpha/2)\nprint(f\"Bootstrap CI for median: {ci_bottom:.2f} - {ci_top:.2f} cm\")\n\nBootstrap CI for median: 118.65 - 124.53 cm\n\n\n\n\nShow the code\nfig, ax = plt.subplots(2,1, figsize=(8, 6), sharex=True)\nax[0].hist(median_list, bins=30, density=True, align='mid')\nax[1].hist(median_list, bins=30, density=True, cumulative=True, align='mid')\n\nax[1].axhline(alpha/2, color='gray', linestyle=':')\nax[1].axhline(1-alpha/2, color='gray', linestyle=':')\n\nxlim = ax[1].get_xlim()\nax[1].text(xlim[1]+0.15, alpha/2, r\"$\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].text(xlim[1]+0.15, 1-alpha/2, r\"$1-\\alpha/2$\",\n           ha=\"left\", va=\"center\")\n\nax[1].annotate(\n     'bottom CI',\n     xy=(ci_bottom, alpha/2), xycoords='data',\n     xytext=(-100, 30), textcoords='offset points',\n     color='black',\n     arrowprops=dict(arrowstyle=\"-&gt;\", color='black',\n                     connectionstyle=\"angle,angleA=0,angleB=90,rad=10\"))\nax[1].annotate(\n     'top CI',\n     xy=(ci_top, 1-alpha/2), xycoords='data',\n     xytext=(-100, 15), textcoords='offset points',\n     color='black',\n     arrowprops=dict(arrowstyle=\"-&gt;\", color='black',\n                     connectionstyle=\"angle,angleA=0,angleB=90,rad=10\"))\n\nax[0].set(ylabel=\"pdf\",\n          title=\"empirical distribution of median heights from bootstrap samples\")\nax[1].set(ylabel=\"cdf\",\n          xlabel=\"height (cm)\")\n\n\n\n\n\n\n\n\n\nClearly, the distribution of median height is not normal. The bootstrap method gives us a way to compute the confidence interval of the median height (or any other statistic of your choosing) without assuming normality.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html",
    "href": "regression/geometry-of-regression.html",
    "title": "13  the geometry of regression",
    "section": "",
    "text": "13.1 a very simple example\nIt’s almost always best to start with a simple and concrete example.\nGoal: We wish to find the best straight line that describes the following data points:\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nimport scipy\n\n# %matplotlib widget\ndefine and plot the simple problem\nx = np.array([1, 2, 3])\ny = np.array([2, 2, 6])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.scatter(x, y, label='data', facecolors='black', edgecolors='black')\n# linear regression\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\nx_domain = np.linspace(0, 4, 101)\nax.plot(x_domain, intercept + slope * x_domain, color='black', label='best line')\n\nax.legend(loc='upper left', fontsize=14, frameon=False)\nax.set(xlim=(0, 4),\n       ylim=(0, 7),\n       xticks=np.arange(0, 5, 1),\n       yticks=np.arange(0, 9, 1),\n       xlabel='X-axis',\n       ylabel='Y-axis');",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#formalizing-the-problem",
    "href": "regression/geometry-of-regression.html#formalizing-the-problem",
    "title": "13  the geometry of regression",
    "section": "13.2 formalizing the problem",
    "text": "13.2 formalizing the problem\nLet’s translate this problem into the language of linear algebra.\nThe independent variable x is the column vector\n\nx=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}\n\nand the dependent variable y is the column vector\n\ny=\n\\begin{pmatrix}\n2 \\\\ 2 \\\\ 6\n\\end{pmatrix}.\n\nBecause we are looking for a straight line, we can express the relationship between x and y as\n\n\\tilde{y} = \\beta_0 + \\beta_1 x.\n\nHere we introduced the notation \\tilde{y} to denote the predicted values of y based on the linear model. It is different from the actual values of y because the straight line usually does not pass exactly on top of y.\nThe parameter \\beta_0 is the intercept and \\beta_1 is the slope of the line.\nWhich values of \\beta_0,\\beta_1 will give us the very best line?",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#higher-dimensions",
    "href": "regression/geometry-of-regression.html#higher-dimensions",
    "title": "13  the geometry of regression",
    "section": "13.3 higher dimensions",
    "text": "13.3 higher dimensions\nIt is very informative to think about this problem not as a scatter plot in the X-Y plane, but as taking place in a higher-dimensional space. Because we have three data points, we can think of the problem in a three-dimensional space. We want to explain the vector y as a linear combination of the vector x and a constant vector (this is what our linear model states).\nIn three dimensions, our building blocks are the vectors c, the intercept, and x, the data points.\n\nc=\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\qquad\nx=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}.\n\nWe can combine these c and x as column vectors in a matrix called design matrix:\n\nX=\n\\begin{pmatrix}\n1 & x_0 \\\\\n| & | \\\\\n1 & x_i \\\\\n| & | \\\\\n1 & x_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n| & | \\\\\n1 & x \\\\\n| & |\n\\end{pmatrix}\n\nWhy is this convenient? Because now the linear combination of \\vec{1} and x can be expressed as a matrix multiplication:\n\n\\begin{pmatrix}\n\\hat{y}_0 \\\\\n\\hat{y}_1 \\\\\n\\hat{y}_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & x_0 \\\\\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\cdot\\beta_0 + x_0\\cdot\\beta_1 \\\\\n1\\cdot\\beta_0 + x_1\\cdot\\beta_1 \\\\\n1\\cdot\\beta_0 + x_2\\cdot\\beta_1\n\\end{pmatrix}\n\nIn short, the linear combination of our two building blocks yields a prediction vector \\hat{y}:\n\n\\hat{y} = X \\beta,\n\nwhere \\beta is the column vector (\\beta_0, \\beta_1)^T.\nThis prediction vector \\hat{y} lies on a plane in the 3d space, it cannot be anywhere in this 3d space. Mathematically, we say that the vector \\hat{y} is in the subspace spanned by the columns of the design matrix X.\nIt will be extremely improbable that the vector y will also lie on this plane, so we will have to find the best prediction \\hat{y} that lies on this plane. Geometrically, our goal is to find the point \\hat{y} on the plane that is closest to the point y in the 3d space.\n\nWhen the distance r=y-\\hat{y} is minimized, the vector r is orthogonal to the plane spanned by the columns of the design matrix X.\nWe call this vector r the residual vector.\nThe residual is orthogonal to each of the columns of X, that is, \\vec{1}\\cdot r=0 and x\\cdot r=0.\n\nI tried to summarize all the above in the 3d image below. This is, for me, the geometry of regression. If you have that in your head, you’ll never forget it.\n\nAnother angle of the image above. This time, because the view direction is within the plane, we see that the residual vector r is orthogonal to the plane spanned by the columns of the design matrix X. \nFor a fully interactive version, see this Geogebra applet.\n\n\nTaking advantage of the matrix notation, we can express the orthogonality condition as follows:\n\n\\begin{pmatrix}\n- & 1 & - \\\\\n- & x & -\n\\end{pmatrix} r =\nX^T r =\n0\n\nLet’s substitute r = y - \\hat{y} = y - X\\beta into the equation above.\n\nX^T(y - X\\beta) = 0\n\nDistributing yields\n\nX^Ty - X^TX\\beta = 0,\n\nand then\n\nX^TX\\beta = X^Ty.\n\nWe need to solve this equation for \\beta, so we left-multiply both sides by the inverse of X^TX,\n\n\\beta = (X^TX)^{-1}X^Ty.\n\nThat’s it. We did it. Given the data points x and y, we can compute the parameters \\beta_0 and \\beta_1 that bring \\hat{y} as close as possible to y. These parameters are the best fit of the straight line to the data points.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#overdetermined-system",
    "href": "regression/geometry-of-regression.html#overdetermined-system",
    "title": "13  the geometry of regression",
    "section": "13.4 overdetermined system",
    "text": "13.4 overdetermined system\nThe design matrix X is a tall and skinny matrix, meaning that it has more rows (n) than columns (m). This is called an overdetermined system, because we have more equations (rows) than unknowns (columns), so we have no hope in finding an exact solution \\beta.\nThis is to say that, almost certainly, the vector y does not lie on the plane spanned by the columns of the design matrix X. No combination of the parameters \\beta will yield a vector \\hat{y} that is exactly equal to y.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#least-squares",
    "href": "regression/geometry-of-regression.html#least-squares",
    "title": "13  the geometry of regression",
    "section": "13.5 least squares",
    "text": "13.5 least squares\nThe method above for finding the best parameters \\beta is called least squares. The name comes from the fact that we are trying to minimize the length of the residual vector\n\nr = y - \\hat{y}.\n\nThe length of the residual is given by the Euclidean norm (or L^2 norm), which is a direct generalization of the Pythagorean theorem for many dimensions.\n\\begin{align}\n\\Vert r\\Vert^2 &= \\Vert y - \\hat{y}\\Vert^2  \\\\\n&= (y_0 - \\hat{y}_0)^2 + (y_1 - \\hat{y}_1)^2 + \\cdots +  (y_{n-1} - \\hat{y}_{n-1})^2 \\\\\n&= r_0^2 + r_1^2 + \\cdots + r_{n-1}^2\n\\end{align}\nThe length (squared) of the residual vector is the sum of the squares of all residuals. The best parameters \\beta are those that yield the least squares, thus the name.\n\n\ndefine and plot the simple problem\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.scatter(x, y, label='data', facecolors='black', edgecolors='black')\nx_domain = np.linspace(0, 4, 101)\nax.plot(x_domain, intercept + slope * x_domain, color='black', label='best line')\n\ndef linear(x, slope, intercept):\n    return intercept + slope * x\n\nfor i, xi in enumerate(x):\n    ax.plot([xi, xi],\n            [y[i], linear(xi, slope, intercept)],\n            color='black', linestyle='--', linewidth=0.5,\n            label='residuals' if i == 0 else None)\n    \nax.legend(loc='upper left', fontsize=14, frameon=False) \nax.set(xlim=(0, 4),\n       ylim=(0, 7),\n       xticks=np.arange(0, 5, 1),\n       yticks=np.arange(0, 9, 1),\n       xlabel='X-axis',\n       ylabel='Y-axis');",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#many-more-dimensions",
    "href": "regression/geometry-of-regression.html#many-more-dimensions",
    "title": "13  the geometry of regression",
    "section": "13.6 many more dimensions",
    "text": "13.6 many more dimensions\nThe concrete example here dealt with only three data points, therefore we could visualize the problem in a three-dimensional space. However, the same reasoning applies to any number of data points and any number of independent variables.\n\nany number of data points: we call the number of data points n, and that makes y be a vector in an n-dimensional space.\nany number of independent variables: we calculated a regression for a straight line, and thus we had only two building blocks, the intercept \\vec{1} and the independent variable x. However, we can have any number of independent variables, say m of them. For example, we might want to predict the data using a polynomial of degree m, or we might have any arbitrary m functions that we wish to use: \\exp(x), \\tanh(x^2), whatever we want. All this will work as long as the parameters \\beta multiply these building blocks. That’s the topic of the next chapter.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html",
    "href": "regression/least-squares.html",
    "title": "14  least squares",
    "section": "",
    "text": "14.1 ordinary least squares (OLS) regression\nLet’s go over a few things that appear in this notebook, statsmodels, Ordinary Least Squares\nimport libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nnp.random.seed(9876789)",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#polynomial-regression",
    "href": "regression/least-squares.html#polynomial-regression",
    "title": "14  least squares",
    "section": "14.2 polynomial regression",
    "text": "14.2 polynomial regression\nLet’s start with a simple polynomial regression example. We will start by generating synthetic data for a quadratic equation plus some noise.\n\n# number of points\nnsample = 100\n# create independent variable x\nx = np.linspace(0, 10, 100)\n# create design matrix with linear and quadratic terms\nX = np.column_stack((x, x ** 2))\n# create coefficients array\nbeta = np.array([5, -2, 0.5])\n# create random error term\ne = np.random.normal(size=nsample)\n\nx and e can be understood as column vectors of length n, while X and \\beta are:\n\nX =\n\\begin{pmatrix}\nx_0 & x_0^2 \\\\\n| & | \\\\\nx_i & x_i^2 \\\\\n| & | \\\\\nx_n & x_n^2 \\\\\n\\end{pmatrix}, \\qquad\n\\beta =\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{pmatrix}.\n\nOops, there is no intercept column \\vec{1} in the design matrix X. Let’s add it:\n\nX = sm.add_constant(X)\nprint(X[:5, :])  # print first 5 rows of design matrix\n\n[[1.         0.         0.        ]\n [1.         0.1010101  0.01020304]\n [1.         0.2020202  0.04081216]\n [1.         0.3030303  0.09182736]\n [1.         0.4040404  0.16324865]]\n\n\nThis add_constant function is smart, it has as default a prepend=True argument, meaning that the intercept is added as the first column, and a has_constant='skip' argument, meaning that it will not add a constant if one is already present in the matrix.\nThe matrix X is now a design matrix for a polynomial regression of degree 2. \nX =\n\\begin{pmatrix}\n1 & x_0 & x_0^2 \\\\\n| & | & | \\\\\n1 & x_i & x_i^2 \\\\\n| & | & | \\\\\n1 & x_n & x_n^2 \\\\\n\\end{pmatrix}\n\nWe now put everything together in the following equation:\n\ny = X \\beta + e\n\nThis creates the dependend variable y as a linear combination of the independent variables in X and the coefficients in \\beta, plus an error term e.\n\ny = np.dot(X, beta) + e\n\nLet’s visualize this:\n\n\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.set(xlabel='x',\n       ylabel='y',\n       title='Simulated Data with Linear and Quadratic Terms'\n       );",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#solving-the-hard-way",
    "href": "regression/least-squares.html#solving-the-hard-way",
    "title": "14  least squares",
    "section": "14.3 solving the “hard way”",
    "text": "14.3 solving the “hard way”\nI’m going to do something that nobody does. I will use the formula we derived in the previous chapter to find the coefficients \\beta of the polynomial regression.\n\n\\beta = (X^TX)^{-1}X^Ty.\n\nTranslating this into code, and keeping in mind that matrix multiplication in Python is done with the @ operator, we get:\n\nbeta_opt = np.linalg.inv(X.T@X)@X.T@y\nprint(f\"beta = {beta_opt}\")\n\nbeta = [ 5.34233516 -2.14024948  0.51025357]\n\n\nThat’s it. We did it (again).\nLet’s take a look at the matrix X^TX. Because X is a tall and skinny matrix of shape (n, 3), the matrix X^T is a wide and short matrix of shape (3, n). This is because we have many more data points n than the number of predictors (\\vec{1},x,x^2), which is of course equal to the number of coefficients (\\beta_0,\\beta_1,\\beta_2).\n\nprint(X.T@X)\n\n[[1.00000000e+02 5.00000000e+02 3.35016835e+03]\n [5.00000000e+02 3.35016835e+03 2.52525253e+04]\n [3.35016835e+03 2.52525253e+04 2.03033670e+05]]\n\n\nWhen we multiply the matrices X^T_{3\\times n} and X_{n\\times 3}, we get a square matrix of shape (3, 3), because the inner dimensions match (the number of columns in X^T is equal to the number of rows in X). The product X^TX is a square matrix of shape (3, 3), which is quite easy to invert. If it were the other way around (X\\,X^T), we would have a matrix of shape (n, n), which is much harder to invert, especially if n is large. Lucky us.\nNow let’s see if the parameters we found are any good.\n\nprint(\"beta parameters used to generate data:\")\nprint(beta)\nprint(\"beta parameters estimated from data:\")\nprint(beta_opt)\n\nbeta parameters used to generate data:\n[ 5.  -2.   0.5]\nbeta parameters estimated from data:\n[ 5.34233516 -2.14024948  0.51025357]\n\n\nPretty good, right? Now let’s see the best fit polynomial on the graph.\n\n\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.plot(x, np.dot(X, beta_opt), color='red', label='fitted line')\nax.legend(frameon=False)\nax.set(xlabel='x',\n       ylabel='y',\n       title='Simulated Data with Linear and Quadratic Terms'\n       );\n\n\n\n\n\n\n\n\n\nWhy did I call it the “hard way”? Because these operations are so common that of course there are libraries that do this for us. We don’t need to remember the equation, we can just use, for example, statsmodels library’s OLS function, which does exactly this. Let’s see how it works.\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nNow we can compare the results of our manual calculation with the results from statsmodels. We should get the same coefficients, and we do.\n\nprint(\"beta parameters used to generate data:\")\nprint(beta)\nprint(\"beta parameters from our calculation:\")\nprint(beta_opt)\nprint(\"beta parameters from statsmodels:\")\nprint(results.params)\n\nbeta parameters used to generate data:\n[ 5.  -2.   0.5]\nbeta parameters from our calculation:\n[ 5.34233516 -2.14024948  0.51025357]\nbeta parameters from statsmodels:\n[ 5.34233516 -2.14024948  0.51025357]",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#statmodels.ols-and-the-summary",
    "href": "regression/least-squares.html#statmodels.ols-and-the-summary",
    "title": "14  least squares",
    "section": "14.4 statmodels.OLS and the summary",
    "text": "14.4 statmodels.OLS and the summary\nStatmodels provides us a lot more information than just the coefficients. Let’s take a look at the summary of the OLS regression.\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.988\nModel:                            OLS   Adj. R-squared:                  0.988\nMethod:                 Least Squares   F-statistic:                     3965.\nDate:                Mon, 23 Jun 2025   Prob (F-statistic):           9.77e-94\nTime:                        12:50:31   Log-Likelihood:                -146.51\nNo. Observations:                 100   AIC:                             299.0\nDf Residuals:                      97   BIC:                             306.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.3423      0.313     17.083      0.000       4.722       5.963\nx1            -2.1402      0.145    -14.808      0.000      -2.427      -1.853\nx2             0.5103      0.014     36.484      0.000       0.482       0.538\n==============================================================================\nOmnibus:                        2.042   Durbin-Watson:                   2.274\nProb(Omnibus):                  0.360   Jarque-Bera (JB):                1.875\nSkew:                           0.234   Prob(JB):                        0.392\nKurtosis:                       2.519   Cond. No.                         144.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nI won’t go into the details of the summary, but I encourage you to take a look at it and see if you can make sense of it.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#r-squared",
    "href": "regression/least-squares.html#r-squared",
    "title": "14  least squares",
    "section": "14.5 R-squared",
    "text": "14.5 R-squared\nR-squared is a measure of how well the model fits the data. It is defined as the proportion of the variance in the dependent variable that is predictable from the independent variables. It can be computed as follows:\n\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n\nwhere SS_{res} and SS_{tot} are defined as follows:\n\\begin{align*}\nSS_{res} &= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\nSS_{tot} &= \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\end{align*}\nThe letters SS mean “sum of squares”, and \\bar{y} is the mean of the dependent variable. Let’s compute it manually, and then compare it with the value from the statsmodels summary.\n\ny_hat = np.dot(X, beta_opt)\nSS_res = np.sum((y - y_hat) ** 2)\nSS_tot = np.sum((y - np.mean(y)) ** 2)\nR2 = 1 - (SS_res / SS_tot)\nprint(\"R-squared (manual calculation): \", R2)\nprint(\"R-squared (from statsmodels): \", results.rsquared)\n\nR-squared (manual calculation):  0.9879144521349076\nR-squared (from statsmodels):  0.9879144521349076\n\n\nThis high R^2 value tells us that the model explains a very large proportion of the variance in the dependent variable.\nHow can we know that the variance has anything to do with the R^2? If we divide both the SS_{res} and SS_{tot} by n-1, we get the sample variances of the residuals and the dependent variable, respectively.\n\\begin{align*}\ns^2_{res} = \\frac{SS_{res}}{n-1} &= \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-1} \\\\\ns^2_{tot} = \\frac{SS_{tot}}{n-1} &= \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n-1}\n\\end{align*}\nThen the R^2 can then be expressed as: \nR^2 = 1 - \\frac{s^2_{res}}{s^2_{tot}}.\n\nI prefer this equation over the first, because it makes it clear that R^2 is the ratio of the variances, which is a more intuitive way to think about it.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#any-function-will-do",
    "href": "regression/least-squares.html#any-function-will-do",
    "title": "14  least squares",
    "section": "14.6 any function will do",
    "text": "14.6 any function will do\nThe formula we derived the the previous chapter works for predictors (independent variables) of any kind, not only polynomials. The formula will work as long as the parameters \\beta are linear in the predictors. For exammple, we could have a nonlinear function like this:\n\ny = \\beta_0 + \\beta_1 e^x + \\beta_2 \\sin(x^2)\n\nbecause each beta multiplies a predictor. On the other hand, the following function would not work, because the parameters are not linear in the predictors:\n\ny = \\beta_0 + e^{\\beta_1 x} + \\sin(\\beta_2 x^2)\n\nLet’s this this in action, I’ll use the same example provided by statsmodels documentation, which is a nonlinear function of the form:\n\ny = \\beta_0 x + \\beta_1 \\sin(x) + \\beta_2(x - 5)^2 + \\beta_3\n\n\nnsample = 50\nsig = 0.5\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((\n    x,\n    np.sin(x),\n    (x - 5) ** 2,\n    np.ones(nsample)\n    ))\nbeta = [0.5, 0.5, -0.02, 5.0]\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.set(xlabel='x',\n       ylabel='y',\n       )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresult = sm.OLS(y, X).fit()\nprint(result.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.933\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     211.8\nDate:                Mon, 23 Jun 2025   Prob (F-statistic):           6.30e-27\nTime:                        12:51:08   Log-Likelihood:                -34.438\nNo. Observations:                  50   AIC:                             76.88\nDf Residuals:                      46   BIC:                             84.52\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.4687      0.026     17.751      0.000       0.416       0.522\nx2             0.4836      0.104      4.659      0.000       0.275       0.693\nx3            -0.0174      0.002     -7.507      0.000      -0.022      -0.013\nconst          5.2058      0.171     30.405      0.000       4.861       5.550\n==============================================================================\nOmnibus:                        0.655   Durbin-Watson:                   2.896\nProb(Omnibus):                  0.721   Jarque-Bera (JB):                0.360\nSkew:                           0.207   Prob(JB):                        0.835\nKurtosis:                       3.026   Cond. No.                         221.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote something interesting: in our design matrix X, we encoded the intercept column as the last column, there is no reason why it should be the first column (although first column is a common choice). The function ‘statsmodels.OLS’ sees this, and when we print the summary, it will show the intercept as the last coefficient. Nice!\nLet’s see a graph of the data and the fitted model.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.plot(x, np.dot(X, result.params), color='red', label='fitted line')\nax.legend(frameon=False)\nax.set(xlabel='x',\n       ylabel='y',\n       )",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html",
    "href": "regression/equivalence.html",
    "title": "15  equivalence",
    "section": "",
    "text": "15.1 orthogonality\nIn the context of linear regression, the orthogonality condition states that the residual vector r is orthogonal to the column space of the design matrix X:\nX^T r = 0\nLet’s substitute r = y - \\hat{y} = y - X\\beta into the equation above.\nX^T(y - X\\beta) = 0\nDistributing yields\nX^Ty - X^TX\\beta = 0,\nand then\nX^TX\\beta = X^Ty.\nWe need to solve this equation for \\beta, so we left-multiply both sides by the inverse of X^TX,\n\\beta = (X^TX)^{-1}X^Ty.\nWe already did that in a previous chapter. Now let’s get exactly the same result using another approach.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#optimization",
    "href": "regression/equivalence.html#optimization",
    "title": "15  equivalence",
    "section": "15.2 optimization",
    "text": "15.2 optimization\nWe wish to minimize the sum of squared errors, which is given by\n\nL = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - X_{ij}\\beta_j)^2.\n\nI’m not exactly sure, but I think we call this L because of the lagrangian function. Later on when we talk about regularization, we will see that the lagrangian function can be constrained by lagrange multipliers. In any case, let’s keep going with the optimization.\nIt is useful to express L in matrix notation. The sum of squared errors can be thought as the dot product of the residual vector with itself:\n\\begin{align*}\nL &= (y - X\\beta)^T(y - X\\beta) \\\\\n&= y^Ty - y^TX\\beta - \\beta^TX^Ty + \\beta^TX^TX\\beta,\n\\end{align*}\nwhere we used the following properties of matrix algebra: 1. The dot product of a vector with itself is the sum of the squares of its components, i.e., a^Ta = \\sum_{i=1}^n a_i^2. We used this to express the sum of squared errors in matrix notation. 2. The dot product is bilinear, i.e., (a - b)^T(c - d) = a^Tc - a^Td - b^Tc + b^Td. We used this to expand the expression for the sum of squared errors. 3. The transpose of a product of matrices is the product of their transposes in reverse order, i.e., (AB)^T = B^TA^T. We used this to compute (X\\beta)^T.\nLet’s use one more property to join the two middle terms, - y^TX\\beta - \\beta^TX^Ty:\n\nThe dot product is symmetric, i.e., a^Tb = b^Ta. This is evident we express the dot product as a summation:\n\n\n  a^Tb = \\sum_{i=1}^n a_i b_i = \\sum_{i=1}^n b_i a_i = b^Ta.\n  \nJoining the two middle terms results in the following L:\n\nL = y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta,\n\n\nThe set of parameters \\beta that minimizes L is that which satisfies the extreme condition of the function L (either maximum or minimum). This means that the gradient of L with respect to \\beta must be zero:\n\n\\frac{\\partial L}{\\partial \\beta} = 0.\n\nLet’s plug in the expression for L:\n\n\\frac{\\partial}{\\partial \\beta} \\left( y^Ty - 2\\beta^TX^Ty + \\beta^TX^TX\\beta \\right) = 0\n\nThe quantity L is a scalar, and also each of the three terms that we are differentiating is a scalar. Let’s differentiate them one by one.\n\nThe first term, y^Ty, is a constant with respect to \\beta, so its derivative is zero.\n\n\nThe second term\n\n\\frac{\\partial}{\\partial \\beta} \\left( - 2\\beta^TX^Ty \\right) = - 2\\frac{\\partial}{\\partial \\beta} \\left( \\beta^TX^Ty \\right).\n\nThe quantity being differentiated is a scalar, it’s the product of the row vector \\beta^T and the column vector X^Ty. Right now we don’t care much about X^Ty, it could be any column vector, so let’s call it c. The derivative of dot product \\beta^T c with respect to a specific element \\beta_k can be written explicitly as\n\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\beta_i c_i \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\beta_1 c_1 + \\beta_2 c_2 + \\ldots + \\beta_p c_p \\right) =  c_k.\n\nWhatever value for the index k we choose, the derivative will be zero for all indices except for k, and that explain the result above.\nSince the gradient \\nabla_{\\beta}(\\beta^T c) is a vector,\n\n\\nabla_{\\beta}(\\beta^T c) = \\frac{\\partial (\\beta^T c)}{\\partial \\beta} =\n\\begin{pmatrix}\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_1} \\\\\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_2} \\\\\n\\vdots \\\\\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_p}\n\\end{pmatrix}\n\nand we have just figured out what each component is, we can write the solution as\n\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta} =\n\\begin{pmatrix}\nc_1 \\\\ c_2 \\\\ \\vdots \\\\ c_p\n\\end{pmatrix}\n= c\n= X^Ty.\n\nSo the derivative of the second term is simply -2 X^Ty.\n\n\nTo solve the derivatie of the third term, \\beta^TX^TX\\beta, we use the following property:\n\n\\frac{\\partial}{\\partial \\beta} \\left( \\beta^T A \\beta \\right) = 2A\\beta,\n\nwhen A is a symmetric matrix. In our case, A = X^TX, which is symmetric because (X^TX)^T = X^T(X^T)^T = X^TX. Therefore we have:\n\n\\frac{\\partial}{\\partial \\beta} \\left( \\beta^T X^TX \\beta \\right) = 2X^TX\\beta,\n\nand that is the derivative of the third term.\nWe still have to prove why the derivative of \\beta^T A \\beta is 2A\\beta. First, let’s use the summation notation to express the term \\beta^T A \\beta:\n\n\\beta^T A \\beta = \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j.\n\nNow, let’s differentiate this expression with respect to \\beta_k, using the chain rule:\n\n\\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j \\right) = \\sum_{i=1}^p A_{ik} \\beta_i + \\sum_{j=1}^p \\beta_j A_{kj}.\n\nUsing the symmetry of A (A_{ij} = A_{ji}):\n\n\\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j \\right) = 2 \\sum_{i=1}^p A_{ik} \\beta_i = 2(A\\beta)_k.\n\nThis is the element k of the vector 2A\\beta. Since this is true for any index k, we can write the gradient as\n\n\\nabla_{\\beta}(\\beta^T A \\beta) = 2A\\beta.\n\n\nNow that we have the derivatives of all three terms, we can write the gradient of L:\n\n\\frac{\\partial L}{\\partial \\beta} = 0 - 2X^Ty + 2X^TX\\beta = 0.\n\nRearranging…\n\nX^TX\\beta = X^Ty\n\n…and solving for \\beta:\n\n\\beta = (X^TX)^{-1}X^Ty",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#discussion",
    "href": "regression/equivalence.html#discussion",
    "title": "15  equivalence",
    "section": "15.3 discussion",
    "text": "15.3 discussion\nUsing two completely different approaches, we arrived at the same result for the least squares solution:\n\n\\beta = (X^TX)^{-1}X^Ty\n.\n\nApproach 1: We used the orthogonality condition, which states that the residual vector is orthogonal to the column space of the design matrix.\nApproach 2: We applied the optimization method, minimizing the sum of squared errors—which corresponds to minimizing the squared length of the residual vector.\n\nThere is a deep connection here. The requirement that the residual vector is orthogonal to the column space of the design matrix is equivalent to minimizing the sum of squared errors. We can see this visually: if the projection of the response vector y onto the column space of X were anywhere else, the residual vector would be not only not orthogonal, but also longer!\n\n\n\\text{orthogonality} \\iff \\text{optimization}\n\n\nThis result even tranfers to other contexts, as long as there is a vector space with a well defined inner product (dot product) and an orthogonal basis. In these cases, the least squares solution can be interpreted as finding the projection of a vector onto a subspace spanned by an orthogonal basis. Some examples include:\n\nFourier series: the Fourier coefficients are the least squares solution to the problem of approximating a function by a sum of sines and cosines, where these functions are an orthogonal basis.\nSVD (Singular Value Decomposition): A matrix can be decomposed into orthogonal matrices, and the singular values can be interpreted as the least squares solution to the problem of approximating a matrix by a sum of outer products of orthogonal vectors.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#other-properties",
    "href": "regression/equivalence.html#other-properties",
    "title": "15  equivalence",
    "section": "15.4 other properties",
    "text": "15.4 other properties\n\n15.4.1 the sum of residuals is zero\nAs long as the model includes an intercept term, the sum of the residuals is zero. The model could be anything, not necessarily linear regression. Let’s prove this property.\n\n\\hat{y}_i = \\beta_0 + f(x_i; \\beta_1, \\beta_2, \\ldots, \\beta_p)\n\nThe Ordinary Least Squares (OLS) estimates of the parameters \\beta minimize the sum of squared residuals L. The equation for \\beta_0 reads:\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_0} &=\n\\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right)^2 \\\\\n&=\n\\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n \\left[y_i - \\beta_0 - f(x_i; \\beta_1, \\beta_2, \\ldots, \\beta_p) \\right]^2 \\\\\n&= -2 \\sum_{i=1}^n \\left[y_i - \\beta_0 - f(x_i; \\beta_1, \\beta_2, \\ldots, \\beta_p) \\right] \\\\\n&= -2 \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right) = 0\n\\end{align*}\nFrom the last line it follows that the sum of the residuals is zero.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/partitioning.html",
    "href": "regression/partitioning.html",
    "title": "16  partitioning of the sum of squares",
    "section": "",
    "text": "16.1 high-dimensional Pythagorean theorem\nOne of the most important equations in regression analysis is the partitioning of the sum of squares (SS).\nSS_\\text{Total} = SS_\\text{Model} + SS_\\text{Error}\nThis equation states that the total variability in the response variable can be partitioned into two components: the variability explained by the regression model and the variability that is not explained by the model (the residuals).\nIn a more precise mathematical language, the equation states that:\n\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 + \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\nwhere:\nWhy is this equation true? It’s easiest to understand this equation by thinking of it as a high-dimensional version of the Pythagorean theorem:\n\\lVert T \\rVert ^2 = \\lVert M \\rVert ^2 + \\lVert E \\rVert ^2,\nwhere:\nI omitted the subscript i to emphasize that these are vectors, not scalars.\nMake sure you understand the figure below, already presented in a previous chapter.\nThe vector r=E in black is the residual or error vector. It is orthogonal to the subspace spanned by the column vector of the design matrix, represented in this image by the blue plane.\nThe vector M is not shown, but it necessarily lies in the blue plane. How do we know that? Because the predicted values \\hat{y} are a linear combination of the columns of the design matrix, and therefore \\hat{y} lies in the column space of the design matrix. Since \\bar{y} is a constant vector (a multiple of the vector of ones), it also lies in the column space of the design matrix. Therefore, M = \\hat{y} - \\bar{y} lies in the column space of the design matrix.\nFrom the above, we can already conclude that E is orthogonal to M. These are the two legs of a right triangle. We now need a hypotenuse. The hypotenuse is the total vector T = y - \\bar{y}, which is the sum of the other two vectors:\nT = M + E.\nThis is easy to see by substituting the definitions of M and E:\nT = y - \\bar{y} = (\\hat{y} - \\bar{y}) + (y - \\hat{y}) = M + E.\nShow the code\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_aspect('equal', adjustable='box')\nax.annotate('', xy=(2, 0), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', width=2, headwidth=10, headlength=10))\nax.annotate('', xy=(2, 1), xytext=(2, 0), arrowprops=dict(facecolor='blue', edgecolor='blue', width=2, headwidth=10, headlength=10))\nax.annotate('', xy=(2, 1), xytext=(0, 0), arrowprops=dict(facecolor='red', edgecolor='red', width=2, headwidth=10, headlength=10))\n\nax.text(1.0, 0.0, 'M', fontsize=20, ha='center', va='bottom')\nax.text(2.03, 0.5, 'E', fontsize=20, ha='left', va='center')\nax.text(1.0, 0.55, 'T', fontsize=20, ha='center', va='bottom')\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\nax.set_xlim(0, 2)\nax.set_ylim(0, 1);",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>partitioning of the sum of squares</span>"
    ]
  },
  {
    "objectID": "regression/partitioning.html#high-dimensional-pythagorean-theorem",
    "href": "regression/partitioning.html#high-dimensional-pythagorean-theorem",
    "title": "16  partitioning of the sum of squares",
    "section": "",
    "text": "T = y - \\bar{y} is the total vector, the vector of deviations of the observed values from their mean;\nM = \\hat{y} - \\bar{y} is the model vector, the vector of deviations of the predicted values from the mean of the observed values;\nE = y - \\hat{y} is the error vector, the vector of residuals, or deviations of the observed values from the predicted values.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>partitioning of the sum of squares</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html",
    "href": "regression/mixed-model.html",
    "title": "17  linear mixed effect model",
    "section": "",
    "text": "17.1 practical example\nA mixed effect model is an expansion of the ordinary linear regression model that includes both fixed effects and random effects. The fixed effects are the same as in a standard linear regression (could be with or without interactions), while the random effects account for variability across different groups or clusters in the data.\nWe are given a dataset of annual income (independent variable) and years of education (independent variable) for individuals that studied different majors in university (categorical variable). We want to predict the annual income based on years of education and the major studied, including an interaction term between years of education and major. One more thing: each individual appears more than once in the dataset, so we can assume that there is a random effect associated with each individual.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as smf\ngenerate synthetic data\n# set seed for reproducibility\nnp.random.seed(42)\n# define parameters\nmajors = ['Juggling', 'Magic', 'Dragon Taming']\nn_individuals = 90  # 30 per major\nyears_per_person = np.random.randint(1, 5, size=n_individuals)  # 1 to 4 time points\n\n# assign majors and person IDs\nperson_ids = [f'P{i+1:03d}' for i in range(n_individuals)]\nmajor_assignment = np.repeat(majors, n_individuals // len(majors))\n\n# simulate data\nrecords = []\nfor i, pid in enumerate(person_ids):\n    major = major_assignment[i]\n    n_years = years_per_person[i]\n    years = np.sort(np.random.choice(np.arange(1, 21), size=n_years, replace=False))\n    \n    # base intercept and slope by major\n    if major == 'Juggling':\n        base_income = 25_000\n        growth = 800\n    elif major == 'Magic':\n        base_income = 20_000\n        growth = 1500\n    elif major == 'Dragon Taming':\n        base_income = 30_000\n        growth = 400  # slower growth\n    \n    # add person-specific deviation\n    personal_offset = np.random.normal(0, 5000)\n    slope_offset = np.random.normal(0, 200)\n    \n    for y in years:\n        income = base_income + personal_offset + (growth + slope_offset) * y + np.random.normal(0, 3000)\n        records.append({\n            'person': pid,\n            'major': major,\n            'years_after_grad': y,\n            'income': income\n        })\n\ndf = pd.DataFrame(records)\nLet’s take a look at the dataset. There are many data points, so we will only see 15 points in three different places.\nShow the code\nprint(df[:5])\nprint(df[90:95])\nprint(df[190:195])\n\n\n  person     major  years_after_grad        income\n0   P001  Juggling                 3  37183.719609\n1   P001  Juggling                 5  35238.112407\n2   P001  Juggling                11  37905.435001\n3   P002  Juggling                 2  27432.186391\n4   P002  Juggling                 4  30617.926804\n   person  major  years_after_grad        income\n90   P034  Magic                 1  14151.072305\n91   P034  Magic                 7  19716.656861\n92   P035  Magic                12  41056.576643\n93   P035  Magic                14  46339.987229\n94   P036  Magic                16  41981.131518\n    person          major  years_after_grad        income\n190   P072  Dragon Taming                 7  36173.437735\n191   P073  Dragon Taming                 8  33450.564557\n192   P074  Dragon Taming                 9  35276.927416\n193   P074  Dragon Taming                17  37271.203018\n194   P075  Dragon Taming                 2  31819.051946\nNow let’s see the data in a plot.\nplot income by major\nfig, ax = plt.subplots(figsize=(8, 6))\n\ngb = df.groupby('major')\nfor major, group in gb:\n    ax.scatter(group['years_after_grad'], group['income'], label=major, alpha=0.6)\n\nax.legend(title='Major', frameon=False)\nax.set(xlabel='years after graduation',\n       ylabel='income',\n       xticks=np.arange(0, 21, 5)\n       );\nThe model we will use is\ny = \\underbrace{X \\beta}_{\\text{fixed effects}} + \\underbrace{Z b}_{\\text{random effects}} + \\underbrace{\\varepsilon}_{\\text{residuals}}\nThe only new term here is Zb, the random effects, where Z is the design matrix for the random effects and b is the vector of random effects coefficients. We will discuss that a bit later. Let’s start with the fixed effects part:\nX \\beta = \\beta_0 + \\beta_1 \\cdot \\text{years} + \\beta_2 \\cdot \\text{major} + \\beta_3 \\cdot (\\text{years} \\cdot \\text{major})\nThe “years” variable is continuous, while the “major” variable is categorical. How to include categorical variables in a linear regression model? We can use dummy coding, where we create binary variables for each category of the categorical variable (except one category, which serves as the reference group). In our case, we have three majors: Juggling, Magic, and Dragon Taming. Let’s use “Juggling” as the reference group. We can create two dummy variables that function as toggles.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#practical-example",
    "href": "regression/mixed-model.html#practical-example",
    "title": "17  linear mixed effect model",
    "section": "",
    "text": "major_Magic: 1 if the major is Magic, 0 otherwise\nmajor_DragonTaming: 1 if the major is Dragon Taming, 0 otherwise",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#visualizing-categories-as-toggles",
    "href": "regression/mixed-model.html#visualizing-categories-as-toggles",
    "title": "17  linear mixed effect model",
    "section": "17.2 visualizing categories as toggles",
    "text": "17.2 visualizing categories as toggles\nIn the equation above, we have only one parameter for “major” (\\beta_2), and only one parameter for the interaction terms (\\beta_3). In reality we have more, see:\n\\begin{align*}\n\\text{income} &= \\beta_0 + \\beta_1 \\cdot \\text{years} \\\\\n&+ \\beta_2 \\cdot \\text{major\\_Magic} + \\beta_3 \\cdot \\text{major\\_DragonTaming} \\\\\n&+ \\beta_4 \\cdot (\\text{years} \\cdot \\text{major\\_Magic}) + \\beta_5 \\cdot (\\text{years} \\cdot \\text{major\\_DragonTaming}) \\\\\n&+ \\epsilon\n\\end{align*}\nThe first line represents the linear relationship between income and education of the reference group (Juggling). The second line adds the effects on the intercept of having studied Magic or Dragon Taming instead, and the third line adds the the effects on the slope of these two majors.\nLet’s see for a few data points how this works. Below, dummy variables represent the pair (major_Magic, major_DragonTaming).\n\n\n\nyears_after_grad\nmajor\nDummy variables\nincome\n\n\n\n\n3\nJuggling\n(0, 0)\n37183.72\n\n\n5\nMagic\n(1, 0)\n35101.07\n\n\n7\nDragon Taming\n(0, 1)\n27179.77\n\n\n10\nJuggling\n(0, 0)\n26366.80\n\n\n12\nMagic\n(1, 0)\n26101.53\n\n\n16\nDragon Taming\n(0, 1)\n39252.76\n\n\n\nThe design matrix X would look like this:\n\nX =\n\\begin{array}{c}\n  \\begin{array}{cccccc}\n    \\beta_0 & \\beta_1 & \\beta_2 & \\beta_3 & \\beta_4 & \\beta_5\n  \\end{array} \\\\\n  \\begin{pmatrix}\n    1 & 3 & 0 & 0 & 0 & 0 \\\\\n    1 & 5 & 1 & 0 & 5 & 0 \\\\\n    1 & 7 & 0 & 1 & 0 & 7 \\\\\n    1 & 10 & 0 & 0 & 0 & 0 \\\\\n    1 & 12 & 1 & 0 & 12 & 0 \\\\\n    1 & 16 & 0 & 1 & 0 & 16\n  \\end{pmatrix}\n\\end{array}.\n\nThe betas above the matrix are there just to label the columns, they are not really part of the matrix. The 3rd and 4th columns are the dummy variables for the majors, and the 5th and 6th columns are the interaction terms between education and the majors.\nIf we were not interested in the random effects, we could stop here, and just use the ordinary least squares (OLS) method already discussed to estimate the coefficients \\beta.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#random-effects",
    "href": "regression/mixed-model.html#random-effects",
    "title": "17  linear mixed effect model",
    "section": "17.3 random effects",
    "text": "17.3 random effects\nThe name “mixed effects” comes from the fact that we have both fixed effects and random effects.\nConceptually, the random effects function in a very similar way to the fixed effects. Instead of a small number of categories, now each person in the dataset is a category. In our example we have 90 different people represented in the dataset, so the quantity Z in Zb is the design matrix for the random effects, which is a matrix with 90 columns, one for each person, and as many rows as there are data points in the dataset. Each row has a 1 in the column corresponding to the person, and 0s elsewhere. The vector b is a vector of random effects coefficients, one for each person.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#implementation",
    "href": "regression/mixed-model.html#implementation",
    "title": "17  linear mixed effect model",
    "section": "17.4 implementation",
    "text": "17.4 implementation\nWe can use statsmodels function smf.mixedlm to do everything for us. We just need to specify the formula, which includes the interaction term, and the data.\nIf you don’t mind which category is the reference group, you can skip the cell below. If you want to make sure a give one is the reference group (Juggling in our case), then you should run it.\n\n\nchoose Juggling as reference major\nfrom pandas.api.types import CategoricalDtype\n# define the desired order: Juggling as reference\nmajor_order = CategoricalDtype(categories=[\"Juggling\", \"Magic\", \"Dragon Taming\"], ordered=True)\ndf[\"major\"] = df[\"major\"].astype(major_order)\n\n\nThe syntax is fairly economic. The formula\nincome ~ years_after_grad * major\nspecifies a linear model where both the baseline income (intercept) and the effect of time since graduation (slope) can vary by major. The * operator includes both the main effects (years after graduation and major) and their interaction, allowing the model to fit a different intercept and slope for each major.\nIn the line\nmodel = smf.mixedlm(formula, data=df, groups=df[\"person\"])\nthe groups argument specifies that the random effects are associated with the “person” variable, meaning that each person can have their own random intercept.\n\n# formula with interaction\nformula = \"income ~ years_after_grad * major\"\n\n# fit mixed model with random intercept for person\nmodel = smf.mixedlm(formula, data=df, groups=df[\"person\"])\nresult = model.fit()\n\nLet’s see the results\n\nprint(result.summary())\n\n                            Mixed Linear Model Regression Results\n==============================================================================================\nModel:                         MixedLM            Dependent Variable:            income       \nNo. Observations:              239                Method:                        REML         \nNo. Groups:                    90                 Scale:                         10690821.7105\nMin. group size:               1                  Log-Likelihood:                -2327.5068   \nMax. group size:               4                  Converged:                     Yes          \nMean group size:               2.7                                                            \n----------------------------------------------------------------------------------------------\n                                           Coef.     Std.Err.   z    P&gt;|z|   [0.025    0.975] \n----------------------------------------------------------------------------------------------\nIntercept                                  25206.095 1349.760 18.675 0.000 22560.615 27851.575\nmajor[T.Magic]                             -2999.754 1995.748 -1.503 0.133 -6911.348   911.840\nmajor[T.Dragon Taming]                      5579.198 1954.661  2.854 0.004  1748.133  9410.263\nyears_after_grad                             723.745   72.028 10.048 0.000   582.573   864.917\nyears_after_grad:major[T.Magic]              635.180  109.599  5.795 0.000   420.370   849.989\nyears_after_grad:major[T.Dragon Taming]     -295.862  106.315 -2.783 0.005  -504.235   -87.488\nGroup Var                               33814137.626 2268.953                                 \n==============================================================================================",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#interpreting-the-results",
    "href": "regression/mixed-model.html#interpreting-the-results",
    "title": "17  linear mixed effect model",
    "section": "17.5 interpreting the results",
    "text": "17.5 interpreting the results\nTo interpret the coefficients, start with the reference group, which in this model is someone who studied Juggling. Their predicted income is:\n\n\\text{income} = 25206.10 + 723.75 \\times \\text{years}\n\nNow, for a person who studied Magic, the model adjusts both the intercept and the slope:\nIntercept shift: -2999.75 Slope shift: +635.18 So for Magic, the predicted income becomes:\n\\begin{align*}\n\\text{income} &= (25206.10 - 2999.75) + (723.75 + 635.18) \\times \\text{years} \\\\\n       &= 22206.35 + 1358.93 \\times \\text{years}\n\\end{align*}\nThis means that compared to Jugglers, Magicians start with a lower baseline salary, but their income grows much faster with each year after graduation.\nThe Coef. column shows the estimated value of each parameter (e.g., intercepts, slopes, interactions). The Std.Err. column reports the standard error of the estimate, reflecting its uncertainty. The z column is the test statistic (estimate divided by standard error), and P&gt;|z| gives the p-value, which helps assess whether the effect is statistically significant. The final two columns, [0.025 and 0.975], show the 95% confidence interval for the coefficient — if this interval does not include zero, the effect is likely meaningful.\nThe line labeled Group Var shows the estimated variance of the random intercepts — in this case, variation in baseline income between individuals. The second number reported is the standard error associated with this estimate, which indicates how much uncertainty there is in the estimate of the variance.\nIf you like, you can print out all the variances for the random effects. They are not explicity shown in the summary, but you can access them through the model’s random_effects attribute:\nresult.random_effects\nFinally, the model as is does not include random slopes, meaning that the effect of years after graduation is assumed to be the same for all individuals. If you want to allow for different slopes for each individual, you can modify the model to include random slopes as well. This would require changing the formula and the groups argument accordingly. Also, result.random_effects will then contain not only the random intercepts, but also the random slopes for each individual.\n\nmodel = smf.mixedlm(\n    \"income ~ years_after_grad * major\",\n    data=df,\n    groups=df[\"person\"],\n    re_formula=\"~years_after_grad\"\n)\nresult = model.fit()\nprint(result.summary())\n\n/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n  warnings.warn(\n\n\n                            Mixed Linear Model Regression Results\n==============================================================================================\nModel:                         MixedLM            Dependent Variable:            income       \nNo. Observations:              239                Method:                        REML         \nNo. Groups:                    90                 Scale:                         10125672.1682\nMin. group size:               1                  Log-Likelihood:                -2323.7559   \nMax. group size:               4                  Converged:                     Yes          \nMean group size:               2.7                                                            \n----------------------------------------------------------------------------------------------\n                                           Coef.     Std.Err.   z    P&gt;|z|   [0.025    0.975] \n----------------------------------------------------------------------------------------------\nIntercept                                  25133.841 1208.050 20.805 0.000 22766.106 27501.576\nmajor[T.Magic]                             -2805.540 1811.051 -1.549 0.121 -6355.135   744.055\nmajor[T.Dragon Taming]                      5980.367 1767.166  3.384 0.001  2516.786  9443.949\nyears_after_grad                             731.399   84.211  8.685 0.000   566.349   896.450\nyears_after_grad:major[T.Magic]              611.065  126.072  4.847 0.000   363.969   858.161\nyears_after_grad:major[T.Dragon Taming]     -329.530  122.977 -2.680 0.007  -570.561   -88.498\nGroup Var                               22392488.656 1835.422                                 \nGroup x years_after_grad Cov               90328.607   75.664                                 \nyears_after_grad Var                       39074.487    7.401                                 \n==============================================================================================",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#back-to-ols",
    "href": "regression/mixed-model.html#back-to-ols",
    "title": "17  linear mixed effect model",
    "section": "17.6 back to OLS",
    "text": "17.6 back to OLS\nIf you went this far, and now realized you don’t care about random effects, you can just use the statsmodels function smf.ols to fit an ordinary least squares regression model. The syntax is similar, but without the groups argument.\n\nimport statsmodels.formula.api as smf\n\n# formula with main effects and interaction\nformula = \"income ~ years_after_grad * major\"\n\n# fit the model with OLS (no random effects)\nols_model = smf.ols(formula, data=df)\nols_result = ols_model.fit()\n\n# print summary\nprint(ols_result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.455\nModel:                            OLS   Adj. R-squared:                  0.443\nMethod:                 Least Squares   F-statistic:                     38.85\nDate:                Tue, 24 Jun 2025   Prob (F-statistic):           6.27e-29\nTime:                        16:16:38   Log-Likelihood:                -2437.0\nNo. Observations:                 239   AIC:                             4886.\nDf Residuals:                     233   BIC:                             4907.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================================\n                                              coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------\nIntercept                                2.486e+04   1450.267     17.141      0.000     2.2e+04    2.77e+04\nmajor[T.Magic]                          -4402.0846   2281.475     -1.929      0.055   -8897.041      92.872\nmajor[T.Dragon Taming]                   7696.8705   2167.061      3.552      0.000    3427.332     1.2e+04\nyears_after_grad                          778.4674    123.280      6.315      0.000     535.582    1021.352\nyears_after_grad:major[T.Magic]           758.4393    185.397      4.091      0.000     393.170    1123.708\nyears_after_grad:major[T.Dragon Taming]  -510.1096    183.456     -2.781      0.006    -871.553    -148.666\n==============================================================================\nOmnibus:                        2.143   Durbin-Watson:                   1.088\nProb(Omnibus):                  0.343   Jarque-Bera (JB):                2.132\nSkew:                           0.176   Prob(JB):                        0.344\nKurtosis:                       2.699   Cond. No.                         93.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html",
    "href": "regression/logistic-regression.html",
    "title": "18  logistic regression",
    "section": "",
    "text": "18.1 question\nWe are given a list of heights for men and women. Given one more data point (180 cm), could we assign a probability that it belongs to either class?",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#discriminative-model",
    "href": "regression/logistic-regression.html#discriminative-model",
    "title": "18  logistic regression",
    "section": "18.2 discriminative model",
    "text": "18.2 discriminative model\nThe idea behind the logistic regression is to find a boundary between our two classes (here men and women). The logistic regression models the probability of a class given a data point, i.e. P(y|x). We can use the logistic function to model this probability:\nP(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\nwhere y is the class (0=women, 1=men), x is the data point (height), and \\beta_0 and \\beta_1 are the parameters of the model.\nOur goal is to find the best s-shaped curve that describes the data. This is done by finding the parameters \\beta_0 and \\beta_1 that maximize the likelihood of the data.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import norm\n\n\n\n\ngenerate data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 20.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\nN_boys = 150\nN_girls = 200\nnp.random.seed(314)  # set scipy seed for reproducibility\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n# pandas dataframe with the two samples in it\ndf = pd.DataFrame({\n    'height (cm)': np.concatenate([sample_boys, sample_girls]),\n    'sex': ['M'] * N_boys + ['F'] * N_girls\n})\ndf = df.sample(frac=1, random_state=314).reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\n\nheight (cm)\nsex\n\n\n\n\n0\n178.558416\nM\n\n\n1\n173.334306\nM\n\n\n2\n183.084154\nM\n\n\n3\n178.236047\nF\n\n\n4\n175.868642\nM\n\n\n...\n...\n...\n\n\n345\n177.387837\nM\n\n\n346\n157.122325\nF\n\n\n347\n166.891746\nF\n\n\n348\n181.090312\nM\n\n\n349\n171.479631\nM\n\n\n\n\n350 rows × 2 columns\n\n\n\n\nX = df['height (cm)'].values.reshape(-1, 1)\ny = df['sex'].map({'M': 1, 'F': 0}).values\nlog_reg_2 = LogisticRegression(penalty=None, solver = 'newton-cg', max_iter= 150).fit(X,y)\nbeta1 = log_reg_2.coef_[0][0]\nbeta0 = log_reg_2.intercept_[0]\ndef logistic_function(x, beta0, beta1):\n    return 1 / (1 + np.exp(-(beta0 + beta1 * x)))\n\n\n\nplot\nfig, ax = plt.subplots()\nax.plot(sample_girls, np.zeros_like(sample_girls),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:orange')\nax.plot(sample_boys, np.ones_like(sample_boys),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:blue')\n\nx_array = np.linspace(140, 200, 300).reshape(-1, 1)\ny_proba = log_reg_2.predict_proba(x_array)[:, 1]\n# ax.plot(x_array, y_proba, color='black')\nax.plot(x_array, logistic_function(x_array, beta0, beta1), color='black', label=\"best fit\")\nax.plot(x_array, logistic_function(x_array, beta0+2, beta1), color='gray', linestyle='--', label=\"worse 1\")\nax.plot(x_array, logistic_function(x_array, beta0, beta1-0.02), color='gray', linestyle=':', label=\"worse 2\")\nax.legend(frameon=False)",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#likelihood",
    "href": "regression/logistic-regression.html#likelihood",
    "title": "18  logistic regression",
    "section": "18.3 likelihood",
    "text": "18.3 likelihood\nHow do we know the parameters of the best s-shaped curve? Let’s pretend we have only three data points:\n\nMan, 180 cm. Data point (180,1).\nMan, 170 cm. Data point (170,1).\nWoman, 165 cm. Data point (165,0).\n\n\n\nrecalculate logistic regression with 3 points\nx3 = np.array([180.0, 170.0, 165.0])\ny3 = np.array([1, 1, 0])\nlog_reg_3 = LogisticRegression(penalty=None, solver = 'newton-cg', max_iter= 150).fit(x3.reshape(-1, 1),y3)\nbeta1 = log_reg_3.coef_[0][0]\nbeta0 = log_reg_3.intercept_[0]\n\n\n\n\nplot\nfig, ax = plt.subplots(1, 3, figsize=(10, 5), sharex=True, sharey=True)\nx_array = np.linspace(140, 200, 300)\n\nya = logistic_function(x_array, beta0, beta1)\nyb = logistic_function(x_array, beta0+344, beta1*0.28)\nyc = logistic_function(x_array, beta0+450, beta1*0.06)\nyhat3a = logistic_function(x3, beta0, beta1)\nyhat3b = logistic_function(x3, beta0+344, beta1*0.28)\nyhat3c = logistic_function(x3, beta0+450, beta1*0.06)\n\nfor axi in ax:\n    axi.plot(x3, y3,\n            linestyle='None', marker='o',\n            markerfacecolor='none', markeredgecolor='black')\n\nax[0].plot(x_array, ya, color='black')\nax[1].plot(x_array, yb, color='gray', linestyle='--')\nax[2].plot(x_array, yc, color='gray', linestyle=':')\n\nfor i in range(len(x3)):\n    ax[0].plot([x3[i], x3[i]], [y3[i], yhat3a[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[1].plot([x3[i], x3[i]], [y3[i], yhat3b[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[2].plot([x3[i], x3[i]], [y3[i], yhat3c[i]], color='tab:red', linestyle='-', lw=0.5)\n\nax[0].set(xlabel=\"height (cm)\",\n          ylabel=\"P\",\n          title=\"best fit\")\nax[1].set(xlabel=\"height (cm)\",\n          title=\"worse 1\")\nax[2].set(xlabel=\"height (cm)\",\n            title=\"worse 2\")\n\n\n\n\n\n\n\n\n\nAs usual, our task in performing the regression is to find the parameters \\beta_0 and \\beta_1 that minimize the distance between the model and the data (the residual). See the figure above, we plotted the same three data points, and in each panel we see a different s-shaped curve (black) and the distance between the model and the data (red lines).\n[Note: this time, because we have only three data points, the best fit gave us an extremely sharp logistic function, that neatly discriminates between the data points. In the first example, the function was much more “shallow”, because of the overlap between the Men and Women datasets.]\nIn the logistic regression, instead of minimizing the residual, we maximize the likelihood of the data given the model parameters. The likelihood is the complement of the residual, see the thick red bars in the figure below.\n\n\nplot\nfig, ax = plt.subplots(1, 3, figsize=(10, 5), sharex=True, sharey=True)\n\nfor axi in ax:\n    axi.plot(x3, y3,\n            linestyle='None', marker='o',\n            markerfacecolor='none', markeredgecolor='black')\n\nax[0].plot(x_array, ya, color='black')\nax[1].plot(x_array, yb, color='gray', linestyle='--')\nax[2].plot(x_array, yc, color='gray', linestyle=':')\n\nfor i in range(len(x3)):\n    ax[0].plot([x3[i], x3[i]], [y3[i], yhat3a[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[0].plot([x3[i], x3[i]], [not bool(y3[i]), yhat3a[i]], color='tab:red', linestyle='-', lw=2)\n    ax[1].plot([x3[i], x3[i]], [y3[i], yhat3b[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[1].plot([x3[i], x3[i]], [not bool(y3[i]), yhat3b[i]], color='tab:red', linestyle='-', lw=2)\n    ax[2].plot([x3[i], x3[i]], [y3[i], yhat3c[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[2].plot([x3[i], x3[i]], [not bool(y3[i]), yhat3c[i]], color='tab:red', linestyle='-', lw=2)\n\nax[2].plot([], [], color='tab:red', linestyle='-', lw=0.5, label=\"residual\")\nax[2].plot([], [], color='tab:red', linestyle='-', lw=2, label=\"likelihood\")\n\nax[0].set(xlabel=\"height (cm)\",\n          ylabel=\"P\",\n          title=\"best fit\")\nax[1].set(xlabel=\"height (cm)\",\n          title=\"worse 1\")\nax[2].set(xlabel=\"height (cm)\",\n            title=\"worse 2\")\nax[2].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)\n\n\n\n\n\n\n\n\n\nWhat is the probability that we would measure the observed data, given the model parameters? For a single data point (height, class), the length of the red bars can be described by the formula below:\n\nL(y_i|P_i) = P_i^{y_i} (1-P_i)^{1-y_i}.\n\nFor instance, if the data point corresponds to a man (y_i=1), we have L=P_i. The likelihood (thick red bars) for men is just the value of the logistic function for that value of x. For women (y_i=0), we have L=1-P_i. The likelihood for women is just the complement (one minus) of the logistic function.\nAssuming that each data point is independent, the likelihood of the entire dataset is the product of the likelihoods of each data point:\n\nL(\\beta_0, \\beta_1) = \\prod_{i=1}^{N} P_i^{y_i} (1-P_i)^{1-y_i}.\n\nIn the example below, the likelihood of the entire dataset for each panel is as follows:\n\n\ncompute likelihoods\nLa = 1.0\nLb = 1.0\nLc = 1.0\nfor i in range(len(x3)):\n    La = La * yhat3a[i]**y3[i] * (1-yhat3a[i])**(1-y3[i])\n    Lb = Lb * yhat3b[i]**y3[i] * (1-yhat3b[i])**(1-y3[i])\n    Lc = Lc * yhat3c[i]**y3[i] * (1-yhat3c[i])**(1-y3[i])\nprint(\"Likelihood for best fit: \", La)\nprint(\"Likelihood for worse 1: \", Lb)\nprint(\"Likelihood for worse 2: \", Lc)\n\n\nLikelihood for best fit:  0.9984099891897481\nLikelihood for worse 1:  0.7711068593851899\nLikelihood for worse 2:  0.3173593316343797\n\n\nAs we increase the number of data points, the likelihood becomes very small (because we are multiplying many numbers between 0 and 1). To avoid numerical issues, we usually work with the log-likelihood.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#log-likelihood",
    "href": "regression/logistic-regression.html#log-likelihood",
    "title": "18  logistic regression",
    "section": "18.4 log-likelihood",
    "text": "18.4 log-likelihood\nThe log-likelihood is the logarithm of the likelihood:\n\n\\ell(\\beta_0, \\beta_1) = \\log L(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} P_i^{y_i} (1-P_i)^{1-y_i}\n\nUsing the properties of logarithms, we can rewrite the log-likelihood as follows:\n\n\\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} \\left( y_i \\log P_i + (1-y_i) \\log (1-P_i) \\right).",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#binary-cross-entropy-or-log-loss",
    "href": "regression/logistic-regression.html#binary-cross-entropy-or-log-loss",
    "title": "18  logistic regression",
    "section": "18.5 binary cross-entropy, or log loss",
    "text": "18.5 binary cross-entropy, or log loss\nWe can use gradient descent to find the parameters that maximize the log-likelihood. Most implementations of gradient descent are designed to minimize a cost function. Therefore, instead of maximizing the log-likelihood, we can minimize the negative log-likelihood:\n\nJ(\\beta_0, \\beta_1) = -\\ell(\\beta_0, \\beta_1) = -\\sum_{i=1}^{N} \\left( y_i \\log P_i + (1-y_i) \\log (1-P_i) \\right).\n This cost function is also known as binary cross-entropy or log loss.\nIt turns out that taking the log of the likelihood is very convenient. What was before only a trick to avoid numerical issues, now has a nice interpretation. The cross-entropy can be thought of as a measure of “surprise”. The more the model is surprised by the data, the higher the cross-entropy, and the poorer the fit. The less surprised the model is by the data, the lower the cross-entropy, and the better the fit.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#wrapping-up",
    "href": "regression/logistic-regression.html#wrapping-up",
    "title": "18  logistic regression",
    "section": "18.6 wrapping up",
    "text": "18.6 wrapping up\nFrom the provided data:\n\nWhat is the probability that a person whose height is 180 cm is a man?\nIf we had to choose one height to discriminate between men and women, what would it be?\n\nLet’s run the code for the logistic regression again:\n\n\ncompute logistic regression and plot\nX = df['height (cm)'].values.reshape(-1, 1)\ny = df['sex'].map({'M': 1, 'F': 0}).values\nlog_reg_2 = LogisticRegression(penalty=None, solver = 'newton-cg', max_iter= 150).fit(X,y)\nbeta1 = log_reg_2.coef_[0][0]\nbeta0 = log_reg_2.intercept_[0]\n\n#| code-summary: \"plot \" \nfig, ax = plt.subplots()\nax.plot(sample_girls, np.zeros_like(sample_girls),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:orange')\nax.plot(sample_boys, np.ones_like(sample_boys),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:blue')\n\nx_array = np.linspace(140, 200, 300).reshape(-1, 1)\ny_proba = log_reg_2.predict_proba(x_array)[:, 1]\n# ax.plot(x_array, y_proba, color='black')\nax.plot(x_array, logistic_function(x_array, beta0, beta1), color='black', label=\"best fit\")\n\nh = 180.0\np180 = log_reg_2.predict_proba(np.array([[h]]))[0, 1]\nax.plot([h, h], [0, p180], color='gray', linestyle=':')\nax.plot([np.min(x_array), h], [p180, p180], color='gray', linestyle=':')\nax.text(h+1, p180-0.05, f\"P({h} cm)={p180:.2f}\", color='gray', fontsize=12)\n\np_50percent = -beta0 / beta1\nax.plot([p_50percent, p_50percent], [0, 0.5], color='gray', linestyle=':')\nax.plot([np.min(x_array), p_50percent], [0.5, 0.5], color='gray', linestyle=':')\nax.text(p_50percent-1, 0.5+0.05, f\"P({p_50percent:.0f} cm)=0.5\", color='gray', fontsize=12, ha='right')\n\n\nax.set(xlim=(140, 200),\n       xlabel=\"height (cm)\",\n       ylabel=\"P\",)\n\n\n\n\n\n\n\n\n\nAnswers:\n\nThe probability that a person 180 cm tall is a man is 92%.\nThe height that best discriminates between men (above) and women (below) is 171 cm.\n\nThis last result follows directly from:\n\\begin{align*}\nP(x) = \\frac{1}{1+\\exp[-(\\beta_0+\\beta_1 x)]} &= \\frac{1}{2} \\\\\n& \\text{therefore} \\\\\n1+\\exp[-(\\beta_0+\\beta_1 x)] &= 2 \\\\\n\\exp[-(\\beta_0+\\beta_1 x)] &= 1 \\\\\n-(\\beta_0+\\beta_1 x) &= 0 \\\\\nx &= -\\frac{\\beta_0}{\\beta_1}\n\\end{align*}\nCompare this result with the one we obtained with the parametric generative model discussed in the Bayes’ theorem section.\nIf you want to see a nice tutorial, see Dr. Roi Yehoshua’s “Mastering Logistic Regression”.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#connection-to-neural-networks",
    "href": "regression/logistic-regression.html#connection-to-neural-networks",
    "title": "18  logistic regression",
    "section": "18.7 connection to neural networks",
    "text": "18.7 connection to neural networks\nThe logistic regression can be understood as a single-layer perceptron neural network model. This is to say, a neural network with no hidden layers, and a single output neuron that uses the logistic (sigmoid) activation function.\n\n\n\n“source: https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae/”",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic_2d.html",
    "href": "regression/logistic_2d.html",
    "title": "19  logistic 2d",
    "section": "",
    "text": "19.1 statistics\nThe figure below represents the distribution of height and weight for boys aged 10 and 13 years old.\nOur job is to find a decision boundary that separates the two classes. Fundamentally, this is the same as the logistic regression we saw in the previous chapter, but now we have two features instead of one. There are two ways to equivalent ways to describe this, the statisics and the machine learning way.\nWe call our two features, height and weight, x_1 and x_2. We can write the logistic regression model as\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\nwhere p is the probability of being 13 years old. The left hand side is called the log-odds or logit. The right hand side is a linear combination of the features.\nThis is, of course, equivalent to the expression with the sigmoid function:\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}}\nSpeaking a “statistics language”, this linear relationship is expressed by writing everything in matrix form: \nz = X\\beta,\n where X is the design matrix, \\beta is the vector of coefficients, and z is the linear predictor.\n\\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\end{pmatrix} = \\begin{pmatrix} 1 & x_{11} & x_{12} \\\\ 1 & x_{21} & x_{22} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_{n1} & x_{n2} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} \\beta_0 + \\beta_1x_{11} + \\beta_2x_{12} \\\\ \\beta_0 + \\beta_1x_{21} + \\beta_2x_{22} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1x_{n1} + \\beta_2x_{n2} \\end{pmatrix}",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>logistic 2d</span>"
    ]
  },
  {
    "objectID": "regression/logistic_2d.html#machine-learning",
    "href": "regression/logistic_2d.html#machine-learning",
    "title": "19  logistic 2d",
    "section": "19.2 machine learning",
    "text": "19.2 machine learning\nIn machine learning, we often call the intercept term the bias, and we call the coefficients weights. We can write the linear predictor as \nz = w_1 x_1 + w_2 x_2 + b\n where w_1 and w_2 are the weights, and b is the bias. In matrix form: \nz = w^T x + b,\n which expands to \n\\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\end{pmatrix} = \\begin{pmatrix} w_1 & w_2 \\end{pmatrix} \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\\\ \\vdots & \\vdots \\\\ x_{n1} & x_{n2} \\end{pmatrix} + \\begin{pmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{pmatrix} = \\begin{pmatrix} w_1x_{11} + w_2x_{12} + b \\\\ w_1x_{21} + w_2x_{22} + b \\\\ \\vdots \\\\ w_1x_{n1} + w_2x_{n2} + b \\end{pmatrix}\n\nThis is the same as the statistics formulation, just with different names for the parameters.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>logistic 2d</span>"
    ]
  },
  {
    "objectID": "regression/logistic_2d.html#solving",
    "href": "regression/logistic_2d.html#solving",
    "title": "19  logistic 2d",
    "section": "19.3 solving",
    "text": "19.3 solving\nI boy from either 7th grade (13 years old) or 5th grade (10 years old) is randomly selected. Given his height (150 cm) and weight (45 kg), we want to predict his age group.\n\n\nfinding the decision boundary\nfig, ax = plt.subplots(figsize=(8, 6))\nX = df[['weight', 'height']]\ny = df['age']\nmodel = LogisticRegression()\nmodel.fit(X, y)\nxx, yy = np.meshgrid(np.linspace(10, 80, 100), np.linspace(100, 200, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\npredict_df = pd.DataFrame(grid_points, columns=['weight', 'height'])\nZ = model.predict(predict_df)\nZ = Z.reshape(xx.shape)\nZ_prob = model.predict_proba(predict_df)[:, 1]\nZ = Z_prob.reshape(xx.shape)\n# contour_fill = ax.contourf(xx, yy, Z, levels=np.arange(0, 1.1, 0.2), cmap='RdBu_r', alpha=0.8)\ncont = ax.contour(xx, yy, Z, levels=[0.1, 0.5, 0.9], colors=['gray', 'black', 'gray'], linestyles=['--', '-', '--'], linewidths=[0.8, 2, 0.8])\nax.clabel(cont, fmt='p=%1.1f', inline=True, fontsize=10)\n\nsns.scatterplot(data=df, x='weight', y='height', hue='age', ax=ax)\nax.set(xlabel='weight (kg)',\n       ylabel='height (cm)');\n\nh1 = 150\nw1 = 45\ndf1 = pd.DataFrame([[w1, h1]], columns=['weight', 'height'])\nax.plot([w1], [h1], ls='None', marker='o', markersize=10, markerfacecolor='None', markeredgecolor='red', label=\"new data point\")\nax.legend(title='age', loc='upper right', frameon=False)\np1 = model.predict_proba(df1)[0, 1]\nprint(f\"Predicted probability of being 13 years old: {p1:.3f}\")\n\n\nPredicted probability of being 13 years old: 0.848\n\n\n\n\n\n\n\n\n\nThe thick line in the figure above is the decision boundary, where the probability of being 13 years old is 0.5. The equation of the line is\n\n0 =  w_1 x_1 + w_2 x_2 + b,\n\nwere feature x_1 is weight and feature x_2 is height. The weights w_1,w_2 and bias b are\n\nmodel.coef_, model.intercept_\n\n(array([[0.19047627, 0.37131303]]), array([-62.54777199]))",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>logistic 2d</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html",
    "href": "correlation/correlation.html",
    "title": "20  correlation",
    "section": "",
    "text": "20.1 variance\nTo understand correlation, we need to start with variance. Let’s say X is a random variable with mean \\mu. The variance of X, denoted \\text{var}(X)=\\sigma^2, is defined as the expected value of the squared deviation from the mean:\n\\sigma^2 = \\text{var}(X) = E[(X - \\mu)^2] = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu)^2.\nI should point out that the formula above is for the population variance. If we are working with a sample, we would use N-1 in the denominator instead of N to get an unbiased estimate of the population variance. Also, the mean and variance for the sample are denoted \\bar{X} and s^2 respectively. In any case, let’s continue with the population variance for simplicity.\nIn simple words, the variance measures how much the values of X deviate from the mean \\mu. A high variance indicates that the data points are spread out over a wider range of values, while a low variance indicates that they are closer to the mean.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#covariance",
    "href": "correlation/correlation.html#covariance",
    "title": "20  correlation",
    "section": "20.2 covariance",
    "text": "20.2 covariance\nNow, let’s consider two random variables, X and Y, with means \\mu_X and \\mu_Y. The covariance between X and Y, denoted \\text{cov}(X, Y), is defined as:\n\n\\text{cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu_X)(Y_i - \\mu_Y).\n\nA high covariance indicates that when X is above its mean, Y tends to be above its mean as well (and vice versa). A low (or negative) covariance indicates that when X is above its mean, Y tends to be below its mean.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#correlation",
    "href": "correlation/correlation.html#correlation",
    "title": "20  correlation",
    "section": "20.3 correlation",
    "text": "20.3 correlation\nThe covariance can be any value, making it difficult to interpret. To standardize the measure, we use the correlation coefficient, denoted \\rho (for population) or r (for sample). The correlation coefficient is defined as:\n\\begin{align*}\n\\rho_{X,Y} &= \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} \\\\\n           &= E\\left[\\frac{(X - \\mu_X)}{\\sigma_X}\\frac{(Y - \\mu_Y)}{\\sigma_Y}\\right] \\\\\n           &= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{X_i - \\mu_X}{\\sigma_x}\\frac{Y_i - \\mu_Y}{\\sigma_Y}.\n\\end{align*}\nwhere \\sigma_X and \\sigma_Y are the standard deviations of X and Y, respectively.\nSomething becomes clear now. If we calculate the correlation of X with itself, we get:\n\n\\rho_{X,X} = \\frac{\\text{cov}(X, X)}{\\sigma_X \\sigma_X} = \\frac{\\text{var}(X)}{\\sigma_X^2} = 1.\n\nThe highest possible correlation is 1, which indicates a perfect positive linear relationship between the two variables. The lowest possible correlation is -1, which indicates a perfect negative linear relationship. A correlation of 0 indicates no linear relationship between the variables.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#pearson-correlation-coefficient",
    "href": "correlation/correlation.html#pearson-correlation-coefficient",
    "title": "20  correlation",
    "section": "20.4 Pearson correlation coefficient",
    "text": "20.4 Pearson correlation coefficient\nWhen we say “correlation”, we usually mean the Pearson correlation coefficient, which is the formula given above. Pearson invented this, so it’s named after him. There are other types of correlation coefficients, such as Spearman’s rank correlation coefficient and Kendall’s tau coefficient, which are used for non-parametric data or ordinal data.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#covariance-of-z-scored-variables",
    "href": "correlation/correlation.html#covariance-of-z-scored-variables",
    "title": "20  correlation",
    "section": "20.5 covariance of z-scored variables",
    "text": "20.5 covariance of z-scored variables\nNotice that the correlation formula can be interpreted as the covariance of the z-scored variables. The z-score of a variable X is defined as:\n\nZ_X = \\frac{X - \\mu_X}{\\sigma_X}\n\nThus, the correlation can be rewritten as:\n\n\\rho_{X,Y} = \\text{cov}(Z_X, Z_Y) = \\frac{1}{N} \\sum_{i=1}^{N} Z_{X_i} Z_{Y_i}\n\nIt is quite easy to compute the correlation on the computer. If X and Y are two arrays, first we standardize (z-score) them, then we compute their dot product (sum of piecewise multiplication), and finally we divide by N (or N-1 for sample correlation).",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#linearity",
    "href": "correlation/correlation.html#linearity",
    "title": "20  correlation",
    "section": "20.6 linearity",
    "text": "20.6 linearity\nThe Pearson correlation coefficient measures the strength and direction of a linear relationship between two variables. It does not capture non-linear relationships. For example, if Y = X^2, the correlation between X and Y may be low or even zero, despite a clear non-linear relationship. When we say “correlation”, it is usually implicit that we are referring to linear correlation.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html",
    "href": "correlation/linear_regression.html",
    "title": "21  correlation and linear regression",
    "section": "",
    "text": "21.1 prelude: finding the intercept and slope\nThe correlation coefficient is closely related to linear regression. We will see below a few instances of this relationship.\nLet’s derive the formulas for the intercept and slope of the regression line. We want to minimize the sum of squared residuals L:\nL = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2,\n\\tag{1}\nwhere\n\\hat{y}_i = \\beta_0 + \\beta_1 x_i.\n\\tag{2}\nTo find the optimal values of \\beta_0 and \\beta_1, we take the partial derivatives of L with respect to \\beta_0 and \\beta_1, set them to zero, and solve the resulting equations.\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_0} &= 0 \\tag{3a}\\\\\n\\frac{\\partial L}{\\partial \\beta_1} &= 0 \\tag{3b}\n\\end{align*}\nWe already did that for a general case, but the calculation had the variables in vector/matrix form. Here we will do it for the simple case of one predictor variable, so that we can see the relationship with correlation more clearly.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html#prelude-finding-the-intercept-and-slope",
    "href": "correlation/linear_regression.html#prelude-finding-the-intercept-and-slope",
    "title": "21  correlation and linear regression",
    "section": "",
    "text": "21.1.1 intercept\nLet’s start with Eq. (3a), and substitute into it Eq. (2):\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_0} &= \\frac{\\partial}{\\partial \\beta_0} (y_i - \\hat{y}_i)^2 \\tag{4a} \\\\\n&= \\frac{\\partial}{\\partial \\beta_0} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\tag{4b} \\\\\n&= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\tag{4c}\n\\end{align*}\nEliminating the constant factor -2 and expanding the summation, we get:\n\n\\sum_{i=1}^n y_i - n \\beta_0 - \\beta_1 \\sum_{i=1}^n x_i = 0\n\\tag{5}\n\nWe now divide by n and rearrange to isolate \\beta_0:\n\n\n\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n\\tag{6}\n\n\nNote: we can rewrite equation (4c) as\n\n\\sum_{i=1}^n (y_i - \\hat{y}_i) = \\sum_{i=1}^n \\text{residuals} = 0,\n\nwhich is a nice thing to know.\n\n\n21.1.2 slope\nNow let’s move on to Eq. (3b), and substitute the result of Eq. (6) into it:\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1} (y_i - \\hat{y}_i)^2 \\tag{7a} \\\\\n&= \\frac{\\partial}{\\partial \\beta_1} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\tag{7b} \\\\\n&= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) x_i = 0 \\tag{7c} \\\\\n&= -2 \\sum_{i=1}^n (y_i - \\bar{y} + \\beta_1 \\bar{x} - \\beta_1 x_i) x_i = 0 \\tag{7d}\n\\end{align*}\nEliminating the constant factor 2 and expanding the summation, we get:\n\n-\\sum_{i=1}^n x_i y_i + \\sum_{i=1}^n x_i \\bar{y} - \\beta_1 \\sum_{i=1}^n x_i \\bar{x} + \\beta_1 \\sum_{i=1}^n x_i^2 = 0\n\\tag{8}\n\nLet’s group the terms involving \\beta_1 on one side and the rest on the other side:\n\n\\beta_1 \\left( \\sum_{i=1}^n x_i^2 - \\sum_{i=1}^n x_i \\bar{x} \\right) = \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\bar{y}\n\\tag{9}\n\nIsolating \\beta_1, we have:\n\n\\beta_1 = \\frac{\\sum x_i y_i - \\sum x_i \\bar{y}}{\\sum x_i^2 - \\sum x_i \\bar{x}} = \\frac{\\text{numerator}}{\\text{denominator}}\n\\tag{10}\n\nIt’s easier to interpret the numerator and denominator separately. To each we will add and subtract a term that will allow us to express them in simpler forms.\nNumerator:\n\n\\text{numerator} = \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\bar{y} + \\sum_{i=1}^n \\bar{x} y_i - \\sum_{i=1}^n \\bar{x} y_i\n\\tag{11}\n\nWe express the third term thus:\n\n\\text{third term} = \\sum_{i=1}^n \\bar{x} y_i = \\bar{x} \\sum_{i=1}^n y_i = n \\bar{x} \\bar{y} = \\sum_{i=1}^n \\bar{x} \\bar{y}\n\\tag{12}\n\nThe numerator now becomes:\n\\begin{align*}\n\\text{numerator} &= \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\bar{y} + \\sum_{i=1}^n \\bar{x} \\bar{y} - \\sum_{i=1}^n \\bar{x} y_i \\tag{13a} \\\\\n&= \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\tag{13b} \\\\\n\\end{align*}\nNow the denominator:\n\n\\text{denominator} = \\sum_{i=1}^n x_i^2 - \\sum_{i=1}^n x_i \\bar{x} + \\sum_{i=1}^n x_i\\bar{x} - \\sum_{i=1}^n x_i\\bar{x}\n\\tag{14}\n\nWe group the second and fourth terms, and express the third term thus:\n\n\\text{third term} = \\sum_{i=1}^n x_i \\bar{x} = \\bar{x} \\sum_{i=1}^n x_i = n \\bar{x}^2 = \\sum_{i=1}^n \\bar{x}^2\n\\tag{15}\n\nThe denominator now becomes:\n\\begin{align*}\n\\text{denominator} &= \\sum_{i=1}^n x_i^2 - 2 \\sum_{i=1}^n x_i \\bar{x} + \\sum_{i=1}^n \\bar{x}^2 \\tag{16a} \\\\\n&= \\sum_{i=1}^n (x_i - \\bar{x})^2 \\tag{16b}\n\\end{align*}\nPutting it all together, we have:\n\n\n\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\tag{17}",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html#slope-and-correlation",
    "href": "correlation/linear_regression.html#slope-and-correlation",
    "title": "21  correlation and linear regression",
    "section": "21.2 slope and correlation",
    "text": "21.2 slope and correlation\nLet’s divide both the numerator and denominator of Eq. (17) by n-1:\n\n\\beta_1 = \\frac{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\tag{18}\n\nThe numerator is the sample covariance \\text{cov}(X, Y), and the denominator is the sample variance \\text{var}(X):\n\n\\beta_1 = \\frac{\\text{cov}(X, Y)}{\\text{var}(X)}\n\\tag{19}\n\nNow, we can express the covariance in terms of the correlation coefficient \\rho_{X,Y} and the standard deviations \\sigma_X and \\sigma_Y (see here):\n\n\\text{cov}(X, Y) = \\rho_{X,Y} \\sigma_X \\sigma_Y\n\\tag{20}\n Substituting Eq. (20) into Eq. (19), we get:\n\n\\beta_1 = \\frac{\\rho_{X,Y} \\sigma_X \\sigma_Y}{\\sigma_X^2}\n\\tag{21}\n\nAnd finally, we have:\n\n\n\\beta_1 = \\rho_{X,Y} \\frac{\\sigma_Y}{\\sigma_X}\n\\tag{22}\n\n\nThis shows that the slope of the regression line is directly proportional to the correlation coefficient. A higher absolute value of the correlation coefficient indicates a steeper slope, while a lower absolute value indicates a flatter slope.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html#r2-square-of-the-correlation-coefficient-r",
    "href": "correlation/linear_regression.html#r2-square-of-the-correlation-coefficient-r",
    "title": "21  correlation and linear regression",
    "section": "21.3 R^2= square of the correlation coefficient r",
    "text": "21.3 R^2= square of the correlation coefficient r\nLet’s start by saying that the correlation coefficient is called \\rho when referring to the population, and r when referring to a sample. I’m playing loose with this convention here, but I hope it’s clear from the context.\nLet’s show now that the coefficient of determination R^2 is equal to the square of the correlation coefficient r.\nWe start with the definition of R^2:\n\nR^2 = 1 - \\frac{\\text{SS}_{\\text{Error}}}{\\text{SS}_{\\text{Total}}} = \\frac{\\text{SS}_{\\text{Total}}+\\text{SS}_{\\text{Error}}}{\\text{SS}_{\\text{Total}}} =  \\frac{\\text{SS}_{\\text{Model}}}{\\text{SS}_{\\text{Total}}}\n\\tag{23}\n\nwhere we used the fact that \\text{SS}_{\\text{Total}} = \\text{SS}_{\\text{Model}} + \\text{SS}_{\\text{Error}}, already seen before.\nWe now substitute into Eq. (23) the definitions of \\text{SS}_{\\text{Model}} and \\text{SS}_{\\text{Total}}:\n\nR^2 = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{24}\n\nWe now substitute into Eq. (24) the expression of \\hat{y}_i from Eq. (2):\n\nR^2 = \\frac{\\sum_{i=1}^n (\\beta_0 + \\beta_1 x_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{25}\n\nNow we substitute into Eq. (25) the expression of \\beta_0 from Eq. (6):\n\nR^2 = \\frac{\\sum_{i=1}^n \\left( \\bar{y} - \\beta_1 \\bar{x} + \\beta_1 x_i - \\bar{y} \\right)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} = \\frac{\\sum_{i=1}^n \\left( \\beta_1 (x_i - \\bar{x}) \\right)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{26}\n\n\\beta_1 is a number, so we can take it out of the summation in the numerator:\n\nR^2 = \\frac{\\beta_1^2 \\sum_{i=1}^n (x_i - \\bar{x})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{26b}\n\nNow, let’s substitute into Eq. (26) the expression of \\beta_1 from Eq. (17):\n\nR^2 = \\frac{\\left( \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)^2 \\sum_{i=1}^n (x_i - \\bar{x})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{27}\n\nWe can simplify Eq. (27) by canceling one instance of the denominator in the squared term with the factor outside the squared term:\n\nR^2 = \\frac{\\left[ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\right]^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{28}\n\nNow, let’s multiply both the numerator and denominator of Eq. (28) by \\frac{1}{(n-1)^2}:\n  \nR^2 = \\frac{\\left[ \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\right]^2}{\\left[ \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\right] \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar{y})^2 \\right]}\n\\tag{29}\n\nThe numerator is the square of the sample covariance \\text{cov}(X, Y), and the denominator is the product of the sample variances \\text{var}(X)=\\sigma_X^2 and \\text{var}(Y)=\\sigma_Y^2:\n\nR^2 = \\frac{\\text{cov}(X, Y)^2}{\\sigma_X^2 \\sigma_Y^2} = \\left( \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} \\right)^2\n\\tag{30}\n\nThis is exactly the square of the correlation coefficient r (see here):\n\n\nR^2 = r^2\n\\tag{31}",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/cosine.html",
    "href": "correlation/cosine.html",
    "title": "22  cosine",
    "section": "",
    "text": "22.1 cosine similarity\nIn the spirit of this website, beautiful things happen when we imagine data in high dimensional spaces. Let’s do that for the correlation between two vectors.\n\\rho(x,y) = \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{x_i - \\bar{x}}{\\sigma_x}\\right)\\left(\\frac{y_i - \\bar{y}}{\\sigma_y}\\right)\n\\tag{1}\nAs we saw before, it is particularly useful to rewrite this formula in terms of z-scored variables:\n\\rho(x,y) = \\frac{1}{N} \\sum_{i=1}^N z_{x_i} z_{y_i}\n\\tag{2}\nwhere z_{x_i} = \\frac{x_i - \\bar{x}}{\\sigma_x} and z_{y_i} = \\frac{y_i - \\bar{y}}{\\sigma_y}.\nNow let’s see the formula for the dot product of two vectors z_x and z_y:\nz_x \\cdot z_y = \\sum_{i=1}^N z_{x_i} z_{y_i} = \\frac{1}{N} \\sum_{i=1}^N z_{x_i} z_{y_i} \\cdot N = \\rho(x,y) \\cdot N\n\\tag{3}\nThere is another formula for the dot product that involves the angle \\theta between the two vectors:\nz_x \\cdot z_y = \\lVert z_x \\rVert \\, \\lVert z_y \\rVert \\cos(\\theta)\n\\tag{4}\nwhere ||z_x|| and ||z_y|| are the magnitudes (or lengths) of the vectors z_x and z_y.\nThe magnitude squared of a z-scored vector is:\n\\begin{align*}\n\\lVert z_x \\rVert ^2 &= \\sum_{i=1}^N z_{x_i}^2 \\\\\n                     &= \\sum_{i=1}^N \\left(\\frac{x_i - \\bar{x}}{\\sigma_x}\\right)^2 \\\\\n                     &= \\frac{1}{\\sigma_x^2} \\sum_{i=1}^N (x_i - \\bar{x})^2 \\\\\n                     &= \\frac{1}{\\sigma_x^2} \\left( \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar{x})^2 \\right) N \\\\\n                     & = N\\frac{\\sigma_x^2}{\\sigma_x^2} \\\\\n                     &= N \\tag{5}\n\\end{align*}\nOf course, the same goes for z_y, so we have \\lVert z_x \\rVert = \\lVert z_y \\rVert = \\sqrt{N}. Substituting this into Eq. (4) gives:\nz_x \\cdot z_y = \\sqrt{N} \\cdot \\sqrt{N} \\cdot \\cos(\\theta) = N \\cos(\\theta)\n\\tag{6}\nFinally, we equate Eqs. (3) and (6):\nThe correlation between two variables is equal to the cosine of the angle between their corresponding z-scored vectors, in a high-dimensional space.\nThe cosine similarity is a measure of similarity between two non-zero vectors. It is defined as:\n\\text{cosine\\_similarity}(x,y) = \\frac{x \\cdot y}{\\lVert x \\rVert \\, \\lVert y \\rVert}\n\\tag{8}\nThis measure is common in text analysis, where a non-zero element of a vector represents the presence of a word in a document, and the value of the element represents the frequency of that word. The cosine similarity measures the cosine of the angle between two vectors, which indicates how similar the two vectors are in terms of their direction, regardless of their magnitude. If we were to z-score the vectors, the absence of a word would be represented by a negative value (below average), which is not useful in this context.\nWhen the vectors are z-scored, the cosine similarity is identical to the Pearson correlation coefficient.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>cosine</span>"
    ]
  },
  {
    "objectID": "correlation/cosine.html#cosine-similarity",
    "href": "correlation/cosine.html#cosine-similarity",
    "title": "22  cosine",
    "section": "",
    "text": "cosine similarity: works on any non-zero vectors, does not require z-scoring. There is no need to alter the reference point.\nPearson correlation: works on z-scored vectors, requires centering and scaling. The reference point is the mean of each variable. Useful when comparing variables with different units or scales.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>cosine</span>"
    ]
  },
  {
    "objectID": "correlation/significance.html",
    "href": "correlation/significance.html",
    "title": "23  significance (p-value)",
    "section": "",
    "text": "Given a correlation coefficient r, we can assess its significance using a p-value. Let’s formulate the hypotheses:\n\nNull Hypothesis (H_0): There is no correlation between the two variables (i.e., r = 0).\nAlternative Hypothesis (H_a): There is a correlation between the two variables (i.e., r \\neq 0).\n\nTo calculate the p-value, we can use the following formula for the test statistic t:\n\nt = \\frac{r \\sqrt{n - 2}}{\\sqrt{1 - r^2}}\n\\tag{1}\n where n is the number of data points.\nThis formula follows the fundamental structure of a t-statistic:\n\nt = \\frac{\\text{Signal}}{\\text{Noise}} = \\frac{\\text{Observed Statistic} - \\text{Null Value}}{\\text{Standard Error of the Statistic}}\n\\tag{2}\n\nLet’s rearrange Eq. (1) to match the structure of Eq. (2):\n\nt = \\frac{r - 0}{\\sqrt{\\frac{1 - r^2}{n - 2}}}\n\\tag{3}\n\nThe numerator is clear enough. Let’s discuss the denominator, which represents the standard error of the correlation coefficient.\nThe term 1 - r^2 is the proportion of unexplained variance in the data. As the correlation r gets stronger (closer to 1 or -1), the unexplained variance gets smaller. This makes intuitive sense: a very strong correlation is less likely to be a result of random chance, so the standard error (noise) should be smaller.\nThe term n−2 is the degrees of freedom. As your sample size n increases, the denominator gets larger, which makes the overall standard error smaller. This also makes sense: a correlation found in a large sample is more reliable and less likely to be a fluke than the same correlation found in a small sample.\nLet’s try a concrete example.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\n\n\ngenerate x and y data with some noise\n# set seed for reproducibility\nnp.random.seed(1)\nN = 100\nx = np.linspace(0, 10, N)\ny = 0.2 * x + 7*np.random.normal(size=x.size)\n\n\n\n\ncalculate a bunch of stuff\n# compute sample z-scores of x, y\nzx = (x - np.mean(x)) / np.std(x, ddof=0)\nzy = (y - np.mean(y)) / np.std(y, ddof=0)\n\n# compute Pearson correlation coefficient\nrho = np.sum(zx * zy) / N\n# compute t-statistic\nt = rho * np.sqrt((N-2) / (1-rho**2))\n# compute two-sided p-value\np = 2 * (1 - stats.t.cdf(np.abs(t), df=N-2))\n\n\n\n\nplot\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.scatter(x, y, label='data', alpha=0.5)\n\n# linear fit and R2 with scipy\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\nax.plot(x, slope*x + intercept, color='red', label=f'linear regression, R²={r_value**2:.2f}')\nax.legend(frameon=False)\n\n# print p-value\nprint(f'our p-value:   {p:.4f}')\nprint(f'scipy p-value: {p_value:.4f}')\n\n# compute correlation coefficients and their p-values\nr = np.corrcoef(x, y)[0,1]\nax.set(xlabel='x',\n       ylabel='y',\n       title=f\"Pearson's r = {rho:.3f}, p-value = {p_value:.4f}\");\n\n\nour p-value:   0.0458\nscipy p-value: 0.0458\n\n\n\n\n\n\n\n\n\nThe linear regression accounts for 4% of the variance in the data, which corresponds to a correlation coefficient of r = 0.2. This correlation is statistically significant at p=0.0458&lt;0.05.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>significance (p-value)</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html",
    "href": "bayes/from-the-ground-up.html",
    "title": "24  Bayes’ theorem from the ground up",
    "section": "",
    "text": "24.1 the scenario\nImagine we’re in a room with a large group of people. We know the group consists of men and women, and we have height measurements for everyone. Someone walks in, we measure their height, but we don’t know if they are a man or a woman. Our goal is to figure out the probability that this person is a man, given their height. For simplicity, let’s say that heights are categorized into three groups: short, medium, and tall. The breakdown of the group is as follows:",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes' theorem from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#the-scenario",
    "href": "bayes/from-the-ground-up.html#the-scenario",
    "title": "24  Bayes’ theorem from the ground up",
    "section": "",
    "text": "Short\nMedium\nTall\n\n\n\n\nMan\n15\n30\n20\n\n\nWoman\n25\n35\n10",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes' theorem from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#joint-and-conditional-probabilities",
    "href": "bayes/from-the-ground-up.html#joint-and-conditional-probabilities",
    "title": "24  Bayes’ theorem from the ground up",
    "section": "24.2 joint and conditional probabilities",
    "text": "24.2 joint and conditional probabilities\nWhat is the probability that a person is both a man and tall?\nThis is the same as asking: what fraction does the rectangle on the bottom left have with respect to the whole area?\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\n\n\n\n\nvisualize\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_aspect('equal')\n\nm1, m2, m3 = 15, 30, 20\nf1, f2, f3 = 25, 35, 10\n\nm = m1 + m2 + m3\nf = f1 + f2 + f3\ntotal = m + f\nh1 = m1 + f1\nh2 = m2 + f2\nh3 = m3 + f3\nh = h1 + h2 + h3\n\np_3 = h3 / total\np_2 = h2 / total\np_1 = h1 / total\np_m = m / total\np_f = f / total\n\np_m_given_1 = m1 / h1\np_f_given_1 = f1 / h1\np_m_given_2 = m2 / h2\np_f_given_2 = f2 / h2\np_m_given_3 = m3 / h3\np_f_given_3 = f3 / h3\n\np_1_given_m = m1 / m\np_2_given_m = m2 / m\np_3_given_m = m3 / m\np_1_given_f = f1 / f\np_2_given_f = f2 / f\np_3_given_f = f3 / f\n\n# tall shaded area\nax.fill_between([0, 1], 0, p_3, color=\"blue\", alpha=0.5, edgecolor=\"none\")\n# medium shaded area\nax.fill_between([0, 1], p_3, p_3 + p_2, color=\"blue\", alpha=0.2, edgecolor=\"none\")\n# man given tall shaded area\nax.fill_between([0, p_m_given_3], 0, p_3, color=\"red\", alpha=0.5, edgecolor=\"none\")\n# man given medium shaded area\nax.fill_between([0, p_m_given_2], p_3, p_3+p_2, color=\"red\", alpha=0.5, edgecolor=\"none\")\n# man given short shaded area\nax.fill_between([0, p_m_given_1], p_3+p_2, 1.0, color=\"red\", alpha=0.5, edgecolor=\"none\")\n\n\nsns.despine(ax=ax, top=True, right=True)\n\n# tall arrow\nax.annotate(\"\",\n            (1.05, 0),\n            (1.05, p_3),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.05, p_3 / 2, f\" tall, {p_3:.2f}\", va=\"center\", rotation=0)\n# medium arrow\nax.annotate(\"\",\n            (1.05, p_3),\n            (1.05, p_2+p_3),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.05, p_3 + (p_2) / 2, f\" medium, {p_2:.2f}\", va=\"center\", rotation=0)\n# short arrow\nax.annotate(\"\",\n            (1.05, p_2+p_3),\n            (1.05, 1.0),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.05, 1.0 - (1.0 - p_2 - p_3) / 2, f\" short, {1.0-p_2-p_3:.2f}\", va=\"center\", rotation=0)\n# man given short arrow\nax.annotate(\"\",\n            (0, 0.90),\n            (p_m_given_1, 0.90),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m_given_1 / 2, 0.92, f\"man | short, {p_m_given_1:.2f}\", ha=\"center\", fontsize=12)\n\n# woman given short arrow\nax.annotate(\"\",\n            (1.0 - p_f_given_1, 0.90),\n            (1.0, 0.90),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f_given_1) / 2, 0.92, f\"woman | short, {p_f_given_1:.2f}\", ha=\"center\", fontsize=12)\n\n# man given medium arrow\nax.annotate(\"\",\n            (0, 0.50),\n            (p_m_given_2, 0.50),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m_given_2 / 2, 0.52, f\"man | medium, {p_m_given_2:.2f}\", ha=\"center\", fontsize=12)\n\n# woman given medium arrow\nax.annotate(\"\",\n            (1.0 - p_f_given_2, 0.50),\n            (1.0, 0.50),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f_given_2) / 2, 0.52, f\"woman | medium, {p_f_given_2:.2f}\", ha=\"center\", fontsize=12)\n\n# man given tall arrow\nax.annotate(\"\",\n            (0, 0.15),\n            (p_m_given_3, 0.15),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m_given_3 / 2, 0.17, f\"man | tall, {p_m_given_3:.2f}\", ha=\"center\", fontsize=12)\n\n# woman given tall arrow\nax.annotate(\"\",\n            (1.0 - p_f_given_3, 0.15),\n            (1.0, 0.15),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f_given_3) / 2, 0.17, f\"woman | tall, {p_f_given_3:.2f}\", ha=\"center\", fontsize=12)\n\n\nax.set(xticks=[],\n       yticks=[],\n       xlabel=\"sex\",\n       ylabel=\"height\",);\n\n\n\n\n\n\n\n\n\n\nThe numbers next to each category denote the proportions. That makes sense: according to the table, most of tall people are men, and most of the short people are women.\n“men | tall” is a short way to write “men given tall”. In simple words, it is the fraction of men, given that we know the person is tall.\n\nThe answer to the question is obvious now. The probability that a person is both man and tall the product of 0.22 with 0.67.\n\n22% of people are tall.\n67% of those are men.\n\nThe answer is 0.22 * 0.67 = 0.1474, or about 15%.\nIn mathematical notation, we write this as:\n\nP(\\text{man } \\cap \\text{ tall}) = P(\\text{tall}) \\cdot P(\\text{man|tall}),\n\\tag{1}\n\nwhere the symbol \\cap means “and”.\n\nP( ) is called the joint probability, because it describes the probability of two events happening together.\nP() is called the conditional probability, because it describes the probability of one event happening, given that another event is already known to have occurred.\n\nWhen Eq. (1) is rewritten in terms of P(\\text{man|tall}), it is called the equation for conditional probability:\n\nP(\\text{man|tall}) = \\frac{P(\\text{man } \\cap \\text{ tall})}{P(\\text{tall})}.\n\n\nOf course, “man” and “tall” are only labels, the general formula we should remember is:\n\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes' theorem from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#different-perspective",
    "href": "bayes/from-the-ground-up.html#different-perspective",
    "title": "24  Bayes’ theorem from the ground up",
    "section": "24.3 different perspective",
    "text": "24.3 different perspective\nWe could have made sense of the data in a different way. Above, we first categorized people by their height, and only then by sex. Let’s try the opposite.\n\n\nvisualize the other way\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_aspect('equal')\n\n# tall given man shaded area\nax.fill_between([0, p_m], 0, p_3_given_m, color=\"blue\", alpha=0.5, edgecolor=\"none\")\n# medium given man shaded area\nax.fill_between([0, p_m], p_3_given_m, p_3_given_m+p_2_given_m, color=\"blue\", alpha=0.2, edgecolor=\"none\")\n\n# man shaded area\nax.fill_between([0, p_m], 0, 1.0, color=\"red\", alpha=0.5, edgecolor=\"none\")\n\n# tall given woman shaded area\nax.fill_between([p_m, 1.0], 0, p_3_given_f, color=\"blue\", alpha=0.5, edgecolor=\"none\")\n# medium given man shaded area\nax.fill_between([p_m, 1.0], p_3_given_f, p_3_given_f+p_2_given_f, color=\"blue\", alpha=0.2, edgecolor=\"none\")\n\nsns.despine(ax=ax, top=True, right=True)\n\n# tall given man arrow\nax.annotate(\"\",\n            (p_m-0.03, 0),\n            (p_m-0.03, p_3_given_m),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m-0.03, p_3_given_m / 2, f\"tall | man, {p_3_given_m:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# medium given man arrow\nax.annotate(\"\",\n            (p_m-0.03, p_3_given_m),\n            (p_m-0.03, p_3_given_m+p_2_given_m),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m-0.03, p_3_given_m + p_2_given_m / 2, f\"medium | man, {p_2_given_m:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# short given man arrow\nax.annotate(\"\",\n            (p_m-0.03, 1.0),\n            (p_m-0.03, 1.0-p_1_given_m),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m-0.03, 1.0 - p_1_given_m / 2, f\"short | man, {p_1_given_m:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n\n# tall given woman arrow\nax.annotate(\"\",\n            (1.0-0.03, 0),\n            (1.0-0.03, p_3_given_f),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0-0.03, p_3_given_f / 2, f\"tall | woman, {p_3_given_f:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# medium given woman arrow\nax.annotate(\"\",\n            (1.0-0.03, p_3_given_f),\n            (1.0-0.03, p_3_given_f+p_2_given_f),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0-0.03, p_3_given_f + p_2_given_f / 2, f\"medium | woman, {p_2_given_f:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# short given woman arrow\nax.annotate(\"\",\n            (1.0-0.03, 1.0),\n            (1.0-0.03, 1.0-p_1_given_f),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0-0.03, 1.0 - p_1_given_f / 2, f\"short | woman, {p_1_given_f:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n\n\n# man arrow\nax.annotate(\"\",\n            (0, 1.05),\n            (p_m, 1.05),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m / 2, 1.07, f\"man, {p_m:.2f}\", ha=\"center\", fontsize=12)\n# woman arrow\nax.annotate(\"\",\n            (1.0 - p_f, 1.05),\n            (1.0, 1.05),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f) / 2, 1.07, f\"woman, {p_f:.2f}\", ha=\"center\", fontsize=12)\n\nax.set(xticks=[],\n       yticks=[],\n       # xlim=(0, 1),\n       # ylim=(0, 1),\n       xlabel=\"sex\",\n       ylabel=\"height\",);\n\n\n\n\n\n\n\n\n\nThe probability that a person is both man and tall is still the area of the purple rectangle on the bottom left. The rectangle has a different shape, but it has to have the same area. The answer to our question now can be understood thus:\n\n48% of people are men.\n31% of those are tall.\n\nThe answer is 0.48 * 0.31 = 15%, exactly the same result as before.\nIn mathematical notation, we write this as:\n\nP(\\text{tall } \\cap \\text{ man}) = P(\\text{man}) \\cdot P(\\text{tall|man}).\n\\tag{2}\n\n\nOne thing should become clear from the images above. The probability that a person is a man, given that they are tall, is not the same as the probability that a person is tall, given that they are a man. In mathematical notation:\n\nP(\\text{man|tall}) \\neq P(\\text{tall|man}).\n\n\nP(\\text{man|tall}): of all tall people, 67% are men.\nP(\\text{tall|man}): of all men, 31% are tall.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes' theorem from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#bayes-theorem",
    "href": "bayes/from-the-ground-up.html#bayes-theorem",
    "title": "24  Bayes’ theorem from the ground up",
    "section": "24.4 Bayes’ theorem",
    "text": "24.4 Bayes’ theorem\nWhen two things are true at the same time, it doesn’t matter the order we choose to write them. In mathematical notation:\n\nP(A \\cap B) = P(B \\cap A).\n\nBecause of this, we equate the right-hand sides of Eqs. (1) and (2), and we get Bayes’ theorem:\n\n\nP(\\text{man|tall}) \\cdot P(\\text{tall}) = P(\\text{tall|man}) \\cdot P(\\text{man})\n\n\nPersonally, I choose to remember this equation, because it is symmetric. But the more common way to write Bayes’ theorem is to solve for P(\\text{man|tall}):\n\nP(\\text{man|tall}) = \\frac{P(\\text{tall|man}) \\cdot P(\\text{man})}{P(\\text{tall})},\n\nor in general terms:\n\n\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}.\n\n\nEach term has a name:\n\nP(A|B) is the posterior. This is what we are trying to find out: the probability of A given that we know B.\nP(B|A) is the likelihood. This is the probability of B given that we know A.\nP(A) is the prior. This is what we know about A before we know anything about B.\nP(B) is the evidence. This is what we know about B before we know anything about A.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes' theorem from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#the-law-of-total-probability",
    "href": "bayes/from-the-ground-up.html#the-law-of-total-probability",
    "title": "24  Bayes’ theorem from the ground up",
    "section": "24.5 the law of total probability",
    "text": "24.5 the law of total probability\nIn the example above, we had enough information to plug all the numbers into Bayes’ theorem. But this is not always the case. Sometimes, we don’t know P(B), the evidence. In this case, we can compute it using the law of total probability. It is best to give a concrete example.\nA famous use of Bayes’ theorem is in desease testing.\n\nA given desease affects 1% of the population.\nA test for the desease is 95% accurate. This means that:\n\nIf a person has the desease, the test will be positive 95% of the time.\nIf a person does not have the desease, the test will be negative 95% of the time.\n\nA person is randomly selected from the population, and they test positive. Should they be worried? What is the probability that they actually have the desease?\n\nLet’s translate this into the language of Bayes’ theorem:\n\nA is the event “the person has the desease”.\nB is the event “the test is positive”.\nWe need to find P(A|B), the probability that the person has the desease, given that they tested positive.\nP(A) = 0.01 is the prior, the probability that a random person has the desease.\nP(B|A) = 0.95 is the likelihood, the probability that the test is positive, given that the person has the desease.\n\nWe don’t know P(B), the evidence, the probability that a random person tests positive. But we can compute it using the law of total probability:\n\\begin{align*}\nP(\\text{test is positive}) &= P(\\text{test is positive } \\textbf{and} \\text{ person has the desease}) \\\\\n                           &+ P(\\text{test is positive } \\textbf{and} \\text{ person doesn't have the desease}).\n\\end{align*}\nThis has to be true, because a person can either have the desease or not. In a more compact form:\n\nP(B) = P(B \\cap A) + P(B \\cap A^c),\n\nwhere A^c is the complement of A, i.e., “the person does not have the desease”.\nUsing the definition of conditional probability, we can rewrite this as: \nP(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c).\n We know all the terms on the right-hand side:\n\nP(B|A) = 0.95, the probability that the test is positive, given that the person has the desease.\nP(A) = 0.01, the probability that a random person has the desease.\nP(A^c) = 0.99, the probability that a random person does not have the desease.\nP(B|A^c) = 0.05, the probability that the test is positive, given that the person does not have the desease. This is 1 minus the accuracy of the test for healthy people.\n\nPlugging in the numbers, we get:\n\nP(B) = 0.95 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.059.\n\nNow we have everything we need to plug the numbers into Bayes’ theorem:\n\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{0.95 \\cdot 0.01}{0.059} = 0.161.\n\nThis means that, even if the person tested positive, there is only a 16.1% chance that they actually have the desease. This is counter-intuitive, but it makes sense when we think about it. The desease is very rare, so even if the test is accurate, most of the positive results will be false positives.\nIn the case that there are several mutually exclusive ways for B to happen, we can generalize the law of total probability:\n\nP(B) = \\sum_i P(B \\cap A_i) = \\sum_i P(B|A_i) \\cdot P(A_i),\n\nwhere the A_i are all the possible ways for B to happen.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Bayes' theorem from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html",
    "href": "bayes/parametric-generative-classification.html",
    "title": "25  parametric generative classification",
    "section": "",
    "text": "25.1 question\nWe are given a list of heights for men and women. Given one more data point (180 cm), could we assign a probability that it belongs to either class?\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm\ngenerate data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 20.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\nN_boys = 150\nN_girls = 200\nnp.random.seed(314)  # set scipy seed for reproducibility\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n# pandas dataframe with the two samples in it\ndf = pd.DataFrame({\n    'height (cm)': np.concatenate([sample_boys, sample_girls]),\n    'sex': ['M'] * N_boys + ['F'] * N_girls\n})\ndf = df.sample(frac=1, random_state=314).reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\n\nheight (cm)\nsex\n\n\n\n\n0\n178.558416\nM\n\n\n1\n173.334306\nM\n\n\n2\n183.084154\nM\n\n\n3\n178.236047\nF\n\n\n4\n175.868642\nM\n\n\n...\n...\n...\n\n\n345\n177.387837\nM\n\n\n346\n157.122325\nF\n\n\n347\n166.891746\nF\n\n\n348\n181.090312\nM\n\n\n349\n171.479631\nM\n\n\n\n\n350 rows × 2 columns",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#explaining-parametric-generative-classification",
    "href": "bayes/parametric-generative-classification.html#explaining-parametric-generative-classification",
    "title": "25  parametric generative classification",
    "section": "25.2 explaining “parametric generative classification”",
    "text": "25.2 explaining “parametric generative classification”\n\nParametric: we assume a specific distribution for the data. In this case, we’ll assume a Gaussian distribution. We call this parametric because the distribution can be fully described by a finite set of parameters (mean and variance for Gaussian).\nGenerative: we model the distribution of each class separately. This would allow us to generate new data points from the learned distributions. “Learned” means estimating the parameters of the distributions from the sample data.\nClassification: we classify a new data point by comparing the likelihoods of it belonging to each class, given the learned distributions.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#visualizing-the-problem",
    "href": "bayes/parametric-generative-classification.html#visualizing-the-problem",
    "title": "25  parametric generative classification",
    "section": "25.3 visualizing the problem",
    "text": "25.3 visualizing the problem\n\n\nShow the code\nare_male = df['sex']=='M'\nboys_sample = df[are_male]['height (cm)'].to_numpy()\ngirls_sample = df[~are_male]['height (cm)'].to_numpy()\n\nxbar_boys = boys_sample.mean()\nxbar_girls = girls_sample.mean()\ns_boys = boys_sample.std(ddof=1)\ns_girls = girls_sample.std(ddof=1)\n\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# plot histogram\nbins = np.arange(135, 210, 5)\nax.hist(boys_sample, bins=bins, alpha=0.3, density=True, label='men', color='tab:blue', histtype='stepfilled')\nax.hist(girls_sample, bins=bins, alpha=0.3, density=True, label='women', color='tab:orange', histtype='stepfilled')\n# plot gaussian pdf based on sample mean and std\nx = np.arange(135, 210, 0.5)\npdf_boys = norm.pdf(x, loc=xbar_boys, scale=s_boys)\npdf_girls = norm.pdf(x, loc=xbar_girls, scale=s_girls)\nax.plot(x, pdf_boys, color='tab:blue')\nax.plot(x, pdf_girls, color='tab:orange')\nh0 = 180\n# plot vertical line at h0\nax.axvline(h0, color='gray', linestyle='--')\n# plot circles where each pdf intersects h0\nlikelihood_boys = norm.pdf(h0, loc=xbar_boys, scale=s_boys)\nlikelihood_girls = norm.pdf(h0, loc=xbar_girls, scale=s_girls)\nax.plot(h0, likelihood_boys, marker='o', color='tab:blue')\nax.plot(h0, likelihood_girls, marker='o', color='tab:orange')\n\nprint(f\"men:   mean={xbar_boys:.1f} cm, std={s_boys:.1f} cm\")\nprint(f\"women: mean={xbar_girls:.1f} cm, std={s_girls:.1f} cm\")\n\nax.legend(frameon=False)\nax.set_xlabel('height (cm)')\nax.set_ylabel('pdf');\n\n\nmen:   mean=177.4 cm, std=6.7 cm\nwomen: mean=163.5 cm, std=7.3 cm\n\n\n\n\n\n\n\n\n\nFrom the sample data, we can compute the mean and standard deviation for each sex (the generative part). We printed these values above. We can then use these parameters to compute the likelihood of the new data point (180 cm) belonging to each class using the Gaussian probability density function (the parametric part). These are plotted as circles in the graph.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#bayes-theorem",
    "href": "bayes/parametric-generative-classification.html#bayes-theorem",
    "title": "25  parametric generative classification",
    "section": "25.4 Bayes’ theorem",
    "text": "25.4 Bayes’ theorem\nWe can then use Bayes’ theorem to compute the posterior probabilities of the new data point belonging to each class (the classification part). Bayes’ theorem states that:\n\nP(\\text{man} | x) = \\frac{P(x | \\text{man})}{P(x)} P(\\text{man})\n\n\nposterior, P(\\text{man} | x). This is what we are looking for, the probability of a new data point corresponding to a man, given that its height is x.\nlikelihood, P(x | \\text{man}). This is the likelihood of observing a height x given that we know it is a man.\nevidence, P(x). This is the total probability of observing a height x across all classes (regardless of sex). It acts as a normalization factor. We calculate the evidence using the law of total probability: \\begin{align*}\nP(x) &= P(x \\cap \\text{man}) + P(x \\cap \\text{woman}) \\\\\n     &=P(x|\\text{man})\\cdot P(\\text{man}) + P(x|\\text{woman})\\cdot P(\\text{woman})\n\\end{align*}\nprior, P(\\text{man}). This is the overall probability of a person being a man in my dataset (regardless of their height).\n\nThink of this as a “battle of likelihoods,” adjusted for the group sizes. You have two groups, men and women, and you’ve modeled their typical heights. When you get a new height, x, you ask two main questions:\n\nLikelihood Question: How “typical” is height x for a man compared to how typical it is for a woman? If men in your data are generally tall and x is a tall height, it’s more likely to be a man. We measure this “typicalness” using a probability distribution.\nPrior Belief Question: In your dataset, are men or women more common? If your dataset contains 90 women and 10 men, any new person is, initially, more likely to be a woman, regardless of their height.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#step-by-step-calculation",
    "href": "bayes/parametric-generative-classification.html#step-by-step-calculation",
    "title": "25  parametric generative classification",
    "section": "25.5 Step-by-Step Calculation",
    "text": "25.5 Step-by-Step Calculation\nStep 1: Model Your Data\nWe assume a Gaussian distribution, and calculate the sample mean and standard deviation for each sex.\n\n\nmen:   mean=177.4 cm, std=6.7 cm\nwomen: mean=163.5 cm, std=7.3 cm\n\n\nStep 2: Calculate the Priors\nThe priors are simply the proportion of each group in the total dataset.\nP(\\text{Man}) = \\frac{N_\\text{men}}{N_\\text{men} + N_\\text{women}}\nP(\\text{Woman}) = \\frac{N_\\text{women}}{N_\\text{men} + N_\\text{women}}\nStep 3: Calculate the Likelihoods\nUsing the normal distribution’s probability density function (PDF), find the likelihood of the new height h for each model. The PDF formula is:\nf(x | \\bar{x}, s) = \\frac{1}{s\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\bar{x}}{s}\\right)^2}\n\nLikelihood for Men: Plug x into the PDF for the men’s model.\nP(x | \\text{Man}) = f(x | \\bar{x}_\\text{men}, s_\\text{men})\nLikelihood for Women: Plug x into the PDF for the women’s model.\nP(x | \\text{Woman}) = f(x | \\bar{x}_\\text{women}, s_\\text{women})\n\nStep 4: Put It All Together\nNow, apply Bayes’ Theorem. The “evidence” term P(x) in the denominator is the sum of all ways you could observe height x:\n\nP(x) = P(x | \\text{Man}) \\cdot P(\\text{Man}) + P(x | \\text{Woman}) \\cdot P(\\text{Woman})\n\nSo, the final calculation for the probability of being a man is:\n\nP(\\text{Man} | x) = \\frac{P(x | \\text{Man}) \\cdot P(\\text{Man})}{P(x | \\text{Man}) \\cdot P(\\text{Man}) + P(x | \\text{Woman}) \\cdot P(\\text{Woman})}\n.\nCrunching the number gives:\n\n\nShow the code\nh0 = 180.0\nlikelihood_boys = norm.pdf(h0, loc=xbar_boys, scale=s_boys)\nlikelihood_girls = norm.pdf(h0, loc=xbar_girls, scale=s_girls)\nprior_boys = N_boys / (N_boys + N_girls)\nprior_girls = N_girls / (N_boys + N_girls)\nevidence = likelihood_boys * prior_boys + likelihood_girls * prior_girls\np_man_given_180 = likelihood_boys * prior_boys / evidence\nprint(f\"Answer: {p_man_given_180 * 100:.2f}%.\")\n\n\nAnswer: 90.92%.\n\n\n\nThe probability that the person is a man, given that their height is 180 cm, is 90.92%",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html",
    "href": "bayes/odds.html",
    "title": "26  odds and log likelihood",
    "section": "",
    "text": "26.1 the scenario\nImagine we are researchers studying a potential link between a specific mutated gene and a certain disease. We have collected data from a sample of 356 people.\nHere’s our data:\nOur goal is to figure out how finding this mutated gene in a person should change our belief about whether they have the disease.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#the-scenario",
    "href": "bayes/odds.html#the-scenario",
    "title": "26  odds and log likelihood",
    "section": "",
    "text": "Has Disease\nNo Disease\n\n\n\n\nHas Mutated Gene\n23\n117\n\n\nNo Mutated Gene\n6\n210",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#prior",
    "href": "bayes/odds.html#prior",
    "title": "26  odds and log likelihood",
    "section": "26.2 prior",
    "text": "26.2 prior\nThe prior probability of someone having the disease is the chance of having the disease before we know anything about their gene status. We can calculate this from our data.\n\nP(\\text{Disease}) = \\frac{\\text{Number of people with the disease}}{\\text{Total number of people}}\n\nFrom our table: \nP(\\text{Disease}) = \\frac{23 + 6}{23 + 117 + 6 + 210} = \\frac{29}{356} \\approx 0.081",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#odds",
    "href": "bayes/odds.html#odds",
    "title": "26  odds and log likelihood",
    "section": "26.3 odds",
    "text": "26.3 odds\nOdds are a different way to express the same information. Odds compare the chance of an event happening to the chance of it not happening.\n\n\\text{Odds} = \\frac{P(\\text{event})}{P(\\text{not event})}\n\nIn our case, “event” is having the disease. Because we have only two choices, the probability of not having the disease is simply 1 - P(\\text{Disease}), therefore:\n\n\\text{Odds(Disease)} = \\frac{P(\\text{Disease})}{1 - P(\\text{Disease})}\n\nPlugging in the numbers:\n\n\\text{Odds(Disease)} = \\frac{29/356}{1 - 29/356} \\approx 0.0887 \\approx \\frac{1}{11}\n\nThis means that for every person with the disease, about 11 do not have it.\n\n26.3.1 log odds\nThe log odds is simply the natural logarithm of the odds.\n\n\\text{Log Odds} = \\ln\\left(\\frac{P}{1-P}\\right)\n\nWe will see soon enough why this is useful. For now, let’s point out that:\n\nif the odds are 1 (meaning a 50/50 chance), the log odds is 0.\nif the odds are greater than 1 (more likely than not), the log odds is positive.\nif the odds are less than 1 (less likely than not), the log odds is negative.\nthe log odds have a symmetric shape around p=1/2, see figure below.\n\nFor our example, the log odds of having the disease is:\n\n\\text{Log Odds(Disease)} = \\ln(\\text{Odds(Disease)}) = \\ln(0.0887) \\approx -2.42\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\n\n\n\n\nvisualize\nfig, ax = plt.subplots(1, 2, figsize=(8, 6))\n\np = np.linspace(0, 1, 100)\nodds = p / (1 - p)\nax[0].plot(p, odds, label=\"odds\", color=\"tab:blue\");\nax[0].axhline(1, ls=\"--\", color=\"gray\")\nax[0].set(ylabel=\"odds\",\n          xlabel=\"probability\",\n          title=r\"f(p) = $\\frac{p}{1-p}$\",\n          ylim=(-1, 10),\n          xlim=(0, 1),\n          xticks=[0,0.5,1]);\nax[0].annotate(\"more likely\\nthan not\", xy=(0.25, 1), xytext=(0.25, 3),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"))\nax[0].annotate(\"less likely\", xy=(0.7, 1), xytext=(0.7, -0.1),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"))\n\nax[1].plot(p, np.log(odds), label=\"odds\", color=\"tab:blue\")\nax[1].axhline(0, ls=\"--\", color=\"gray\")\nax[1].yaxis.set_ticks_position('right')\nax[1].yaxis.set_label_position('right')\nax[1].set(ylabel=\"log odds\",\n          xlabel=\"probability\",\n          title=r\"f(p) = log$\\frac{p}{1-p}$\",\n          xlim=(0, 1),\n          xticks=[0,0.5,1]);\nax[1].annotate(\"more likely\\nthan not\", xy=(0.25, 0), xytext=(0.25, 1),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"))\nax[1].annotate(\"less likely\", xy=(0.75, 0), xytext=(0.75, -1),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"));\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10104/1581004145.py:5: RuntimeWarning: divide by zero encountered in divide\n  odds = p / (1 - p)\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10104/1581004145.py:19: RuntimeWarning: divide by zero encountered in log\n  ax[1].plot(p, np.log(odds), label=\"odds\", color=\"tab:blue\")",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#bayes-theorem-in-odds-form",
    "href": "bayes/odds.html#bayes-theorem-in-odds-form",
    "title": "26  odds and log likelihood",
    "section": "26.4 Bayes’ theorem in odds form",
    "text": "26.4 Bayes’ theorem in odds form\nThe standard form of Bayes’ theorem is:\n\nP(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}.\n\\tag{1}\n\nIn our example, the hypothesis H is “having the disease”, and the evidence E is detecting the mutated gene.\nLet’s write Bayes’ theorem for the alternative hypothesis \\neg H (“not having the disease”):\n\nP(\\neg H|E) = \\frac{P(E|\\neg H) \\cdot P(\\neg H)}{P(E)}.\n\\tag{2}\n\nThe odds form of Bayes’ theorem is the ratio of these two equations:\n\n\\underbrace{\\frac{P(H|E)}{P(\\neg H|E)}}_ {\\text{posterior odds}} = \\underbrace{\\frac{P(E|H)}{P(E|\\neg H)}}_{\\text{likelihood ratio}} \\cdot \\underbrace{\\frac{P(H)}{P(\\neg H)}}_{\\text{prior odds}}\n\\tag{3}\n\nWe already discussed the prior odds. The posterior odds represent the odds of having the desease after we have seen the evidence (the mutated gene). The new piece we need is the likelihood ratio.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#likelihood-ratio",
    "href": "bayes/odds.html#likelihood-ratio",
    "title": "26  odds and log likelihood",
    "section": "26.5 likelihood ratio",
    "text": "26.5 likelihood ratio\nThe likelihood ratio (LR) tells us how much more likely we are to see the evidence if the hypothesis is true compared to if it is false.\n\n\\text{LR} = \\frac{P(E|H)}{P(E|\\neg H)}\n\nWe can compute this from our data:\n\nP(E|H): the probability of having the mutated gene given that the person has the disease. From the left column in our table, this is \\frac{23}{23+6} \\approx 0.793.\nP(E|\\neg H): the probability of having the mutated gene given that the person does not have the disease. From the right column in our table, this is \\frac{117}{117+210} \\approx 0.358.\n\nFinally:\n\n\\text{LR} = \\frac{23/29}{117/327} \\approx 2.22\n\nThe interpretation is that seeing the mutated gene is about 2.22 times more likely if the person has the disease than if they do not.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#log-likelihood-ratio",
    "href": "bayes/odds.html#log-likelihood-ratio",
    "title": "26  odds and log likelihood",
    "section": "26.6 log likelihood ratio",
    "text": "26.6 log likelihood ratio\nHere too, taking the logarithm transforms a quantity between 0 and infinity into a number between negative infinity and positive infinity. The log likelihood ratio is: \n\\text{Log-LR} = \\ln(\\text{LR}) = \\ln(2.22) \\approx 0.797\n\nThe fact that this is a positive number says that seeing the mutated gene increases our belief that the person has the disease.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#bayes-theorem-in-log-odds-form",
    "href": "bayes/odds.html#bayes-theorem-in-log-odds-form",
    "title": "26  odds and log likelihood",
    "section": "26.7 Bayes’ theorem in log odds form",
    "text": "26.7 Bayes’ theorem in log odds form\nTaking the logarith of Eq. (3) gives us the log odds form of Bayes’ theorem:\n\n\\underbrace{\\ln\\left(\\frac{P(H|E)}{P(\\neg H|E)}\\right)}_{\\text{posterior log odds}} = \\underbrace{\\ln\\left(\\frac{P(E|H)}{P(E|\\neg H)}\\right)}_{\\text{log-likelihood ratio}} + \\underbrace{\\ln\\left(\\frac{P(H)}{P(\\neg H)}\\right)}_{\\text{prior log odds}}\n\\tag{4}\n\nPlugging in our numbers, we can see how our belief about the person having the disease changes after seeing the evidence (the mutated gene).\n\\begin{align*}\n\\text{Posterior Log Odds} &= \\text{Log-LR} + \\text{Prior Log Odds} \\\\\n&= 0.797 + (-2.42) \\\\\n&\\approx -1.623\n\\end{align*}\nFrom the posterior log odds, we can get back to the posterior odds by exponentiating:\n\n\\text{Posterior Odds} = e^{\\text{Posterior Log Odds}} = e^{-1.623} \\approx 0.197\n\nFinally, we can convert the posterior odds back to a probability: \nP(H|E) = \\frac{\\text{Posterior Odds}}{1 + \\text{Posterior Odds}} = \\frac{0.197}{1 + 0.197} \\approx 0.164\n This means that after seeing the mutated gene, our estimate of the probability that the person has the disease has increased from about 8.1% to about 16.4%.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/logistic-connection.html",
    "href": "bayes/logistic-connection.html",
    "title": "27  logistic connection",
    "section": "",
    "text": "27.1 from Bayes the logistic\nThe arguments below follow those in subsection 12.2 of “Introduction to Environmental Data Science” by William W. Hsieh.\nWe start with Bayes’ theorem for two classes C_1 and C_2:\nP(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}\n\\tag{1}\nUsing the law of total probability in the denominator, we get:\nP(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}\n\\tag{2}\nWe now divide the numerator and denominator by P(x|C_1)P(C_1):\nP(C_1|x) = \\frac{1}{1 + \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}\n\\tag{3}\nWe now note that the ratio P(C_2|x)/P(C_1|x) can be expressed as:\n\\frac{P(C_2|x)}{P(C_1|x)} = \\frac{\\frac{P(x|C_2)P(C_2)}{P(x)}}{\\frac{P(x|C_1)P(C_1)}{P(x)}} = \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}\n\\tag{4}\nIn the expression above, we used the Bayes’ theorem in (1) to express P(C_2|x) and P(C_1|x) in terms of P(x|C_2) and P(x|C_1). We can now rewrite (3) as: \nP(C_1|x) = \\frac{1}{1 + \\frac{P(C_2|x)}{P(C_1|x)}} = \\frac{1}{1 + \\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right)^{-1}}\n\\tag{5}\nThe posterior probability P(C_1|x) is a function of the ratio P(C_1|x)/P(C_2|x). This ratio is called the posterior odds, or simply odds. We can make this function look like a sigmoid function by taking the logarithm of the posterior odds. The logarithm of the posterior odds is called the log-odds or logit: \n\\text{logit} = u = \\ln\\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right)\n\\tag{6}\nWe can now rewrite (5) in terms of the logit: \nP(C_1|x) = \\frac{1}{1 + e^{-u}}\n\\tag{7}\nFinally, we assume that there is a linear relationship between u and the features x:\nu = \\sum_j w_j x_j + w_0 = \\mathbf{w}^T \\mathbf{x} + w_0\n\\tag{8}\nWe now have the logistic function that connects the features x to the posterior probability P(C_1|x): \nP(C_1|x) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + w_0)}}\n\\tag{9}\nThis seems a rather arbitrary assumption. Why does this make sense?",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>logistic connection</span>"
    ]
  },
  {
    "objectID": "bayes/logistic-connection.html#from-bayes-the-logistic",
    "href": "bayes/logistic-connection.html#from-bayes-the-logistic",
    "title": "27  logistic connection",
    "section": "",
    "text": "The one assumption that is needed to make the connection from Bayes’ theorem to the logistic function is that there is a linear relationship between the log-odds and the features x:\n\n\\ln\\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right) = \\mathbf{w}^T \\mathbf{x} + w_0\n\\tag{10}\n\n\n\n\nA linear relationship between the log odds and the features is simple and easy to interpret.\nLinear models are easy to implement and computationally efficient.\nIn a few specific cases (see below) the linearity doesn’t have to be assumed, it emerges naturally from the model.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>logistic connection</span>"
    ]
  },
  {
    "objectID": "bayes/logistic-connection.html#emergent-linearity",
    "href": "bayes/logistic-connection.html#emergent-linearity",
    "title": "27  logistic connection",
    "section": "27.2 emergent linearity",
    "text": "27.2 emergent linearity\nLet’s start from the log odds definition in (6):\n\nu = \\ln\\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right) = \\ln\\left(\\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}\\right)\n\\tag{11}\n\nWe rewrite this as:\n\\begin{align*}\nu &= \\ln \\frac{P(x|C_1)}{P(x|C_2)} + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\ln P(x|C_1) - \\ln P(x|C_2) + \\ln \\frac{P(C_1)}{P(C_2)}. \\tag{12}\n\\end{align*}\nWe now make the assumption that the likelihoods P(x|C_k) are Gaussian distributions. For simplicity, let’s assume that x is a single feature (univariate case).\n\nP(x|C_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma_k^2}\\right),\n\\tag{13}\n\nwhere C_k are the two classes we have, C_1 and C_2.\nWe now calculate the log of the likelihoods:\n\n\\ln P(x|C_k) = -\\ln \\sqrt{2\\pi \\sigma_k^2} - \\frac{(x-\\mu_k)^2}{2\\sigma_k^2}.\n\\tag{14}\n\nWe now substitute this into Eq. (12) for the log odds:\n\\begin{align*}\nu &= -\\ln \\sqrt{2\\pi \\sigma_1^2} - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\ln \\sqrt{2\\pi \\sigma_2^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\ln \\frac{\\sigma_2}{\\sigma_1} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\ln \\frac{P(C_1)}{P(C_2)}.\n  \\tag{15}\n\\end{align*}\nKEY ASSUMPTION: if we assume that the two classes have the same variance, \\sigma_1 = \\sigma_2 = \\sigma, the expression simplifies to:\n\\begin{align*}\nu &= \\frac{1}{2\\sigma^2} \\left( (x-\\mu_2)^2 - (x-\\mu_1)^2 \\right) + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\frac{1}{2\\sigma^2} \\left( x^2 - 2x\\mu_2 + \\mu_2^2 - x^2 + 2x\\mu_1 - \\mu_1^2 \\right) + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\frac{\\mu_1 - \\mu_2}{\\sigma^2} x + \\frac{\\mu_2^2 - \\mu_1^2}{2\\sigma^2} + \\ln \\frac{P(C_1)}{P(C_2)}.\n  \\tag{16}\n\\end{align*}\nThe first term depends on x linearly, and the other two terms are constants. We can thus rewrite the log odds u as:\n\nu = wx + w_0,\n\\tag{17}\n\nwhere \nw = \\frac{\\mu_1 - \\mu_2}{\\sigma^2}, \\quad w_0 = \\frac{\\mu_2^2 - \\mu_1^2}{2\\sigma^2} + \\ln \\frac{P(C_1)}{P(C_2)}.\n\\tag{18}\n\nUnder the assumption that the distributions have equal variance, the posterior probability can be expressed as a logistic function of a linear combination of the input feature.\nThis is probably the simplest example of a connection between a generative model (Gaussian distributions for each class) and a discriminative model (logistic regression). It would work for other distributions from the exponential family, e.g., Poisson, Bernoulli, Exponential, etc. The one condition they all need to satisfy is that the non-linear part of the log-likelihoods cancels out when we compute the log-odds, leaving a linear function of x.\nWhen we have real data in our hands, we usually don’t know the underlying distributions. The calculation above showed us that a linear relationship between the log odds and the features naturally emerges in a few cases, and this is the motivation for the wider assumption in (10).",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>logistic connection</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html",
    "href": "bayes/conjugate-prior.html",
    "title": "28  conjugate prior",
    "section": "",
    "text": "28.1 question\nWe will learn about conjugate priors in Bayesian statistics from a concrete example. Once we understand it, we will generalize the idea.\nLet’s use here the same example from the chapter on cross-entropy and KL divergence:\nThe specific question we want to answer is: how can my friend update their belief about the probability of rain when they arrive in city A?",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#question",
    "href": "bayes/conjugate-prior.html#question",
    "title": "28  conjugate prior",
    "section": "",
    "text": "Assume I live in city A, where it rains 50% of the days. A friend of mine lives in city B, where it rains 10% of the days. What happens when my friend visits me in city A and, not knowing any better, assumes that it rains 10% of the days?",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#bayes-theorem",
    "href": "bayes/conjugate-prior.html#bayes-theorem",
    "title": "28  conjugate prior",
    "section": "28.2 bayes’ theorem",
    "text": "28.2 bayes’ theorem\nWe will use Bayes’ theorem to update our friend’s belief. To makes things easier to remember, let’s call the hypothesis p (the probability of rain), and the evidence R (a specific observation of rain or no rain). Thus, Bayes’ theorem can be rewritten as:\n\\begin{align}\nP(p|R) &= \\frac{P(R|p)\\cdot P(p)}{P(R)} \\\\\n\\text{posterior}&= \\frac{\\text{likelihood}\\cdot \\text{prior}}{\\text{evidence}}\n\\end{align}\nwhere:\n\np is the hypothesis, the probability of rain.\nR is the evidence, the observation of rain or no rain.\nP(p) is the prior probability, our friend’s initial belief about the probability of rain.\nP(R|p) is the likelihood, the likelihood of observing rain given the hypothesis that it rains with a certain probability.\nP(R) is the evidence, the total probability of observing the evidence.\nP(p|R) is the posterior probability, this is what we want to find: our friend’s updated belief about the probability of rain p after observing the evidence.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#modeling-the-likelihood",
    "href": "bayes/conjugate-prior.html#modeling-the-likelihood",
    "title": "28  conjugate prior",
    "section": "28.3 modeling the likelihood",
    "text": "28.3 modeling the likelihood\nIn this problem, every day that passes it either rains or it does not rain. This can be understood as a Bernoulli process, where each day is an independent trial with two possible outcomes. “Success” would be ocurrence of rain (R=1), which happens with probability p. “Failure” would be no rain (R=0), which happens with probability 1-p. In mathematical terms, the likelihood can be modeled as:\n\nP(R=r|p) =\n\\begin{cases}\np & \\text{if } r=1 \\\\\n1-p & \\text{if } r=0\n\\end{cases}.\n\nThis can be more compactly written as:\n\nP(R=r|p) = p^r (1-p)^{1-r}.\n\nThis equation describes only one observation. However, we can extend it to multiple observations. Suppose our friend observes, over a total of n days, k days of rain and n-k days of no rain. The likelihood of observing this specific sequence of rain and no rain, given the probability p, can be modeled using the binomial distribution, which is the natural extension of the Bernoulli process for multiple trials:\n\nP(R=k|p) = \\binom{n}{k} p^k (1-p)^{n-k}.\n\nThis is the time to be more precise. When we previously said that “p is the hypothesis, the probability of rain”, we left behind the modeling aspect. We assumed here a generative model, the Bernoulli process. Rephrasing the statement more precisely: “p is the parameter of the generative model (Bernoulli process) that generates the observations of rain and no rain”.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#modeling-the-prior",
    "href": "bayes/conjugate-prior.html#modeling-the-prior",
    "title": "28  conjugate prior",
    "section": "28.4 modeling the prior",
    "text": "28.4 modeling the prior\nThe prior in the Bayesian framework is not a single value. From the question above, we might think that our friend’s prior belief about the probability of rain is simply 0.1=10\\%. However, in Bayesian statistics, the prior is represented as a probability distribution over all possible values of p. In would make sense to choose a probability distribution that is highest around 0.1 and lower elsewhere. There are infinite possible distributions that could represent this belief, so which should we choose? See below three examples of possible prior distributions, all have their mean at 0.1.\n\n\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.gridspec as grid_spec\nfrom matplotlib.lines import Line2D\nfrom scipy.stats import beta, norm, uniform, binom\n\n\n\n\nplot various possible distributions\nfig, ax = plt.subplots(figsize=(6, 4))\nlocation = 0.1\np = np.linspace(0, 1, 1000)\nax.plot(p, beta.pdf(p, 2, 2/location-2), label='Beta(2, 18)', color=\"black\")\nax.plot(p, norm.pdf(p, loc=location, scale=0.02), label='Normal(0.1, 0.05)', color=\"tab:orange\")\nax.plot(p, uniform.pdf(p, loc=location-0.05, scale=0.1), label='Uniform(0.05, 0.15)', color=\"tab:blue\")\nax.legend(frameon=False)\nax.set(xlabel='Probability of rain (p)',\n       ylabel='Probability Density',\n       title='Possible prior distributions for the probability of rain');\n\n\n\n\n\n\n\n\n\nA particular good choice is the Beta distribution. The Beta distribution is defined on the interval [0, 1], which makes it suitable for modeling probabilities. It is parameterized by two positive shape parameters, \\alpha and \\beta, which determine the shape of the distribution. The probability density function (PDF) of the Beta distribution is given by:\n\n\\text{Beta}(p|\\alpha, \\beta) = \\frac{p^{\\alpha - 1} (1-p)^{\\beta - 1}}{B(\\alpha, \\beta)},\n\nwhere B(\\alpha, \\beta) is the Beta function, which serves as a normalization constant to ensure that the total probability integrates to 1.\nThe Beta distribution in the graph above is \\text{Beta}(p|\\alpha=2, \\beta=18). How did I choose these parameters? The mean of a Beta distribution is given by:\n\n\\text{mean} = \\frac{\\alpha}{\\alpha + \\beta}.\n\nThe derivation of this formula is not shown here, but it involves calculating the expected value of the distribution using its pdf, and using properties of the Beta function and of the Gamma function. Indeed, by choosing \\alpha=2 and \\beta=18, we get a mean of 0.1.\nIntuitively, a rain probability of 0.1 means that out of every n days, we expect it to rain on average 0.1n days. It terms of “successes” and “failures”:\n\\begin{align*}\n\\text{mean} &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n&= \\frac{\\text{expected successes}}{\\text{expected successes} + \\text{expected failures}} \\\\\n&= \\frac{\\text{expected successes}}{\\text{total number of trials}}\n\\end{align*}\nInstead of choosing \\alpha=2 and \\beta=18, we could choose other values that maintain the same mean but represent different levels of confidence or prior knowledge. See the three Beta distributions plotted below. All have their mean at 0.1, but the black one (\\alpha=2, \\beta=18) is more spread out, indicating less certainty about the probability of rain when only 2+18=20 days are considered. Increasing the number of days to 50 (orange, \\alpha=5, \\beta=45) or 200 (blue, \\alpha=20, \\beta=180) makes the distribution more peaked around the mean, indicating greater confidence in the estimate of the probability of rain.\n\n\nthree beta distributions\nfig, ax = plt.subplots(figsize=(6, 4))\nlocation = 0.1\np = np.linspace(0, 1, 1000)\nax.plot(p, beta.pdf(p, 2, 2/location-2), label='Beta(2, 18)', color=\"black\")\nax.plot(p, beta.pdf(p, 5, 5/location-5), label='Beta(5, 45)', color=\"tab:orange\")\nax.plot(p, beta.pdf(p, 20, 20/location-20), label='Beta(20, 180)', color=\"tab:blue\")\nax.legend(frameon=False)\nax.set(xlabel='Probability of rain (p)',\n       ylabel='Probability Density',\n       title='Beta prior distributions with mean at 0.1');",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#posterior-looks-like-an-updated-prior",
    "href": "bayes/conjugate-prior.html#posterior-looks-like-an-updated-prior",
    "title": "28  conjugate prior",
    "section": "28.5 posterior looks like an updated prior",
    "text": "28.5 posterior looks like an updated prior\nWhy is this distribution particulary convenient? Note that it has the factors p^{\\alpha - 1} and (1-p)^{\\beta - 1}, which are similar to the factors in the likelihood function p^k (1-p)^{n-k}.\nAccording to Bayes’ theorem, the posterior distribution is proportional to the product of the likelihood and the prior. Thus, if we choose a Beta distribution as the prior, the posterior distribution will also be a Beta distribution, but with updated parameters. Let’s see how this works mathematically:\n\\begin{align*}\nP(p|R=k) &= \\frac{1}{\\underbrace{P(R=k)}_{\\text{evidence}}} \\cdot \\underbrace{P(R=k|p)}_{\\text{likelihood}} \\cdot \\underbrace{P(p)}_{\\text{prior}} \\\\\n&= \\frac{1}{P(R=k)} \\left( \\binom{n}{k} p^k (1-p)^{n-k} \\right) \\cdot \\left( \\frac{p^{\\alpha - 1} (1-p)^{\\beta - 1}}{B(\\alpha, \\beta)} \\right) \\\\\n&= \\underbrace{ \\frac{1}{P(R=k)} \\binom{n}{k} \\frac{1}{B(\\alpha, \\beta)} }_{\\text{normalization constant}} p^{k + \\alpha - 1} (1-p)^{n - k + \\beta - 1}\\\\\n& = \\text{Beta}(p | \\alpha + k, \\beta + n - k) \\\\\n& = \\text{Beta}(p | \\alpha + \\text{successes}, \\beta + \\text{failures}).\n\\end{align*}\nThis isn’t magic. We simply chose a prior distribution (Beta) that, when multiplied by the likelihood (Binomial), results in a posterior distribution of the same family (Beta). This property is what defines conjugate priors. Why is this useful?\n\nComputational Simplicity: The posterior distribution can be computed analytically without the need for complex numerical methods.\nIntuitive Interpretation: The parameters of the posterior distribution can be interpreted as updated counts of successes and failures, making it easy to understand how new data influences our beliefs.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#iterative-updating-of-beliefs",
    "href": "bayes/conjugate-prior.html#iterative-updating-of-beliefs",
    "title": "28  conjugate prior",
    "section": "28.6 iterative updating of beliefs",
    "text": "28.6 iterative updating of beliefs\nLet’s say that my friends Bob and Alice move to city A, where it rains on 50% of the days. Bob has a prior belief modeled as a Beta distribution with parameters \\alpha=2 and \\beta=18, reflecting his initial belief that it rains 10% of the days, but with a low level of confidence. Alice, however, has an initial belief closer to the truth, 20%, but with a much higher level of confidence, modeled as a Beta distribution with parameters \\alpha=100 and \\beta=400. Every week that passes, they observe the weather and write down the number of rainy days, thus collecting the following data over 52 weeks (one year):\n\n\ngenerate binomial data for 52 weeks\nnp.random.seed(6)\nN_weeks = 52\nsuccess_array_daily = binom.rvs(n=1, p=0.5, size=N_weeks*7)  # first generate daily data\nsuccess_array = success_array_daily.reshape(-1, 7).sum(axis=1)  # aggragate to weekly data\n# success_array = binom.rvs(n=7, p=0.5, size=N_weeks)  # do this if you don't need daily data\nfailure_array = 7 - success_array\nsf_array = np.vstack([success_array, failure_array]).T\nprint(success_array)\n\n\n[4 4 5 6 4 4 2 3 5 4 2 3 5 3 3 5 4 3 1 3 4 4 2 2 3 5 1 2 6 4 3 2 3 5 6 3 4\n 5 3 5 2 3 4 3 3 5 5 3 5 6 5 2]\n\n\nNow we just use the updating formula iteratively over the 52 weeks of observations:\n\n\nupdate priors over 52 weeks and print parameters\nbobs_parameters = np.array([[2, 18]])\nalices_parameters = np.array([[100, 400]])\nprint(\"\\t\\tBob\\t\\tBob\\t\\tAlice\\tAlice\")\nprint(\"week\\talpha\\tbeta\\talpha\\tbeta\")\nprint(f\"{0}\\t\\t{bobs_parameters[0][0]}\\t\\t{bobs_parameters[0][1]}\\t\\t{alices_parameters[0][0]}\\t\\t{alices_parameters[0][1]}\")\nfor week in np.arange(N_weeks):\n    bobs_last_weeks_parameters = bobs_parameters[-1]\n    bobs_this_weeks_parameters = bobs_last_weeks_parameters + sf_array[week]\n    bobs_parameters = np.vstack([bobs_parameters, bobs_this_weeks_parameters])\n\n    alices_last_weeks_parameters = alices_parameters[-1]\n    alices_this_weeks_parameters = alices_last_weeks_parameters + sf_array[week]\n    alices_parameters = np.vstack([alices_parameters, alices_this_weeks_parameters])\n\n    print(f\"{week+1}\\t\\t{bobs_this_weeks_parameters[0]}\\t\\t{bobs_this_weeks_parameters[1]}\\t\\t{alices_this_weeks_parameters[0]}\\t\\t{alices_this_weeks_parameters[1]}\")\n\n\n        Bob     Bob     Alice   Alice\nweek    alpha   beta    alpha   beta\n0       2       18      100     400\n1       6       21      104     403\n2       10      24      108     406\n3       15      26      113     408\n4       21      27      119     409\n5       25      30      123     412\n6       29      33      127     415\n7       31      38      129     420\n8       34      42      132     424\n9       39      44      137     426\n10      43      47      141     429\n11      45      52      143     434\n12      48      56      146     438\n13      53      58      151     440\n14      56      62      154     444\n15      59      66      157     448\n16      64      68      162     450\n17      68      71      166     453\n18      71      75      169     457\n19      72      81      170     463\n20      75      85      173     467\n21      79      88      177     470\n22      83      91      181     473\n23      85      96      183     478\n24      87      101     185     483\n25      90      105     188     487\n26      95      107     193     489\n27      96      113     194     495\n28      98      118     196     500\n29      104     119     202     501\n30      108     122     206     504\n31      111     126     209     508\n32      113     131     211     513\n33      116     135     214     517\n34      121     137     219     519\n35      127     138     225     520\n36      130     142     228     524\n37      134     145     232     527\n38      139     147     237     529\n39      142     151     240     533\n40      147     153     245     535\n41      149     158     247     540\n42      152     162     250     544\n43      156     165     254     547\n44      159     169     257     551\n45      162     173     260     555\n46      167     175     265     557\n47      172     177     270     559\n48      175     181     273     563\n49      180     183     278     565\n50      186     184     284     566\n51      191     186     289     568\n52      193     191     291     573\n\n\nFinally, we can plot how the probability densities of Bob and Alice are updated over the 52 weeks.\n\n\nridge plot\nN_weeks = 52\nN_panels = N_weeks + 1\ngs = grid_spec.GridSpec(N_panels,1)\nfig = plt.figure(figsize=(10,12))\np = np.linspace(0, 1, 1000)\nbob_colors = mpl.cm.Blues(np.linspace(0.4,0.8,N_panels))\nalice_colors = mpl.cm.Reds(np.linspace(0.4,0.8,N_panels))\nax_objs = []\n\nfor week in np.arange(N_panels)[::-1]:\n    # creating new axes object, start from top = week 52\n    ax_objs.append(fig.add_subplot(gs[N_panels-week-1:N_panels-week, 0:]))\n    \n    bobs_params = bobs_parameters[week]\n    alices_params = alices_parameters[week]\n\n    # don't plot the whole distribution, only when greater than threshold\n    range_bob = beta.pdf(p, bobs_params[0], bobs_params[1])\n    domain = np.where(range_bob &gt; 1e-3*np.max(range_bob))\n    ax_objs[-1].fill_between(p[domain], range_bob[domain],\n                             color=bob_colors[week], alpha=1.0,\n                             clip_on=False, ec=\"white\", zorder=N_panels-week,\n                             label=\"Bob\")\n    range_alice = beta.pdf(p, alices_params[0], alices_params[1])\n    domain = np.where(range_alice &gt; 1e-3*np.max(range_alice))\n    ax_objs[-1].fill_between(p[domain], range_alice[domain],\n                             color=alice_colors[week], alpha=1.0,\n                             clip_on=False, ec=\"white\", zorder=N_panels-week,\n                             label=\"Alice\")\n\n    ax_objs[-1].set(xlim=(0,0.6),\n                    ylim=(0,15),\n                    yticks=[])\n    if week&gt;0:\n        ax_objs[-1].set_xticks([])\n\n    # make background transparent\n    rect = ax_objs[-1].patch\n    rect.set_alpha(0)\n    ax_objs[-1].set_yticklabels([])\n\n    if week == N_panels-1:\n        ax_objs[-1].legend(frameon=False, loc=\"upper left\", fontsize=12, ncol=2)\n    if week%4==0:\n        ax_objs[-1].set_yticks([0])\n        ax_objs[-1].set_yticklabels([f\"week {week}\"])\n        ax_objs[-1].tick_params(axis='y', length=0)\n        ax_objs[-1].axhline(0, color=\"black\", lw=1, zorder=N_panels-week+1)\n    spines = [\"top\",\"right\",\"left\",\"bottom\"]\n    for s in spines:\n        ax_objs[-1].spines[s].set_visible(False)\n\nax_objs[-1].set(xlabel='Probability densities updated over 52 weeks')\ngs.update(hspace=-0.7)\n\nax_top = ax_objs[0]\nax_bottom = ax_objs[-1]\npos_top = ax_top.get_position().extents # left, bottom, right, top\npos_bottom = ax_bottom.get_position().extents # left, bottom, right, top\nx_dotted_lines = [0.1,0.2,0.3,0.4,0.5,0.6]\nfor x in x_dotted_lines:\n    display_coord = ax_bottom.transData.transform([x, 0.0])  # convert data coordinates to display coordinates\n    fig_coord = fig.transFigure.inverted().transform(display_coord)  # convert display coordinates to figure coordinates\n    line = Line2D([fig_coord[0], fig_coord[0]],     # x coordinates\n                [fig_coord[1], pos_top[1]],          # y coordinates\n                transform=fig.transFigure, color='black', ls=':', lw=0.5)\n    fig.add_artist(line)\n\n\n\n\n\n\n\n\n\nAlthough Alice starts with a more accurate prior, Bob’s belief converges faster towards the true probability of rain (50%) over time, because his prior at week 0 was less confident (more spread out), allowing new evidence to have a greater impact on his posterior belief. For each person’s pdf, we can plot its mean and standard deviation over time. For a Beta distribution \\text{Beta}(p|\\alpha, \\beta), the mean and variance are given by:\n\\begin{align*}\n\\text{mean} &= \\frac{\\alpha}{\\alpha + \\beta}, \\\\\n\\text{variance} &= \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n\\end{align*}\n\n\nBob and Alice’s estimates over time\nfig, ax = plt.subplots(figsize=(6,4))\nt = np.arange(N_weeks+1)\n\ndef beta_mean(alpha, beta):\n    return alpha / (alpha + beta)\ndef beta_variance(alpha, beta):\n    return (alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1))\n\nbob_means = beta_mean(bobs_parameters[:,0], bobs_parameters[:,1])\nbob_sqrt_variance = np.sqrt(beta_variance(bobs_parameters[:,0], bobs_parameters[:,1]))\nalice_means = beta_mean(alices_parameters[:,0], alices_parameters[:,1])\nalice_sqrt_variance = np.sqrt(beta_variance(alices_parameters[:,0], alices_parameters[:,1]))\n\nline_bob, = ax.plot(t, bob_means, color=\"tab:blue\", label=\"Bob's Mean\")\nfill_bob = ax.fill_between(t, bob_means - bob_sqrt_variance, bob_means + bob_sqrt_variance,\n                color=\"tab:blue\", alpha=0.2, label=\"Bob's Std Dev\")\nline_alice, = ax.plot(t, alice_means, color=\"tab:red\", label=\"Alice's Mean\")\nfill_alice = ax.fill_between(t, alice_means - alice_sqrt_variance, alice_means + alice_sqrt_variance,\n                color=\"tab:red\", alpha=0.2, label=\"Alice's Std Dev\")\n\nax.axhline(0.5, color=\"black\", ls=\"--\", lw=1)\nax.legend([(line_bob, fill_bob), (line_alice, fill_alice)],\n          [r\"Bob's mean$\\pm$std\", r\"Alice's mean$\\pm$std\"],\n          handler_map={tuple: mpl.legend_handler.HandlerTuple(ndivide=1)},\n          frameon=False)\n\nax.set(xlabel='Weeks',\n       ylabel='Estimated Expected Probability of Rain',\n       title='Convergence of Bob and Alice\\'s Estimates Over Time',\n       ylim=(0,0.6),\n       xlim=(0,N_weeks));",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#comparison-with-frequentist-approach",
    "href": "bayes/conjugate-prior.html#comparison-with-frequentist-approach",
    "title": "28  conjugate prior",
    "section": "28.7 comparison with frequentist approach",
    "text": "28.7 comparison with frequentist approach\nLet’s add to the graph above one more line, for Charlie, who follows a frequentist approach to estimate the probability of rain over time. Charlie doesn’t have any prior belief; instead, he simply counts the number of rainy days observed so far (k) and divides it by the total number of days that passed (n). As shown in the chapter MLE and summary statistics, when assuming a generative model that is a Bernoulli process (or Binomial distribution for multiple trials), the maximum likelihood estimate (MLE) of the probability of success (rain) is given by the ratio of the number of successes to the total number of trials:\n\n\\hat{p} = \\frac{k}{n}.\n\nThe conclusion here is that, even if Charlied doesn’t know it, his estimate is based on the Maximum Likelihood Estimation (MLE) principle.\nThis time, instead of plotting the weekly estimates, let’s see what happens if each of the residents of city A updates their beliefs daily.\n\n\ncompute daily parameter updates for Bob and Alice\nfailure_array_daily = 1 - success_array_daily\nsf_array_daily = np.vstack([success_array_daily, failure_array_daily]).T\n\nbobs_parameters_daily = np.array([[2, 18]])\nalices_parameters_daily = np.array([[100, 400]])\n\nfor day in np.arange(N_weeks*7):\n    bobs_yesterdays_parameters = bobs_parameters_daily[-1]\n    bobs_todays_parameters = bobs_yesterdays_parameters + sf_array_daily[day]\n    bobs_parameters_daily = np.vstack([bobs_parameters_daily, bobs_todays_parameters])\n    alices_yesterdays_parameters = alices_parameters_daily[-1]\n    alices_todays_parameters = alices_yesterdays_parameters + sf_array_daily[day]\n    alices_parameters_daily = np.vstack([alices_parameters_daily, alices_todays_parameters])\n\n\n\n\nfrequentist vs Bayesian daily updates\nfig, ax = plt.subplots(figsize=(6,4))\n\nt = 7*np.arange(N_weeks+1)\nt_day= np.arange(N_weeks*7+1)\n\n\nbob_means_daily = beta_mean(bobs_parameters_daily[:,0], bobs_parameters_daily[:,1])\nbob_sqrt_variance_daily = np.sqrt(beta_variance(bobs_parameters_daily[:,0], bobs_parameters_daily[:,1]))\nalice_means_daily = beta_mean(alices_parameters_daily[:,0], alices_parameters_daily[:,1])\nalice_sqrt_variance_daily = np.sqrt(beta_variance(alices_parameters_daily[:,0], alices_parameters_daily[:,1]))\n\nline_bob_daily, = ax.plot(t_day, bob_means_daily, color=\"tab:blue\", label=\"Bob's Mean (Daily)\", alpha=0.5, ls=\"-\")\nfill_bob_daily = ax.fill_between(t_day, bob_means_daily - bob_sqrt_variance_daily, bob_means_daily + bob_sqrt_variance_daily,\n                color=\"tab:blue\", alpha=0.2, label=\"Bob's Std Dev\")\nline_alice_daily, = ax.plot(t_day, alice_means_daily, color=\"tab:red\", label=\"Alice's Mean (Daily)\", alpha=0.5, ls=\"-\")\nfill_alice_daily = ax.fill_between(t_day, alice_means_daily - alice_sqrt_variance_daily, alice_means_daily + alice_sqrt_variance_daily,\n                color=\"tab:red\", alpha=0.2, label=\"Alice's Std Dev\")\n\nbob_parameters_frequentist = bobs_parameters_daily - bobs_parameters_daily[0]\nbob_means_frequentist = bob_parameters_frequentist[1:,0] / (bob_parameters_frequentist[1:,0] + bob_parameters_frequentist[1:,1])\nline_bob_frequentist, = ax.plot(t_day[1:], bob_means_frequentist, color=\"purple\", label=\"Bob's Frequentist Estimate\", ls=\"-\", marker=\"o\", markersize=3, alpha=0.5, clip_on=True)\n\nax.axhline(0.5, color=\"black\", ls=\"--\", lw=1)\nax.legend([(line_bob_daily, fill_bob_daily), (line_alice_daily, fill_alice_daily), (line_bob_frequentist,)],\n          [r\"Bob's mean$\\pm$std\", r\"Alice's mean$\\pm$std\", \"Charlie, the frequentist\"],\n          handler_map={tuple: mpl.legend_handler.HandlerTuple(ndivide=1)},\n          frameon=False)\n\nax.set(xlabel='Days',\n       ylabel='Estimated Expected Probability of Rain',\n       title='Frequentist vs Bayesian Daily Updates',\n       ylim=(0,1),\n       xlim=(0,150)\n       );\n\n\n\n\n\n\n\n\n\nLet’s digest what we see above.\n\nThe frequentist approach does not use prior beliefs, it simply estimates the probability based on the observed data.\nAt day zero, Bob and Alice have beliefs about the world, but Charlie doesn’t have any data to base his estimate on, so we see nothing for him at day zero.\nThe very first day was a rainy day, so Charlie’s estimate begins at 100%.\nIn the following days, Charlie’s estimate fluctuates more wildly than Bob’s and Alice’s, especially in the early weeks when the amount of data is still small. As more data is collected, Charlie’s estimate stabilizes and converges towards the true probability of rain (50%).\nAs the number of observations increases, all three estimates (Bob’s, Alice’s, and Charlie’s) converge towards the true probability of rain (50%).\n\nPierre-Simon Laplace came up with a solution to Charlie’s “small sample size” problem, where he estimates 100% probability of rain on day one. Laplace did that in a similar context, answering the question “will the sun rise tomorrow?”. Instead of not assuming anything like Charlie, Laplace proposes the “rule of succession”, which says that we assume one success and one failure even before observing any data. Translating that to a Beta prior, it means starting with \\alpha=1 and \\beta=1, which is a uniform prior over p, see the graph below.\n\n\nLaplace vs frequentist vs Bayesian daily updates\nlaplaces_parameters_daily = np.array([[1, 1]])\nfor day in np.arange(N_weeks*7):\n    laplaces_yesterdays_parameters = laplaces_parameters_daily[-1]\n    laplaces_todays_parameters = laplaces_yesterdays_parameters + sf_array_daily[day]\n    laplaces_parameters_daily = np.vstack([laplaces_parameters_daily, laplaces_todays_parameters])\n\nfig, ax = plt.subplots(1, 2, figsize=(8,6))\n\nlaplaces_means_daily = beta_mean(laplaces_parameters_daily[:,0], laplaces_parameters_daily[:,1])\nlaplaces_sqrt_variance_daily = np.sqrt(beta_variance(laplaces_parameters_daily[:,0], laplaces_parameters_daily[:,1]))\n\nline_bob_daily, = ax[0].plot(t_day, bob_means_daily, color=\"black\", label=\"Bob's Mean (Daily)\", alpha=0.5, ls=\"-\")\nfill_bob_daily = ax[0].fill_between(t_day, bob_means_daily - bob_sqrt_variance_daily, bob_means_daily + bob_sqrt_variance_daily,\n                color=\"tab:blue\", alpha=0.2, label=\"Bob's Std Dev\")\nline_alice_daily, = ax[0].plot(t_day, alice_means_daily, color=\"black\", label=\"Alice's Mean (Daily)\", alpha=0.5, ls=\"-\")\nfill_alice_daily = ax[0].fill_between(t_day, alice_means_daily - alice_sqrt_variance_daily, alice_means_daily + alice_sqrt_variance_daily,\n                color=\"tab:red\", alpha=0.2, label=\"Alice's Std Dev\")\n\nbob_parameters_frequentist = bobs_parameters_daily - bobs_parameters_daily[0]\nbob_means_frequentist = bob_parameters_frequentist[1:,0] / (bob_parameters_frequentist[1:,0] + bob_parameters_frequentist[1:,1])\nline_bob_frequentist, = ax[0].plot(t_day[1:], bob_means_frequentist, color=\"purple\", label=\"Bob's Frequentist Estimate\", ls=\"-\", marker=\"o\", markersize=3, alpha=0.5, clip_on=True)\n\nline_laplaces_daily, = ax[0].plot(t_day, laplaces_means_daily, color=\"tab:orange\", label=\"Laplace's Mean (Daily)\", alpha=0.5, ls=\"-\")\nfill_laplaces_daily = ax[0].fill_between(t_day, laplaces_means_daily - laplaces_sqrt_variance_daily, laplaces_means_daily + laplaces_sqrt_variance_daily,\n                color=\"tab:orange\", alpha=0.2, label=\"Laplace's Std Dev\")\n\nax[0].axhline(0.5, color=\"black\", ls=\"--\", lw=1)\nax[0].legend([(line_bob_daily, fill_bob_daily), (line_alice_daily, fill_alice_daily), (line_bob_frequentist,), (line_laplaces_daily, fill_laplaces_daily)],\n          [r\"Bob's mean$\\pm$std\", r\"Alice's mean$\\pm$std\", \"Charlie, the frequentist\", \"Laplace\"],\n          handler_map={tuple: mpl.legend_handler.HandlerTuple(ndivide=1)},\n          frameon=False)\n\nax[0].set(xlabel='Days',\n       ylabel='Estimated Expected Probability of Rain',\n       title='Frequentist vs Bayesian Daily Updates',\n       ylim=(0,1),\n       xlim=(0,20)\n       );\n\np = np.linspace(0, 1, 1000)\nax[1].plot(p, beta.pdf(p, laplaces_parameters_daily[0,0], laplaces_parameters_daily[0,1]), label='Beta(1, 1) - Laplace', color=\"black\")\nax[1].plot(p, beta.pdf(p, bobs_parameters_daily[0,0], bobs_parameters_daily[0,1]), label=f'Beta({bobs_parameters_daily[0,0]}, {bobs_parameters_daily[0,1]}) - Bob', color=\"tab:orange\")\nax[1].plot(p, beta.pdf(p, alices_parameters_daily[0,0], alices_parameters_daily[0,1]), label=f'Beta({alices_parameters_daily[0,0]}, {alices_parameters_daily[0,1]}) - Alice', color=\"tab:blue\")\nax[1].legend(frameon=False)\nax[1].set(xlabel='Probability of rain (p)',\n       ylabel='Probability Density',\n       title='Beta prior distributions with mean at 0.1');\n\n\n\n\n\n\n\n\n\nLaplace’s estimate doesn’t suffer from Charlie’s extreme initial estimate. Just after a few days, however, it is almost indistinguishable from Charlie’s estimate.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#conjugate-pairs",
    "href": "bayes/conjugate-prior.html#conjugate-pairs",
    "title": "28  conjugate prior",
    "section": "28.8 conjugate pairs",
    "text": "28.8 conjugate pairs\nIn the example above, we saw how convenient it is to use a prior function that is conjugate to the likelihood function. The generating process behind the observations was a Bernoulli process (or Binomial distribution for multiple trials), so we chose its conjugate, the Beta distribution.\nSome examples of processes that can be described by a Binomial distribution (or Bernoulli process) are:\n\nConversion Rates: A marketing team has a prior belief about how many people will click an email link (Success) vs. ignore it (Failure).\nQuality Control: A factory manager has a prior belief about the proportion of defective items (Success) vs. non-defective items (Failure) in a production batch.\nMedical Trials: A researcher has a prior belief about the effectiveness of a new drug (Success) vs. ineffectiveness (Failure) based on preliminary studies.\n\nWhat if the generating process was different?\n\n28.8.1 Poisson distribution\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space (the “count”). The Gamma distribution is its conjugate prior because it is defined for positive values (0 to \\infty), making it perfect for modeling the rate (\\lambda) at which these events happen. It can be used in scenarios such as:\n\nEcohydrology: Modeling the number of rainfall pulses in a desert ecosystem during the growing season.\nCustomer Service: Estimating the rate of phone calls arriving at a help desk per hour.\nRadioactive Decay: Predicting the number of particles emitted by a substance over a specific duration.\n\n\n\n28.8.2 normal distribution (known variance)\nWhen you assume your data follows a Normal distribution (like measurement errors) and you know the variance, the conjugate prior for the mean is also a Normal distribution. This creates a beautiful “weighted average” effect: the posterior mean will sit somewhere between your prior guess and the data’s mean, depending on which one is more certain. It is useful in scenarios such as:\n\nMeasurement Errors: Estimating the true value of a physical quantity when measurements are subject to random errors.\nQuality Control: Determining the average weight of products in a manufacturing process where individual weights vary normally around a true mean.\nPsychometrics: Estimating the average score of a psychological test when individual scores are normally distributed with known variance.\n\n\n\n28.8.3 categorical / multinomial distribution\nThis is the multi-dimensional version of the Beta distribution. Instead of just “Success/Failure,” you have multiple categories (e.g., “Rain/Sun/Clouds”). The Dirichlet distribution allows you to track the probabilities of all these categories simultaneously. It is useful in scenarios such as:\n\nTopic Modeling: In Machine Learning, determining the “theme” of a document by looking at the frequency of different words (each word is a category).\nPolitical Polling: Estimating the support for four different candidates in an upcoming election.\nGenetics: Modeling the frequency of different alleles (gene variants) within a specific population.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/conjugate-prior.html#uniform-distribution-known-bounds",
    "href": "bayes/conjugate-prior.html#uniform-distribution-known-bounds",
    "title": "28  conjugate prior",
    "section": "28.9 uniform distribution (known bounds)",
    "text": "28.9 uniform distribution (known bounds)\nThis is used when you are trying to find the maximum possible value (\\theta) of a process. If you assume the data is spread evenly (Uniform) up to some unknown limit, the Pareto distribution is the conjugate prior that helps you “narrow in” on where that upper limit actually sits. It is useful in scenarios such as:\n\nThe German Tank Problem: Estimating the total number of tanks produced by an enemy based on the highest serial number found on captured tanks.\nEcological Limits: Estimating the maximum possible size a specific fish species can reach based on the largest specimens caught.\nQuality Control: Determining the maximum defect size in a batch of manufactured items based on the largest defect observed in a sample.\nProject Management: Estimating the maximum time required to complete a project based on the longest time taken for similar past projects.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>conjugate prior</span>"
    ]
  },
  {
    "objectID": "bayes/boy-girl-paradox.html",
    "href": "bayes/boy-girl-paradox.html",
    "title": "29  the boy-girl paradox",
    "section": "",
    "text": "29.1 analytical solution\nMary has two children.\nSuccessively, we added more stringent condtions on the original problem. We will first solve this problem analytically and then visually.\nLet’s denote by j \\in {1,2} the child index, and by B_j and G_j the events that child j is a boy or a girl. We call Q_j=B_j \\cap T_j the “Qualified Boy” event, meaning that child j is a boy and has the trait T_j (being born during daytime, or on a Sunday, or on the 1st of the month). Because the probability of being a boy is P(B_j)=1/2, and the probability of having trait T_j is P(T_j), we have:\nP(Q_j) = P(B_j) \\cdot P(T_j) = \\frac{1}{2} P(T_j),\n where we assumed independence between sex and trait.\nAll of the questions above ask, in mathematical notation,\nP(B_1 \\cap B_2 \\mid Q_1 \\cup Q_2).\nLet’s translate that to English:\nThe table below summarizes the qualifications and their probabilities for each question:\nQuestion Q0 is a special case where there is no qualification, so Q_j is always true, with probability 1. We can solve that one directly: the possible combinations of children are BB, BG, GB, GG, so the probability that both are boys is 1/4. In mathematical notation: \nP(B_1 \\cap B_2) = P(B_1)P(B_2) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}.\nTo solve the qualified cases (Q1 through Q4), we use Bayes’ theorem:\nP(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)},\nwhere\nSo we need to solve\nP(B_1 \\cap B_2 \\mid Q_1 \\cup Q_2) = \\frac{P\\left( \\left(Q_1 \\cup Q_2\\right) \\mid \\left(B_1 \\cap B_2\\right)\\right) P\\left(B_1 \\cap B_2\\right)}{P\\left(Q_1 \\cup Q_2\\right)}.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>the boy-girl paradox</span>"
    ]
  },
  {
    "objectID": "bayes/boy-girl-paradox.html#analytical-solution",
    "href": "bayes/boy-girl-paradox.html#analytical-solution",
    "title": "29  the boy-girl paradox",
    "section": "",
    "text": "What is the probability, P()\nthat child 1 is a boy, B_1\nand, \\cap\nthat child 2 is a boy, B_2,\ngiven that, \\mid\neither child (at least one), \\cup\nis a qualified boy, Q_1 or Q_2.\n\n\n\n\n\nQuestion\nQualification Q_j\nP(Q_j)\n\n\n\n\nQ0\nNone\n1\n\n\nQ1\nBoy\n1/2\n\n\nQ2\nBoy born during daytime\n1/2\\times 1/2\n\n\nQ3\nBoy born born on a Sunday\n1/2\\times 1/7\n\n\nQ4\nBoy born on the 1st of month\n1/2\\times 1/30\n\n\n\n\n\n\n\n\nA is the event that both children are boys, B_1 \\cap B_2,\nB is the event that at least one child is a qualified boy, Q_1 \\cup Q_2.\n\n\n\n\n29.1.1 numerator\n\nP\\left( \\left(Q_1 \\cup Q_2\\right) \\mid \\left(B_1 \\cap B_2\\right)\\right) P\\left(B_1 \\cap B_2\\right).\n\nThe second term is easy: \nP\\left(B_1 \\cap B_2\\right) = P(B_1)P(B_2) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4},\n since the children’s sexes are independent.\nThe first term is: \nP\\left( \\left(Q_1 \\cup Q_2\\right) \\mid \\left(B_1 \\cap B_2\\right)\\right),\n or in English: given that we know both children are boys, what is the probability that at least one of them is a qualified boy? We formulated the qualified boy as Q_j = B_j \\cap T_j, but now we know that B_j is true, that is, B_j=1, so we can simplify Q_j to just T_j. So we need to calculate \nP\\left( T_1 \\cup T_2\\right).\n\nThe probability that at least one child has trait T is more easily calculated via its complement, the probability that neither child has trait T:\n\nP\\left( T_1 \\cup T_2\\right) = 1 - P\\left(\\text{neither } T_1 \\text{ nor } T_2 \\right) = 1 - (1-p)^2 = 2p - p^2,\n where we used independence between the two children, and denoted p=P(T_j).\nThus, the numerator is: \nP\\left( \\left(Q_1 \\cup Q_2\\right) \\mid \\left(B_1 \\cap B_2\\right)\\right) P\\left(B_1 \\cap B_2\\right) = (2p - p^2) \\cdot \\frac{1}{4} = \\frac{p}{4}(2 - p).\n\n\n\n29.1.2 denominator\n\nP\\left(Q_1 \\cup Q_2\\right).\n Using the inclusion-exclusion principle, we have: \nP\\left(Q_1 \\cup Q_2\\right) = P(Q_1) + P(Q_2) - P(Q_1 \\cap Q_2).\n Because the two children are independent, we have: \nP(Q_1 \\cap Q_2) = P(Q_1)P(Q_2) = \\left(\\frac{p}{2}\\right)^2 = \\frac{p^2}{4}.\n Thus, the denominator is: \nP\\left(Q_1 \\cup Q_2\\right) = \\frac{p}{2} + \\frac{p}{2} - \\frac{p^2}{4} = p - \\frac{p^2}{4} = \\frac{p}{4}\\left(4 - p\\right).\n\n\n\n29.1.3 final result\nFinally, putting everything together, we have: \nP(B_1 \\cap B_2 \\mid Q_1 \\cup Q_2) = \\frac{\\frac{p}{4}(2 - p)}{\\frac{p}{4}\\left(4 - p\\right)} = \\frac{2 - p}{4 - p}.\n We can now plug in the values of p for each question:\n\n\n\n\n\n\n\n\nQuestion\nP(T_j)=p\nAnswer\n\n\n\n\nQ0\nNone\n1/4=0.25\n\n\nQ1\n1\n\\frac{1}{3} \\approx 0.333\n\n\nQ2\n1/2\n\\frac{3}{7} \\approx 0.429\n\n\nQ3\n1/7\n\\frac{13}{27} \\approx 0.481\n\n\nQ4\n1/30\n\\frac{59}{119} \\approx 0.496\n\n\n\nWhen no qualifications are made, the probability is 1/4. When qualifications are made, the probability increases, approaching 1/2 as the qualification becomes more stringent.\nThe algebra is done, but I’m left with an uneasy feeling. WHY?! What does it really matter what day of the week the boy was born on? How can that possibly affect the result?!",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>the boy-girl paradox</span>"
    ]
  },
  {
    "objectID": "bayes/boy-girl-paradox.html#visual-solution",
    "href": "bayes/boy-girl-paradox.html#visual-solution",
    "title": "29  the boy-girl paradox",
    "section": "29.2 visual solution",
    "text": "29.2 visual solution\n\n29.2.1 Q0, what is the probability that both children are boys?\nIn this non-qualified case, there are four equally likely possibilities for the two children: BB, BG, GB, GG. It is obvious that only one of these four possibilities corresponds to both children being boys (purple square), so the probability is 1/4.\n\n\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\n\n\nvisualizing Q0\nfig, ax = plt.subplots(figsize=(6, 6))\nN = 2\na = np.ones((N,N))\na[0,0] = 0\nim = ax.imshow(a,\n               interpolation='none', vmin=0, vmax=1, aspect='equal')\n# Major ticks\nax.set_xticks(np.arange(0, N, 1))\nax.set_yticks(np.arange(0, N, 1))\n\n# Labels for major ticks\nax.set_xticklabels([\"B\", \"G\"])\nax.set_yticklabels([\"B\", \"G\"])\n\n# Minor ticks\nax.set_xticks(np.arange(-.5, N, 1), minor=True)\nax.set_yticks(np.arange(-.5, N, 1), minor=True)\n\n# Gridlines based on minor ticks\nax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n\n# Remove minor ticks\nax.tick_params(which='minor', bottom=False, left=False)\n\nax.set_title(\"Mary has two children. Probability that both are boys?\");\n\n\n\n\n\n\n\n\n\n\n\n29.2.2 Q1, what is the probability that both children are boys, given that at least one is a boy?\nWe divide the question into two parts: the qualification and the target event.\nThe target event is the purple square, where both children are boys.\nThe qualification is represented by the red rectangles.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 6))\nN = 2\na = np.ones((N,N))\na[0,0] = 0\nim = ax.imshow(a,\n               interpolation='none', vmin=0, vmax=1, aspect='equal')\n# Major ticks\nax.set_xticks(np.arange(0, N, 1))\nax.set_yticks(np.arange(0, N, 1))\n\n# Labels for major ticks\nax.set_xticklabels([\"B\", \"G\"])\nax.set_yticklabels([\"B\", \"G\"])\n\n# Minor ticks\nax.set_xticks(np.arange(-.5, N, 1), minor=True)\nax.set_yticks(np.arange(-.5, N, 1), minor=True)\n\n# Gridlines based on minor ticks\nax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n\n# Remove minor ticks\nax.tick_params(which='minor', bottom=False, left=False)\n\nrect = patches.Rectangle((-0.5, -0.5), N, 1, \n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\nrect = patches.Rectangle((-0.5, -0.5), 1, N,\n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\n\nax.set_title(\"Mary has two children. Probability that both are boys\\ngiven that at least one is a boy?\");\n\n\n\n\n\n\n\n\n\nThe horizontal rectangle corresponds to the first child being a boy, and the vertical rectangle corresponds to the second child being a boy. The squares inside the red rectangles represent the remaining possibilities after the qualification. Each rectangle is 2\\times 1, the number 1 representing the constraint that at least one child is a boy, and the number 2 representing the two equally likely possibilities for the other child. The total number of squares inside the red rectangles is:\nD = (2 \\times 1)\\cdot 2 - 1 = 3.\n\n(2 \\times 1) is the size of each rectangle,\n\\cdot 2 because there are two rectangles,\n-1 because the square on the top left corner is counted twice (it is inside both rectangles).\n\nOf the remaining possibilities (D=3), only one (N=1) corresponds to both children being boys (the purple square).\nThe probability is therefore: \nP = \\frac{N}{D} = \\frac{1}{3} \\approx 0.333.\n\n\n\n29.2.3 Q1, what is the probability that both children are boys, given that at least one is a boy born during daytime?\nThe target event is still the purple square, where both children are boys.\nThe qualification is now represented by larger red rectangles.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 6))\nN = 4  # 2*2\na = np.ones((N,N))\na[0:2,0:2] = 0\n# a[0,:] = 0.2\n\nim = ax.imshow(a, interpolation='none', vmin=0, vmax=1, aspect='equal')\n\n\n# Major ticks\nax.set_xticks(np.arange(0, N, 1))\nax.set_yticks(np.arange(0, N, 1))\n\n# Labels for major ticks\nax.set_xticklabels([\"B-day\", \"B-night\", \"G-day\", \"G-night\"])\nax.set_yticklabels([\"B-day\", \"B-night\", \"G-day\", \"G-night\"])\n\n# Minor ticks\nax.set_xticks(np.arange(-.5, N, 1), minor=True)\nax.set_yticks(np.arange(-.5, N, 1), minor=True)\n\n# Gridlines based on minor ticks\nax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n\n# Remove minor ticks\nax.tick_params(which='minor', bottom=False, left=False)\n\nrect = patches.Rectangle((-0.5, -0.5), N, 1, \n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\nrect = patches.Rectangle((-0.5, -0.5), 1, N,\n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\n\nax.set_title(\"Mary has two children. Probability that both are boys\\ngiven that at least one is a boy born during daytime?\");\n\n\n\n\n\n\n\n\n\nEach rectangle is 2\\cdot 2 over 1 squares, where\n\nthe number 1 represents the constraint that at least one child is a boy born during daytime,\nthe first 2 represents the two equally likely possibilities for the sex of the other child,\nthe second 2 represents the two equally likely possibilities for the time of birth (daytime or nighttime).\n\nThe total number of squares inside the red rectangles is: \nD = (2 \\cdot 2)\\cdot 2 - 1 = 7.\n\nThe factor (2 \\cdot 2) is the length of the rectangles, we multiply by 2 because there are two rectangles, and we subtract 1 because the square on the top left corner is counted twice (it is inside both rectangles).\nHow many of those squares correspond to both children being boys (the intersection of the red rectangle and the purple square)? The reasoning of discounting the overlap square still holds, but clearly half of each original red rectangle should be discarded, since only half lay in the purple region:\n\nN = \\frac{(2 \\cdot 2)}{2}\\cdot 2 - 1 = 3.\n\nThe probability is therefore: \nP = \\frac{N}{D} = \\frac{3}{7} \\approx 0.429.\n\n\n\n29.2.4 Q2, what is the probability that both children are boys, given that at least one is a boy born on a Sunday?\nIt seems that a pattern is emerging.\nThe target event is still the purple square, as before.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 6))\nN = 2*7\na = np.ones((N,N))\na[0:7,0:7] = 0\n# a[0,:] = 0.2\n\nim = ax.imshow(a, interpolation='none', vmin=0, vmax=1, aspect='equal')\n\n\n# Major ticks\nax.set_xticks(np.arange(0, N, 1))\nax.set_yticks(np.arange(0, N, 1))\n\n# Labels for major ticks\nlabels = [f\"B{x+1:d}\" for x in range(7)] + [f\"G{x+1:d}\" for x in range(7)]\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\n\n# Minor ticks\nax.set_xticks(np.arange(-.5, N, 1), minor=True)\nax.set_yticks(np.arange(-.5, N, 1), minor=True)\n\n# Gridlines based on minor ticks\nax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n\n# Remove minor ticks\nax.tick_params(which='minor', bottom=False, left=False)\n\nrect = patches.Rectangle((-0.5, -0.5), N, 1, \n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\nrect = patches.Rectangle((-0.5, -0.5), 1, N,\n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\n\nax.set_title(\"Mary has two children. Probability that both are boys\\ngiven that at least one is a boy born on a Sunday?\");\n\n\n\n\n\n\n\n\n\nThe red rectangles are now 2\\cdot 7 over 1 squares, where 1 represents the constraint that at least one child is a boy born on a Sunday, 2 represents the two equally likely possibilities for the sex of the other child, and 7 represents the seven equally likely possibilities for the day of the week of birth.\nThe total number of squares inside the red rectangles is: \nD = (2 \\cdot 7)\\cdot 2 - 1 = 27.\n\nOf those squares, the number that lay in the purple region is: \nN = \\frac{(2 \\cdot 7)}{2}\\cdot 2 - 1 = 13.\n\nThe probability is therefore: \nP = \\frac{N}{D} = \\frac{13}{27} \\approx 0.481.\n\n\n\n29.2.5 generalization\nWe can generalize the calculation above for any trait with k equally likely possibilities:\n\\begin{align*}\nD & = (2 \\cdot k)\\cdot 2 - 1 = 4k - 1, \\\\\nN & = \\frac{(2 \\cdot k)}{2}\\cdot 2 - 1 = 2k - 1, \\\\\nP & = \\frac{N}{D} = \\frac{2k - 1}{4k - 1}.\n\\end{align*}\nThis is very similar to the analytical result we obtained before. Indeed, substituting k=1/p in the result above yields the same expression: \nP = \\frac{2 - p}{4 - p}.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>the boy-girl paradox</span>"
    ]
  },
  {
    "objectID": "bayes/boy-girl-paradox.html#the-moral-explanation",
    "href": "bayes/boy-girl-paradox.html#the-moral-explanation",
    "title": "29  the boy-girl paradox",
    "section": "29.3 the moral explanation",
    "text": "29.3 the moral explanation\nMathematicians use the word “moral” to refer to explanations that capture the underlying reason why something must be true, rather than just providing a mechanical, step-by-step verification. See this last image, for the case that at least one on the children is a boy born on the 1st of the month.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 6))\nN = 2*30\na = np.ones((N,N))\na[0:30,0:30] = 0\n# a[0,:] = 0.2\n\nim = ax.imshow(a, interpolation='none', vmin=0, vmax=1, aspect='equal')\n\n\n# Major ticks\nax.set_xticks(np.arange(0, N, 1))\nax.set_yticks(np.arange(0, N, 1))\n\n# Labels for major ticks\n# labels = [f\"B{x+1:d}\" for x in range(7)] + [f\"G{x+1:d}\" for x in range(7)]\nax.set_xticklabels([])\nax.set_yticklabels([])\n\n# Minor ticks\nax.set_xticks(np.arange(-.5, N, 1), minor=True)\nax.set_yticks(np.arange(-.5, N, 1), minor=True)\n\n# Gridlines based on minor ticks\nax.grid(which='minor', color='w', linestyle='-', linewidth=1)\n\n# Remove minor ticks\nax.tick_params(which='minor', bottom=False, left=False)\n\nrect = patches.Rectangle((-0.5, -0.5), N, 1, \n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\nrect = patches.Rectangle((-0.5, -0.5), 1, N,\n                         linewidth=4, \n                         edgecolor='red', \n                         facecolor='none',\n                         clip_on=False,\n                         zorder=10)\nax.add_patch(rect)\n\nax.set_title(\"Mary has two children. Probability that both are boys\\ngiven that at least one is a boy born on the 1st of the month?\");\n\n\n\n\n\n\n\n\n\nWe can see that the answer we are seeking is the ratio between the number of purple squares inside the red rectangles and the total number of squares inside the red rectangles (discounting the overlap square for each of them). As the qualification becomes more stringent (the trait has more equally likely possibilities), the red rectangles become taller and taller, and this ratio approaches 1/2. The act of discounting the overlap square becomes negligible for large k, since it is only one square out of many.\nIn question Q1, saying that one child is a boy is a vague statement. Which of them? Could be either. This great vagueness comes from the huge overlap between the two red rectangles. In question Q4, saying that one child is a boy born on the 1st of the month is a very specific statement. Almost certainly it could only be one of the children, since it is very unlikely that both children were born on the 1st of the month. This lack of vagueness comes from the tiny overlap between the two red rectangles. A super specific qualification effectively points at a specific child, and thus the problem reduces to finding the probability that the other child is a boy, which is clearly 1/2. That is the moral argument.\nI have two children. One is a boy, born on 29 February, he plays the flute, goes to karate classes, and got stiches in his chin last summer after a minor bike accident. What is the probability that my other child is also a boy? We can all agree that there is very little doubt that I’m talking about a specific child, since it is so unlikely that both children share all those traits. Therefore, the probability that my other child is a boy is 1/2.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>the boy-girl paradox</span>"
    ]
  },
  {
    "objectID": "bayes/boy-girl-paradox.html#information",
    "href": "bayes/boy-girl-paradox.html#information",
    "title": "29  the boy-girl paradox",
    "section": "29.4 information",
    "text": "29.4 information\nWhen I first heard this “paradox”, I could not fathom how adding irrelevant information (the day of the week of birth) could possibly affect the answer. As we have seen, it does. I want to quantify the amount of information added by the qualification.\nAs we saw in the cross-entropy chapter, we can use the Kullback-Leibler divergence to quantify the information gained when updating our beliefs from a prior distribution to a posterior distribution:\n\nD_{KL}(P \\| Q) = \\sum_i P(i) \\log\\frac{P(i)}{Q(i)},\n where P is the posterior distribution and Q is the prior distribution. The greater the difference between the two distributions, the greater the information gain. Another way to say this is that the qualification that most changes our beliefs is the one that provides the most information.\nBefore we plug in the numbers, we need to define the prior and posterior distributions.\n\n\\mathbf{q}, Prior distribution: our belief about the sex of the second child before knowing anything about the first child. This is simply \n\\mathbf{q} = \\begin{bmatrix} P(B_2) \\\\ P(G_2) \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ 1/2 \\end{bmatrix}.\n\n\\mathbf{p}, Posterior distribution: our belief about the sex of the second child after knowing that at least one child is a qualified boy. This is \n\\mathbf{p} = \\begin{bmatrix} P(B_2) \\\\ P(G_2) \\end{bmatrix} = \\begin{bmatrix} \\frac{2 - p}{4 - p} \\\\ 1-\\frac{2 - p}{4 - p} \\end{bmatrix}.\n\n\nPlugging in these values, we can calculate the Kullback-Leibler divergence for a qualification with probability p:\n\nD_{KL}(\\mathbf{p} \\| \\mathbf{q}) = \\frac{2 - p}{4 - p} \\log\\left(\\frac{2 - p}{4 - p} \\cdot \\frac{1}{1/2}\\right) + \\left(1-\\frac{2 - p}{4 - p}\\right) \\log\\left(\\left(1-\\frac{2 - p}{4 - p}\\right) \\cdot \\frac{1}{1/2}\\right).\n\nThere are other possibilities to consider regarding the prior distribution. For example, what if we considered the joint distribution of both children, instead of just the second child? In this case, the prior distribution would be: \n\\mathbf{q} = \\begin{bmatrix} P(B_1 \\cap B_2) \\\\ P(B_1 \\cap G_2) \\\\ P(G_1 \\cap B_2) \\\\ P(G_1 \\cap G_2) \\end{bmatrix} = \\begin{bmatrix} 1/4 \\\\ 1/4 \\\\ 1/4 \\\\ 1/4 \\end{bmatrix}.\n\nand the posterior distribution would be: \n\\mathbf{p} = \\begin{bmatrix} P(B_1 \\cap B_2) \\\\ P(B_1 \\cap G_2) \\\\ P(G_1 \\cap B_2) \\\\ P(G_1 \\cap G_2) \\end{bmatrix} = \\begin{bmatrix} \\frac{2 - p}{4 - p} \\\\ \\frac{1}{4 - p} \\\\ \\frac{1}{4 - p} \\\\ 0 \\end{bmatrix}.\n\n\nThe first entry corresponds to both children being boys. This expression is what we found before.\nThe last entry corresponds to both children being girls, which is impossible given the qualification, so its probability is zero.\nThe two middle entries correspond to one boy and one girl, which are equally likely, so they have the same probability. Since the total probability must sum to one, each of them has probability \\frac{1}{4 - p}.\n\nLet’s plot the Kullback-Leibler divergence for both definitions of prior and posterior distributions, as a function of p.\n\n\nShow the code\nfig, ax = plt.subplots(3, 1,figsize=(8, 8), sharex=True)\nfig.subplots_adjust(hspace=0.1)\n\np = np.linspace(0.001, 1, 100)\nq1A, q2A = [1/2, 1/2]\nq1B, q2B = [1/4, 1/4]\np1 = lambda p: (2 - p) / (4 - p)\np2 = lambda p: 1 - p1(p)\nDKL_A = lambda p: p1(p) * np.log2(p1(p) / q1A) + p2(p) * np.log2(p2(p) / q2A)\nDKL_B = lambda p: p1(p) * np.log2(p1(p) / q1B) + 2*(p2(p)/2) * np.log2((p2(p)/2) / q2B)\n\nax[0].plot(p, p1(p), lw=2, label=r\"$P(\\mathrm{BB} \\mid \\mathrm{B})$\")\nax[0].plot(p, 0*p+0.5, lw=1, ls='--', color='gray')\n\nax[1].plot(p, DKL_A(p), lw=2)\n# ax[1].plot(p, 0*p, lw=1, ls='--', color='gray')\nax[0].set(ylabel=\"probability that the\\nother child is a boy\")\nax[1].set(ylabel=r\"Information gain ($D_{KL}$, bits)\")\nax[2].plot(p, DKL_B(p), lw=2)\nax[2].set(xlabel=\"p: probability of the specific trait\",\n          ylabel=r\"Information gain ($D_{KL}$, bits)\")\n\nlabels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\nps = [1, 1/2, 1/7, 1/30]\nfor i, p_val in enumerate(ps):\n    ax[0].plot([p_val], [p1(p_val)], ls=None, marker='o', markerfacecolor='None', markeredgecolor='red', markersize=8, markeredgewidth=2)\n    ax[0].text(p_val+0.02, p1(p_val), labels[i], ha='left')\n\n    ax[1].plot([p_val], [DKL_A(p_val)], ls=None, marker='o', markerfacecolor='None', markeredgecolor='red', markersize=8, markeredgewidth=2)\n    ax[1].text(p_val-0.06, DKL_A(p_val), labels[i], ha='left')\n\n    ax[2].plot([p_val], [DKL_B(p_val)], ls=None, marker='o', markerfacecolor='None', markeredgecolor='red', markersize=8, markeredgewidth=2)\n    ax[2].text(p_val-0.06, DKL_B(p_val), labels[i], ha='left')\n\nax[1].text(0.5, 0.98, \"Prior: other child is equally likely to be a boy or a girl\", ha='center', transform=ax[1].transAxes, va='top')\nax[2].text(0.5, 0.98, \"Prior: joint distribution of both children\", ha='center', transform=ax[2].transAxes, va='top');\n\n\n\n\n\n\n\n\n\nSome considerations:\n\nQuestion Q0 does not appear on the graph because there is no qualification, there is no p to plot.\nIn the top graph we see that as the qualification becomes more stringent (smaller p), the probability that the other child is a boy increases from 1/3 to 1/2.\nIn the middle and bottom graphs we see divergent behavior of the information gain as p approaches zero, depending on the choice of prior distribution.\nWhen considering only the second child (middle graph), the information decreases to zero with increasing qualification (read graph from right to left). This makes sense, because we initally assumed that the other child was equally likely to be a boy or a girl, and that’s exactly what we end up believing when the qualification becomes very stringent. The qualification that gives maximum information is “at least one of the children is a boy”.\nWhen considering the joint distribution of both children (bottom graph), the information increases with increasing qualification (read graph from right to left): we initially believed that all four combinations of children were equally likely (1/4), but the more stringent the qualification, the more certain we become that we’re talking about a specific child, and thus the problem reduces to finding the probability that the other child. In this case, the qualification that gives maximum information is the most specific one.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>the boy-girl paradox</span>"
    ]
  },
  {
    "objectID": "bayes/boy-girl-paradox.html#obsevation-vs-information",
    "href": "bayes/boy-girl-paradox.html#obsevation-vs-information",
    "title": "29  the boy-girl paradox",
    "section": "29.5 obsevation vs information",
    "text": "29.5 obsevation vs information\nThis is a beautiful example of the difference between observation and information. Data alone does not contain information. In the information theory sense, information is a measure of how much our beliefs (priors) change when we observe data.\n\nThe simple observation “at least one child is a boy” provides the maximum information for one choice of prior distribution (middle graph), and the least information for the other choice of prior distribution (bottom graph).\nThe very specific and long observation “at least one child is a boy born on 29 February, he likes sushi but doesn’t like pasta, and he dreams of becoming an animal ophthalmologist” provides almost zero information for one choice of prior distribution, and (almost) maximum information for the other choice of prior distribution.\n\nThe arguments above are very much related to the idea of “theory-ladenness of observation” in the philosophy of science. Laden means “heavily loaded”, and the idea is that every observation we make only has meaning within a theoretical framework: our prior beliefs, theories, and expectations. This philosophical idea goes against empiricism, which states that knowledge comes only or primarily from sensory experience. The accumulation of sensory experiences alone does not lead to knowledge without our expectations of how the world works.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>the boy-girl paradox</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html",
    "href": "bayes/monty-hall.html",
    "title": "30  monty hall",
    "section": "",
    "text": "30.1 Bayes’ Theorem\nThe famous Monty Hall problem is a great opportunity to apply Bayes’ theorem. The problem is named after Monty Hall, the original host of the television game show Let’s Make a Deal.\nImagine you’re a contestant on the show, and you’re presented with three doors. Behind one door is a car (the prize you want), and behind the other two doors are goats (which you don’t want). You pick a door, say door 1. Monty, who knows what’s behind each door, then opens another door, say door 2, which has a goat behind it. He then gives you the option to switch your choice to the remaining unopened door (door 3) or stick with your original choice (door 1). Would it be smarter to switch or stay?\nWe can formulate this problem as the conditional probability:\nP(\\text{car behind 1} \\mid \\text{Monty opens 2})\nWe can use Bayes’ theorem to compute this:\nP(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)},\nwhere\nLet’s rewrite Bayes’ theorem for our specific events:\nP(\\text{car behind 1} \\mid \\text{Monty opens 2}) = \\frac{P(\\text{Monty opens 2} \\mid \\text{car behind 1}) P(\\text{car behind 1})}{P(\\text{Monty opens 2})}\nLet’s compute each term.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#bayes-theorem",
    "href": "bayes/monty-hall.html#bayes-theorem",
    "title": "30  monty hall",
    "section": "",
    "text": "A is the event “car behind door 1”\nB is the event “Monty opens door 2”\n\n\n\n\n\n30.1.1 likelihood\nP(\\text{Monty opens 2} \\mid \\text{car behind 1})\nIn English: assuming we know the car is behind door 1, what is the probability that Monty opens door 2? In that case, Monty could have opened either door 2 or door 3 with equal probability, so the answer is 1/2.\n\n\n30.1.2 prior\nP(\\text{car behind 1})\nBefore Monty opened any door, the probability that the car is behind door 1 is 1/3.\n\n\n30.1.3 evidence\nP(\\text{Monty opens 2})\nWe can compute this using the law of total probability. The evidence is a marginal probability, and it is the sum of the joint probabilities over all possible locations of the car:\n\\begin{align*}\nP(\\text{Monty opens 2}) &= P(\\text{Monty opens 2} \\cap \\text{car behind 1}) \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind 2}) \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind 3})\n\\end{align*}\nBy the rules of the game, the second term is zero, because Monty will never ruin the fun by opening the door with the car behind it. For the other two terms, we can use the definition of conditional probability:\n\nFirst term: \n  P(\\text{Monty opens 2} \\cap \\text{car behind 1}) = P(\\text{Monty opens 2} \\mid \\text{car behind 1}) \\cdot P(\\text{car behind 1})\n   The first part is the likelihood we computed above (1/2), and the second part is the prior (1/3). So the first term is 1/6.\nThird term: \n  P(\\text{Monty opens 2} \\cap \\text{car behind 3}) = P(\\text{Monty opens 2} \\mid \\text{car behind 3}) \\cdot P(\\text{car behind 3})\n   In this case, if the car is behind door 3, Monty has no choice but to open door 2, so the first part is 1. The second part is again the prior (1/3). So the third term is 1/3.\n\nPutting it all together, the evidence is: \nP(\\text{Monty opens 2}) = \\frac{1}{6} + 0 + \\frac{1}{3} = \\frac{1}{2}\n\nFinally, we can plug everything back into Bayes’ theorem: \nP(\\text{car behind 1} \\mid \\text{Monty opens 2}) = \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{\\frac{1}{2}} = \\frac{1}{3}\n\nThe probability that the car is behind door 1 is still 1/3. The logical conclusion is that the probability that the car is behind door 3 is 2/3, so you should switch!",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#reflections",
    "href": "bayes/monty-hall.html#reflections",
    "title": "30  monty hall",
    "section": "30.2 reflections",
    "text": "30.2 reflections\nI posed this question to my children, and they said the same thing: “What does it matter if Monty reveals that another door has a goat? That doesn’t change anything! There are now two doors, each equally likely to have the car behind it.” (those weren’t exactly their words, I’m paraphrasing).\nThere is both truth and confusion in what they said!\n“It doesn’t change anything!” This is “sorta” true. Our prior probability that the car is behind door 1 was 1/3, and our posterior probability is still 1/3. The information that Monty revealed a goat behind door 2 did not change our belief about door 1. Nothing changed here!\n“There are now two doors, each equally likely to have the car behind it.” Since the probability that the car is behind door 1 is 1/3, the probability that it is behind door 3 must be 2/3. The two doors are not equally likely to have the car behind them!\nHow could all the probability from door 2 have been fully transferred to door 3?",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#monty-selects-at-random",
    "href": "bayes/monty-hall.html#monty-selects-at-random",
    "title": "30  monty hall",
    "section": "30.3 Monty selects at random",
    "text": "30.3 Monty selects at random\nLet’s imagine a different game, where Monty opens a door at random (one you haven’t chosen), and it just so happens that he reveals a goat. How would the calculation change?\nThe likelihood P(\\text{Monty opens 2} \\mid \\text{car behind 1}) remains the same (1/2). If the car is behind door 1, Monty could have opened either door 2 or door 3 with equal probability.\nThe prior P(\\text{car behind 1}) is also the same (1/3).\nNow let’s see the evidence:\n\\begin{align*}\nP(\\text{Monty opens 2}) &= P(\\text{Monty opens 2} \\cap \\text{car behind 1}) \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind 2}) \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind 3}),\n\\end{align*}\nbecomes\n\\begin{align*}\nP(\\text{Monty opens 2}) &= P(\\text{Monty opens 2} \\mid \\text{car behind 1})P(\\text{car behind 1}) \\\\\n                        &+ P(\\text{Monty opens 2} \\mid \\text{car behind 2})P(\\text{car behind 2}) \\\\\n                        &+ P(\\text{Monty opens 2} \\mid \\text{car behind 3})P(\\text{car behind 3}),\n\\end{align*}\nAll three terms are the same now! Monty will randomly open one of the remaining two doors, regardless of where the car is. Sure, that could ruin the game, but bear with me. The three terms above are all the product of 1/2 (the probability that Monty randomly opens door 2) and 1/3 (the prior probability that the car is behind each door). So we have:\n\nP(\\text{Monty opens 2}) = \\frac{1}{2}\\cdot \\frac{1}{3} + \\frac{1}{2}\\cdot \\frac{1}{3} + \\frac{1}{2}\\cdot \\frac{1}{3} = \\frac{1}{2}\n\nHmmm… I just assumed that the evidence would be different from the original problem, but it turns out to be the same (1/2). All results being the same, the posterior probability that the car is behind door 1 is still 1/3. Where’s the catch?!\nI think that now we know that with a 1/3 probability Monty would have revealed the car behind door 2, ruining the game. But since he revealed a goat, we are in the 2/3 probability case where the car is behind either door 1 or door 3. Since we calculated that the probability that the car is behind door 1 is still 1/3, it follows that the probability that it is behind door 3 is 1/3. When Monty opens a door at random there is no “transfer of probability”, and it doesn’t matter if we switch or not! The transfer of probability happens only when Monty judiciously chooses which door to open. Let’s try another variation.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#many-doors-to-select-from",
    "href": "bayes/monty-hall.html#many-doors-to-select-from",
    "title": "30  monty hall",
    "section": "30.4 many doors to select from",
    "text": "30.4 many doors to select from\nWhat if the game had N doors (think 100, something big), and you picked door 1? Let’s call “door 2” the door that Monty opens, revealing a goat. How do the probabilities change now?\nPrior: P(\\text{car behind 1}) = \\frac{1}{N}\nLikelihood: P(\\text{Monty opens 2} \\mid \\text{car behind 1}) = \\frac{1}{N-1}. If the car is behind door 1, Monty can open any of the other N-1 doors with equal probability.\nEvidence:\n\\begin{align*}\nP(\\text{Monty opens 2}) &= P(\\text{Monty opens 2} \\cap \\text{car behind 1}) \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind 2}) \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind 3}) \\\\\n                        &+ \\ldots \\\\\n                        &+ P(\\text{Monty opens 2} \\cap \\text{car behind N})\n\\end{align*}\nThis becomes\n\\begin{align*}\nP(\\text{Monty opens 2}) &= P(\\text{Monty opens 2} \\mid \\text{car behind 1})\\cdot P(\\text{car behind 1}) \\\\\n                        &+ 0 \\\\\n                        &+ P(\\text{Monty opens 2} \\mid \\text{car behind 3})\\cdot P(\\text{car behind 3}) \\\\\n                        &+ \\ldots \\\\\n                        &+ P(\\text{Monty opens 2} \\mid \\text{car behind N})\\cdot P(\\text{car behind N})\n\\end{align*}\nAgain, Monty will never open the door with the car behind it, so the second term is zero. The first term is the product of the likelihood and the prior, while all the rest of the N-2 terms are the same: if the car is behind any of those doors (3, 4, …, N), Monty can open door 2 with probability 1/(N-2), because door 1 is taken by you, and the door with the car behind it cannot be opened. The second part of those terms is the prior, which is 1/N. So we have:\n\\begin{align*}\nP(\\text{Monty opens 2}) &= \\frac{1}{N-1}\\cdot \\frac{1}{N} \\\\\n                        &+ 0 \\\\\n                        &+ \\frac{1}{N-2} \\cdot \\frac{1}{N} \\\\\n                        &+ \\ldots \\\\\n                        &+ \\frac{1}{N-2} \\cdot \\frac{1}{N}\n\\end{align*}\nClearly, there are N-2 identical terms in the sum, so we can write:\n\\begin{align*}\nP(\\text{Monty opens 2}) &= \\frac{1}{N-1}\\cdot \\frac{1}{N} + \\frac{1}{N} \\\\\n                        &= \\frac{1}{N}\\left( \\frac{1}{N-1} + 1 \\right) \\\\\n                        &= \\frac{1}{N}\\cdot \\frac{N}{N-1} \\\\\n                        &= \\frac{1}{N-1}\n\\end{align*}\nThis “miraculous” simplification is surely the key to understanding the transfer of probability! Let’s keep going.\nPosterior: Plugging everything back into Bayes’ theorem: \nP(\\text{car behind 1} \\mid \\text{Monty opens 2}) = \\frac{\\frac{1}{N-1} \\cdot \\frac{1}{N}}{\\frac{1}{N-1}} = \\frac{1}{N}.\n\nThe probability that the car is behind door 1 is still 1/N. All remaining doors (3, 4, …, N) are equally likely to have the car behind them, so the probability that the car is behind any of those doors is the complement of 1/N divided by the number of those doors (N-2).",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#transfer-of-probability",
    "href": "bayes/monty-hall.html#transfer-of-probability",
    "title": "30  monty hall",
    "section": "30.5 transfer of probability",
    "text": "30.5 transfer of probability\nThe fact that Monty knows where the car is and will never open a door with the car behind it creates a dependency between the doors. The “miraculous” simplification we saw before means that the evidence term cancels out the likelihood term when computing the posterior for door 1, leaving its probability unchanged.\nBayes’ theorem can be written as:\n\n\\text{Posterior} = \\left( \\frac{\\text{Likelihood}}{\\text{Evidence}} \\right) \\times \\text{Prior}\n\nThe term in the parentheses, what we found to be 1 in this case, is called the Bayes factor. In some contexts, this term is also called updating factor, because it tells us how much to update our prior belief based on the evidence we observed. Here, the updating factor for door 1 is 1, meaning that our belief about door 1 does not change.\nWhen Monty opens door 2, all the probability mass that was assigned to that door is equally redistributed among the remaining doors (except door 1, which we already established remains at 1/N).",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#information",
    "href": "bayes/monty-hall.html#information",
    "title": "30  monty hall",
    "section": "30.6 information",
    "text": "30.6 information\nHow much information did we get from Monty opening door 2? We can measure this using the Kullback-Leibler divergence between the prior and posterior distributions:\n\nD_{KL}(\\mathbf{p} \\| \\mathbf{q}) = \\sum_i \\mathbf{p}(i) \\log\\frac{\\mathbf{p}(i)}{\\mathbf{q}(i)},\n where \\mathbf{p} is the posterior distribution and \\mathbf{q} is the prior distribution.\nIn the case of three doors, the prior and posterior distributions are:\n\n\\mathbf{q} = \\left[ \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3} \\right], \\quad \\mathbf{p} = \\left[ \\frac{1}{3}, 0, \\frac{2}{3} \\right].\n\nWhen summing over all doors, the first term contributes 0, since the prior and posterior probabilities are the same (1/3). The second term also contributes 0, since the posterior probability is 0. Only the third term contributes to the information gain:\n\nD_{KL}(\\mathbf{p} \\| \\mathbf{q}) = \\frac{2}{3} \\log\\frac{\\frac{2}{3}}{\\frac{1}{3}} = \\frac{2}{3} \\log 2 \\approx 0.462.\n\nFor N doors, the prior and posterior distributions are:\n\n\\mathbf{q} = \\left[ \\frac{1}{N}, \\frac{1}{N}, \\ldots, \\frac{1}{N} \\right], \\quad \\mathbf{p} = \\left[ \\frac{1}{N}, 0, \\ldots, \\frac{1}{N-2} \\cdot \\frac{N-1}{N}, \\ldots, \\frac{1}{N-2} \\cdot \\frac{N-1}{N} \\right].\n\nOnce more, the first two terms contribute 0 to the information gain, and only the remaining N-2 terms contribute: \nD_{KL}(\\mathbf{p} \\| \\mathbf{q}) = (N-2) \\cdot \\frac{1}{N-2} \\cdot \\frac{N-1}{N} \\log\\frac{\\frac{1}{N-2} \\cdot \\frac{N-1}{N}}{\\frac{1}{N}} = \\frac{N-1}{N} \\log\\frac{N-1}{N-2}.\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 3))\nN = np.arange(3,21)\nDKL = ((N-1)/N) * np.log2((N-1)/(N-2))\nax.plot(N, DKL, marker='o')\nax.set(xlabel=\"Number of doors (N)\",\n       ylabel=\"Information Gain (bits)\",\n       title=\"Information Gain from Monty Opening a Door\",\n       xticks=N,\n       ylim=(0, 1)\n       );\n\n\n\n\n\n\n\n\n\nWe learn from computing the posterior that it is equal to the prior after Monty opens door 2. That, of course, is not the whole story. The calculation of the information gain makes it explicit that the posterior is not a number, but a distribution. This is important to remember when we work with Bayesian statistics. Let’s state the first sentence in a more precise way: The probability that the car is behind door 1 remains unchanged, but the probabilities of the other doors change, and we learn something from that change. The more doors there are, the less informative is the revelation that door 2 has a goat behind it, as shown in the plot above.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "bayes/monty-hall.html#the-best-explanations",
    "href": "bayes/monty-hall.html#the-best-explanations",
    "title": "30  monty hall",
    "section": "30.7 the best explanations",
    "text": "30.7 the best explanations\nA few days after I’ve written this chapter on the Monty Hall problem, I saw the following post by David Deutsch on X:\n\nThe first link is to the paper “Right for the wrong reasons: common bad arguments for the correct answer to the Monty Hall Problem” by Don Fallis and Peter J. Lewis, published on 15 January 2026 (exactly one week ago from when I’m writing this), in the philosophy journal Synthese. This is a great paper, I highly recommend it. There, we find this beautiful argument:\n\nThe Wi-Phi Probability Swap argument\nThere is a 1/3 chance that the car is behind the door that you initially chose. So 1/3 of the times that you play the game you will win by sticking with that door. And 2/3 of the times that you play you will lose by sticking. However if you switch to the door that Monty did not open you will lose all of the times that you would have won by sticking. And you will win all of the times that you would have lost by sticking. (In those cases, there is a car behind one of the two remaining doors and a goat behind the other. So, when Monty opens one of those doors and reveals a goat, you will win by switching to the other door.) So, you should switch to the last remaining door when Monty opens a door and reveals a goat\n\nThe second link is really the mind blowing one. It leads to a very short post in David Deutsch’s website, titled ” Monty Hall Problem”, published on 26 October 2013. There, Deutsch writes:\n\nConsider a different problem first: you’re faced with the same three boxes but now you can choose any one box OR any two boxes, and in the latter case receive the better of the two contents. It’s always better to choose two boxes, right? But the rules of the original game allow you to choose two! Here’s how. First point to the remaining box i.e. the one you’re not going to choose. Then Monty will open the worse of the two boxes you chose, and you take the better one.\n\nBeautiful! Thanks, David 🤯",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>monty hall</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html",
    "href": "svd_and_pca/svd_image_compression.html",
    "title": "31  SVD for image compression",
    "section": "",
    "text": "31.1 the image\nThis chapter is partially based on these sources:\nCheck out also this cool demo by Tim Baumann.\nI will use a black-and-white version of the photo below as the matrix to decompose. There are two reasons to use this image:\nimport libraries\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.decomposition import TruncatedSVD\nload jpg into numpy array, convert to grayscale, and display it\nimage = Image.open('../archive/images/splat.jpg')\ngray_image = image.convert('L')  # convert to grayscale\nimage_array = np.array(gray_image)  # make it a numpy array\n# display the image\nfig, ax = plt.subplots()\nax.imshow(image_array, cmap='gray')\nax.axis('off')  # Hide axis",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#the-image",
    "href": "svd_and_pca/svd_image_compression.html#the-image",
    "title": "31  SVD for image compression",
    "section": "",
    "text": "it is a tall and skinny matrix (width 1600 px, height 2600 px). Tall and skinny matrices are usually used in overdetermined systems, which are common in data science.\nthis is the image of the juice of a tomato I ate, as it fell on the concrete floor. I found this splat pattern so beautiful that I took a picture, and I wanted to immortalize it in this tutorial. You’re welcome.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#decomposition",
    "href": "svd_and_pca/svd_image_compression.html#decomposition",
    "title": "31  SVD for image compression",
    "section": "31.2 decomposition",
    "text": "31.2 decomposition\n\n\ndecompose with numpy in one line of code\nU, S, Vt = np.linalg.svd(image_array)\n\n\nLet’s see how the magnitude of the singular values decreases as we go from the first to the last (k goes from zero to 900-1). Also, let’s see how much of the total energy accumulated up to the k-th singular value squared.\n\n\nplot singular values and cumulative energy\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nlenS = len(S)\nax[0].plot(np.arange(lenS), S, marker='o', markeredgecolor=\"black\", markerfacecolor='None', markersize=5, linestyle='None', alpha=0.5)\nax[0].set_yscale('log')  # Set y-axis to log scale\nax[0].set(xlabel='k',\n          ylabel='singular value'\n         )\nax[0].grid(True)  # Enable grid on the first panel\n\ncumS2 = np.cumsum(S**2) / np.sum(S**2)\nax[1].plot(np.arange(lenS), cumS2, marker='o', markeredgecolor=\"black\", markerfacecolor='None', markersize=5, linestyle='None', alpha=0.5)\nklist = [5, 20, 100, 400]\nfor k in klist:\n    ax[1].plot([k-1], [cumS2[k-1]], marker='o', markeredgecolor=\"tab:red\", markerfacecolor='tab:red', markersize=5, linestyle='None')\n    ax[1].text(k+20, cumS2[k-1]-0.001, f'k={k}', color='tab:red', fontsize=12, ha='left')\n\nax[1].set(xlabel='k',\n          ylabel='cumulative energy'\n         )\nax[1].grid(True)  # Enable grid on the second panel\nax[1].yaxis.set_label_position(\"right\")  # Move ylabel to the right\nax[1].yaxis.tick_right()  # Move yticks to the right\n\n\n\n\n\n\n\n\n\nThe square of the Frobenius norm of the matrix X is equal to the sum of the squares of all its singular values.\n\n\\lVert X \\rVert_F^2 = \\sum_{i=1}^{r} \\sigma_i^2\n\nThe Frobenius norm is a measure of the “magnitude” or “size” of the matrix, which can be interpreted as the total amount of “information” in the data. By taking the cumulative sum of the squared singular values, we are effectively measuring how much of this total information is retained with each successive truncation. The term “energy” is an analogy from physics and signal processing. In these fields, the total energy of a signal is often defined as the integral of its squared magnitude over time. This concept carries over to data analysis where the squared singular values are a direct measure of the variance in the data along each singular vector, and the sum of these squares represents the total variance.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#truncation-and-reconstruction",
    "href": "svd_and_pca/svd_image_compression.html#truncation-and-reconstruction",
    "title": "31  SVD for image compression",
    "section": "31.3 truncation and reconstruction",
    "text": "31.3 truncation and reconstruction\nOur original matrix X has dimensions (1600, 2600) and rank 1600. Therefore, it has 1600 non-zero singular values. We can truncate the SVD to a lower rank k &lt; 1600 and reconstruct an approximation of the original matrix using only the first k singular values and their corresponding singular vectors:\n\nX_k = U_k \\Sigma_k V_k^T = \\sum_{i=1}^{k} \\sigma_i \\cdot \\text{outer}(u_i, v_i^T)\n\nwhere U_k is the matrix of the first k left singular vectors, \\Sigma_k is the diagonal matrix of the first k singular values, and V_k^T is the transpose of the matrix of the first k right singular vectors. The outer product \\text{outer}(u_i, v_i^T) creates a rank-1 matrix from the i-th left and right singular vectors.\nUsing the same truncation values k shown in red in the plot above, we can reconstruct approximations of the original image. As k increases, the reconstructed image becomes more detailed and closer to the original image.\n\n\nreconstruct image from first k singular values/vectors\ndef reconstruct_image(U, S, Vt, k):\n    X_reconstructed = np.zeros_like(image_array, dtype=np.float64)\n    for i in range(k): \n        X_reconstructed += S[i] * np.outer(U[:, i], Vt[i, :])   \n    X_reconstructed = np.clip(X_reconstructed, 0, 255)  # ensure values are in byte range\n    X_reconstructed = X_reconstructed.astype(np.uint8)  # convert to uint8\n    return X_reconstructed\n\nreconstructed_images = []\nfor k in klist:\n    X_reconstructed = reconstruct_image(U, S, Vt, k)\n    reconstructed_images.append(X_reconstructed)\n\n\n\n\nvisualize reconstructed images\nfig = plt.figure(figsize=(6, 8))\nfor i, k in enumerate(klist):\n    ax = fig.add_subplot(2, 2, i+1)\n    X_k = reconstructed_images[i]\n    ax.imshow(X_k, cmap='gray')\n    ax.set_title(f'k={k}')\n    ax.axis('off')  # Hide axis\n\n\n\n\n\n\n\n\n\nThe truncation for k=5 gives a blurry image, but for k=20 it is recognizably a tomato splat. The reconstructions for k=100 and k=400 seem indistinguishable at this resolution. Let’s zoom in on a small section of the image to see the differences more clearly.\n\n\nvisualize zoomed in reconstructed images\nfig = plt.figure(figsize=(6, 6))\nfor i, k in enumerate(klist):\n    ax = fig.add_subplot(2, 2, i+1)\n    X_k = reconstructed_images[i][700:1000, 700:1000]  # zoom in\n    ax.imshow(X_k, cmap='gray')\n    ax.set_title(f'k={k}')\n    ax.axis('off')  # Hide axis\n\n\n\n\n\n\n\n\n\nTo capture all the details in the concrete floor we need more than 100 singular values. If we’re interested in the overall shape of the splat, 100 singular values are more than enough. This justifies the name of this chapter: SVD for image compression. We can compress images by storing only the first k singular values and their corresponding singular vectors, instead of the entire image matrix.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#computational-efficiency",
    "href": "svd_and_pca/svd_image_compression.html#computational-efficiency",
    "title": "31  SVD for image compression",
    "section": "31.4 computational efficiency",
    "text": "31.4 computational efficiency\nWe calculated the reconstruction “the hard way”, by explicitly forming the outer products and summing them. However, we can also use matrix multiplication to achieve the same result more efficiently.\n\nX_k = U_k \\Sigma_k V_k^T\n where:\n\nU_k is the matrix formed by the first k columns of U.\n\\Sigma_k is the k \\times k diagonal matrix formed by the first k singular values, \\Sigma_{11}=\\sigma_1, \\Sigma_{22}=\\sigma_2, etc.\nV_k^T is the matrix formed by the first k rows of V^T.\n\nLet’s leverage matrix multiplication to compute the reconstruction: X_k = U_k (\\Sigma_k V_k^T).\nFirst, let’s look at the product of the diagonal singular value matrix \\Sigma_k and the truncated V^T matrix, V_k^T. \\Sigma_k is a k \\times k diagonal matrix with singular values \\sigma_1, \\sigma_2, ..., \\sigma_k on the diagonal. V_k^T is a k \\times n matrix where each row is a singular vector v_i^T.\n\n\\Sigma_k V_k^T = \\begin{bmatrix}\n\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma_k\n\\end{bmatrix}\n\\begin{bmatrix}\n— & v_1^T & — \\\\\n— & v_2^T & — \\\\\n& \\vdots & \\\\\n— & v_k^T & —\n\\end{bmatrix}\n\nMultiplying a diagonal matrix by another matrix from the left scales each row of the second matrix by the corresponding diagonal element of the first matrix. \n\\Sigma_k V_k^T = \\begin{bmatrix}\n— & \\sigma_1 v_1^T & — \\\\\n— & \\sigma_2 v_2^T & — \\\\\n& \\vdots & \\\\\n— & \\sigma_k v_k^T & —\n\\end{bmatrix}\n\nThis matrix contains the scaled singular vectors as its rows.\nNow, we multiply the truncated U matrix, U_k, with the result from the first step. U_k is an m \\times k matrix whose columns are the singular vectors u_1, u_2, ..., u_k. Let’s call the result from the first step, the matrix A. The product is U_k A.\n\nX_k = U_k A = \\begin{bmatrix}\n| & | & & | \\\\\nu_1 & u_2 & \\dots & u_k \\\\\n| & | & & |\n\\end{bmatrix}\n\\begin{bmatrix}\n— & \\sigma_1 v_1^T & — \\\\\n— & \\sigma_2 v_2^T & — \\\\\n& \\vdots & \\\\\n— & \\sigma_k v_k^T & —\n\\end{bmatrix}\n\nMatrix multiplication can be seen as a sum of outer products of the columns of the first matrix and the rows of the second matrix.\n\nX_k = \\sum_{i=1}^{k} (\\text{column } i \\text{ of } U_k) \\cdot (\\text{row } i \\text{ of } A)\n\n\nX_k = \\sum_{i=1}^{k} u_i (\\sigma_i v_i^T)\n\n\nX_k = \\sum_{i=1}^{k} \\sigma_i (u_i v_i^T)\n\nLet’s time the two methods of reconstruction to see the efficiency gain from using matrix multiplication.\n\nk = 100\n\nstart_time1 = time.time()\nX1 = reconstruct_image(U, S, Vt, k)\nend_time1 = time.time()\n\nstart_time2 = time.time()\nX2 = np.dot(U[:, :k], np.dot(np.diag(S[:k]), Vt[:k, :])) \nend_time2 = time.time()\n\nT1 = end_time1 - start_time1\nT2 = end_time2 - start_time2\n\nprint(f\"explicitly computing the outer products: {T1:.6f} seconds\")\nprint(f\"leveraging matrix multiplication: {T2:.6f} seconds\")\nprint(f\"speedup: {T1/T2:.2f}x\")\n\nexplicitly computing the outer products: 1.508656 seconds\nleveraging matrix multiplication: 0.011750 seconds\nspeedup: 128.40x\n\n\nThere are two equivalent ways of leveraging matrix multiplication. Because of the associativity of matrix multiplication, we can compute the product in two different orders:\n\nFirst compute B = \\Sigma_k V_k^T, then compute X_k = U_k B.\nFirst compute C = U_k \\Sigma_k, then compute X_k = C V_k^T.\n\nFor a tall-and-skinny matrix like our image (m&gt;n), the first method is more efficient because it involves multiplying a smaller intermediate matrix B (of size k \\times n) with U_k (of size m \\times k). The second method would involve multiplying a larger intermediate matrix C (of size m \\times k) with V_k^T (of size k \\times n), which is less efficient. For our 2600x1600 image, the difference is tiny, but for larger datasets it can be significant.\nSVD is such a common operation that most numerical computing libraries have highly optimized implementations. See below sklearn’s TruncatedSVD, which uses scipy.sparse.linalg.svds under the hood. It is designed to compute only the first k singular values and vectors, making it more efficient for large datasets where only a few singular values are needed.\n\n\nShow the code\nstart_time2b = time.time()\nX2b = np.dot(np.dot(U[:, :k], np.diag(S[:k])), Vt[:k, :])\nend_time2b = time.time()\n\nsvd = TruncatedSVD(n_components=100)\ntruncated_image = svd.fit_transform(image_array)\nstart_time3 = time.time()\nX3 = svd.inverse_transform(truncated_image)\nend_time3 = time.time()\n\nT2b = end_time2b - start_time2b\nT3 = end_time3 - start_time3\n\nprint(f\"matrix multiplication, option 2: {T2b:.6f} seconds\")\nprint(f\"using sklearn's TruncatedSVD: {T3:.6f} seconds\")\n\n\nmatrix multiplication, option 2: 0.068557 seconds\nusing sklearn's TruncatedSVD: 0.011218 seconds",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html",
    "href": "svd_and_pca/svd_regression.html",
    "title": "32  SVD for regression",
    "section": "",
    "text": "32.1 the problem with Ordinary Least Squares\nThis tutorial is partly based on the following sources:\nLet’s say I want to predict the weight of a person based on their height. I have the following data:\nimport libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nvisualize data\nh = np.array([150 ,160, 170, 180, 190])\nw = np.array([53,  65,  68,  82,  88])\nfig, ax = plt.subplots()\nax.scatter(h, w)\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"Weight (kg)\")\n\n\nText(0, 0.5, 'Weight (kg)')\nI can try to predict the weight using a linear model:\n\\text{weight} = \\beta_0 + \\beta_1 \\cdot \\text{height}.\n\\tag{1}\nIn a general form, we can write this as:\nX\\beta = y,\n\\tag{2}\nwhere X is the design matrix, \\beta is the vector of coefficients, and y is the vector of outputs (weights). This problem probably has no exact solution for \\beta, because the design matrix X is not square (there are more data points than parameters). So we want to find the best approximation \\hat{\\beta} that minimizes the error:\n\\hat{\\beta} = \\arg\\min_\\beta \\|y - X\\beta\\|^2.\n\\tag{3}\nWe know how to solve this, we use the equation we derived in the chapter “the geometry of regression”:\n\\hat{\\beta} = (X^TX)^{-1}X^Ty,\n\\tag{4}\nFor a linear model, the design matrix is:\nX =\n\\begin{bmatrix}\n| & | \\\\\n\\mathbf{1} & h\\\\\n| & |\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 150 \\\\\n1 & 160 \\\\\n1 & 170 \\\\\n1 & 180 \\\\\n1 & 190\n\\end{bmatrix}\n.\n\\tag{5}\nWhat does the matrix X^T X look like?\n\\begin{align*}\nX^TX &=\n\\begin{bmatrix}\n- & \\mathbf{1}^T & - \\\\\n- & h^T & -\n\\end{bmatrix}\n\\begin{bmatrix}\n| & | \\\\\n\\mathbf{1} & h\\\\\n| & |\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\mathbf{1}^T\\mathbf{1} & \\mathbf{1}^Th \\\\\nh^T\\mathbf{1} & h^Th\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n5 & 850 \\\\\n850 & 153000\n\\end{bmatrix}\n\\tag{6}\n\\end{align*}\nThere’s no problem inverting this matrix, so we can find the coefficient estimates \\hat{\\beta} using the formula above.\nSuppose now that we have a new predictor, the height of the person in inches. The design matrix now looks like this:\nX =\n\\begin{bmatrix}\n| & | & | \\\\\n\\mathbf{1} & h_{cm} & h_{inch}\\\\\n| & | & |\n\\end{bmatrix}\n\\tag{7}\nObviously, the columns h_{cm} and h_{inch} are linearly dependent (h_{cm}=ah_{inch}). This means that the matrix X^TX also has linearly dependent columns:\n\\begin{align*}\nX^TX &=\n\\begin{bmatrix}\n- & \\mathbf{1}^T & - \\\\\n- & h_{cm}^T & - \\\\\n- & h_{inch}^T & -\n\\end{bmatrix}\n\\begin{bmatrix}\n| & | & | \\\\\n\\mathbf{1} & h_{cm} & h_{inch}\\\\\n| & | & |\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\n\\mathbf{1}^T\\mathbf{1} & \\mathbf{1}^Th_{cm} & \\mathbf{1}^Th_{inch} \\\\\nh_{cm}^T\\mathbf{1} & h_{cm}^Th_{cm} & h_{cm}^Th_{inch} \\\\\nh_{inch}^T\\mathbf{1} & h_{inch}^Th_{cm} & h_{inch}^Th_{inch}\n\\end{bmatrix}\n\\tag{8}\n\\end{align*}\nUsing the fact that h_{cm}=ah_{inch}, we can see that the second and third columns are linearly dependent. This means that the matrix X^TX is not invertible, and we cannot use the formula above to find the coefficient estimates \\hat{\\beta}. What now?\nThis is an extreme case, but problems similar to this can happen in real life. For example, if we have two predictors that are highly correlated, the matrix X^TX will be close to singular (not invertible). In this case, the coefficients \\beta will be very sensitive to small changes in the data.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html#the-problem-with-ordinary-least-squares",
    "href": "svd_and_pca/svd_regression.html#the-problem-with-ordinary-least-squares",
    "title": "32  SVD for regression",
    "section": "",
    "text": "Height (cm)\nWeight (kg)\n\n\n\n\n150\n53\n\n\n160\n65\n\n\n170\n68\n\n\n180\n82\n\n\n190\n88",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html#svd-to-the-rescue",
    "href": "svd_and_pca/svd_regression.html#svd-to-the-rescue",
    "title": "32  SVD for regression",
    "section": "32.2 SVD to the rescue",
    "text": "32.2 SVD to the rescue\nLet’s use Singular Value Decomposition (SVD) to find the coefficients \\beta. SVD is a powerful technique that can handle multicollinearity and other issues in the data.\nThe SVD of a matrix X is given by:\n\nX = U\\Sigma V^T,\n\\tag{9}\n\nwhere U and V are orthogonal matrices and \\Sigma is a diagonal matrix with singular values on the diagonal.\nWe can plug the SVD of X into the least squares problem, which is to find the \\hat{\\beta} that best satisfies X\\hat{\\beta} = \\hat{y}:\n\nU\\Sigma V^T\\hat{\\beta} = \\hat{y}.\n\\tag{10}\n\nWe define now the Moore-Penrose pseudo-inverse of X, which is given by:\n\nX^+ = V\\Sigma^+U^T,\n\\tag{11}\n\nwhere \\Sigma^+ is obtained by taking the reciprocal of the non-zero singular values in \\Sigma and transposing the resulting matrix.\nThis pseudo-inverse has the following properties:\n\nX^+X = I. This means that X^+ is a left-inverse of X.\nXX^+ is a projection matrix onto the column space of X. In other words, left-multiplying XX^+ to any vector gives the projection of that vector onto the column space of X. In particular, XX^+y = \\hat{y}.\nThis is very similar to the property we used in the chapter “the geometry of regression”, where we had P_Xy = \\hat{y}, with P_X being the projection matrix onto the column space of X. There, we found that\n\n\\hat{\\beta} = (X^TX)^{-1}X^Ty,\n\nLeft-multiplying both sides by X gives: \nX\\hat{\\beta} = X(X^TX)^{-1}X^Ty = P_Xy = \\hat{y}.\n So we found that in the OLS case, X(X^TX)^{-1}X^T is the projection matrix onto the column space of X. Here, we have a more general result that works even when X^TX is not invertible: XX^+ is the projection matrix onto the column space of X.\n\nWe left-multiply Eq. (10) by X^+, and also substitute \\hat{y}=XX^+y as we just saw:\n\\begin{align*}\nX^+ (U\\Sigma V^T\\hat{\\beta}) &= X^+XX^+y\\\\\n(V\\Sigma^+U^T)(U\\Sigma V^T)\\hat{\\beta} &= X^+y\n\\tag{12}\n\\end{align*}\nThe term (V\\Sigma^+U^T)(U\\Sigma V^T) simplifies to the identity matrix, so we have:\n\n\\hat{\\beta} = X^+y = V\\Sigma^+U^Ty.\n\\tag{13}\n\nThis is the formula we will use to find the coefficients \\hat{\\beta} using SVD. This formula works even when X^TX is not invertible, and it is more stable than the OLS formula.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html#in-practice",
    "href": "svd_and_pca/svd_regression.html#in-practice",
    "title": "32  SVD for regression",
    "section": "32.3 in practice",
    "text": "32.3 in practice\nLet’s go back to the problematic example with height in cm and inches. We can use the SVD to find the coefficients \\hat{\\beta}.\n\n# design matrix with intercept, height in cm and height in inches\nconversion_factor = 2.54\nX = np.vstack([np.ones(len(h)), h, h/conversion_factor]).T\n# compute SVD of X\nU, S, VT = np.linalg.svd(X, full_matrices=False)\nSigma = np.diag(S)\nV = VT.T\n# compute coefficients using SVD\ny = w\nSigma_plus = np.zeros(Sigma.T.shape)\nfor i in range(len(S)):\n    if S[i] &gt; 1e-10:  # avoid division by zero\n        Sigma_plus[i, i] = 1 / S[i]\nX_plus = V @ Sigma_plus @ U.T\nbeta_hat = X_plus @ y\n# make predictions\ny_hat = X @ beta_hat\n\n\n\nplot results\nfig, ax = plt.subplots()\nax.scatter(h, w, label=\"data\")\n# plot predictions\nax.plot(h, y_hat, color=\"tab:red\", label=\"SVD fit\")\nax.legend()\nax.set_xlabel(\"height (cm)\")\nax.set_ylabel(\"weight (kg)\");",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html",
    "href": "decision_trees/CART_classification.html",
    "title": "33  CART: classification",
    "section": "",
    "text": "33.1 a jungle party\nImagine you’re throwing a party in the jungle. You know you have three types of guests—koalas, foxes, and bonobos—but you don’t know who is who. To make sure you serve the right food, you need to automatically figure out which animal is which. Koalas only eat eucalyptus leaves, foxes prefer meat, and bonobos love fruit, so it’s important to get it right!\nTo solve this problem, you’ll use a decision tree. You’ve gathered some data on your past animal guests, and for each one, you have their height and weight, as well as their species (their label). Your goal is to build a system that can learn from this historical data to correctly classify a new, unlabeled animal based on its height and weight alone.\nThe data is structured as:\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nWe are using the famous Iris dataset structure, but pretending they are animals for this example.\nload iris dataset and prepare data\niris = load_iris()\nX = iris.data[:, [0, 1]]\ny = iris.target\nplot\nfig, ax = plt.subplots(figsize=(6, 6))\nmarkers = ['o', 's', '+']\ncolors = ['black', 'None', 'tab:red']\niris.target_names = ['koala', 'fox', 'bonobo']\niris.feature_names = ['height', 'weight', 'age']\nfor i, marker in enumerate(markers):\n    ax.scatter(X[y == i, 0], X[y == i, 1], \n               c=colors[i], \n               edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\nax.legend()\nax.set_xlabel(iris.feature_names[0])\nax.set_ylabel(iris.feature_names[1])\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/3638754974.py:8: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax.scatter(X[y == i, 0], X[y == i, 1],\n\n\nText(0, 0.5, 'weight')\nWe will use the CART (Classification and Regression Trees) algorithm to build a decision tree. There are many other methods, but CART is one of the most popular, it’s important to know it well.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>CART: classification</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#a-jungle-party",
    "href": "decision_trees/CART_classification.html#a-jungle-party",
    "title": "33  CART: classification",
    "section": "",
    "text": "Features: height and weight, continuous, numerical features.\nCategories (Classes): Koala, Fox, Bonobo",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>CART: classification</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#the-split",
    "href": "decision_trees/CART_classification.html#the-split",
    "title": "33  CART: classification",
    "section": "33.2 the split",
    "text": "33.2 the split\nWe will cut the data into two parts along one of the features. We will try all possible cut thresholds for each of the features and see which one gives us the best split. Ideally, we want to end up with two groups, “left” and “right”, that are as “pure” as possible, meaning that each group contains animals of only one species. It might not be possible to get perfect homogeneity, so we will need to quantify how good a split is. We will use two different metrics to evaluate the quality of a split:\n\nInformation Gain (based on Entropy): This measures how much uncertainty in the data is reduced after the split. A higher information gain means a better split. \n  IG(T, X) = \\underbrace{H(T)}_{\\text{original entropy}} - \\underbrace{\\frac{N_\\text{left}}{N} H(T_\\text{left})}_{\\text{left group}} - \\underbrace{\\frac{N_\\text{right}}{N} H(T_\\text{right})}_{\\text{right group}}\n where T_\\text{left} and T_\\text{right} are the subsets after the split, and N denotes the total number of samples in a (sub) dataset.\nGini Impurity: This measure is often explained as the impurity of a split. A better point of view is to interpret is as the probability of misclassification. If we picked an element at random from the dataset, what is the probability that we would misclassify it if we labeled it according to the distribution of classes in the dataset? The probability of picking an element of class i is P_i, and we would misclassify it with probability 1 - P_i. So the total probability of misclassification is P_i(1 - P_i). If we sum this over all classes, we get the Gini Impurity: \n  G(T) = \\sum_{i=1}^{C} P_i (1 - P_i) = 1 - \\sum_{i=1}^{C} P_i^2\n\n\nNow let’s take our dataset and try to find the best split using both metrics.\n\n33.2.1 entropy\nWe will start with the entropy metric.\n\n\ndefine useful functions\ndef calculate_gini(y_subset):\n    \"\"\"Calculates the Gini Impurity for a given subset of class labels.\"\"\"\n    # If the subset is empty, there's no impurity.\n    if len(y_subset) == 0:\n        return 0.0\n    \n    # Get the counts of each unique class in the subset.\n    unique_classes, counts = np.unique(y_subset, return_counts=True)\n    \n    # Calculate the probability of each class.\n    probabilities = counts / len(y_subset)\n    \n    # Gini Impurity formula: 1 - sum(p^2) for each class\n    return 1.0 - np.sum(probabilities**2)\n\ndef calculate_entropy(y_subset):\n    \"\"\"Calculates the Entropy for a given subset of class labels.\"\"\"\n    if len(y_subset) == 0:\n        return 0.0\n    unique_classes, counts = np.unique(y_subset, return_counts=True)\n    probabilities = counts / len(y_subset)\n    epsilon = 1e-9\n    return -np.sum(probabilities * np.log2(probabilities + epsilon))\n\ndef find_best_split(X_feature, y, criterion='entropy'):\n    \"\"\"\n    Finds the best split point for a single feature based on a specified criterion.\n    Returns the best threshold and the score (Information Gain or Gini Impurity) after the split.\n    \"\"\"\n    # Get the unique values of the feature to consider as split points.\n    unique_values = np.sort(np.unique(X_feature))\n    differences = np.diff(unique_values)\n    threshold_candidates = unique_values[:-1] + differences / 2\n    if criterion == 'entropy':\n        initial_score = calculate_entropy(y)\n        best_score = 0.0  # We want to maximize Information Gain\n        criterion_function = calculate_entropy\n    elif criterion == 'gini':\n        best_score = 1.0  # We want to minimize Gini Impurity\n        criterion_function = calculate_gini\n    else:\n        raise ValueError(\"Criterion must be 'entropy' or 'gini'\")\n\n    best_threshold = None\n\n    # Iterate through each unique value as a potential split point\n    for threshold in threshold_candidates:\n        # Split the data into two groups based on the threshold\n        condition = X_feature &lt;= threshold\n        y_left = y[condition]    # condition is True\n        y_right = y[~condition]  # condition is False\n\n        score_left = criterion_function(y_left)\n        score_right = criterion_function(y_right)\n        fraction_left = len(y_left) / len(y)\n        fraction_right = len(y_right) / len(y)\n        weighted_score = fraction_left * score_left + fraction_right * score_right\n\n        if criterion == 'entropy':\n            information_gain = initial_score - weighted_score\n            # If this split is the best so far, save it!\n            if information_gain &gt; best_score:  # Max Information Gain\n                best_score = information_gain\n                best_threshold = threshold\n        \n        elif criterion == 'gini':\n            # If this split is the best so far, save it!\n            if weighted_score &lt; best_score:  # Min Gini Impurity\n                best_score = weighted_score\n                best_threshold = threshold\n\n    return best_threshold, best_score\n\ndef quantify_all_splits(X_feature, y, criterion='entropy'):\n    \"\"\"\n    Finds the best split point for a single feature based on a specified criterion.\n    Returns the best threshold and the score (Information Gain or Gini Impurity) after the split.\n    \"\"\"\n    # Get the unique values of the feature to consider as split points.\n    unique_values = np.sort(np.unique(X_feature))\n    differences = np.diff(unique_values)\n    threshold_candidates = unique_values[:-1] + differences / 2\n    score_list = []\n    if criterion == 'entropy':\n        initial_score = calculate_entropy(y)\n        best_score = 0.0  # We want to maximize Information Gain\n        criterion_function = calculate_entropy\n    elif criterion == 'gini':\n        best_score = 1.0  # We want to minimize Gini Impurity\n        criterion_function = calculate_gini\n    else:\n        raise ValueError(\"Criterion must be 'entropy' or 'gini'\")\n\n    best_threshold = None\n\n    # Iterate through each unique value as a potential split point\n    for threshold in threshold_candidates:\n        # Split the data into two groups based on the threshold\n        condition = X_feature &lt;= threshold\n        y_left = y[condition]    # condition is True\n        y_right = y[~condition]  # condition is False\n\n        score_left = criterion_function(y_left)\n        score_right = criterion_function(y_right)\n        fraction_left = len(y_left) / len(y)\n        fraction_right = len(y_right) / len(y)\n        weighted_score = fraction_left * score_left + fraction_right * score_right\n\n        if criterion == 'entropy':\n            information_gain = initial_score - weighted_score\n            score_list.append((threshold, information_gain))\n            # If this split is the best so far, save it!\n\n        elif criterion == 'gini':\n            score_list.append((threshold, weighted_score))\n\n    return np.array(score_list)\n\n\n\n\nplot\nfig = plt.figure(1, figsize=(8, 6))\ngs = gridspec.GridSpec(2, 2, width_ratios=[0.2,1], height_ratios=[1,0.2])\ngs.update(left=0.16, right=0.86,top=0.88, bottom=0.13, hspace=0.05, wspace=0.05)\n\nax_main = plt.subplot(gs[0, 1])\nax_height = plt.subplot(gs[1, 1])\nax_weight = plt.subplot(gs[0, 0])\n\nfor i, marker in enumerate(markers):\n    ax_main.scatter(X[y == i, 0], X[y == i, 1], \n                    c=colors[i], \n                    edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nsplit_feature_height = quantify_all_splits(X[:, 0], y, criterion='entropy')\nsplit_feature_weight = quantify_all_splits(X[:, 1], y, criterion='entropy')\n\nax_height.plot(split_feature_height[:, 0], split_feature_height[:, 1], marker='o')\nax_weight.plot(split_feature_weight[:, 1], split_feature_weight[:, 0], marker='o')  \n\nbest_height_split = np.argmax(split_feature_height[:, 1])\nbest_weight_split = np.argmax(split_feature_weight[:, 1])\n\nax_main.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_height.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_main.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\nax_weight.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\n\nax_main.set(xticklabels=[],\n            yticklabels=[],\n            title=\"splits using Entropy\",\n            xlim=(4, 8),\n            ylim=(1.5, 5.0)\n            )\nax_main.legend()\n\nax_height.set(ylim=(0, 0.6),\n              xlim=(4, 8),\n              xlabel=\"height\"\n              )\nax_weight.set(xlim=(0, 0.6),\n              ylim=(1.5, 5.0),\n                ylabel=\"weight\",\n              )\n\nax_weight.spines['top'].set_visible(False)\nax_weight.spines['right'].set_visible(False)\nax_height.spines['top'].set_visible(False)\nax_height.spines['right'].set_visible(False)\n\nax_height.text(-0.15, 0.5, \"Information\\nGain\", rotation=0, va='center', ha='center', transform=ax_height.transAxes)\nplt.tight_layout()\n\nprint(f\"Height: highest Information Gain: {split_feature_height[best_height_split, 1]:.2f} at height={split_feature_height[best_height_split, 0]:.2f}\")\nprint(f\"Weight: highest Information Gain: {split_feature_weight[best_weight_split, 1]:.2f} at weight={split_feature_weight[best_weight_split, 0]:.2f}\")\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/2101311549.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax_main.scatter(X[y == i, 0], X[y == i, 1],\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/2101311549.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\nHeight: highest Information Gain: 0.56 at height=5.55\nWeight: highest Information Gain: 0.28 at weight=3.35\n\n\n\n\n\n\n\n\n\n\nThe best split for the ‘height’ feature is at a height 5.55, with IG=0.56.\nThe best split for the ‘weight’ feature is at a weight 3.35, with IG=0.28.\n\nUsing the Entropy metric, the first split will be on the ‘height’ feature at a height of 5.55, since it has the highest information gain.\n\n\n33.2.2 Gini impurity\nNow let’s try the Gini impurity metric.\n\n\nplot\nfig = plt.figure(1, figsize=(8, 6))\ngs = gridspec.GridSpec(2, 2, width_ratios=[0.2,1], height_ratios=[1,0.2])\ngs.update(left=0.16, right=0.86,top=0.88, bottom=0.13, hspace=0.05, wspace=0.05)\n\nax_main = plt.subplot(gs[0, 1])\nax_height = plt.subplot(gs[1, 1])\nax_weight = plt.subplot(gs[0, 0])\n\nfor i, marker in enumerate(markers):\n    ax_main.scatter(X[y == i, 0], X[y == i, 1], \n                    c=colors[i], \n                    edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nsplit_feature_height = quantify_all_splits(X[:, 0], y, criterion='gini')\nsplit_feature_weight = quantify_all_splits(X[:, 1], y, criterion='gini')\n\nax_height.plot(split_feature_height[:, 0], split_feature_height[:, 1], marker='o')\nax_weight.plot(split_feature_weight[:, 1], split_feature_weight[:, 0], marker='o')  \n\nbest_height_split = np.argmin(split_feature_height[:, 1])\nbest_weight_split = np.argmin(split_feature_weight[:, 1])\n\nax_main.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_height.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_main.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\nax_weight.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\n\nax_main.set(xticklabels=[],\n            yticklabels=[],\n            title=\"splits using Gini Impurity\",\n            xlim=(4, 8),\n            ylim=(1.5, 5.0)\n            )\nax_main.legend()\n\nax_height.set(ylim=(0.4, 0.70),\n              xlim=(4, 8),\n              xlabel=\"height\"\n              )\nax_weight.set(xlim=(0.4, 0.70),\n              ylim=(1.5, 5.0),\n                ylabel=\"weight\",\n              )\n\nax_weight.spines['top'].set_visible(False)\nax_weight.spines['right'].set_visible(False)\nax_height.spines['top'].set_visible(False)\nax_height.spines['right'].set_visible(False)\n\nax_height.text(-0.15, 0.5, \"Gini\\nImpurity\", rotation=0, va='center', ha='center', transform=ax_height.transAxes)\nplt.tight_layout()\n\nprint(f\"Height: lowest Gini Impurity: {split_feature_height[best_height_split, 1]:.2f} at height={split_feature_height[best_height_split, 0]:.2f}\")\nprint(f\"Weight: lowest Gini Impurity: {split_feature_weight[best_weight_split, 1]:.2f} at weight={split_feature_weight[best_weight_split, 0]:.2f}\")\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/3659548085.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax_main.scatter(X[y == i, 0], X[y == i, 1],\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/3659548085.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\nHeight: lowest Gini Impurity: 0.44 at height=5.45\nWeight: lowest Gini Impurity: 0.54 at weight=3.35\n\n\n\n\n\n\n\n\n\n\nThe best split for the ‘height’ feature is at a height 5.45, with Gini=0.44.\nThe best split for the ‘weight’ feature is at a weight 3.35, with Gini=0.54.\n\nUsing the Gini Impurity metric, the first split will be on the ‘height’ feature at a height of 5.45, since it has the lowest Gini impurity.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>CART: classification</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#successive-splits",
    "href": "decision_trees/CART_classification.html#successive-splits",
    "title": "33  CART: classification",
    "section": "33.3 successive splits",
    "text": "33.3 successive splits\nThe same idea used for the first split is then applied to each of the subsets, recursively. The process continues until one of the stopping criteria is met, such as:\n\nAll samples in a node belong to the same class.\nThe maximum depth of the tree is reached.\nThe number of samples in a node is less than a predefined minimum.\n\nI find it useful to visualize the decision tree as a series of splits in the feature space: \nSource: “Predicting University Students’ Academic Success and Major Using Random Forests”, by Cédric Beaulac and Jeffrey S. Rosenthal\nSplitting the feature space with vertical and horizontal lines reminds me of a classic 1990’s computer game, JazzBall. Check out a video of the game here, and see it reminds you of the basic algorithm discussed so far.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>CART: classification</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#sklearn-tree-decisiontreeclassifier",
    "href": "decision_trees/CART_classification.html#sklearn-tree-decisiontreeclassifier",
    "title": "33  CART: classification",
    "section": "33.4 sklearn tree DecisionTreeClassifier",
    "text": "33.4 sklearn tree DecisionTreeClassifier\nWe will now use the DecisionTreeClassifier from sklearn.tree to build a decision tree classifier. Let’s restrict the maximum depth of the tree to 3, so we can visualize it easily. All the hard work was already coded for use, it’ll take us only two lines of code to create and fit the model.\n\n\nbuild and fit decision tree classifiers\n# using gini impurity\nclassifier_gini = DecisionTreeClassifier(criterion='gini', max_depth=3)#, random_state=42)\nclassifier_gini.fit(X, y)\n\n# using entropy\nclassifier_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3)#, random_state=42)\nclassifier_entropy.fit(X, y)\n\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'entropy'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \n3\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nNow let’s visualize the results.\n\n\ndefine useful functions\n# Helper function to plot decision boundaries\ndef plot_decision_boundaries(ax, model, X, y, title):\n    \"\"\"\n    Plots the decision boundaries for a given classifier.\n    \"\"\"\n    # Define a mesh grid to color the background based on predictions\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                         np.arange(y_min, y_max, 0.02))\n\n    # Predict the class for each point in the mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the colored regions\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n\n    # Set labels and title\n    ax.set_title(title)\n    ax.set_xlabel(iris.feature_names[0])\n    ax.set_ylabel(iris.feature_names[1])\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\n\n\nplot boundaries for each criterion\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplot_decision_boundaries(ax[0], classifier_entropy, X, y, \"Entropy Criterion\")\nfor i, marker in enumerate(markers):\n    ax[0].scatter(X[y == i, 0], X[y == i, 1], \n                  c=colors[i], \n                  edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nplot_decision_boundaries(ax[1], classifier_gini, X, y, \"Gini Criterion\")\nfor i, marker in enumerate(markers):\n    ax[1].scatter(X[y == i, 0], X[y == i, 1], \n                  c=colors[i], \n                  edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/759299291.py:5: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax[0].scatter(X[y == i, 0], X[y == i, 1],\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/759299291.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax[1].scatter(X[y == i, 0], X[y == i, 1],\n\n\n\n\n\n\n\n\n\nJust by using different criteria, we get different boundaries! We can now predict the species of a new animal based on its height and weight.\nFor example, an animal with height 6.3 and weight 4.0 would be classified as:\n\n\npredict\n# Predict the species of a new animal with height 5.7 and weight 4.0 using the entropy-based classifier\nsample = np.array([[6.3, 4.0]])\npredicted_class_entropy = classifier_entropy.predict(sample)\npredicted_class_gini = classifier_gini.predict(sample)\nprint(f\"Predicted species (Entropy): {iris.target_names[predicted_class_entropy[0]]}\")\nprint(f\"Predicted species (Gini): {iris.target_names[predicted_class_gini   [0]]}\")\n\n\nPredicted species (Entropy): koala\nPredicted species (Gini): bonobo\n\n\nSee also the decision trees for each criterion.\n\n\nplot decision tree for entropy criterion\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplot_tree(classifier_entropy,\n          filled=True,\n          rounded=True,\n          class_names=iris.target_names,\n          feature_names=[iris.feature_names[0], iris.feature_names[1]],\n          ax=ax);\n\n\n\n\n\n\n\n\n\n\n\nplot decision tree for gini criterion\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplot_tree(classifier_gini,\n          filled=True,\n          rounded=True,\n          class_names=iris.target_names,\n          feature_names=[iris.feature_names[0], iris.feature_names[1]],\n          ax=ax);",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>CART: classification</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#overfitting",
    "href": "decision_trees/CART_classification.html#overfitting",
    "title": "33  CART: classification",
    "section": "33.5 overfitting",
    "text": "33.5 overfitting\nWhat would happen if we chose to grow our decision tree until all leaves are pure? This would lead to a very complex tree that perfectly classifies the training data, but might not generalize well to new, unseen data. This is known as overfitting.\n\n\nno max_depth, grow until pure\nclassifier_gini = DecisionTreeClassifier(criterion='gini')\nclassifier_gini.fit(X, y)\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nplot boundaries and decision tree\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplot_decision_boundaries(ax[0], classifier_gini, X, y, \"Decision Boundaries in feature space (Gini Criterion)\")\nfor i, marker in enumerate(markers):\n    ax[0].scatter(X[y == i, 0], X[y == i, 1], \n                  c=colors[i], \n                  edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nplot_tree(classifier_gini,\n          filled=True,\n          rounded=True,\n          class_names=iris.target_names,\n          feature_names=[iris.feature_names[0], iris.feature_names[1]],\n          ax=ax[1]);\nax[1].set_title(\"Decision Tree (Gini Criterion)\");\nfig.savefig(\"decision_boundaries_no_max_depth.png\", dpi=300)\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/359429702.py:5: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax[0].scatter(X[y == i, 0], X[y == i, 1],\n\n\n\n\n\n\n\n\n\nWe can see that some of the regions are very small and specific to the training data points. This means that the model has learned not only the underlying patterns in the data but also the noise and outliers, which can lead to poor performance on new data.\nTo avoid overfitting, we can use techniques such as:\n\nSetting a maximum depth. We limit how deep the tree can grow. Read about max_depth.\nSetting a minimum number of samples required to split a node. Read about min_samples_split.\nPost-pruning: Cutting back the tree after it has been grown to remove branches that do not provide significant predictive power. Read about cost_complexity_pruning_path.\n\nOther machine learning algorithms, such as Random Forests and Gradient Boosted Trees, use ensemble methods that combine multiple decision trees to improve performance and reduce overfitting.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>CART: classification</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_regression.html",
    "href": "decision_trees/CART_regression.html",
    "title": "34  CART: regression",
    "section": "",
    "text": "34.1 the split\nIn the previous classification example, we saw how to put each animal into a category (koala, fox, bonobo) based on its features (height and weight). If you haven’t read it yet, go back to the classification example.\nIn regression, instead of putting things into categories, we predict a continuous value. Building on the previous example, let’s say we want to predict the age of an animal based on its height and weight.\nAgain, we are using the famous Iris dataset structure:\nWe will follow a similar procedure to split the data along the features, but this time our target variable is continuous (age) instead of categorical (animal type). In classification we wanted to have leaves as pure as possible, and we quantified that either with Gini impurity or entropy. In regression, we want to minimize the variance of the target variable within each leaf. In our example, this means that we want to split the data in a way that the ages of the animals in each leaf are as similar as possible.\nThe cost function we will use to evaluate the quality of a split is the Weighted Mean Squared Error (MSE):\nJ(j,s) = \\frac{N_\\text{left}}{N_\\text{total}} \\cdot \\text{MSE}_\\text{left} + \\frac{N_\\text{right}}{N_\\text{total}} \\cdot \\text{MSE}_\\text{right},\n where N_\\text{left} and N_\\text{right} are the number of samples in the left and right child nodes, respectively, and N_\\text{total} is the total number of samples in the parent node. \\text{MSE}_\\text{left} and \\text{MSE}_\\text{right} are the mean squared errors of the target variable in the left and right child nodes:\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2,\n where y_i are the target values in the node, \\bar{y} is the mean target value in the node, and N is the number of samples in the node.\nThe cost function is weighted by the number of samples in each child node to account for the fact that larger nodes have a greater impact on the overall variance.\nSo how do we know which split is the best? We will evaluate all possible split thresholds s along all features j and choose the one that minimizes the Weighted MSE. In a mathematical language:\n(j^*, s^*) = \\arg\\min_{j,s} J(j,s).",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>CART: regression</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_regression.html#sklearn-tree-decisiontreeregressor",
    "href": "decision_trees/CART_regression.html#sklearn-tree-decisiontreeregressor",
    "title": "34  CART: regression",
    "section": "34.2 sklearn tree DecisionTreeRegressor",
    "text": "34.2 sklearn tree DecisionTreeRegressor\nWe will use the DecisionTreeRegressor class from sklearn.tree to build our regression tree.\n\n\nShow the code\nregressor = DecisionTreeRegressor(max_depth=3)\nregressor.fit(X, y)\n\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'squared_error'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \n3\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nShow the code\n# Helper function to plot decision boundaries\ndef plot_decision_boundaries(ax, model, X, y, title):\n    \"\"\"\n    Plots the decision boundaries for a given classifier.\n    \"\"\"\n    # Define a mesh grid to color the background based on predictions\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                         np.arange(y_min, y_max, 0.02))\n\n    # Predict the class for each point in the mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the colored regions\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap='plasma', vmin=1, vmax=7)\n\n    # Set labels and title\n    ax.set_title(title)\n    ax.set_xlabel(iris.feature_names[0])\n    ax.set_ylabel(iris.feature_names[1])\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Helper function to plot decision boundaries\ndef plot_decision_boundaries_3d(ax, model, X, y, title):\n    # Define a mesh grid\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    \n    # Predict the values for each point in the mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the surface\n    surf = ax.plot_surface(xx, yy, Z, alpha=0.5, cmap='plasma', vmin=1, vmax=7)\n    \n    # Plot the actual data points\n    scatter = ax.scatter(X[:, 0], X[:, 1], y, c=y, cmap='plasma', edgecolor='k', s=50, vmin=1, vmax=7)\n    \n    # Set labels and title\n    ax.set_xlabel(iris.feature_names[0])\n    ax.set_ylabel(iris.feature_names[1])\n    ax.set_zlabel(iris.feature_names[2])\n    ax.set_title(title)\n    \n    # fig.colorbar(scatter, ax=ax, label='Age (years)', shrink=0.5)\n    \n    return fig, ax\n\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_decision_boundaries(ax, regressor, X, y, \"Regression Tree Predictions\")\nscatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='plasma', edgecolor='k', s=50, vmin=1, vmax=7)\nfig.colorbar(scatter, label='Age (years)')\n\n\n\n\n\n\n\n\n\nEach region (leaf) of the tree will predict the mean age of the training samples that fall into that region. Visualizing this in 3d shows us steps, because the regression tree creates a piecewise constant approximation of the target variable (age) over the feature space (height and weight).\n\n\nShow the code\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(121, projection='3d')\nplot_decision_boundaries_3d(ax, regressor, X, y, \"3d plot\")\nax.view_init(elev=20, azim=110)\n\n\n\n\n\n\n\n\n\nThe same consideration regarding overfitting applies here as in classification.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>CART: regression</span>"
    ]
  },
  {
    "objectID": "decision_trees/random_forest.html",
    "href": "decision_trees/random_forest.html",
    "title": "35  random forest",
    "section": "",
    "text": "35.1 step 1: bootstrap sampling\nThe motivation behind random forests is to avoid the weird regions in the feature space that a single decision tree might create. In the tutorial for classification with CART, we saw this:\nThe small blue regions inside the yellow region are highly undesirable. The decision tree learned very well the data it was given, but it will probably not generalize well to new data points. Random forests solve this problem in three steps.\nInstead of running the decision tree algorithm once on the entire training set, we run it multiple times on different bootstrap samples of the training set. We already learned about bootstrap sampling in the empirical confidence interval tutorial. In a nutshell, if we have a data set with N data points, we create a bootstrap sample by sampling N data points with replacement from the original data set. This means that some data points will appear multiple times in the bootstrap sample, while others will not appear at all. We can choose how many bootstrap samples we want to create. The default used by sklearn’s RandomForestRegressor is 100 (this argument is called n_estimators).\nWhat fraction of the dataset will a given bootstrap sample contain, on average? The probability of choosing a specific data point in one draw is 1/N. Therefore, the probability of not choosing that data point in one draw is 1 - 1/N. If we draw N times with replacement, the probability of never choosing that data point is\n\\left(1 - \\frac{1}{N}\\right)^N\nAs N becomes large…\n\\lim_{N \\to \\infty} \\left(1 - \\frac{1}{N}\\right)^N = e^{-1} \\approx 0.37.\nThis means that, on average, a bootstrap sample will contain about 63% of the original data points (because about 37% of them will not be chosen at all).",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>random forest</span>"
    ]
  },
  {
    "objectID": "decision_trees/random_forest.html#step-1-bootstrap-sampling",
    "href": "decision_trees/random_forest.html#step-1-bootstrap-sampling",
    "title": "35  random forest",
    "section": "",
    "text": "This follows from the definition of the exponential function: \ne^x = \\lim_{n \\to \\infty} \\left(1 + \\frac{x}{n}\\right)^n,\n just set x = -1.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>random forest</span>"
    ]
  },
  {
    "objectID": "decision_trees/random_forest.html#step-2-random-feature-selection",
    "href": "decision_trees/random_forest.html#step-2-random-feature-selection",
    "title": "35  random forest",
    "section": "35.2 step 2: random feature selection",
    "text": "35.2 step 2: random feature selection\nWhen training each decision tree on a bootstrap sample, we also randomly select a subset of the features to consider for each split in the tree. For example, if we have 10 features in total, we might randomly select 3 of them to consider for each split. This further increases the diversity among the trees in the forest, which helps to reduce overfitting.\nWhy is this important? Imagine a dataset where feature number 1 is very strongly correlated with the target variable. In that case, most decision trees will likely use that feature for the top split, leading to similar trees and less diversity in the forest.\nAs a rule of thumb, we typically use the square root or the logarithm (base 2) of the total number of features as the number of features to consider for each split. The argument in sklearn’s RandomForestRegressor that controls this is called max_features, and its default value is 1.0. This means that if we don’t specify anything, it will use 100% of the features in each split, and we will not have “feature decorrelation”. In the documentation, search for max_features to find more details.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>random forest</span>"
    ]
  },
  {
    "objectID": "decision_trees/random_forest.html#step-3-bagging",
    "href": "decision_trees/random_forest.html#step-3-bagging",
    "title": "35  random forest",
    "section": "35.3 step 3: bagging",
    "text": "35.3 step 3: bagging\nFinally, to make a prediction for a new data point, we pass it through each of the decision trees in the forest and average their predictions (for regression) or take a majority vote (for classification). This process is called “bagging” (short for bootstrap aggregating). By averaging the predictions of multiple trees, we can reduce the variance of the model and improve its generalization performance.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>random forest</span>"
    ]
  },
  {
    "objectID": "decision_trees/random_forest.html#example-iris-dataset",
    "href": "decision_trees/random_forest.html#example-iris-dataset",
    "title": "35  random forest",
    "section": "35.4 example: iris dataset",
    "text": "35.4 example: iris dataset\n\n\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\n\nload iris dataset and prepare data\niris = load_iris()\n\n# 1. Prepare Data (3 Inputs, 1 Output)\n# columns: 0=SepalLen, 1=SepalWid, 2=PetalLen, 3=PetalWid\nX_full = iris.data[:, [0, 1, 2]]  # 3 features\ny_full = iris.data[:, 3]          # Target: Petal Width\n\n# 2. Train Models\ntree_model = DecisionTreeRegressor(random_state=42)\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\ntree_model.fit(X_full, y_full)\nrf_model.fit(X_full, y_full)\n\n# 3. Compare Errors (The numerical proof)\nprint(f\"Single Tree Score: {tree_model.score(X_full, y_full):.3f}\")\nprint(f\"Random Forest Score: {rf_model.score(X_full, y_full):.3f}\")\n\n# X = iris.data[:, [0, 1]]\n# y = iris.data[:,[2]].flatten()\n# iris.feature_names = ['height', 'weight', 'age']\n\n\nSingle Tree Score: 0.999\nRandom Forest Score: 0.991\n\n\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# 1. Split the data (80% for training, 20% for testing)\nX_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n# 2. Train on ONLY the training set\ntree_model.fit(X_train, y_train)\nrf_model.fit(X_train, y_train)\n\n# 3. Test on the unseen data\nprint(f\"Single Tree Test Score:   {tree_model.score(X_test, y_test):.3f}\")\nprint(f\"Random Forest Test Score: {rf_model.score(X_test, y_test):.3f}\")\n\n\nSingle Tree Test Score:   0.845\nRandom Forest Test Score: 0.931",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>random forest</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html",
    "href": "information_theory/entropy.html",
    "title": "36  entropy",
    "section": "",
    "text": "36.1 image-generating machines\nLet’s derive the formula for entropy from first principles.\nWe’re given machines that generate images of cats (😺) and dogs (🐶). See the following outputs from the machines:\nmachine 1:\n😺 😺 😺 😺 😺 🐶, 5 cats and 1 dog\nmachine 2:\n😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺, 14 cats and 0 dogs\nmachine 3:\n🐶 😺 🐶 😺 🐶 🐶 🐶 😺 😺 🐶, 4 cats and 6 dogs\nmachine 4:\n🐶 😺 🐶 🐶 🐶 😺 😺 😺 🐶 😺 🐶 😺, 6 cats and 6 dogs",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#surprise",
    "href": "information_theory/entropy.html#surprise",
    "title": "36  entropy",
    "section": "36.2 surprise",
    "text": "36.2 surprise\nHow surprised are we by each output?\nFor machine 2, we wouldn’t be surprised at all if the next image it generates is a cat. It’s been generating only cats so far. However, if it generates a dog, that would be quite surprising.\nMachine 4 produced equal numbers of cats and dogs. So we would be equally surprised if the next image is a cat or a dog.\nMachines 1 and 3 are somewhere in between.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#information",
    "href": "information_theory/entropy.html#information",
    "title": "36  entropy",
    "section": "36.3 information",
    "text": "36.3 information\nWe could say similar things about information. If machine 2 generates another cat, we don’t learn anything new, it’s the same machine as ever. But if it generates a dog, we learn a lot about this machine’s behavior. In contrast, machine 4 generates equal amounts of information whether it produces a cat or a dog, since both have had the same number so far.\nTo say that we are surprised by an event is the same as saying that we learn some information from it. If an event is not surprising at all, then it doesn’t provide us with any new information.\n\nThe sun will rise tomorrow.\n\nThis is neither surprising nor informative.\n\nI have a dragon living in my garage.\n\nThis is both surprising and informative.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#quantifying-surprise-and-information",
    "href": "information_theory/entropy.html#quantifying-surprise-and-information",
    "title": "36  entropy",
    "section": "36.4 quantifying surprise and information",
    "text": "36.4 quantifying surprise and information\nAn event that is very likely to happen is not surprising, and doesn’t provide us with much information. An event that is unlikely to happen is surprising, and provides us with a lot of information. It seems reasonable to say that the amount of information I we gain from an event x, whose probability is P(x), is inversely proportional to the probability:\n\nI(P(x)) = \\frac{1}{P(x)}\n\\tag{1}",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#problems-with-the-inverse-formula",
    "href": "information_theory/entropy.html#problems-with-the-inverse-formula",
    "title": "36  entropy",
    "section": "36.5 problems with the inverse formula",
    "text": "36.5 problems with the inverse formula\nThere are at least two problems with this inverse formula.\nProblem one\nAn event with zero probability would provide us with infinite information. Maybe that’s okay, since I could be infinitely surprised if the sun didn’t rise tomorrow. However, a certain event (probability 1) would provide us with only 1 unit of information. That doesn’t seem right. A certain event should provide us with no information at all, zero.\nProblem two\nLet’s say that we learn about two completely independent events that happened yesterday, x and y:\n\nx: My sister flipped a coin and got “heads”.\ny: My cousin has won the lottery.\n\nSince the events are independent, the probability that both have happened is the product of their probabilities:\n\nP(x \\text{ and } y) = P(x)P(y)\n\\tag{2}\n\nNow let’s calculate the information we gained from learning that both events happened:\n\\begin{align*}\nI(P(x \\text{ and } y)) &= I(P(x)P(y)) \\\\\n                       &= \\frac{1}{P(x)P(y)} \\\\\n                       &= \\frac{1}{P(x)} \\cdot \\frac{1}{P(y)} \\\\\n                       &= I(P(x)) \\cdot I(P(y))\n                       \\tag{3}\n\\end{align*}\nThe total information I gained is the product of the information I gained from each event. There’s something wrong with that. Assume that my sister flipped a fair coin, so the probability of heads is 1/2. From the inverse formula, this means that the “heads” outcome doubled the information I got from yesterday’s events. My surprise from learning that my cousin won the lottery was multiplied by 2 by a mere coin toss. That’s obviously not ok.\nWhat would make sense is if the total information I gained from both events was the sum of the information I gained from each independent event. Then, if my sister flipped a fair coin, the information I gained from learning that my cousin won the lottery would be increased by a fixed amount, regardless of how surprising the lottery win was.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#a-new-and-better-formula",
    "href": "information_theory/entropy.html#a-new-and-better-formula",
    "title": "36  entropy",
    "section": "36.6 a new and better formula",
    "text": "36.6 a new and better formula\nWe are looking for a function I(P(x)) such that:\n\nI(1) = 0\nA sure event provides no information.\nI(P(x)P(y)) = I(P(x)) + I(P(y))\nThe information from two independent events is the sum of the information from each event.\n\nWe can guess another formula that satisfies these two properties:\n\nI(P(x)) = \\log\\left(\\frac{1}{P(x)}\\right)\n\\tag{4}\n\n\nThis formula satisfies the first property, since \\log(1) = 0.\nIt also satisfies the second property, since \\log(ab) = \\log(a) + \\log(b).\n\nClaude Shannon, the father of information theory, proposed this formula in his seminal 1948 paper “A Mathematical Theory of Communication”. There, he proved (see Appendix 2) that this is the only function that satisfies three properties, related to the two stated above, but more restrictive. See Shannon’s three properties, stated in Section 6 of his paper, titled “Choice, Uncertainty, and Entropy”.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#entropy",
    "href": "information_theory/entropy.html#entropy",
    "title": "36  entropy",
    "section": "36.7 entropy",
    "text": "36.7 entropy\nShannon writes:\n\nSuppose we have a set of possible events whose probabilities of occurrence are p_1, p_2, \\ldots, p_n. These probabilities are known but that is all we know concerning which event will occur. Can we find a measure of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?\n\nAfter introducing the three properties that a measure H must satisfy to answer the question above, Shannon presents the formula for the entropy:\n\nWe shall call \nH = - \\sum p_i \\log(p_i)\n\\tag{5}\n the entropy of the set of probabilities p_1, \\ldots, p_n.\n\nThis is the standard formula for entropy, but a better rendition is:\n\nH = \\sum P_i \\log\\left(\\frac{1}{P_i}\\right)\n\\tag{6}\n\n(Shannon uses p_i, but I’ll go back to using capital P for probabilities.)\nThis formula is better because it clearly answers the following question:\n\nWhat would be the expected amount of information (or surprise) we would gain from a probabilistic event?\n\nWe don’t know what image our machines will produce next. But we do know the probabilities of each outcome. So we can calculate the expected information from the next image:\n\nH = \\sum_i P(x_i) I(P(x_i)) = \\sum_i P(x_i) \\log\\left(\\frac{1}{P(x_i)}\\right)\n\\tag{7}\n\nThe information from each possible outcome x_i is weighted by the probability of that outcome, and we sum over all possible outcomes.\nFor instance, machine 1 produces dogs with a probability of 1/6, and cats with a probability of 5/6. The expected information from the next image is the information from a dog times the probability of getting a dog, plus the information from a cat times the probability of getting a cat. If our machine produced N possible images, we would do the same, summing over all N possible images with their respective probabilities as weights.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#binary-machines",
    "href": "information_theory/entropy.html#binary-machines",
    "title": "36  entropy",
    "section": "36.8 binary machines",
    "text": "36.8 binary machines\nIn the example of the machines that produce only two outcomes, we have that one probability is P (say, for getting cats) and the other is Q=1-P. So we can write the entropy as a function of a single probability:\n\nH(P) = - P \\log(P) - (1-P) \\log(1-P)\n\n\n\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nimport pandas as pd\nimport matplotlib.gridspec as gridspec\n\n\n\n\nplot\nfig, ax = plt.subplots()\np = np.linspace(0, 1, 100)\neps = 1e-10\ndef entropy_H(p):\n    return -p*np.log2(p+eps)-(1-p)*np.log2(1-p+eps)\nax.plot(p, entropy_H(p), label=r\"$  H(p)$\")\n\np_machine1 = 5/6\nax.plot([p_machine1], [entropy_H(p_machine1)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine1-0.05, entropy_H(p_machine1), \"M1\", ha='center', color='tab:orange')\n\np_machine2 = 1.0\nax.plot([p_machine2], [entropy_H(p_machine2)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine2-0.05, entropy_H(p_machine2), \"M2\", ha='center', color='tab:orange')\n\np_machine3 = 4/10\nax.plot([p_machine3], [entropy_H(p_machine3)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine3-0.05, entropy_H(p_machine3), \"M3\", ha='center', color='tab:orange')\n\np_machine4 = 6/12\nax.plot([p_machine4], [entropy_H(p_machine4)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine4, entropy_H(p_machine4)-0.05, \"M4\", ha='center', va='top', color='tab:orange')\n\nax.set_xlabel(\"P\")\nax.set_ylabel(\"bits\")\nax.set_title(\"Binary Entropy Function\")\n\n\nText(0.5, 1.0, 'Binary Entropy Function')\n\n\n\n\n\n\n\n\n\nThe expected information is zero for machine 2, which certainly produces only cats (P=1). The expected information is maximal for machine 4, which produces cats and dogs with equal probabilities (P=1/2).\nIn Equations (5) and (6), we didn’t explicitly say what the base of the logarithm is. It doesn’t matter, since changing the base only changes the units of information. In the example above, we use base 2 because we have a binary choice, and the units for entropy are called bits, suggested by JW Tukey, meaning binary digits.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/entropy.html#one-last-example",
    "href": "information_theory/entropy.html#one-last-example",
    "title": "36  entropy",
    "section": "36.9 one last example",
    "text": "36.9 one last example\nConsider the frequency of the letters in the Hebrew alphabet (there are 22 letters). The most common letter is “י”, which appears with a frequency of about 11.06%. The least common letter is “ט”, which appears with a frequency of about 1.24%. If we opened a book written in Hebrew to a random page, and picked a random letter on that page, what would be the expected information from that letter?\n\n\nimport data and plot\ndf = pd.read_csv(\"../archive/data/letter_frequency_hebrew.csv\", sep=\"\\t\")\n\nfig= plt.figure(1, figsize=(8, 8))\n\ngs = gridspec.GridSpec(3, 2, width_ratios=[1,0.2], height_ratios=[1,1,1])\ngs.update(left=0.16, right=0.86,top=0.88, bottom=0.13, hspace=0.05, wspace=0.05)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[1, 0], sharex=ax0)\nax2 = plt.subplot(gs[2, 0], sharex=ax0)\nax3 = plt.subplot(gs[:, 1])\n\nbar_width = 0.8\n\nax0.bar(df['Letter'], df['Frequency(%)']/100, width=bar_width)\nfor i, letter in enumerate(df['Letter']):\n    ax0.text(i, df['Frequency(%)'][i]/100 + 0.002, letter, ha='center', va='bottom', fontsize=10)\nax0.set_xticklabels(df['Letter'], rotation=0);\nax0.set_ylabel('prob. mass function', fontsize=12)\nax0.set(xlim=(-0.5, len(df['Letter'])-0.5))\nax0.text(0.4, 0.95, 'letter frequency', transform=ax0.transAxes, ha='center', va='top', fontsize=14)\nax0.text(0.4, 0.85, r'$P_i$', transform=ax0.transAxes, ha='center', va='top', fontsize=16)\n\nax1.bar(df['Letter'], np.log2(1/(df['Frequency(%)']/100)), width=bar_width)\nax1.set_ylabel('self-information\\n(bits)', fontsize=12)\nax1.text(0.4, 0.95, 'letter self-information', transform=ax1.transAxes, ha='center', va='top', fontsize=14)\nax1.text(0.4, 0.85, r'$\\log_2(1/P_i)$', transform=ax1.transAxes, ha='center', va='top', fontsize=16)\n\nHi = (df['Frequency(%)']/100) * np.log2(1/(df['Frequency(%)']/100))\ncolors = sns.color_palette(\"deep\", n_colors=len(df))\nfor i, (letter, h) in enumerate(zip(df['Letter'], Hi)):\n    ax2.bar(letter, h, color=colors[i % len(colors)], width=bar_width)\nax2.set_ylabel('entropy (bits)', fontsize=12)\nax2.set(yticks=[0,1,2])\nax2.set_ylim(0,4.5/3)\nax2.text(0.4, 0.95, 'letter entropy contribution', transform=ax2.transAxes, ha='center', va='top', fontsize=14)\nax2.text(0.4, 0.85, r'$P_i\\cdot\\log_2(1/P_i)$', transform=ax2.transAxes, ha='center', va='top', fontsize=16)\n\n# Annotate arrow from x=10 to x=20 on ax2\nax2.annotate(\n    '', \n    xy=(len(df['Letter'])-1, 0.3), \n    xytext=(10, 0.3), \n    arrowprops=dict(arrowstyle='-&gt;', lw=2, color='black')\n)\nax2.text(15.5, 0.35, r'$H=\\sum P_i \\log_2(1/P_i)$', ha='center', va='bottom', fontsize=16)\nfor spine in ['top', 'right']:\n    ax0.spines[spine].set_visible(False)\n    ax1.spines[spine].set_visible(False)\n    ax2.spines[spine].set_visible(False)\n\n# Plot stacked bars of the entropy contributions in ax3\nbottom = 0\nfor i, H in enumerate(Hi):\n    p = ax3.bar(0, H, width=bar_width, label=df['Letter'][i], bottom=bottom)\n    ax3.text(0, bottom + H/2, f\"{df['Letter'][i]}\", ha='center', va='center', color='black', fontsize=10)\n    bottom += H\nax3.set_xlim(-len(df['Letter'])/2*0.2, len(df['Letter'])/2*0.2)\nax3.set_ylim(0,4.5)\nfor spine in ['left', 'top', 'right']:\n    ax3.spines[spine].set_visible(False)\nax3.yaxis.tick_right()\n\ntotal_H = np.sum(Hi)\nax3.axhline(total_H, color='gray', linestyle=':', linewidth=1)\nax3.set(\n    xticks=[],\n    yticks=[0,1,2,3,4,total_H],\n    yticklabels=[0,1,2,3,4, f'H = {total_H:.2f} bits']\n)\nax3.tick_params(axis='y', which='major', length=0)\n\nfig.tight_layout();\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_99634/300921586.py:19: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax0.set_xticklabels(df['Letter'], rotation=0);\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_99634/300921586.py:74: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  fig.tight_layout();\n\n\n\n\n\n\n\n\n\nThe expected information would be 4.14 bits. Because Hebrew has structure (like any other language), this is less than the maximum possible information. If all 22 letters were equally likely, the expected information would be:\n\\begin{align*}\nH_\\text{max} &= \\sum_{i=1}^{22} \\frac{1}{22} \\log_2\\left(\\frac{1}{\\frac{1}{22}}\\right) \\\\\n             &= \\log_2(22) \\\\\n             &\\approx 4.46 \\text{ bits}\n\\end{align*}\nFrom this exercise, we learned that in the case that all outcomes are equally likely, the entropy is simply the logarithm of the number of possible outcomes. This is a useful fact to remember.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "information_theory/cross-entropy.html",
    "href": "information_theory/cross-entropy.html",
    "title": "37  cross-entropy and KL divergence",
    "section": "",
    "text": "37.1 wrong model\nAssume I live in city A, where it rains 50% of the days. A friend of mine lives in city B, where it rains 10% of the days.\nWhat happens when my friend visits me in city A and, not knowing any better, assumes that it rains 10% of the days?\nWe have two probability distributions, the true weather distribution P, and the assumed weather distribution Q:\nWhat will be the expected surprise of my friend, when he visits me in city A? Now that we have discussed surprise (information) and entropy, we can calculate the following quantity, called cross-entropy:\nH(P, Q) = - \\sum_x P(x) \\log Q(x)\nMy friend will evaluate his surprise using the mental model that he has, i.e., the assumed distribution Q. For example, because he comes from a dry city, every time it rains he is surprised a lot more than when it does not rain.\nHowever, since my friend is visiting me in city A, he will actually experience the weather according to the true distribution P. He will not weigh the big surprise of rain with the probability of rain in city B (10%), but with the probability of rain in city A (50%).\nThis reasoning explains the asymetry of the cross-entropy: the first argument is the true distribution, which determines how often each event happens, while the second argument is the assumed distribution, which determines how surprised my friend will be when each event happens.\nLet’s compute my friend’s expected surprise when he visits me in city A:\n\\begin{align*}\nH(P, Q) &= - \\sum_x P(x) \\log Q(x) \\\\\n        &= - (P(\\text{rain}) \\log Q(\\text{rain}) + P(\\text{no rain}) \\log Q(\\text{no rain})) \\\\\n        &= - (0.5 \\log 0.1 + 0.5 \\log 0.9) \\\\\n        &= - (0.5 \\cdot -1 + 0.5 \\cdot -0.045757) \\\\\n        &= 1.74 \\text{ bits},\n\\end{align*}\nwhere we used the base-2 logarithm, so the result is in bits.\nTo see the asymetry of the cross-entropy, let’s compute my expected surprise when I visit my friend in city B:\n\\begin{align*}\nH(Q, P) &= - \\sum_x Q(x) \\log P(x) \\\\\n        &= - (Q(\\text{rain}) \\log P(\\text{rain}) + Q(\\text{no rain}) \\log P(\\text{no rain})) \\\\\n        &= - (0.1 \\log 0.5 + 0.9 \\log 0.5) \\\\\n        &= - (0.1 \\cdot -1 + 0.9 \\cdot -1) \\\\\n        &= 1 \\text{ bits},\n\\end{align*}\nMy friend’s expected surprise will be higher when he visits me in city A (1.74 bits) than my expected surprise when I visit him in city B (1 bit).",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "information_theory/cross-entropy.html#wrong-model",
    "href": "information_theory/cross-entropy.html#wrong-model",
    "title": "37  cross-entropy and KL divergence",
    "section": "",
    "text": "True distribution: P(rain) = 0.5, P(no rain) = 0.5\nAssumed distribution: Q(rain) = 0.1, Q(no rain) = 0.9",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "information_theory/cross-entropy.html#kullback-leibler-divergence",
    "href": "information_theory/cross-entropy.html#kullback-leibler-divergence",
    "title": "37  cross-entropy and KL divergence",
    "section": "37.2 Kullback-Leibler divergence",
    "text": "37.2 Kullback-Leibler divergence\nNot all of my friend’s surprise is due to the fact that he has an inaccurate mental model of the weather. Some of his surprise is simply due to the inherent randomness of the weather. This would be the same surprise that I myself experience when I live in city A, and I have the correct mental model of the weather.\nIt would make sense to separate the surprise that is due to the inherent randomness of the weather from the surprise that is due to my friend’s wrong mental model. We can do this by subtracting the entropy of the true distribution P from the cross-entropy H(P, Q):\nD_{KL}(P \\| Q) = H(P, Q) - H(P)\nThis quantity is called the Kullback-Leibler divergence, and it measures the amount of surprise that is only due to my friend’s wrong mental model.\nLet’s use the properties of logarithms to rewrite the KL divergence in a more convenient form:\n\\begin{align*}\nD_{KL}(P \\| Q) &= H(P, Q) - H(P)\\\\\n                &= - \\sum_x P(x) \\log Q(x) + \\sum_x P(x) \\log P(x) \\\\\n                &= \\sum_x P(x) (\\log P(x) - \\log Q(x)) \\\\\n                &= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}.\n\\end{align*}\nThis is the most common form of the KL divergence.\nWhen our model is perfect, i.e., when P = Q, the KL divergence is zero (\\log(P/P) = 0), because there is no extra surprise due to a wrong mental model. When our model is not perfect, the KL divergence is always positive, because we are always more surprised when our mental model is wrong.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "information_theory/cross-entropy.html#model-training-and-objective-functions",
    "href": "information_theory/cross-entropy.html#model-training-and-objective-functions",
    "title": "37  cross-entropy and KL divergence",
    "section": "37.3 model training and objective functions",
    "text": "37.3 model training and objective functions\nWe might want to train a model that classifies photos. We have a dataset of photos, and for each photo we know the correct label (cat, dog, elephant, etc.). The goal of the model is to predict the correct label for each photo.\nAt every step of the training process, we need to evaluate how well the model is doing. The true data distribution P is given by the labels in the training dataset, while the model’s predicted distribution Q is given by the model’s output. Ideally, our model’s predicted distribution Q should be as close as possible to the true data distribution P. That’s sounds like a job for the KL divergence!\nWe will adjust the model’s parameters to minimize the KL divergence between the true data distribution P and the model’s predicted distribution Q. In practice, we will minimize the cross-entropy H(P, Q) instead of the KL divergence D_{KL}(P \\| Q), because the entropy H(P) does not depend on the model’s parameters, and therefore does not affect the optimization process. Think about it: no matter what the model’s parameters are, the entropy of the true data distribution P will always be the same. So minimizing the KL divergence is equivalent to minimizing the cross-entropy. We don’t care if they differ by a constant.",
    "crumbs": [
      "information theory",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "likelihood/probability_and_likelihood.html",
    "href": "likelihood/probability_and_likelihood.html",
    "title": "38  probability and likelihood",
    "section": "",
    "text": "38.1 Q1: deduction and probability\nIn the bus stop next to where I live, buses arrive on average every 20 minutes. The timing is not exact, due to traffic, weather, or the number of passengers getting on and off.\nBefore we go about solving these questions, we note that they require of us two different things.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>probability and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood/probability_and_likelihood.html#q1-deduction-and-probability",
    "href": "likelihood/probability_and_likelihood.html#q1-deduction-and-probability",
    "title": "38  probability and likelihood",
    "section": "",
    "text": "38.1.1 A slightly different situation\nIf we knew that buses arrive exactly every 20 minutes, the question would be quite easy to answer. The probability of waiting more than 30 minutes would be zero. The only uncertainty would be when we arrive at the bus stop relative to the bus schedule. We can assume that we arrive uniformly at random between two bus arrivals, so the probability density of us waiting x minutes would look like this:\n\n\nplot\nfig, ax = plt.subplots(figsize=(6, 3))\nx = np.arange(0,41)\npdf = np.where(x &lt; 20, 1/20, 0)\nax.plot(x, pdf, drawstyle='steps-post', color='blue', lw=2)\nax.plot(x, 0*pdf, color='black', lw=1, ls='--')\nax.fill_between(x, 0, pdf, where=((x&gt;=10) & (x&lt;=15)), color='orange', alpha=0.5, step='post', label='Area = 0.25')\nax.set(title='Probability Density Function for Bus Wait Time',\n       xlabel='Wait Time (minutes)',\n       ylabel='Probability Density',\n       xlim=(0,40));\n\n\n\n\n\n\n\n\n\nThis is a probability density function (pdf), because the probability that we wait exactly x minutes is zero. Instead, we can calculate the probability that we wait between two times, say between 10 and 15 minutes, by calculating the area under the curve between those two times. The shaded region in the plot above has area 5\\times0.05=0.25, meaning that there is a 25% chance that we will wait between 10 and 15 minutes. It would be impossible to wait more than 20 minutes because the area under the curve after 20 minutes is zero. Of course, if we ask what is the probability that we wait for more than zero minutes, the answer is 1=100%, because logically there is no other option. This tells us that the total area under the curve must be equal to 1, not only for this specific example, but for any valid probability density function.\n\n\n38.1.2 now the real situation\nLet’s contrast the real situation with the simplified one from before:\n\n\n\n\n\n\n\n1st case\n2nd case\n\n\n\n\nBuses arrive exactly every 20 minutes.\nBuses arrive on average every 20 minutes.\n\n\nUncertainty only in our arrival time.\nUncertainty in both our arrival time and bus arrival times.\n\n\nEvery minute that passes I feel more certain that the bus is coming soon.\nEvery minute that passes, my expected wait time doesn’t change! The fact that I already waited 20 minutes doesn’t decrease my expected time, the universe doesn’t “owe” me a bus just because I’ve been standing there.\n\n\nI can wait at most 20 minutes.\nThere is not upper bound, I could wait for a very long time.\n\n\nMy expected wait time is 10 minutes.\nMy expected wait time is 20 minutes.\n\n\n\nI will make the follwing assumption: every minute has the same chance of seeing a bus arrive. No matter how long I already waited, the chance of seeing a bus in the next minute is the same. This is called the memoryless property. It means that the bus arrival process has no memory of what happened before. This can be a reasonable assumption for buses arriving randomly, but it would not be reasonable for something like a train schedule, where if I just missed a train, I know I will have to wait a long time for the next one.\nWhat is the probability that I will have to wait more than t minutes?\n\n\n38.1.3 full derivation\nThis is not strictly necessary to understand the rest of the notebook, but I’m a nerd and these derivations are fun. If you don’t agree just skip to the final result below.\nLet’s start by imagining the 20-minute average wait. Instead of looking at the full 20 minutes, let’s slice time into tiny, microscopic intervals of size \\Delta t (e.g., one millisecond). In each tiny slice, only two things can happen, either the bus arrives, or it does not. Since the average wait is 20 minutes, the rate \\lambda is 1/20 buses per minute. In a tiny slice of time \\Delta t, the probability p of a bus arriving is p\\approx \\lambda \\Delta t.\nIf I am waiting for the bus at time t, it means the bus failed to show up in every single tiny slice of time from 0 up to t. If each slice is \\Delta t, the number of slices in time t is n=t/\\Delta t. The probability of not seeing a bus in one slice is (1−p). Therefore, the probability of not seeing a bus in n consecutive slices is:\n\nPr(T &gt; t) = (1 - p)^n = (1 - \\lambda \\Delta t)^{t/\\Delta t}.\n\nTo make this accurate, we want the slices to be “infinitely small” (\\Delta t\\rightarrow 0). As \\Delta t gets smaller, n gets larger. Let’s rewrite the exponent to use the definition of e: \nPr(T &gt; t) = \\left[(1 - \\lambda \\Delta t)^{\\frac{1}{\\lambda \\Delta t}}\\right]^{\\lambda t}.\n\nRecall the fundamental definition of the exponential constant:\n\n\\lim_{x \\to 0} (1 - x)^{1/x} = e^{-1}\n\nAs \\Delta t \\to 0, the term inside the brackets becomes e^{-1}.\n\nPr(T&gt;t)=e^{-\\lambda t}\n\nWhat we just found is the Survival Function (the chance you are still waiting at time t). To get the Cumulative Distribution Function (CDF)—the chance the bus has arrived by time t:\n\nF(t) = Pr(T \\leq t) = 1 - Pr(T &gt; t) = 1 - e^{-\\lambda t}\n\nFinally, the Probability Density Function (PDF) is the rate of change (derivative) of the CDF:\n\\begin{align*}\nf(t) &= \\frac{d}{dt}F(t) \\\\\n&= \\frac{d}{dt}\\left(1 - e^{-\\lambda t}\\right) \\\\\n&= \\lambda e^{-\\lambda t}\n\\end{align*}\n\n\n38.1.4 the result\nLet’s remember the question:\nQ1: What is the probability that I will have to wait more than 30 minutes for the next bus?\nNow that we have the probability density function,\n\nf(t) = \\lambda e^{-\\lambda t}\n\nwe can calculate the probability of waiting more than t minutes as the integral of the PDF from T to infinity:\n\\begin{align*}\nPr(T &gt; t) &= \\int_{t}^{\\infty} \\lambda e^{-\\lambda t} dt\\\\\n&= \\left[-e^{-\\lambda t}\\right]_{t}^{\\infty} \\\\\n&= 0 - \\left(-e^{-\\lambda \\cdot t}\\right) \\\\\n&= e^{-\\lambda \\cdot t}\n\\end{align*}\nFor our specific case, with an average wait of 20 minutes (\\lambda=1/20) and t=30 minutes:\n\nPr(T &gt; 30) = e^{-\\frac{1}{20} \\cdot 30} = e^{-1.5} \\approx 0.22.\n\nThere is a 22% chance that I will have to wait more than 30 minutes for the next bus. \\square",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>probability and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood/probability_and_likelihood.html#q2-induction-and-likelihood",
    "href": "likelihood/probability_and_likelihood.html#q2-induction-and-likelihood",
    "title": "38  probability and likelihood",
    "section": "38.2 Q2: induction and likelihood",
    "text": "38.2 Q2: induction and likelihood\nAssume now that we don’t know the average interval between buses. Given that I waited 15 minutes today, what is the most likely average interval between buses?\nIn other words, given a specific observed wait time, what is the most likely value of \\lambda?\n\nIf \\lambda is very small, this means that the average interval between buses is very long, say, one a day. In that case, waiting 15 minutes would be quite unlikely, it would be an astonishing stroke of luck.\nIf \\lambda is very large, this means that the average interval between buses is very short, say, one every 10 seconds. In that case, waiting 15 minutes would also be quite unlikely, it would be an astonishing stroke of bad luck.\n\nIt is clear from the reasoning above that there is a sweet spot, an intermediate value of \\lambda that makes waiting 15 minutes most likely.\nThe likelihood is written as:\n\n\\mathcal{L}(\\lambda \\mid t).\n\nIt should not be read as “the probability of \\lambda given t”, because \\lambda is not a random variable, it’s a parameter. Instead, it should be read as “the likelihood of \\lambda given t”, meaning how plausible is the value of \\lambda in light of the observed data t.\nFor given observed wait time t=15, the likelihood function is equal to the probability density function evaluated at t=15:\n\n\\mathcal{L}(\\lambda \\mid t=15) = P(t=15 \\mid \\lambda)\n\nLet’s vizualize this. On the left plot below we see the PDF for a few different values of \\lambda. For each curve, I marked the observed wait time of 15 minutes with a circle. It is hard to see which value of \\lambda makes waiting 15 minutes most likely just by looking at the left plot.\nOn the right plot we see the likelihood function for the observed wait time of 15 minutes. Clearly there is a peak for the likelihood.\n\n\nShow the code\nfig, ax = plt.subplots(1, 2, figsize=(6, 3))\nt = np.linspace(0,40, 101)\nf = lambda t, lam: lam * np.exp(-lam * t)\ntime = np.array([5, 10, 15, 20, 25, 30, 60])\nlam = 1 / time\n\ncolor_min = 0.4\ncolor_max = 0.99\ncolors = plt.cm.Blues(np.linspace(color_min, color_max, len(lam)))\nfor i,l in enumerate(lam):\n    ax[0].plot(t, f(t, l), lw=1, ls='--', label=f'λ=1/{time[i]:.0f}', color=colors[i])\n    ax[0].plot([15], [f(15, l)], ls=None, marker='o', color=colors[i], markersize=3)\nax[0].legend()\n# ax.plot(t, f(t, lam), color='blue', lw=2)\n# ax.plot(t, 0*t, color='black', lw=1, ls='--')\nax[0].set(xlabel='Wait Time (minutes)',\n       ylabel='Probability Density',\n       xlim=(0,40),\n       ylim=(0,0.12));\n\nl_array = np.linspace(0.001, 0.3, 100)\nt_obs = 15\nL = l_array * np.exp(-l_array * t_obs)\nax[1].plot(l_array, L, color=colors[-1], lw=2)\nax[1].set(xlabel=r'Rate ($\\lambda$, min$^{-1}$)',\n       ylabel='Likelihood',\n       xlim=(0,0.3),\n       ylim=(0, 0.03)\n       );\nax[1].yaxis.tick_right()\nax[1].yaxis.set_label_position(\"right\")\n\n\n\n\n\n\n\n\n\n\n38.2.1 maximum likelihood\nThe final step here is obvious. We need to find which lambda maximizes the likelihood function. This value is called the Maximum Likelihood Estimate (MLE) of lambda, denoted \\hat{\\lambda}:\n\n\\hat{\\lambda} = \\underset{\\lambda}{\\mathrm{argmax}} \\ \\mathcal{L}(\\lambda \\mid t=15),\n\nor in English, “the value of lambda that maximizes the argument of the likelihood function, given the observation t=15.”\nLet’s take the derivative of the likelihood function with respect to lambda, set it to zero, and solve for lambda:\n\\begin{align*}\n\\mathcal{L}(\\lambda \\mid t) &= \\lambda e^{-\\lambda t} \\\\\n\\frac{d}{d\\lambda} \\mathcal{L}(\\lambda \\mid t) &= e^{-\\lambda t} - \\lambda t e^{-\\lambda t} \\\\\n0 &= e^{-\\lambda t} - \\lambda t e^{-\\lambda t} \\\\\n\\lambda t e^{-\\lambda t} &= e^{-\\lambda t} \\\\\n\\lambda t &= 1 \\\\\n\\hat{\\lambda} &= \\frac{1}{t}\n\\end{align*}\nWe found that the MLE of lambda given a single observation t is simply the reciprocal of t. In our case, with t=15 minutes, \\hat{\\lambda} = 1/15 buses per minute, meaning that the most likely average interval between buses is 15 minutes. \\square\n\n\n38.2.2 likelihood surface\nI find it useful to visualize both the probability and likelihood functions together in a 3D surface plot. When we derived f(t), we wrote it as a function of time, and the parameter \\lambda was assumed to be known. The joint probability distribution is the same equation, but we now recognize that it is a function of both t and \\lambda:\n\nf(t, \\lambda) = \\lambda e^{-\\lambda t}.\n\nThe joint pdf can be visualized as a surface, representing the exponential distribution’s PDF across a range of wait times (t) and rate parameters (\\lambda). The common expression we see when we learn about probability and likelihood is: \n\\mathcal{L}(\\theta \\mid x) = P(x \\mid \\theta),\n where x is the data and \\theta is the parameter (wait time t and rate \\lambda in our example). At first this seemed baffling to me to define the likelihood function using the same formula as the probability density function. What’s the point of having two names for the same formula? I now picture the probability and likelihood functions as slices (projections) of the same surface. Keeping the parameters fixed and varying the data gives us the probability function. Keeping the data fixed and varying the parameters gives us the likelihood function.\nThis also helps me understand why the likelihood function is not a valid probability distribution over the parameters. For a fixed data point, the area under the likelihood curve over all possible parameter values does not equal 1. Instead, it can take any value, depending on the observed data.\n\n\nShow the code\n# Define ranges\nt = np.linspace(0, 60, 100)\nlmbda = np.linspace(1/120, 1/4, 100)\nT, L = np.meshgrid(t, lmbda)\n\n# Exponential PDF: f(t; lambda) = lambda * exp(-lambda * t)\njoint_pdf = lambda T, L: L * np.exp(-L * T)\nZ = joint_pdf(T, L)\n\n# 1. 3D Surface Plot\nfig = plt.figure(figsize=(8, 6), constrained_layout=True)\nax = fig.add_subplot(111, projection='3d')\n# fig.subplots_adjust(right=0.80)\nsurf = ax.plot_surface(T, L, Z, color='lightgray', alpha=0.3, edgecolor='none', rcount=20, ccount=20)\n\n# Highlight slices\nfixed_lambda = 1/15\nfixed_t = 15\n\n# Probability slice (fixed lambda, varying t)\nt_slice = t\nz_prob = joint_pdf(t_slice, fixed_lambda)\nax.plot(t_slice, [fixed_lambda]*len(t_slice), z_prob, color='red', lw=3, label=f'Probability Slice $f(t|\\lambda=1/15)$')\nfor lbd_i in [1/5, 1/7, 1/10, 1/20, 1/30, 1/45]:\n    zi = joint_pdf(t_slice, lbd_i)\n    ax.plot(t_slice, [lbd_i]*len(t_slice), [zi], color='red', alpha=0.5)\n\n# Likelihood slice (fixed t, varying lambda)\nl_slice = lmbda\nz_lik = joint_pdf([fixed_t]*len(l_slice), l_slice)\nax.plot([fixed_t]*len(l_slice), l_slice, z_lik, color='blue', lw=3, label=f'Likelihood Slice $L(\\lambda|t={fixed_t})$')\nfor ti in [5, 10, 20, 30, 45]:\n    zi = joint_pdf([ti]*len(l_slice), l_slice)\n    ax.plot([ti]*len(l_slice), l_slice, [zi], color='blue', alpha=0.5)\n\n\nax.set_xlabel('Wait Time ($t$)')\nax.set_ylabel('Rate ($\\lambda$)')\nax.set_zlabel('Joint PDF', labelpad=0)\nax.legend()\nax.view_init(elev=15, azim=-20)\n# fig.tight_layout()",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>probability and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood/probability_and_likelihood.html#q3-likelihood-with-multiple-observations",
    "href": "likelihood/probability_and_likelihood.html#q3-likelihood-with-multiple-observations",
    "title": "38  probability and likelihood",
    "section": "38.3 Q3: likelihood with multiple observations",
    "text": "38.3 Q3: likelihood with multiple observations\nThis week I waited 10, 25, 30, 2, and 20 minutes for the bus on different days. Given this data, what is the most likely average interval between buses?\nWe will now make an important assumption: the wait times on different days are independent of each other. This means that the likelihood of observing all the wait times is the product of the individual likelihoods:\n\n\\mathcal{L}(\\lambda \\mid t_1, t_2, \\ldots, t_n) = \\prod_{i=1}^{n} \\mathcal{L}(\\lambda \\mid t_i) = \\prod_{i=1}^{n} P( t_i \\mid \\lambda).\n\nGiven\n\nP(t) = \\lambda e^{-\\lambda t}\n\nwe have\n\\begin{align*}\n\\mathcal{L}(\\lambda \\mid t_1, t_2, \\ldots, t_n) &= \\prod_{i=1}^{n} \\lambda e^{-\\lambda t_i} \\\\\n&= \\lambda^n e^{-\\lambda \\sum_{i=1}^{n} t_i}\n\\end{align*}\nLet’s take the derivative of the likelihood function with respect to lambda, set it to zero, and solve for lambda:\n\\begin{align*}\n\\mathcal{L}(\\lambda \\mid t_1, t_2, \\ldots, t_n) &= \\lambda^n e^{-\\lambda \\sum_{i=1}^{n} t_i} \\\\\n\\frac{d}{d\\lambda} \\mathcal{L}(\\lambda \\mid t_1, t_2, \\ldots, t_n) &= n \\lambda^{n-1} e^{-\\lambda \\sum t_i} - \\lambda^n \\left(\\sum t_i\\right) e^{-\\lambda \\sum t_i} \\\\\n0 &= n \\lambda^{n-1} e^{-\\lambda \\sum t_i} - \\lambda^n \\left(\\sum t_i\\right) e^{-\\lambda \\sum t_i} \\\\\n\\lambda^n \\left(\\sum t_i\\right) e^{-\\lambda \\sum t_i} &= n \\lambda^{n-1} e^{-\\lambda \\sum t_i} \\\\\n\\lambda \\left(\\sum t_i\\right) &= n \\\\\n\\hat{\\lambda} &= \\frac{n}{\\sum_{i=1}^{n} t_i} \\\\\n\\hat{\\lambda} &= \\frac{1}{\\frac{\\sum_{i=1}^{n} t_i}{n}} \\\\\n\\hat{\\lambda} &= \\frac{1}{\\bar{t}},\n\\end{align*}\nwhere \\bar{t} is the sample mean of the observed wait times. Because the average of 10, 25, 30, 2, and 20 minutes is 17.4 minutes, the MLE of lambda is approximately 1/17.4 buses per minute, quite close to the value 1/20 in the first problem. \\square\nThe result is (perhaps) surprisingly simple, certainly compared with the full derivation above. There is something deep to be said about the fact that the formula for the mean naturally emerges from the principle of maximum likelihood. This, and much more, is the subject of the next chapter.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>probability and likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE.html",
    "href": "likelihood/MLE.html",
    "title": "39  maximum likelihood estimation",
    "section": "",
    "text": "39.1 log likelihood\nThe one idea that permeates all of statistics and machine learning is that of maximum likelihood estimation. I cannot overstate its centrality.\nIn the chapter on probability and likelihood, we solved a practical example of estimating the rate parameter of an exponential distribution using maximum likelihood estimation. For the case where we observed many samples, we found that the formula for the mean naturally emerged as the maximum likelihood estimator for the rate parameter. We will dig deeper into this idea, and show deep connections between maximum likelihood estimation and many other statistical concepts.\nWe already saw the formula for the likelihood for multiple independent observations:\nL(\\lambda \\mid t_1, t_2, \\ldots, t_n) = \\prod_{i=1}^{n} L(\\lambda \\mid t_i),\nWe justified the product form by assuming that the observations are “iid”, meaning independent and identically distributed. Instead of working with the product form, it is often more convenient to work with the log likelihood, which converts the product into a sum:\n\\ell(\\theta \\mid x_{1:n}) = \\log L(\\theta \\mid x_{1:n}) = \\sum_{i=1}^{n} \\log L(\\theta \\mid x_i) = \\sum_{i=1}^{n} \\log P( x_i \\mid \\theta).\nHere we changed the notation a bit to use \\theta for the parameters (which could be a vector) and x for the data vector, which is more common in statistics. There are a few reasons why we prefer to work with the log likelihood instead of the likelihood itself:\nIn the following chapters, we will see the deep connections between maximum likelihood estimation and many fundamental concepts in statistics and machine learning.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE.html#log-likelihood",
    "href": "likelihood/MLE.html#log-likelihood",
    "title": "39  maximum likelihood estimation",
    "section": "",
    "text": "Numerical Stability (Underflow): In likelihood with multiple observations, we saw that likelihoods are products: \\prod P(x_i). If you have 100 observations, you are multiplying 100 small numbers, which can lead to numerical underflow where a computer rounds the result to zero. Summing logs, \\sum \\ln P(x_i​), keeps the numbers in a range computers can handle.\nMathematical Elegance: Many common distributions (like the Exponential or Normal) use e, Euler’s number. The natural log cancels the exponent, turning products of complex terms into simple sums that are much easier to differentiate.\nPreservation of the Maximum: Because ln(x) is a monotonically increasing function, the value of \\theta that maximizes the log-likelihood is identical to the value that maximizes the likelihood.\nOptimization Surface: Taking the log reshapes the likelihood surface. It stretches out the extremely steep slopes near the peak, creating a well-behaved “hill” that optimization algorithms can navigate reliably without the fluctuations caused by nearly-vertical gradients.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html",
    "href": "likelihood/MLE_and_summary_statistics.html",
    "title": "40  MLE and summary statistics",
    "section": "",
    "text": "40.1 exponential distribution revisited\nLet’s do the following exercise. Suppose we have n iid observations x from a model with parameters \\theta. Using the definition of the log likelihood, derive the maximum likelihood estimator for \\theta.\nWe can now revisit our original example of estimating the rate parameter \\lambda of an exponential distribution using maximum likelihood estimation. Recall that the joint probability density function of the exponential distribution is given by: \nf(x, \\lambda) = \\lambda e^{-\\lambda x}.\n The log likelihood reads:\n\\begin{align*}\n\\ell(\\lambda \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i, \\lambda) \\\\\n&= \\sum_{i=1}^{n} \\left( \\log \\lambda - \\lambda x_i \\right) \\\\\n&= n \\log \\lambda - \\lambda \\sum_{i=1}^{n} x_i.\n\\end{align*} To find the maximum likelihood estimator for \\lambda, we take the derivative of the log likelihood with respect to \\lambda and set it to zero:\n\\begin{align*}\n\\frac{d\\ell}{d\\lambda} &= \\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i = 0.\n\\end{align*}\nSolving for \\lambda gives: \n\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} x_i} = \\frac{1}{\\bar{x}},\n where \\bar{x} is the sample mean of the observed data. This result aligns with our previous derivation using the likelihood function directly.\nLet’s plot the log likelihood surface (the joint probability distribution), and we’ll see how the four reasons for using the log likelihood come into play.\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon, uniform, norm\nShow the code\npdf_expon = lambda x, lambd: lambd * np.exp(-lambd * x) * (x &gt;= 0)\nfig, ax = plt.subplots(2,3, figsize=(8,6), sharex='col')\nfig.subplots_adjust(wspace=0.4, hspace=0.4)\nax[1,0].set_axis_off()\nx = np.linspace(0, 5, 1000)\nlmbda = np.linspace(1e-5, 3, 1000)\n\nideal_lambda = 0.7\nnp.random.seed(seed=1)\ndata = expon.rvs(size=100, scale=1/ideal_lambda)\n\nax[0,0].plot(x, pdf_expon(x, ideal_lambda))\nax[0,0].plot(data, np.zeros_like(data), ls='None', marker='|', color='red')\nax[0,0].set(xlabel='x', ylabel='pdf')\n\nLAM, X = np.meshgrid(lmbda, x)\nZ = pdf_expon(X, LAM)\nax[0,1].contourf(LAM, X, Z, levels=10)\n\nax[0,2].contourf(LAM, X, np.log(Z), levels=10)\n\nlike = [np.prod(pdf_expon(data, l)) for l in lmbda]\nlog_like = [np.sum(np.log(pdf_expon(data, l))) for l in lmbda]\nax[1,1].plot(lmbda, like)\nax[1,2].plot(lmbda, log_like)\n\nfor i, xx in enumerate(data):\n    ax[0,1].axhline(xx, color='white', ls='-', alpha=0.1)\n    ax[0,2].axhline(xx, color='white', ls='-', alpha=0.1)\n\nprint(f\"{ideal_lambda} was the ideal lambda used to generate the data.\")\nprint(f\"{1/np.mean(data):.3f} is the MLE estimate of lambda from the data.\")\n\n\nax[0,0].set(xlim=(0, 5))\nax[0,1].set(ylim=(0, 5),\n            xlim=(0, 3),\n            title=\"joint pdf\",\n            xlabel=r\"$\\lambda$\",\n            )\nax[0,1].set_ylabel(r\"$x$\", rotation=0)\nax[0,2].set(ylim=(0, 5),\n            xlim=(0, 3),\n            title=\"joint pdf, log scale\",\n            xlabel=r\"$\\lambda$\",\n            )\nax[0,2].set_ylabel(r\"$x$\", rotation=0)\nax[1,1].set(title=\"likelihood\",\n             xlabel=r\"$\\lambda$\",\n             ylabel=r\"$\\prod P(x\\mid\\lambda)$\")\nax[1,2].yaxis.tick_right()\nax[1,2].yaxis.set_label_position(\"right\")\nax[1,2].set(title=\"log likelihood\",\n             xlabel=r\"$\\lambda$\",\n             ylabel=r\"$\\sum \\ln P(x\\mid\\lambda)$\")\nax[0,0].text(0.97, 0.97, r\"a\", transform=ax[0,0].transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontweight=\"bold\")\nax[0,1].text(0.97, 0.97, r\"b\", transform=ax[0,1].transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontweight=\"bold\", color=\"white\")\nax[0,2].text(0.97, 0.97, r\"c\", transform=ax[0,2].transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontweight=\"bold\", color=\"white\")\nax[1,1].text(0.97, 0.97, r\"d\", transform=ax[1,1].transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontweight=\"bold\")\nax[1,2].text(0.97, 0.97, r\"e\", transform=ax[1,2].transAxes,\n         horizontalalignment='right', verticalalignment='top',\n         fontweight=\"bold\");\n\n\n0.7 was the ideal lambda used to generate the data.\n0.738 is the MLE estimate of lambda from the data.\nBecause this problem has only one parameter, all we have to do to find the MLE is to find the peak of the curve in panel e. We can do this analytically by taking the derivative of the log likelihood function we derived above, or numerically using optimization methods. For 100 data points drawn from an exponential distribution with \\lambda=0.7, the MLE estimate of \\lambda is approximately 0.0.738, which is close to the true value.\nThe same ideas shown here apply to all the examples below. For models with more than one parameter, the log likelihood surface becomes a multi-dimensional surface, and we cannot visualize it easily. However, the principles remain the same: we seek the set of parameters \\{\\theta_1, \\theta_2, \\ldots, \\theta_k\\} that maximize the log likelihood.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#exponential-distribution-revisited",
    "href": "likelihood/MLE_and_summary_statistics.html#exponential-distribution-revisited",
    "title": "40  MLE and summary statistics",
    "section": "",
    "text": "Panel a: graph of the exponential PDF for \\lambda=0.7. I drew 100 random samples from this distribution, shown as small vertical red lines.”\nPanel b: joint probability distribution in the space of x and \\lambda. The horizontal white lines correspond to the observed data points. The Brighter colors represent higher values.\nPanel c: log joint probability distribution in the space of x and \\lambda. Note how it is much easier to see the structure of the surface.\nPanel d: likelihood as a function of \\lambda for the observed data. How to read this: for each value of \\lambda on the x-axis, look up to see where it intersects each of the horizontal white lines in panel b, read off the corresponding probability density values, and multiply them together to get the likelihood value at that \\lambda. Note the following:\n\nThe curve is extremely steep near the peak.\nThe likelihood values are very small.\n\nPanel e: log likelihood as a function of \\lambda for the observed data. Note the following:\n\nThe curve is much smoother.\nThe log likelihood values are in a “manageable” range.\nThe maximum occurs at the same \\lambda value as in panel d.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#normal-distribution",
    "href": "likelihood/MLE_and_summary_statistics.html#normal-distribution",
    "title": "40  MLE and summary statistics",
    "section": "40.2 normal distribution",
    "text": "40.2 normal distribution\nThe normal distribution is defined by two parameters: the mean \\mu and the standard deviation \\sigma. As an example, consider measuring the heights of a group of people. We can assume that the heights are normally distributed with some unknown mean and standard deviation.\nIts joint probability density function is given by: \nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\n\nThe log likelihood reads:\n\\begin{align*}\n\\ell(\\mu, \\sigma \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i; \\mu, \\sigma) \\\\\n&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right) \\\\\n&= \\sum_{i=1}^{n} \\left( -\\log(\\sigma \\sqrt{2\\pi}) - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right) \\\\\n&= -n \\log(\\sigma) -n \\log(\\sqrt{2\\pi}) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2.\n\\end{align*}\nMaximizing the log likelihood with respect to \\mu and \\sigma involves taking partial derivatives, setting them to zero, and solving for the parameters.\nFor \\mu, we have:\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial \\mu} &= \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) \\\\\n&= \\frac{1}{\\sigma^2} \\left( \\sum_{i=1}^{n} x_i - n\\mu \\right) = 0.\n\\end{align*}\nSolving for \\mu gives: \n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i,\n which is the sample mean.\nNow, for \\sigma, we have:\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial \\sigma} &= -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\\\\n&= \\frac{1}{\\sigma^3} \\left( -n\\sigma^2 + \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right) = 0.\n\\end{align*}\nSolving for \\sigma gives: \n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2,\n which is the sample variance (using n in the denominator for MLE).\nGoing back to the height example, our MLEs for the mean and standard deviation of heights are simply the sample mean and sample standard deviation of the observed heights.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#binomial-distribution",
    "href": "likelihood/MLE_and_summary_statistics.html#binomial-distribution",
    "title": "40  MLE and summary statistics",
    "section": "40.3 binomial distribution",
    "text": "40.3 binomial distribution\nThe binomial distribution models the number of successes in n independent Bernoulli trials, each with success probability p. A very common example is flipping a biased coin, whose probability of yielding heads is p, flipping it n times and counting the number of heads k. Its joint probability mass function is given by: \nP(X = k; n, p) = \\binom{n}{k} p^k (1 - p)^{n - k}.\n The log likelihood reads:\n\\begin{align*}\n\\ell(p \\mid k, n) &= \\log P(X = k; n, p) \\\\\n&= \\log \\left( \\binom{n}{k} p^k (1 - p)^{n - k} \\right) \\\\\n&= \\log \\binom{n}{k} + k \\log p + (n - k) \\log (1 - p).\n\\end{align*}\nTo find the maximum likelihood estimator for p, we take the derivative of the log likelihood with respect to p and set it to zero:\n\\begin{align*}\n\\frac{d\\ell}{dp} &= \\frac{k}{p} - \\frac{n - k}{1 - p} = 0.\n\\end{align*}\nSolving for p gives: \n\\hat{p} = \\frac{k}{n},\n which is the sample proportion of successes. Going back to the coin example, if we flip the coin 100 times and get 60 heads, our MLE for p would be \\hat{p} = 60/100 = 0.6.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#laplace-distribution",
    "href": "likelihood/MLE_and_summary_statistics.html#laplace-distribution",
    "title": "40  MLE and summary statistics",
    "section": "40.4 Laplace distribution",
    "text": "40.4 Laplace distribution\nThe Laplace distribution, also known as the double exponential distribution, is characterized by its location parameter \\mu and scale parameter b. We use the normal distribution for ‘well-behaved’ data like human heights. We use the Laplace distribution for ‘wild’ data like stock market data, where outliers aren’t just mistakes—they are part of the system. For example, most days the price of a stock barely moves, but rare events (market crashes or surges) happen much more often than a Normal distribution would allow.\nIts joint probability density function is given by: \nf(x; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x - \\mu|}{b}\\right).\n\nThe log likelihood reads:\n\\begin{align*}\n\\ell(\\mu, b \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i; \\mu, b) \\\\\n&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{2b} \\exp\\left(-\\frac{|x_i - \\mu|}{b}\\right) \\right) \\\\\n&= \\sum_{i=1}^{n} \\left( -\\log(2b) - \\frac{|x_i - \\mu|}{b} \\right) \\\\\n&= -n \\log(2b) - \\frac{1}{b} \\sum_{i=1}^{n} |x_i - \\mu|.\n\\end{align*}\nMaximizing the log likelihood with respect to \\mu and b involves taking partial derivatives, setting them to zero, and solving for the parameters.\nFor \\mu, we have:\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial \\mu} &= \\frac{1}{b} \\sum_{i=1}^{n} \\text{sgn}(x_i - \\mu) = 0,\n\\end{align*} where \\text{sgn}(x) is the sign function, which returns -1 for negative values, 1 for positive values, and 0 for zero.\nSolving for \\mu is essentially a balancing act. It requires us finding a value for \\mu such that the number of positive ones (points above \\mu) exactly cancels out the number of negative ones (points below \\mu). This happens when \\mu is the median of the data: \n\\hat{\\mu} = \\text{median}(x_{1:n}).\n\nNow, for b, we have:\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial b} &= -\\frac{n}{b} + \\frac{1}{b^2} \\sum_{i=1}^{n} |x_i - \\mu| = 0.\n\\end{align*}\nSolving for b gives: \n\\hat{b} = \\frac{1}{n} \\sum_{i=1}^{n} |x_i - \\hat{\\mu}|,\n which is the mean absolute deviation (MAD) from the median. This measure of spread is more robust to outliers than the standard deviation.\nGoing back to the stock market example, our MLEs for the location and scale parameters are simply the sample median and mean absolute deviation from the median of the observed returns.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#poisson-distribution",
    "href": "likelihood/MLE_and_summary_statistics.html#poisson-distribution",
    "title": "40  MLE and summary statistics",
    "section": "40.5 Poisson distribution",
    "text": "40.5 Poisson distribution\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence \\lambda. A common example is counting the number of emails received in an hour, or the number of radioactive decays detected in a piece of radioactive material over a certain period. Its joint probability mass function is given by: \nP(X = k, \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}.\n The log likelihood reads:\n\\begin{align*}\n\\ell(\\lambda \\mid k_{1:n}) &= \\sum_{i=1}^{n} \\log P(X = k_i, \\lambda) \\\\\n&= \\sum_{i=1}^{n} \\left( k_i \\log \\lambda - \\lambda - \\log(k_i!) \\right) \\\\\n&= \\left(\\sum_{i=1}^{n} k_i \\right) \\log \\lambda - n \\lambda - \\sum_{i=1}^{n} \\log(k_i!).\n\\end{align*}\nTo find the maximum likelihood estimator for \\lambda, we take the derivative of the log likelihood with respect to \\lambda and set it to zero:\n\\begin{align*}\n\\frac{d\\ell}{d\\lambda} &= \\frac{\\sum_{i=1}^{n} k_i}{\\lambda} - n = 0.\n\\end{align*}\nSolving for \\lambda gives: \n\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^{n} k_i,\n which is the sample mean of the observed counts.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#uniform-distribution",
    "href": "likelihood/MLE_and_summary_statistics.html#uniform-distribution",
    "title": "40  MLE and summary statistics",
    "section": "40.6 uniform distribution",
    "text": "40.6 uniform distribution\nThe uniform distribution models a situation where all outcomes in a given range are equally likely. A common example is rolling a fair die, where each face (1 through 6) has an equal probability of landing face up. Its joint probability density function is given by: \nf(x; a, b) = \\frac{1}{b - a} \\text{ for } a \\leq x \\leq b.\n\nThe log likelihood reads:\n\\begin{align*}\n\\ell(a, b \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i; a, b) \\\\\n&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{b - a} \\right) \\\\\n&= -n \\log(b - a).\n\\end{align*}\nIn this instance, it is much easier to find the maximum likelihood estimators for a and b using the probability density function directly, rather than taking derivatives of the log likelihood. The likelihood reads: \nL(a, b \\mid x_{1:n}) = \\left( \\frac{1}{b - a} \\right)^n.\n\nThe likelihood doesn’t have a peak in the usual sense; it increases as the interval [a, b] narrows. In order to maximize the likelihood, the interval must be as small as possible while still containing all observed data points. If at least one data point falls outside the interval, the likelihood becomes zero, because the uniform distribution assigns zero probability to values outside [a, b], and the product of probabilities will include a zero term. The smallest interval that contains all observed data points is defined by the minimum and maximum of the data. Therefore, \n\\hat{a} = \\min(x_{1:n}), \\quad \\hat{b} = \\max(x_{1:n}).",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_summary_statistics.html#summary",
    "href": "likelihood/MLE_and_summary_statistics.html#summary",
    "title": "40  MLE and summary statistics",
    "section": "40.7 summary",
    "text": "40.7 summary\nBy appliying the reasoning of maximum likelihood estimation to various probability distributions, we have derived the maximum likelihood estimators for their parameters. Notably, we found that:\n\nFor the exponential distribution, the MLE is the reciprocal of the sample mean.\nFor the normal distribution, the MLEs are the sample mean and sample standard deviation.\nFor the binomial distribution, the MLE is the sample proportion of successes.\nFor the Laplace distribution, the MLEs are the sample median and mean absolute deviation from the median.\nFor the Poisson distribution, the MLE is the sample mean of the observed counts.\nFor the uniform distribution, the MLEs are the sample minimum and sample maximum.\n\nAll these estimators are intuitive and align with common summary statistics used in data analysis, highlighting the deep connection between maximum likelihood estimation and descriptive statistics.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>MLE and summary statistics</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_linear_regression.html",
    "href": "likelihood/MLE_and_linear_regression.html",
    "title": "41  MLE and linear regression",
    "section": "",
    "text": "The equation for linear regression is given by:\n\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k + \\epsilon,\n where \\beta_0 is the intercept, \\beta_1, \\beta_2, \\ldots, \\beta_k are the coefficients for the predictor variables x_1, x_2, \\ldots, x_k, and \\epsilon is the error term.\nReminder: the regression is called linear because it is linear in the parameters \\beta_i, not necessarily in the predictor variables x_i. For example, a model like y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon is still considered linear regression because it is linear in \\beta_0, \\beta_1, and \\beta_2. If you are still not convinced, consider that you can always create new predictor variables that are transformations of the original ones (e.g., x_2 = x^2) and include them in a linear regression model.\nThe \\epsilon term is typically assumed to be normally distributed with mean 0 and constant variance \\sigma^2. This assumption is key! It allows us to use maximum likelihood estimation to estimate the parameters \\beta_0, \\beta_1, \\ldots, \\beta_k. Let’s see how.\nFirst, we solve the equation above for \\epsilon:\n\n\\epsilon = y - (\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k).\n\nSee the figure below for a practical example.\n\nleft panel: scatter plot of data points (blue dots) and the underlying model (parabola) used as the basis (red line).\nmiddle panel: scatter plot for epsilon, which is, of course, just the vertical distance from each data point to the red line in the left panel. This distance is the error term, also called the residual.\nright panel: the probability density function from which the epsilon values were drawn (normal distribution with mean 0 and standard deviation 5).\n\n\n\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon, uniform, norm\n\n\n\n\nShow the code\nfig, ax = plt.subplots(1,3, figsize=(8,3), gridspec_kw=dict(width_ratios=[1,1, 0.2]))\nfig.subplots_adjust(wspace=0.4)\nbeta0 = 2\nbeta1 = 13\nbeta2 = -1\n\nnp.random.seed(seed=1)\nx = uniform.rvs(size=100) * 15\nx = np.sort(x)\nepsilon = norm.rvs(size=100, scale=5)\ny = beta0 + beta1 * x + beta2 * x**2 + epsilon\n\nax[0].scatter(x, y, alpha=0.5, label=r\"$y=\\beta_0 + \\beta_1 x + \\beta_2 x^2x + \\epsilon$\")\nax[0].plot(x, beta0 + beta1 * x + beta2 * x**2, color='red', lw=2, label=r\"$\\beta_0 + \\beta_1 x$\", alpha=0.5)\nax[0].set(xlabel=\"x\",\n          ylim=(-30, 90),\n          )\nax[0].legend(frameon=False, loc='upper left')\n\nax[1].scatter(x, epsilon, alpha=0.5)\nax[1].axhline(0, color='black', ls='--')\nax[1].set(xlabel=\"x\")\nax[1].set_ylabel(r\"$\\epsilon$\", rotation=0)\nax[1].yaxis.set_label_position(\"right\")\nax[1].yaxis.tick_right()\n\neps_vec = np.linspace(-12, 12, 1000)\nax[2].plot(norm.pdf(eps_vec, loc=0, scale=5), eps_vec)\nax[2].set(ylim=(-12, 12),\n          xlabel='pdf',\n          xticks=[],\n          yticks=[]\n          );\n\n\n\n\n\n\n\n\n\nWe can now “forget” about the y and x variables for a moment, and model the error term \\epsilon directly. Since we assumed that \\epsilon is normally distributed with mean 0 and variance \\sigma^2, we can write the following joint probability density function for \\epsilon:\n\nf(\\epsilon \\mid \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{\\epsilon^2}{2\\sigma^2}\\right).\n\nUsing this expression for the joint pdf in the log likelihood function yields: \n\\ell(\\sigma^2 \\mid \\epsilon_{1:n}) = \\sum_{i=1}^{n} \\log f(\\epsilon_i \\mid \\sigma^2).\n\nThis is the time to go back to the original variables. Substituting the expression for \\epsilon into the log likelihood function gives:\n\\begin{align*}\n\\ell(\\beta_0, \\beta_1, \\ldots, \\beta_k, \\sigma^2 \\mid y_{1:n}, x_{1:n}) &= \\sum_{i=1}^{n} \\log f\\left(y_i - (\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik}) \\mid \\sigma^2\\right)\\\\\n&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik}))^2}{2\\sigma^2}\\right) \\right)\\\\\n&= \\sum_{i=1}^{n} \\left( -\\frac{1}{2} \\log(2\\pi \\sigma^2) - \\frac{(y_i - (\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik}))^2}{2\\sigma^2} \\right)\\\\\n&= -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik}))^2.\n\\end{align*}\nIn the expression above, instead of solving for the specific case of two predictor variables (as shown in the graph), I solved for the general case of k predictor variables, and therefore we would like to find the best values for the k+1 coefficients \\beta_0, \\beta_1, \\ldots, \\beta_k. Notice that the first term in the log likelihood does not depend on the \\beta parameters, so we can ignore it for the purpose of maximization.\nNow, here’s the kicker: because of the negative sign in front of the second term, maximizing the log likelihood is equivalent to minimizing the sum of squared errors (SSE):\n\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik}))^2.\n\nMind blown.\nThe general case that the best set of parameters \\hat{\\theta} are those that maximize the log likelihood, \n\\hat{\\theta} = \\arg\\max_{\\theta_i} \\ell(\\theta_i \\mid x_{1:n}),\n translates in the specific case of linear regression to “the best set of parameters \\hat{\\beta} are those that minimize the sum of squared errors”: \n\\hat{\\beta} = \\arg\\min_{\\beta_i} \\text{SSE}.\n\nFinal thought: whenever we use the Least Squares method to fit a linear regression model, we are assuming (implicitly or explicitly) that the error terms are normally distributed.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>MLE and linear regression</span>"
    ]
  },
  {
    "objectID": "likelihood/MLE_and_information_theory.html",
    "href": "likelihood/MLE_and_information_theory.html",
    "title": "42  MLE and information theory",
    "section": "",
    "text": "42.1 implication\nWe will show here how the maximizing the log likelihood naturally connects to minimizing the Kullback-Leibler (KL) divergence.\nWe will start from a true data generating distribution P(x), and we will assume we have n independent and identically distributed (i.i.d.) observations x_1, x_2, \\ldots, x_n drawn from it. We model the data using a parametric family of distributions Q(x | \\theta), where \\theta is the parameter we want to estimate. Our goal is to find the parameters \\theta that make the model distribution Q(x | \\theta) as close as possible to the true distribution P(x). This set of parameters is typically found using Maximum Likelihood Estimation (MLE):\n\\hat{\\theta} = \\arg\\max_{\\theta} \\ell(\\theta \\mid x_{1:n})\nIn English: our best estimation of the parameters \\theta is the one that maximizes the parameters (also called arguments) of the log likelihood of the observed data x_{1:n} = (x_1, x_2, \\ldots, x_n).\nThe log likelihood function for the observed data given the parameters \\theta is defined as:\n\\ell(\\theta \\mid x_{1:n}) = \\sum_{i=1}^{n} \\log Q(x_i | \\theta)\nHere we used Q to denote the model distribution, to distinguish it from the true data generating distribution P. We will now divide both sides by n to express the average log likelihood per observation:\n\\frac{1}{n} \\ell(\\theta \\mid x_{1:n}) = \\frac{1}{n} \\sum_{i=1}^{n} \\log Q(x_i | \\theta)\nThis step can be justified in a few ways:\nNote that now the right-hand side is the sample mean of the log probabilities \\log Q(x | \\theta) evaluated at the observed data points x_i. This sample mean can also be understood as an empirical expectation over the observed data, where each data point is given equal weight 1/n:\n\\begin{align*}\n\\frac{1}{n} \\ell(\\theta \\mid x_{1:n}) &= \\frac{1}{n} \\sum_{i=1}^{n} \\log Q(x_i | \\theta) \\\\\n&= \\mathbb{E}_{x \\sim \\hat{P}_n}[\\log Q(x | \\theta)],\n\\end{align*}\nwhere the subscript x \\sim \\hat{P}_n indicates that the expectation (\\mathbb{E}) is taken with respect to the empirical distribution \\hat{P}_n defined by the observed data points x_1, x_2, \\ldots, x_n.\nNow, what happens as we collect more and more data, i.e., as n approaches infinity? Our “data-driven” distribution \\hat{P}_n starts to look exactly like the “true” distribution P, therefore:\n\\begin{align*}\n\\lim_{n \\to \\infty} \\mathbb{E}_{x \\sim \\hat{P}_n}[\\log Q(x | \\theta)] &= \\mathbb{E}_{x \\sim P}[\\log Q(x | \\theta)] \\\\\n&= \\sum_{x} P(x) \\log Q(x | \\theta)\n\\end{align*}\nWe’re almost there. Remember when we defined the cross-entropy in the chapter cross-entropy and KL divergence? It was defined as:\nH(P, Q) = - \\sum_x P(x) \\log Q(x),\nwhich is exactly the negative of what we got before. We have shown so far that maximizing the average log likelihood is equivalent to minimizing the cross-entropy between the true distribution P and the model distribution Q. The caveat for this statement is that this equivalence holds in the limit of infinite data. The very last step is to connect this to the KL divergence, which we defined as:\nD_{KL}(P \\| Q) = H(P, Q) - H(P).\nThe term H(P) is the entropy of the true distribution P, which does not depend on the model parameters \\theta. Therefore, minimizing the cross-entropy H(P, Q) is equivalent to minimizing the KL divergence D_{KL}(P \\| Q).\nThe minimization of the KL divergence is ubiquitous in machine learning, and it is often used as a loss function to train models. This connection between MLE and KL divergence provides a theoretical foundation for many machine learning algorithms.",
    "crumbs": [
      "likelihood",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>MLE and information theory</span>"
    ]
  },
  {
    "objectID": "generalization/bias-variance-tradeoff.html",
    "href": "generalization/bias-variance-tradeoff.html",
    "title": "46  bias-variance tradeoff",
    "section": "",
    "text": "46.1 expected squared error\nLet’s say I’m running an experiment where I observe some data points (x, y). This experiment could represent whatevery you like, but I always like to have a concrete example in mind. Assume that x is the number of years a person has gone to school/university, and y is their income. The process that generates this data has a true underlying function f(x), but I don’t know what it is. What’s worse, nature is noisy, so the observed data points y are not exactly on the true function, but are spread around it:\ny = f(x) + \\epsilon.\nHere the noise \\epsilon has zero mean and some standard deviation \\sigma. In our example, the noise could represent all the other factors that affect a person’s income, such as their family background, their social skills, their luck, and so on.\nSee below an example.\nNote: don’t pay much attention to the scale of the axes, the concrete example is just for illustration purposes, and the numbers are not meant to be realistic.\nLet’s say that I learn that John Doe belongs to the same population as the data I observed, and I know that John Doe has x_0 years of education. Can I predict his income? My job is to find a function \\hat{f}(x) that best describes the process that generates the data, f(x). I can then use this function and give a prediction for John’s income, \\hat{f}(x_0). There are infinite functions to choose from, what should I do? Not knowing much about the true function, I decide in this tutorial to describe it using a polynomial function. The problem is not only that I need to find the best coefficients for the polynomial, but I also need to decide on the degree of the polynomial! It could be a straight line (degree 1), a parabola (degree 2), or something a lot more complex, such as a 15-degree polynomial. How do I decide on the degree of the polynomial?\nLet’s play a little bit. At first, I’ll choose a 3rd degree polynomial (a cubic function). I’ll repeat the experiment 8 times, which in our example means that I’ll conduct 8 surveys, each with a different sample of people. For each experiment, I’ll fit a cubic function to each experiment using the least-squares method, and see how the fitted functions look like.\nNote that in each experiment m we observe a different set of data points, and therefore we get a different fitted function {\\hat{f}}_m(x). We will do the same for 200 experiments, and plot it as a “spaghetti plot”:\nA pattern seems to emerge!\nNow, let’s focus on a tiny slice of the graph, centered at x=0.6 (the vertical dotted line)\nThe pink dot denotes a “test point”. We can think of it as representing a future measurement at x=0.6 (another person interviewed that we know has x=0.6 years of education). Since we now have a bunch of fitted functions, we can ask how well do we expect them to predict the value of y at x=0.6? Or, turning it around, how badly do we expect them to err the value of y at x=0.6? The error for a specific fitted function at a specific x value is the vertical distance between it and the test point:\n\\text{error} = \\hat{f}_m(x) - y.\nThe expected squared error over all repetitions of the experiment, that is, over all fitted functions, is:\n\\mathbb{E}_D[(\\hat{f}(x) - y)^2\\mid x],\nwhere the symbols |x mean “given x”, meaning that we are looking at the error at a specific value of x.\nNote that:\nWhat follows is a derivation of the expected squared error. If you don’t care about how we get to the formula, you can skip to the next section.\nWe start by substituting the value of y=f(x)+\\epsilon in the expected error: \n\\mathbb{E}_D[(\\hat{f}(x) - f(x) - \\epsilon)^2\\mid x].\nLet’s call A = \\hat{f}(x) - f(x). Then the expected squared error becomes:\n\\mathbb{E}_D[(A - \\epsilon)^2\\mid x].\nExpanding the square, we get:\n\\begin{align*}\n\\mathbb{E}_D[(A - \\epsilon)^2\\mid x] &= \\mathbb{E}_D[A^2 - 2A\\epsilon + \\epsilon^2\\mid x] \\\\\n&= \\underbrace{\\mathbb{E}_D[A^2\\mid x]}_{\\text{term 1}} - 2\\underbrace{\\mathbb{E}_D[A\\epsilon\\mid x]}_{\\text{term 2}} + \\underbrace{\\mathbb{E}_D[\\epsilon^2\\mid x]}_{\\text{term 3}}.\n\\end{align*}\nLet’s take stock of what we have so far. The expected error is now:\n\\begin{align*}\n\\mathbb{E}_D[(\\hat{f}(x) - y)^2\\mid x] &= \\mathbb{E}_D[A^2\\mid x] + \\sigma^2 \\\\\n&= \\mathbb{E}_D[(\\hat{f}(x) - f(x))^2\\mid x] + \\sigma^2.\n\\end{align*}\nThe first term is the error from the model \\hat{f} not matching the ground truth f, and the second term is an irreducible error from the noise in the data. Let’s analyze the first term a bit more.\nWe define the mean predictor at x to be:\n\\mu(x) = \\mathbb{E}_D[\\hat{f}(x)\\mid x].\nIn the graph above, it would simply be the mean of all the 200 blue lines at a specific x value. Now let’s perform a little trick: we add and subtract \\mu(x) inside the parenthesis in the first term:\n\\hat{f}(x) - f(x) = \\left(\\hat{f}(x) - \\mu(x)\\right) + \\Bigl(\\mu(x) - f(x)\\Bigr).\nNow squaring it:\n(\\hat{f} - f)^2 = \\left(\\hat{f} - \\mu\\right)^2 + \\Bigl(\\mu - f\\Bigr)^2 + 2\\left(\\hat{f} - \\mu\\right)\\Bigl(\\mu - f\\Bigr).\nWhat we wanted to analyze is the expected value of this expression:\n\\begin{align*}\n\\mathbb{E}_D[(\\hat{f} - f)^2\\mid x] &=\n\\mathbb{E}_D\\left[\\left(\\hat{f} - \\mu\\right)^2\\mid x\\right] \\\\\n&+ \\Bigl(\\mu - f\\Bigr)^2 \\\\\n&+ 2\\Bigl(\\mu - f\\Bigr)\\mathbb{E}_D\\left[\\left(\\hat{f} - \\mu\\right)\\mid x\\right]\n\\end{align*}\nIn the 2nd and 3rd terms, the term (\\mu - f) got out of the expectation operator \\mathbb{E} because it is a constant, not a random variable. This expression looks complicated, but it can get simpler. Let’s start with the last term. Since \\mu is the expected value of \\hat{f}, we have:\n\\mathbb{E}_D\\left[\\left(\\hat{f} - \\mu\\right)\\mid x\\right] = \\mathbb{E}_D\\left[\\hat{f}\\mid x\\right] - \\mu = \\mu - \\mu = 0.\nThe very last thing we do now it to give names to the first two terms. The first term is called the variance of the model,\n\\text{Var}\\left[\\hat{f}(x)\\right] = \\mathbb{E}_D\\left[\\left(\\hat{f}(x) - \\mu(x)\\right)^2\\mid x\\right],\nand the second term is the square of the bias of the model,\n\\text{Bias}^2\\left[\\hat{f}(x)\\right] = \\Bigl(\\mu(x) - f(x)\\Bigr)^2.",
    "crumbs": [
      "generalization and model complexity",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "generalization/bias-variance-tradeoff.html#expected-squared-error",
    "href": "generalization/bias-variance-tradeoff.html#expected-squared-error",
    "title": "46  bias-variance tradeoff",
    "section": "",
    "text": "term 2: the test noise \\epsilon is independent of the training dataset D, therefore independent of \\hat{f}(x) and of A. Remember also that we assumed that the noise has zero mean, so \\mathbb{E}[\\epsilon] = 0. Therefore:\n\n  \\mathbb{E}_D[A\\epsilon\\mid x] = \\mathbb{E}_D[A\\mid x]\\cdot\\mathbb{E}[\\epsilon] = \\mathbb{E}_D[A\\mid x]\\cdot 0 = 0.\n  \nterm 3: we can use the identity that for a random variable \\epsilon with finite variance, \n  \\mathbb{E}[\\epsilon^2] = \\text{Var}(\\epsilon) + (\\mathbb{E}[\\epsilon])^2.\n   Since we assumed that \\epsilon has zero mean, the last term is zero, and we get: \n  \\mathbb{E}_D[\\epsilon^2\\mid x] = \\text{Var}(\\epsilon) = \\sigma^2.",
    "crumbs": [
      "generalization and model complexity",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "generalization/bias-variance-tradeoff.html#biasvariancenoise-decomposition",
    "href": "generalization/bias-variance-tradeoff.html#biasvariancenoise-decomposition",
    "title": "46  bias-variance tradeoff",
    "section": "46.2 bias–variance–noise decomposition",
    "text": "46.2 bias–variance–noise decomposition\nPutting everything together, we get the following decomposition of the expected squared error:\n\n\\mathbb{E}_D[(\\hat{f}(x) - y)^2\\mid x] = \\text{Var}\\left[\\hat{f}(x)\\right] + \\text{Bias}^2\\left[\\hat{f}(x)\\right] + \\sigma^2.\n\nWe did all this hard work of deriving this decomposition to understand the different sources of error in our model. Hopefully, a good model will give a low expected error. Let’s see a few examples of repeated experiments, this time for polynomials of degree 3, 6, and 11.\n\n\ncalculate mean and std for all degrees from 1 to 15\nnp.random.seed(0)\ndegree_max = 15  # degree of the fitted polynomial\nsigma = 0.3      # standard deviation of the noise\nM = 200          # number of repeated experiments\nn_train = 20     # number of observed points used to fit in each experiment\n\n# we'll store the predictions of all M fitted models on the full x_grid in this array\npreds15 = np.empty((M, N, degree_max))\n\nfor deg in range(1, degree_max+1):\n\n    for m in range(M):\n        # generate a new noisy realization of the data on the fixed grid\n        noise = np.random.normal(0, sigma, size=N)\n        y = f(x_grid) + noise\n\n        # choose which n_train grid points are \"observed\" in this experiment\n        idx = np.random.choice(N, size=n_train, replace=False)\n\n        # # fit a degree-deg polynomial to the observed points\n        # coeffs = np.polyfit(x_grid[idx], y[idx], deg)\n\n        # # turn the fitted coefficients into a callable polynomial\n        # p = np.poly1d(coeffs)\n        # fit with a numerically stable polynomial basis (chebyshev)\n        # we get a callable polynomial\n        p = Chebyshev.fit(x_grid[idx], y[idx], deg, domain=[0, 1])\n\n        # evaluate the fitted model on the full grid (common x for averaging across fits)\n        preds15[m, :, deg-1] = p(x_grid)\n\n# calculate the mean prediction and its standard deviation across the M fitted models, for every point on the x-grid\nmean_pred = preds15.mean(axis=0)\nstd_pred  = preds15.std(axis=0)\n\n\n\n\nplot 8 fitted functions from repeated experiments\nnp.random.seed(0)\ndegr = [3, 6, 11] # list of degrees to fit\nsigma = 0.3      # standard deviation of the noise\nM5 = 5          # number of repeated experiments\nn_train = 20     # number of observed points used to fit in each experiment\nfig, ax = plt.subplots(3, 6, figsize=(8, 4), sharex=True, sharey=True)\nfor d, deg in enumerate(degr):\n    ax[d,0].set_ylabel(f\"deg={deg}\", labelpad=10)\n    for m in range(M):\n        ax[d, 5].plot(x_grid, preds15[m, :, deg-1], color=blue, alpha=0.02)\n        if d ==0:\n            ax[d, 5].set_title(f\"200 fits\")\n    ax[d, 5].plot(x_grid, f(x_grid), color='black', label='true function', alpha=1)\n    for i in range(M5):\n        idx = np.random.choice(N, size=n_train, replace=False)\n        noise = np.random.normal(0, sigma, size=n_train)\n        x_m = x_grid[idx]\n        y_m = f(x_m) + noise\n        p = Chebyshev.fit(x_m, y_m, deg, domain=[0, 1])\n        l_true, = ax[d, i].plot(x_grid, f(x_grid), color='black', label='true function', alpha=0.2)\n        l_data = ax[d, i].scatter(x_m, y_m, s=10, alpha=0.3, color=blue)\n        l_fitted, = ax[d, i].plot(x_grid, p(x_grid), color=blue, label='fitted function')\n        if d ==0:\n            ax[0,i].set_title(f\"m={i+1}\")\n        ax[d, i].set(xlim=(x_grid[0], x_grid[-1]),\n                ylim=(-1, 4),\n                xticks=[],\n                yticks=[])\n\n\n\n\n\n\n\n\n\nIn the top row we fitted a 3rd degree polynomial. None of the five examples does a great job at approximating the true function, and this is quantified by their high bias. On the other hand, at least they are all pretty similar to each other. Remember: each repeated experiment drew a different set of data points, but all 3rd order polynomials are still pretty similar to each other. This is quantified by their low variance. The last plot on the right shows 200 fitted functions, and we can see that they are all pretty similar to each other (the spaghetti is not too messy), but the overall shape in the aggregate is far from the true black curve.\nThe bottom row shows the opposite. The 11th degree polynomials are very flexible, and they can approximate the true function very well, which is quantified by their low bias. However, this flexibility comes at a cost: the fitted functions are very different from each other, and this is quantified by their high variance. Here, the spaghetti plot on the right is very messy, and the aggregate of all fitted functions is very close to the true function.\n\nHigh bias: We can understand that high bias means that a model is not flexible enough to capture the true function, and therefore it is systematically wrong in its predictions.\nHigh variance: We can understand that high variance means that a model is highly sensitive to the specific data points it was trained on. If we were to train the same model on a different dataset, we would get a very different fitted function.\n\nInstead of showing spaghetti plots, we can compute the mean and standard deviation of all repeated experiments. It the plot below, the mean is the blue line, and the shaded area represents ±1 standard deviation.\n\n\nplot results for all degrees from 1 to 15\nfig, ax = plt.subplots(3, 5, figsize=(10, 6), sharex=True, sharey=True)\nax = ax.flatten()\nfor i in range(15):\n    ax[i].plot(x_grid, f(x_grid), color=\"black\", label=\"true function\")\n    ax[i].plot(x_grid, mean_pred[:, i], label=f\"mean prediction (deg={i+1})\", color=blue)\n    ax[i].fill_between(x_grid, mean_pred[:, i] - std_pred[:, i], mean_pred[:, i] + std_pred[:, i], alpha=0.25, label=\"±1 std\", color=blue)\n\n    ax[i].text(0.97, 0.97, f\"deg={i+1}\", transform=ax[i].transAxes,\n            horizontalalignment='right', verticalalignment='top',)\n    if i &gt;= 10:\n        ax[i].set(xlabel=\"x\")\n    if i % 5 == 0:\n        ax[i].set_ylabel(\"y\", rotation=0, labelpad=10)\n    ax[i].set(xlim=(x_grid[0], x_grid[-1]),\n              ylim=(-1, 4),\n              xticks=[],\n              yticks=[])\n\n\n\n\n\n\n\n\n\nWe can actually see the bias and variance in these plots. To make it more clear, let’s focus on two polynomials: the 3rd degree and the 8th degree. The bias squared and the variance, shown in the middle and right columns, are just the square of the distances marked on the left column.\n\n\nbias-variance decomposition for deg=3 and deg=8\nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\n\ndeg_A = 3\nax[0,0].plot(x_grid, f(x_grid), color=\"black\", label=\"true function\")\nax[0,0].plot(x_grid, mean_pred[:, deg_A-1], label=f\"mean prediction (deg={deg_A})\", color=blue)\nax[0,0].fill_between(x_grid, mean_pred[:, deg_A-1] - std_pred[:, deg_A-1], mean_pred[:, deg_A-1] + std_pred[:, deg_A-1], alpha=0.25, label=\"±1 std\", color=blue)\nax[0,0].set_box_aspect(1)\nax[0,0].set(xlim=(x_grid[0], x_grid[-1]),\n        ylim=(-1, 4),\n        xticks=[],\n        yticks=[],\n        xlabel=\"x\",\n        ylabel=\"deg=3\")\nidx = int(0.6 * len(x_grid))\nax[0,0].plot([x_grid[idx]]*2, [f(x_grid[idx]), mean_pred[idx, deg_A-1]], color=gold, lw=2)\nax[0,0].plot([x_grid[idx]]*2, [mean_pred[idx, deg_A-1], mean_pred[idx, deg_A-1]-std_pred[idx, deg_A-1]], color=pink, lw=2)\n\nax[0,1].plot(x_grid, (f(x_grid)-mean_pred[:, deg_A-1])**2, color=\"black\", label=\"true function\")\nax[0,1].fill_between(x_grid, 0, (f(x_grid)-mean_pred[:, deg_A-1])**2, color=gold, label=\"bias^2\")\nax[0,2].plot(x_grid, (std_pred[:, deg_A-1])**2, color=\"black\", label=\"true function\")\nax[0,2].fill_between(x_grid, 0, (std_pred[:, deg_A-1])**2, color=pink, label=\"variance\")\nax[0,1].set_box_aspect(1)\nax[0,2].set_box_aspect(1)\nax[0,1].set(xlim=(x_grid[0], x_grid[-1]),\n        ylim=(0, 2),\n        xticks=[],\n        xlabel=\"x\",\n        yticks=[0,2])\nax[0,2].set(xlim=(x_grid[0], x_grid[-1]),\n        ylim=(0, 2),\n        xticks=[],\n        xlabel=\"x\",\n        yticks=[0,2])\n\nax[0,1].text(0.5, 0.97, r\"Bias$^2$\", transform=ax[0,1].transAxes,\n         horizontalalignment='center', verticalalignment='top',)\nax[0,2].text(0.5, 0.97, r\"Variance\", transform=ax[0,2].transAxes,\n         horizontalalignment='center', verticalalignment='top',)\n\n\n\n###\n\ndeg_B = 8\nax[1,0].plot(x_grid, f(x_grid), color=\"black\", label=\"true function\")\nax[1,0].plot(x_grid, mean_pred[:, deg_B-1], label=f\"mean prediction (deg={deg_B})\", color=blue)\nax[1,0].fill_between(x_grid, mean_pred[:, deg_B-1] - std_pred[:, deg_B-1], mean_pred[:, deg_B-1] + std_pred[:, deg_B-1], alpha=0.25, label=\"±1 std\", color=blue)\nax[1,0].set_box_aspect(1)\nax[1,0].set(xlim=(x_grid[0], x_grid[-1]),\n        ylim=(-1, 4),\n        xticks=[],\n        yticks=[],\n        xlabel=\"x\",\n        ylabel=\"deg=8\")\nidx = int(0.13 * len(x_grid))\nax[1,0].plot([x_grid[idx]]*2, [f(x_grid[idx]), mean_pred[idx, deg_B-1]], color=gold, lw=2)\nax[1,0].plot([x_grid[idx]]*2, [mean_pred[idx, deg_B-1], mean_pred[idx, deg_B-1]+std_pred[idx, deg_B-1]], color=pink, lw=2)\n\nax[1,1].plot(x_grid, (f(x_grid)-mean_pred[:, deg_B-1])**2, color=\"black\", label=\"true function\")\nax[1,1].fill_between(x_grid, 0, (f(x_grid)-mean_pred[:, deg_B-1])**2, color=gold, label=\"bias^2\")\nax[1,2].plot(x_grid, (std_pred[:, deg_B-1])**2, color=\"black\", label=\"true function\")\nax[1,2].fill_between(x_grid, 0, (std_pred[:, deg_B-1])**2, color=pink, label=\"variance\")\nax[1,1].set_box_aspect(1)\nax[1,2].set_box_aspect(1)\nax[1,1].set(xlim=(x_grid[0], x_grid[-1]),\n        ylim=(0, 2),\n        xticks=[],\n        xlabel=\"x\",\n        yticks=[0,2])\nax[1,2].set(xlim=(x_grid[0], x_grid[-1]),\n        ylim=(0, 2),\n        xticks=[],\n        xlabel=\"x\",\n        yticks=[0,2])\n\nax[1,1].text(0.5, 0.97, r\"Bias$^2$\", transform=ax[1,1].transAxes,\n         horizontalalignment='center', verticalalignment='top',)\nax[1,2].text(0.5, 0.97, r\"Variance\", transform=ax[1,2].transAxes,\n         horizontalalignment='center', verticalalignment='top',);",
    "crumbs": [
      "generalization and model complexity",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "generalization/bias-variance-tradeoff.html#tradeoff",
    "href": "generalization/bias-variance-tradeoff.html#tradeoff",
    "title": "46  bias-variance tradeoff",
    "section": "46.3 tradeoff",
    "text": "46.3 tradeoff\nWe quantified the bias and the variance at every specific value of x. It makes sense to ask: if we aggregate the bias and the variance across all x values, can we get a single number that quantifies the overall bias and variance of the model? In mathematical terms, we can take the expected value of the squared error across all x values:\n\\begin{align*}\n\\mathbb{E}_x\\left[\\mathbb{E}_D[(\\hat{f}(x) - y)^2\\mid x]\\right] &= \\mathbb{E}_x\\left[\\text{Var}\\left[\\hat{f}(x)\\right]\\right] \\\\\n&+ \\mathbb{E}_x\\left[\\text{Bias}^2\\left[\\hat{f}(x)\\right]\\right]\\\\\n&+ \\sigma^2\n\\end{align*}\nOnce more the noise term gets out of the expectation operator because it is a constant, not a random variable. The actual calculation that we execute with \\mathbb{E}_x requires us to specify a distribution over x values. See three possible choices:\n\nWe can assume that x is uniformly distributed in the interval [0, 1]. This means that we are equally interested in all x values between 0 and 1, and we want to give them equal weight when calculating the overall bias and variance. In our concrete example, this would mean that all levels of education between 0 and 1 are equally important to us, and we want to give them equal weight when calculating the overall bias and variance of our model. This is a common choice in theoretical analyses, but it may not always be the most realistic one.\nWe can assume that x is distributed according to the distribution of the data points that we actually observed in the dataset. The “empirical distribution” of the data points gives more weight to x values that are more common in the observed data, and less weight to x values that are less common. For instance, our dataset may have more people with an education level lower than 0.2 than people with an education level higher than 0.8, so it wouldn’t make sense to give them equal weight when calculating the overall bias and variance.\nWe might know the true distribution of x in the population, and use it to calculate the overall bias and variance. In our example, we might have access to census data that tells us the distribution of education levels in the population, and we can use this information to calculate the overall bias and variance of our model.\n\nFor simplicity’s sake, let’s assume that x is uniformly distributed in the interval [0, 1]. See how the overall bias and variance change as we increase the degree of the polynomial.\n\n\ncompute overall bias and variance\np_order = np.arange(1, 9)\nbias2 = np.empty(len(p_order))\nvar = np.empty(len(p_order))\nfor i, deg in enumerate(p_order):\n    bias2[i] = np.mean((f(x_grid) - mean_pred[:, deg-1])**2)\n    var[i] = np.mean(std_pred[:, deg-1]**2)\n\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(p_order, bias2, label=\"Bias²\", marker='o', color=gold)\nax.plot(p_order, var, label=\"Variance\", marker='o', color=pink)\nax.set(xlabel=\"Polynomial Degree\",\n       ylabel=\"Error\",\n       xticks=p_order,\n       title=\"Bias-Variance Tradeoff\",\n       ylim=(0, 2));\nax.legend(loc='upper left', frameon=False);\n\n\n\n\n\n\n\n\n\nIn broad terms, and for fixed data size and noise level, increasing the degree of the polynomial tends to decrease the bias and increase the variance. This is what we call the bias-variance tradeoff. A model with low bias and low variance would be ideal, but with finite data these two goals are typically in tension. Increasing model complexity can reduce bias, but often at the cost of increased variance; decreasing complexity has the opposite effect.\nThe curves above are quite jagged, and we do not observe a smooth monotonic decrease in bias or increase in variance. This is partly because we are estimating bias and variance from a finite number of repeated experiments. To reduce this Monte Carlo noise and make the underlying trends clearer, we repeat the analysis below using 5000 experiments instead of 200, and sampling 50 data points per experiment instead of 20.\n\n\ncompute overall bias and variance for all degrees from 1 to 15\n# setup\nnp.random.seed(0)\nN = 100\nx_grid = np.linspace(0, 1, N)\nf = lambda x: np.sin(4*np.pi*x) + 3*x\n\nsigma = 0.3\nM = 5000\nn_train = 50\ndegrees = np.arange(1, 13)\n\n# pre-generate randomness once (shared across all degrees)\nnoise_mat = np.random.normal(0, sigma, size=(M, N))\nidx_mat = np.array([np.random.choice(N, size=n_train, replace=False) for _ in range(M)])\n\nbias2_list = []\nvar_list = []\n\nfor deg in degrees:\n    preds = np.empty((M, N))\n\n    for m in range(M):\n        # generate dataset m (same x_grid, new noise)\n        y = f(x_grid) + noise_mat[m]\n\n        # observe subset for training\n        idx = idx_mat[m]\n\n        # fit with a numerically stable polynomial basis (chebyshev)\n        p = Chebyshev.fit(x_grid[idx], y[idx], deg, domain=[0, 1])\n\n        # evaluate on the common grid\n        preds[m] = p(x_grid)\n\n    mean_pred = preds.mean(axis=0)\n    std_pred = preds.std(axis=0)\n\n    # bias^2 and variance aggregated over x\n    bias2_list.append(np.mean((mean_pred - f(x_grid))**2))\n    var_list.append(np.mean(std_pred**2))\n\nbias2_arr = np.array(bias2_list)\nvar_arr = np.array(var_list)\n\n\n\n\nplot tradeoff\nfig, ax = plt.subplots()\nax.plot(degrees, bias2_arr + var_arr, lw=5, alpha=0.3, label=\"bias² + variance\", color='black')\nax.plot(degrees, bias2_arr, marker=\"o\", label=\"bias²\", color=gold)\nax.plot(degrees, var_arr, marker=\"o\", label=\"variance\", color=pink)\nax.set(xlabel=\"polynomial degree\", ylabel=\"error\", title=\"bias-variance tradeoff\")\nax.set_yscale(\"log\")\nax.legend(frameon=False);\n\n\n\n\n\n\n\n\n\nIf our aim is to minimize the expected error, we should choose a polynomial degree that minimizes the sum of bias and variance. In the plot above, the sweet spot of the bias-variance tradeoff for this specific problem is around a degree of 7.",
    "crumbs": [
      "generalization and model complexity",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "generalization/bias-variance-tradeoff.html#nerding-out",
    "href": "generalization/bias-variance-tradeoff.html#nerding-out",
    "title": "46  bias-variance tradeoff",
    "section": "46.4 nerding out",
    "text": "46.4 nerding out\nThe bias–variance–noise decomposition we derived is\n\n\\mathbb{E}_D[(\\hat{f}(x) - y)^2\\mid x] = \\text{Var}\\left[\\hat{f}(x)\\right] + \\text{Bias}^2\\left[\\hat{f}(x)\\right] + \\sigma^2.\n\nThis equation is so similar to the Pythagorean theorem, that we might wonder if there’s anything in common between them. The answer is yes! In a way, the bias–variance–noise decomposition is a stochastic analogue of the sum-of-squares decomposition in linear regression, with orthogonality defined by expectation rather than by Euclidean inner products over data points. Let’s see how this works.\nLet’s call \\omega a specific experimental realization (e.g. 20 pairs of x,y data poins as well as a test point). \\omega is randomly drawn from the distribution of all possible experimental realizations, which we denote by \\Omega. For a fixed input x, we can associate to each of these experimental realizations \\omega a random variable, for example:\n\n\\epsilon(\\omega). Each experimental realization \\omega has its own noise value \\epsilon, and therefore we can think of \\epsilon as a random variable.\n\\hat{f}(x)(\\omega). Each experimental realization \\omega has its own fitted function \\hat{f}(x), and therefore we can think of \\hat{f}(x) as a random variable. Note: \\hat{f}(x) is calculated for a fixed value of x, so it is a real number.\n\\left(\\hat{f}(x) - f(x)\\right)(\\omega). Each experimental realization \\omega has its own value of \\hat{f}(x) - f(x), and therefore we can think of \\hat{f}(x) - f(x) as a random variable. Again, this is calculated for a fixed value of x, so it is a real number.\nYou get the idea.\n\nIt’s useful to see a figure from before to grasp these ideas. Below, you see 200 \\omega experimental realizations, each with its own fitted function \\hat{f}(x) in blue, plus a test point in pink. At the specific value of x marked by the vertical dotted line, we can compute all sorts of random variables. Each of these random variables is a list of 200 real numbers, one for each experimental realization \\omega. We can imagine that each random variable is a vector in a 200-dimensional space, where each dimension corresponds to a specific experimental realization \\omega.\n\n\na thin slice\nfig, ax = plt.subplots(figsize=(8, 5))\ntest_point = [0.6, f(0.6)+0.8]\nax.plot(*test_point, ls=\"None\", marker=\"o\", markersize=5, mec=pink, mfc=pink)\nax.plot(x_grid, f(x_grid), color=\"black\", label=r\"true function, $f(x)$\")\nax.annotate(\n    \"\",\n    xy=(0.6, f(0.6)),\n    xytext=tuple(test_point),\n    arrowprops=dict(arrowstyle=\"&lt;-&gt;\", color=pink, lw=1.5, connectionstyle=\"bar,fraction=-0.5\",\n                    shrinkA=5, shrinkB=5),\n)\nax.text(test_point[0]+0.010, test_point[1], r\"$y=f(x)+\\epsilon$\", color=pink, fontsize=14, ha=\"left\", va=\"center\")\nax.plot([], [], color=blue, label=r\"fitted functions, $\\hat{f}_m(x)$\")\nax.legend(loc='upper left', frameon=False)\nfor m in range(M):\n    ax.plot(x_grid, preds[m, :], color=blue, alpha=0.05)\nax.axvline(x=0.6, color=gold, linestyle=\":\")\nax.set(xlim=(0.54, 0.66),\n       ylim=(-1, 4),\n       xlabel=\"x\",);\nax.set_ylabel(\"y\", rotation=0);\n\n\n\n\n\n\n\n\n\nWhen we talked about the bias-variance tradeoff, we ran a simulation where we increased the number of experiments to 5000. There, the random variables were vectors that “lived” in a 5000-dimensional space, and we had 5000 real numbers for each random variable, one for each experimental realization \\omega.\nThe idea now is that we can imagine taking an infinite number of experimental realizations \\omega from \\Omega, and for each of them we can compute the random variables mentioned above. Now, the random variables are vectors in an infinite-dimensional space.\nThis infinite dimensional space is a Hilbert space. A Hilbert space is a complete inner product space. Let’s unpack this definition, from the last word to the first:\n\nspace: a set of objects (in our case, random variables) that we can add together and multiply by scalars, and that satisfy the usual vector space properties (e.g. associativity, commutativity, distributivity, etc.).\ninner product: a function that takes two objects from the space and returns a real number, that satisfies certain properties (e.g. positivity, linearity, symmetry, etc.). In our case, the inner product is defined as the expected value of the product of two (square-integrable) random variables: \n\\langle A, B \\rangle = \\mathbb{E}[A \\cdot B].\n From the inner product, we also get the notion of a norm, which is defined as the square root of the inner product of a random variable with itself: \n  \\|A\\| = \\sqrt{\\langle A, A \\rangle} = \\sqrt{\\mathbb{E}[A^2]}.\n The norm gives us a way to measure the “length” of a random variable, and the inner product gives us a way to measure the “angle” between two random variables. In particular, if the inner product of two random variables is zero, we say that they are orthogonal. Finally, the qualifier “square-integrable” means that a random variable A has finite norm, that is, \\|A\\| &lt; \\infty (equivalently, \\mathbb{E}[A^2] &lt; \\infty).\ncomplete: informally, a space is complete if it contains all the limits of sequences formed from its own elements. In our setting, this means that if a sequence of random variables becomes closer and closer in average squared distance, then their limiting random variable also belongs to the space. Completeness ensures that no “gaps” exist: limits of meaningful constructions do not fall outside the space.\n\nSee how the whole derivation of the bias-variance-noise decomposition can be understood as operations on vectors in a Hilbert space, where the inner product is defined by expectation. The point-wise error at x was defined as: The point-wise error at x was defined as:\n\n\\text{error} = \\hat{f} - y = \\hat{f} - f - \\epsilon.\n\nWe will do the same trick of adding and subtracting \\mu(x), but first let’s interpret what it means:\n\n\\mu(x) = \\mathbb{E}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)\\cdot 1] = \\langle \\hat{f}(x), 1 \\rangle.\n\nThe mean predictor can be understood as the projection of the random variable \\hat{f}(x) onto the constant random variable 1. Attention: \\mu is a constant scalar, not a random variable, and we will treat it as such in what follows.\nNow we can rewrite the error as:\n\\begin{align*}\n\\text{error} &= \\underbrace{\\left(\\hat{f} - \\mu\\right)}_{\\text{variance vector}} + \\underbrace{\\left(\\mu - f\\right)}_{\\text{bias vector}} + \\underbrace{\\epsilon}_{\\text{irreducible noise vector}}\\\\\n&\\text{or simply...}\\\\\n\\text{error} &= \\vec{V} +\\vec{B} +\\vec{\\epsilon}\n\\end{align*}\nWe wish the best possible model, and that is one that minimizes the length of the error vector. The squared length of the error vector is given by its squared norm:\n\\begin{align*}\n\\|\\text{error}\\|^2 &= \\mathbb{E}\\left[\\left(\\vec{V} +\\vec{B} +\\vec{\\epsilon}\\right)^2\\right] \\\\\n&= \\mathbb{E}\\left[\\left(\\vec{V} +\\vec{B} +\\vec{\\epsilon}\\right)\\cdot\\left(\\vec{V} +\\vec{B} +\\vec{\\epsilon}\\right)\\right] \\\\\n&= \\mathbb{E}\\left[\\vec{V}^2 +\\vec{B}^2 +\\vec{\\epsilon}^2 + 2\\vec{V}\\cdot\\vec{B} + 2\\vec{V}\\cdot\\vec{\\epsilon} + 2\\vec{B}\\cdot\\vec{\\epsilon}\\right] \\\\\n&= \\mathbb{E}\\left[\\vec{V}^2 + \\vec{B}^2 + \\vec{\\epsilon}^2\\right] + 2\\mathbb{E}\\left[\\vec{V}\\cdot\\vec{B} + \\vec{V}\\cdot\\vec{\\epsilon} + \\vec{B}\\cdot\\vec{\\epsilon}\\right]\n\\end{align*} The last line has two terms. The first term is the sum of the squared lengths of the variance, bias, and noise vectors. This is our end result. All we need to show now is that all the mixed terms are zero, that is, that the vectors for bias, variance, and noise are mutually orthogonal.\n\n\\vec{V}\\cdot\\vec{B} = 0: The inner product of these two vectors is: \n  \\langle \\vec{V}, \\vec{B} \\rangle = \\mathbb{E}\\left[(\\hat{f} - \\mu)(\\mu - f)\\right].\n   Since both \\mu and f are constants, we can take them out of the expectation operator, and we get: \n  \\langle \\vec{V}, \\vec{B} \\rangle = \\mathbb{E}\\left[\\hat{f}(\\mu - f)\\right] - \\mu(\\mu - f) = \\mu(\\mu - f) - \\mu(\\mu - f) = 0.\n  \n\\vec{V}\\cdot\\vec{\\epsilon} = 0: The inner product of these two vectors is: \n  \\langle \\vec{V}, \\vec{\\epsilon} \\rangle = \\mathbb{E}\\left[(\\hat{f} - \\mu)\\epsilon\\right].\n   Since \\epsilon is independent of \\hat{f} and has zero mean, we have: \n  \\langle \\vec{V}, \\vec{\\epsilon} \\rangle = \\mathbb{E}\\left[\\hat{f}\\epsilon\\right] - \\mu\\mathbb{E}\\left[\\epsilon\\right] = 0 - \\mu\\cdot 0 = 0.\n  \n\\vec{B}\\cdot\\vec{\\epsilon} = 0: The inner product of these two vectors is: \n  \\langle \\vec{B}, \\vec{\\epsilon} \\rangle = \\mathbb{E}\\left[(\\mu - f)\\epsilon\\right].\n   Since \\epsilon is independent of \\mu and f, and has zero mean, we have: \n  \\langle \\vec{B}, \\vec{\\epsilon} \\rangle = (\\mu - f)\\mathbb{E}\\left[\\epsilon\\right] = (\\mu - f)\\cdot 0 = 0.\n  \n\nThis is it, now we have\n\n\\|\\text{error}\\|^2 = \\mathbb{E}\\left[\\vec{V}^2 + \\vec{B}^2 + \\vec{\\epsilon}^2\\right] = \\text{Var}\\left[\\hat{f}(x)\\right] + \\text{Bias}^2\\left[\\hat{f}(x)\\right] + \\sigma^2.\n\nThe image that I have in my head, and that shows that the result above is a manifestation of the Pythagorean theorem, is this:\n\n\nerror vector in 3d\n# create figure and 3d axis\nfig = plt.figure(figsize=(6, 4), tight_layout=True)\nax = fig.add_subplot(111, projection='3d')\n\nx0 = 1.5; y0 = 1; z0 = 0.8\n# define vectors\nx1 = np.array([x0, 0, 0])\nx2 = np.array([0, y0, 0])\nx3 = np.array([0, 0, z0])\n\n# plot coordinate axes\nax.quiver(0, 0, 0, *x1, color='black', linewidth=2, arrow_length_ratio=0.1/x0)\nax.quiver(0, 0, 0, *x2, color='black', linewidth=2, arrow_length_ratio=0.1)\nax.quiver(0, 0, 0, *x3, color='black', linewidth=2, arrow_length_ratio=0.1/z0)\n\n# label axes\nax.text(x0 + 0.05, 0, 0, r'$\\vec{V}$', fontsize=12)\nax.text(0, y0 + 0.05, 0, r'$\\vec{B}$', fontsize=12)\nax.text(0, 0, z0 + 0.05, r'$\\vec{\\epsilon}$', fontsize=12)\n\n# define cube corners\n\ncorners = np.array([\n    [0, 0, 0],\n    [x0, 0, 0],\n    [x0, y0, 0],\n    [0, y0, 0],\n    [0, 0, z0],\n    [x0, 0, z0],\n    [x0, y0, z0],\n    [0, y0, z0],\n])\n\n# cube edges\nedges = [\n    (0, 1), (1, 2), (2, 3), (3, 0),\n    (4, 5), (5, 6), (6, 7), (7, 4),\n    (0, 4), (1, 5), (2, 6), (3, 7),\n]\n\n# draw cube\nfor i, j in edges:\n    ax.plot(*zip(corners[i], corners[j]), color='black', linewidth=1)\n\n# diagonal vector\ndiag = x1 + x2 + x3\ndiag_length = np.linalg.norm(diag)\nax.quiver(0, 0, 0, *diag, color='red', linewidth=3, arrow_length_ratio=0.1/diag_length)\n\n# label diagonal\nax.text(*(diag + 0.05), r'error$=\\vec{V} + \\vec{B} + \\vec{\\epsilon}$', color='red', fontsize=12, ha=\"right\")\n\n# limits and aspect\nax.set_xlim(0, 1.2)\nax.set_ylim(0, 1.2)\nax.set_zlim(0, 1.2)\nax.set_box_aspect([1, 1, 1])\n\n# hide axes completely\nax.set_axis_off()\n\n# lower viewing angle\nax.view_init(elev=10, azim=-80)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs usual in Math, we first had to think of our entities in a more abstract (and maybe less intuitive) way, so we could get a short a beautiful derivation of the bias-variance-noise decomposition.",
    "crumbs": [
      "generalization and model complexity",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>bias-variance tradeoff</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html",
    "href": "misc/trend_test.html",
    "title": "53  trend test",
    "section": "",
    "text": "53.1 linear regression\nHow to determine if there is a trend (positive or negative) between two variables?\nOne of the simplest ways to determine if there is a trend between two variables is to use linear regression.\nfit linear model and plot\nslope, intercept, _, _, _ = linregress(x, y)\ny_hat = slope * x + intercept\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data')\nax.plot(x, y_hat, color='red', label=f'model: y = {slope:.2f}x + {intercept:.2f}')\nax.set(xlabel='X')\nax.set_ylabel('Y', rotation=0, labelpad=20)\nax.legend()\nGreat, we found that there is a positive slope, its value is 0.13. It this enough to say that there is a trend?\nWe need to determine if the slope is significantly different from zero. For that we can use a t-test.\nNull Hypothesis: The slope is equal to zero (no trend).\nAlternative Hypothesis: The slope is not equal to zero (there is a trend).\nThe t-statistic is calculated as:\nt = \\frac{\\text{slope} - 0}{SE_\\text{slope}},\nwhere SE_\\text{slope} is the standard error of the slope, and it is given by:\nSE_\\text{slope} = \\frac{SD_\\text{residuals}}{\\sqrt{\\sum (x_i - \\bar{x})^2}}.\nwhere SD_\\text{residuals} is the standard deviation of the residuals, x_i are the individual x values, and \\bar{x} is the mean of the x values.\nThe standard deviation of the residuals is calculated as:\nSD_\\text{residuals} = \\sqrt{\\frac{\\sum (y_i - \\hat{y}_i)^2}{n - 2}},\nwhere y_i are the observed y values, \\hat{y}_i are the predicted y values from the regression, and n is the number of data points. The number of the degrees of freedom is n - 2 because we are estimating two parameters (the slope and the intercept) from the data.\nLet’s compute SE_\\text{slope}, the t-statistic, and the p-value for our example.\nmanual computation vs scipy\n# calculate the residuals\nresiduals = y - y_hat\n# calculate the sum of squared residuals (SSR)\nsum_squared_residuals = np.sum(residuals**2)\n# calculate the Residual Standard Error (s_e)\nn = len(x)\ndegrees_of_freedom = n - 2\nresidual_std_error = np.sqrt(sum_squared_residuals / degrees_of_freedom)\n# calculate the sum of squared deviations of x from its mean\nx_mean = np.mean(x)\nsum_squared_x_deviations = np.sum((x - x_mean)**2)\n# put it all together to get SE_slope\n# SE_slope = (typical error) / (spread of x)\nSE_slope = residual_std_error / np.sqrt(sum_squared_x_deviations)\n# verify the result against the value directly from scipy\nscipy_slope, scipy_intercept, scipy_r, scipy_p, scipy_se = linregress(x, y)\nprint(f\"manually calculated SE_slope:          {SE_slope:.6f}\")\nprint(f\"SE_slope from scipy.stats.linregress:  {scipy_se:.6f}\")\n\n\nmanually calculated SE_slope:          0.057695\nSE_slope from scipy.stats.linregress:  0.057695\ncalculate p-value manually\nt_statistic = (slope-0) / SE_slope\np_value = 2 * (1 - scipy.stats.t.cdf(np.abs(t_statistic), df=degrees_of_freedom))\nprint(f\"manually calculated p-value: {p_value:.6f}\")\nprint(f'scipy p-value:               {scipy_p:.6f}')\n\n\nmanually calculated p-value: 0.028344\nscipy p-value:               0.028344\nIf we choose a significance level \\alpha=0.05, the p-value we found indicates that we can reject the null hypothesis and conclude that there is a significant trend between x and y.\nIf instead of testing if the slope is different from zero, but rather if it is greater than zero (i.e., a one-sided test), we would divide the p-value by 2.\none-sided p-value\np_value = (1 - scipy.stats.t.cdf(np.abs(t_statistic), df=degrees_of_freedom))\nscipy_slope, scipy_intercept, scipy_r, scipy_p, scipy_se = linregress(x, y, alternative='greater')\nprint(f\"manually calculated p-value: {p_value:.6f}\")\nprint(f'scipy p-value:               {scipy_p:.6f}')\n\n\nmanually calculated p-value: 0.014172\nscipy p-value:               0.014172\nOne last remark. What does the formula for the standard error of the slope mean?",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#linear-regression",
    "href": "misc/trend_test.html#linear-regression",
    "title": "53  trend test",
    "section": "",
    "text": "inside the square root, we have a quantity dependent on y squared divided by a quantity dependent on x squared. Dimensionally this makes sense, because the standard error of the slope should have the same dimension as the slope \\Delta y/\\Delta x.\nthe larger the variability of the residuals (i.e., the more scattered the data points are around the regression line), the larger the standard error of the slope, and thus the less precise our estimate of the slope is.\nWe can manipulate the formula a little bit to get more intuition: \n  SE_\\text{slope} = \\sqrt{\\frac{1}{n-2}}\\frac{SD_y}{SD_x}\\sqrt{1-r^2},\n where SD_y and SD_x are the standard deviations of y and x, respectively, and r is the correlation coefficient between x and y. From this formula we can see that:\n\nthe standard error of the slope decreases with increasing sample size n (more data points lead to a more precise estimate of the slope);\nimagine all the data points in a rectangular box, and all the possible slopes that can be drawn within that box. If you change the dimensions of the box, you need to account for that, and that is the second term.\nThe last term acounts for the spread of the points about the line.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#mann-kendall-trend-test",
    "href": "misc/trend_test.html#mann-kendall-trend-test",
    "title": "53  trend test",
    "section": "53.2 Mann-Kendall Trend Test",
    "text": "53.2 Mann-Kendall Trend Test\nThe method above assumed that the relationship between x and y is linear, and that the residuals are normally distributed. If these assumptions are not met, we can use a non-parametric test like the Mann-Kendall trend test. The intuition behind this test works like a voting system. You go through your data and compare every data point to all points that come after it.\n\nif a later points is higher, you give a +1 vote\nif a later point is lower, you give a -1 vote\nif they are equal, you give a 0 vote\n\nAll these votes are summed up. A large positive sum indicates an increasing trend, a large negative sum indicates a decreasing trend, and a sum close to zero indicates no trend. We can then calculate a test statistic Z based on the sum of votes, and use it to determine the p-value. If the p-value is less than our chosen significance level (e.g., 0.05), we can reject the null hypothesis of no trend. We can use the package pymannkendall to perform the Mann-Kendall trend test.\n\n\napplying the Mann-Kendall test\nfrom pymannkendall import original_test\nmk_result = original_test(y)\nprint(mk_result)\n\n\nMann_Kendall_Test(trend='increasing', h=True, p=0.04970274086760851, z=1.9625134103851736, Tau=0.25517241379310346, s=111.0, var_s=3141.6666666666665, slope=0.14036402565970577, intercept=11.836643578083883)\n\n\nThe test concluded that there is an increasing trend, with a p-value of 0.0497.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#spearmans-rank-correlation",
    "href": "misc/trend_test.html#spearmans-rank-correlation",
    "title": "53  trend test",
    "section": "53.3 Spearman’s Rank Correlation",
    "text": "53.3 Spearman’s Rank Correlation\nThis is another non-parametric test. It assesses how well the relationship between two variables can be described using a monotonic function. It does this by converting the data to ranks and then calculating the Pearson correlation coefficient on the ranks. The Spearman’s rank correlation coefficient, denoted by \\rho (rho), ranges from -1 to 1, where:\n\n1 indicates a perfect positive monotonic relationship,\n-1 indicates a perfect negative monotonic relationship,\n0 indicates no monotonic relationship.\n\nThis test is robust to outliers and does not assume a linear relationship between the variables.\nWe can use the scipy.stats.spearmanr function to calculate Spearman’s rank correlation coefficient and the associated p-value.\n\n\napplying the Spearman’s Rank Correlation test\nspearman_corr, spearman_p = scipy.stats.spearmanr(x, y)\nprint(f\"Spearman's correlation: {spearman_corr:.6f}, p-value: {spearman_p:.6f}\")\n\n\nSpearman's correlation: 0.361958, p-value: 0.049356\n\n\nWe found that there is a positive monotonic relationship between x and y, with a p-value of 0.0494, indicating that the relationship is statistically significant at the 0.05 significance level.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#theil-sen-estimator",
    "href": "misc/trend_test.html#theil-sen-estimator",
    "title": "53  trend test",
    "section": "53.4 Theil-Sen Estimator",
    "text": "53.4 Theil-Sen Estimator\nThe Theil-Sen estimator is a robust method for estimating the slope of a linear trend. It is particularly useful when the data contains outliers or is not normally distributed. The Theil-Sen estimator calculates the slope as the median of all possible pairwise slopes between data points. We can use the scipy.stats.theilslopes function to calculate the Theil-Sen estimator and the associated confidence intervals.\n\n\napplying the Theil-Sen Estimator\nfrom scipy.stats import theilslopes\ntheil_slope, theil_intercept, theil_lower, theil_upper = theilslopes(y, x, 0.95)\nprint(f\"Theil-Sen slope: {theil_slope:.6f}, intercept: {theil_intercept:.6f}\")\n\n\nTheil-Sen slope: 0.140364, intercept: 11.836644",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>trend test</span>"
    ]
  }
]