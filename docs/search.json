[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Machine Learning",
    "section": "",
    "text": "home\nI’m teaching myself statistics and machine learning, and the best way to truly understand is to use the new tools I’ve acquired. This is what this website is for. It is mainly a reference guide for my future self.",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Statistics and Machine Learning",
    "section": "books",
    "text": "books\nThese are the books that I’ve read and recommend.\n\nModern Statistics: Intuition, Math, Python, R\nby Mike X Cohen\n\nGithub\nThis is a really approachable book, the author has a very nice conversational style, and I enjoyed it a lot. Highly recommended\n\n\n\nData-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control\nby Steven L. Brunton, J. Nathan Kutz\n\nThe whole book is available in this website.\nThis is the sort of books that is suitable for those who already know the subject. I would not recommend it as a first read. In any case, some chapters gave me new intuition on the subject. I do highly recommend Steve Brunton’s youtube channel, it’s fantastic.\n\n\n\nNeural Networks and Deep Learning\nby Michael Nielsen\n\nThis is an online book, freely available here. It can be tiring to read a whole book on a computer screen, so you can find Anton Vladyka’s LaTeX rendition of this book in his GitHub repository. I wanted to read the pdf in my tiny kindle reader, so I recompiled Anton’s LaTeX code to make it fit the screen, and on the way changed the font, and corrected typos here and there. Overleaf project. Download pdf.\nNielsen writes very well, I really enjoyed this book. The part on backprogation is a bit confusing, I would recommend watching 3b1b’s youtube video on that.\n\n\n\nIntroduction to Environmental Data Science\nby William W. Hsieh\n\nThis book’s best quality is that it covers a bunch of topics, methods, techniques. It is not a good book to learn concepts for the first time, it’s more useful as a menu of what exists, and maybe a brief reminder of topics you studied in the past but forgot. The “environmental” aspect is completely incidental, in my opinion. Hsieh brings examples from the Environment, but you don’t need to have a background in environmental science to be able to read it.",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "index.html#websites",
    "href": "index.html#websites",
    "title": "Statistics and Machine Learning",
    "section": "websites",
    "text": "websites\n\nDr. Roi Yehoshua’s tutorials\nReally good tutorials, you should check this out:\nhttps://towardsdatascience.com/author/roiyeho/\nIt seems the he wrote a book, I haven’t read it, but should be good:\nMachine Learning Foundations, Volume 1: Supervised Learning",
    "crumbs": [
      "home"
    ]
  },
  {
    "objectID": "data/height.html",
    "href": "data/height.html",
    "title": "1  height data",
    "section": "",
    "text": "I found growth curves for girls and boys in Israel:\n\nurl girls, pdf girls\nurl boys, pdf boys\nurl both, png boys, png girls.\n\nFor example, see this:\n\nI used the great online resource Web Plot Digitizer v4 to extract the data from the images files. I captured all the growth curves as best as I could. The first step now is to get interpolated versions of the digitized data. For instance, see below the 50th percentile for boys:\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.optimize import curve_fit\nfrom scipy.special import erf\nfrom scipy.interpolate import UnivariateSpline\nimport matplotlib.animation as animation\nfrom scipy.stats import norm\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n# %matplotlib widget\n\n\n\n\ndefine useful arrays\nage_list = np.round(np.arange(2.0, 20.1, 0.1), 1)\nheight_list = np.round(np.arange(70, 220, 0.1), 1)\n\n\n\n\nimport sample data, boys 50th percentile\ndf_temp_boys_50th = pd.read_csv('../archive/data/height/boys-p50.csv', names=['age','height'])\nspline = UnivariateSpline(df_temp_boys_50th['age'], df_temp_boys_50th['height'], s=0.5)\ninterpolated = spline(age_list)\n\n\n\n\nplot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(df_temp_boys_50th['age'], df_temp_boys_50th['height'], label='digitized data',\n        marker='o', markerfacecolor='None', markeredgecolor=\"black\", markersize=6, linestyle='None')\nax.plot(age_list, interpolated, label='interpolated', color=\"black\", linewidth=2)\nax.set(xlabel='age (years)',\n       ylabel='height (cm)',\n       xticks=np.arange(2, 21, 2),\n       title=\"boys, 50th percentile\"\n       )\nax.legend(frameon=False);\n\n\n\n\n\n\n\n\n\nLet’s do the same for all the other curves, and then save them to a file.\n\n\ninterpolate all growth curves\ncol_names = ['p05', 'p10', 'p25', 'p50', 'p75', 'p90', 'p95']\nfile_names_boys = ['boys-p05.csv', 'boys-p10.csv', 'boys-p25.csv', 'boys-p50.csv',\n                   'boys-p75.csv', 'boys-p90.csv', 'boys-p95.csv',]\nfile_names_girls = ['girls-p05.csv', 'girls-p10.csv', 'girls-p25.csv', 'girls-p50.csv',\n                   'girls-p75.csv', 'girls-p90.csv', 'girls-p95.csv',]\n\n# create dataframe with age column\ndf_boys = pd.DataFrame({'age': age_list})\ndf_girls = pd.DataFrame({'age': age_list})\n# loop over file names and read in data\nfor i, file_name in enumerate(file_names_boys):\n    # read in data\n    df_temp = pd.read_csv('../archive/data/height/' + file_name, names=['age','height'])\n    spline = UnivariateSpline(df_temp['age'], df_temp['height'], s=0.5)\n    df_boys[col_names[i]] = spline(age_list)\nfor i, file_name in enumerate(file_names_girls):\n    # read in data\n    df_temp = pd.read_csv('../archive/data/height/' + file_name, names=['age','height'])\n    spline = UnivariateSpline(df_temp['age'], df_temp['height'], s=0.5)\n    df_girls[col_names[i]] = spline(age_list)\n\n# make age index\ndf_boys.set_index('age', inplace=True)\ndf_boys.index = df_boys.index.round(1)\ndf_boys.to_csv('../archive/data/height/boys_height_vs_age_combined.csv', index=True)\ndf_girls.set_index('age', inplace=True)\ndf_girls.index = df_girls.index.round(1)\ndf_girls.to_csv('../archive/data/height/girls_height_vs_age_combined.csv', index=True)\n\n\nLet’s take a look at what we just did.\n\ndf_girls\n\n\n\n\n\n\n\n\n\np05\np10\np25\np50\np75\np90\np95\n\n\nage\n\n\n\n\n\n\n\n\n\n\n\n2.0\n79.269087\n80.794167\n83.049251\n85.155597\n87.475854\n89.779822\n90.882059\n\n\n2.1\n80.202106\n81.772053\n84.052858\n86.207778\n88.713405\n90.883740\n92.409913\n\n\n2.2\n81.130687\n82.706754\n85.011591\n87.211543\n89.856186\n91.940642\n93.416959\n\n\n2.3\n82.048325\n83.601023\n85.928399\n88.170313\n90.914093\n92.953965\n94.270653\n\n\n2.4\n82.948516\n84.457612\n86.806234\n89.087509\n91.897022\n93.927147\n95.226089\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19.6\n152.520938\n154.812286\n158.775277\n163.337149\n167.699533\n171.531349\n173.969235\n\n\n19.7\n152.534223\n154.814440\n158.791925\n163.310864\n167.704618\n171.519600\n173.980150\n\n\n19.8\n152.548001\n154.827666\n158.815071\n163.275852\n167.708562\n171.504730\n173.990964\n\n\n19.9\n152.562338\n154.853760\n158.845506\n163.231563\n167.711342\n171.486629\n174.001704\n\n\n20.0\n152.577300\n154.894521\n158.884019\n163.177444\n167.712936\n171.465189\n174.012396\n\n\n\n\n181 rows × 7 columns\n\n\n\n\n\n\nshow all interpolated curves for girls\nfig, ax = plt.subplots(figsize=(8, 6))\n# loop over col_names and plot each column\ncolors = sns.color_palette(\"Oranges\", len(col_names))\nfor col, color in zip(col_names, colors):\n    ax.plot(df_girls.index, df_girls[col], label=col, color=color)\nax.set(xlabel='age (years)',\n       ylabel='height (cm)',\n       xticks=np.arange(2, 21, 2),\n       title=\"growth curves for girls\\npercentile curves: 5, 10, 25, 50, 75, 90, 95\",\n       );\n\n\n\n\n\n\n\n\n\nLet’s now see the percentiles for girls age 20.\n\n\nplot cdf for girls, age 20\nfig, ax = plt.subplots(figsize=(8, 6))\npercentile_list = np.array([5, 10, 25, 50, 75, 90, 95])\ndata = df_girls.loc[20.0]\nax.plot(data, percentile_list, ls='', marker='o', markersize=6, color=\"black\")\nax.set(xlabel='height (cm)',\n         ylabel='percentile',\n         yticks=percentile_list,\n         title=\"cdf for girls, age 20\"\n         );\n\n\n\n\n\n\n\n\n\nI suspect that the heights in the population are normally distributed. Let’s check that. I’ll fit the data to the integral of a gaussian, because the percentiles correspond to a cdf. If a pdf is a gaussian, its cumulative is given by\n\n\\Phi(x) = \\frac{1}{2} \\left( 1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma \\sqrt{2}}\\right) \\right)\n\nwhere \\mu is the mean and \\sigma is the standard deviation of the distribution. The error function \\text{erf} is a sigmoid function, which is a good approximation for the cdf of the normal distribution.\n\n\ndefine functions\ndef erf_model(x, mu, sigma):\n    return 50 * (1 + erf((x - mu) / (sigma * np.sqrt(2))) )\n# initial guess for parameters: [mu, sigma]\np0 = [150, 6]\n# Calculate R-squared\ndef calculate_r2(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - (ss_res / ss_tot)\n\n\n\n\nfit model to data\ndata = df_girls.loc[20.0]\nparams, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                        bounds=([100, 3],   # lower bounds for mu and sigma\n                                [200, 10])  # upper bounds for mu and sigma\n                        )\n# store the parameters in the dataframe\npercentile_predicted = erf_model(data, *params)\n# R-squared value\nr2 = calculate_r2(percentile_list, percentile_predicted)\n\n\n\n\nshow results\nfig, ax = plt.subplots(figsize=(8, 6))\npercentile_list = np.array([5, 10, 25, 50, 75, 90, 95])\ndata = df_girls.loc[20.0]\nax.plot(data, percentile_list, ls='', marker='o', markersize=6, color=\"black\", label='data')\nfit = erf_model(height_list, *params)\nax.plot(height_list, fit, label='fit', color=\"red\", linewidth=2)\nax.text(150, 75, f'$\\mu$ = {params[0]:.1f} cm\\n$\\sigma$ = {params[1]:.1f} cm\\nR$^2$ = {r2:.6f}',\n        fontsize=14, bbox=dict(facecolor='white', alpha=0.5))\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       xlim=(140, 190),\n         ylabel='percentile',\n         yticks=percentile_list,\n         title=\"the data is very well fitted by a normal distribution\"\n         );\n\n\n\n\n\n\n\n\n\nAnother way of making sure that the model fits the data is to make a QQ plot. In this plot, the quantiles of the data are plotted against the quantiles of the normal distribution. If the data is normally distributed, the points should fall on a straight line.\n\n\nshow QQ plot\nfitted_quantiles = norm.cdf(data, loc=params[0], scale=params[1])\nexperimental_quantiles = percentile_list / 100\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_aspect('equal', adjustable='box')\nax.plot(experimental_quantiles, fitted_quantiles,\n        ls='', marker='o', markersize=6, color=\"black\",\n        label='qq points')\nax.plot([0, 1], [0, 1], color='red', linewidth=2, label=\"1:1 line\")\nax.set(xlabel='empirical quantiles',\n       ylabel='fitted quantiles',\n       xlim=(0, 1),\n       ylim=(0, 1),\n       title=\"QQ plot\")\nax.legend(frameon=False)\n\n\n\n\n\n\n\n\n\nGreat, now we just need to do exactly the same for both sexes, and all the ages. I chose to divide age from 2 to 20 into 0.1 intervals.\n\n\ncreate dataframes to store the parameters mu, sigma, r2\ndf_stats_boys = pd.DataFrame(index=age_list, columns=['mu', 'sigma', 'r2'])\ndf_stats_boys['mu'] = 0.0\ndf_stats_boys['sigma'] = 0.0\ndf_stats_boys['r2'] = 0.0\ndf_stats_girls = pd.DataFrame(index=age_list, columns=['mu', 'sigma', 'r2'])\ndf_stats_girls['mu'] = 0.0\ndf_stats_girls['sigma'] = 0.0\ndf_stats_girls['r2'] = 0.0\n\n\n\n\nfit model to all the data\np0 = [80, 3]\n# loop over ages in the index, calculate mu and sigma\nfor i in df_boys.index:\n    # fit the model to the data\n    data = df_boys.loc[i]\n    params, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                          bounds=([70, 2],   # lower bounds for mu and sigma\n                                  [200, 10])  # upper bounds for mu and sigma\n                         )\n    # store the parameters in the dataframe\n    df_stats_boys.at[i, 'mu'] = params[0]\n    df_stats_boys.at[i, 'sigma'] = params[1]\n    percentile_predicted = erf_model(data, *params)\n    # R-squared value\n    r2 = calculate_r2(percentile_list, percentile_predicted)\n    df_stats_boys.at[i, 'r2'] = r2\n    p0 = params\n# same for girls\np0 = [80, 3]\nfor i in df_girls.index:\n    # fit the model to the data\n    data = df_girls.loc[i]\n    params, _ = curve_fit(erf_model, data, percentile_list, p0=p0,\n                          bounds=([70, 3],   # lower bounds for mu and sigma\n                                  [200, 10])  # upper bounds for mu and sigma\n                         )\n    # store the parameters in the dataframe\n    df_stats_girls.at[i, 'mu'] = params[0]\n    df_stats_girls.at[i, 'sigma'] = params[1]\n    percentile_predicted = erf_model(data, *params)\n    # R-squared value\n    r2 = calculate_r2(percentile_list, percentile_predicted)\n    df_stats_girls.at[i, 'r2'] = r2\n    p0 = params\n\n# save the dataframes to csv files\ndf_stats_boys.to_csv('../archive/data/height/boys_height_stats.csv', index=True)\ndf_stats_girls.to_csv('../archive/data/height/girls_height_stats.csv', index=True)\n\n\nLet’s see what we got. The top panel in the graph shows the average height for boys and girls, the middle panel shows the coefficient of variation (\\sigma/\\mu), and the bottom panel shows the R2 of the fit (note that the range is very close to 1).\n\ndf_stats_boys\n\n\n\n\n\n\n\n\n\nmu\nsigma\nr2\n\n\n\n\n2.0\n86.463069\n3.563785\n0.999511\n\n\n2.1\n87.374895\n3.596583\n0.999676\n\n\n2.2\n88.269676\n3.627433\n0.999742\n\n\n2.3\n89.148086\n3.657263\n0.999752\n\n\n2.4\n90.010783\n3.686764\n0.999733\n\n\n...\n...\n...\n...\n\n\n19.6\n176.802810\n7.134561\n0.999991\n\n\n19.7\n176.845789\n7.135786\n0.999994\n\n\n19.8\n176.892196\n7.137430\n0.999995\n\n\n19.9\n176.942521\n7.139466\n0.999990\n\n\n20.0\n176.997255\n7.141858\n0.999976\n\n\n\n\n181 rows × 3 columns\n\n\n\n\n\n\nplot results\nfig, ax = plt.subplots(3,1, figsize=(8, 10), sharex=True)\nfig.subplots_adjust(left=0.15)\nax[0].plot(df_stats_boys['mu'], label='boys', lw=2)\nax[0].plot(df_stats_girls['mu'], label='girls', lw=2)\nax[0].legend(frameon=False)\n\nax[1].plot(df_stats_boys['sigma'] / df_stats_boys['mu'], lw=2)\nax[1].plot(df_stats_girls['sigma'] / df_stats_girls['mu'], lw=2)\n\nax[2].plot(df_stats_boys.index, df_stats_boys['r2'], label=r'$r2$ boys', lw=2)\nax[2].plot(df_stats_girls.index, df_stats_girls['r2'], label=r'$r2$ girls', lw=2)\n\nax[0].set(ylabel='average height (cm)',)\nax[1].set(ylabel='CV',\n          ylim=[0,0.055])\nax[2].set(xlabel='age (years)',\n            ylabel=r'$R^2$',\n            xticks=np.arange(2, 21, 2),\n          );\n\n\n\n\n\n\n\n\n\nLet’s see how the pdfs for boys and girls move and morph as age increases.\n\n\nproduce dataframes for pre-calculated pdfs\nage_list_string = age_list.astype(str).tolist()\ndf_pdf_boys = pd.DataFrame(index=height_list, columns=age_list_string)\ndf_pdf_girls = pd.DataFrame(index=height_list, columns=age_list_string)\n\nfor age in df_pdf_boys.columns:\n    age_float = round(float(age), 1)\n    df_pdf_boys[age] = norm.pdf(height_list,\n                                loc=df_stats_boys.loc[age_float]['mu'],\n                                scale=df_stats_boys.loc[age_float]['sigma'])\nfor age in df_pdf_girls.columns:\n    age_float = round(float(age), 1)\n    df_pdf_girls[age] = norm.pdf(height_list,\n                                loc=df_stats_girls.loc[age_float]['mu'],\n                                scale=df_stats_girls.loc[age_float]['sigma'])\n\n\n\ndf_pdf_girls\n\n\n\n\n\n\n\n\n\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n...\n19.1\n19.2\n19.3\n19.4\n19.5\n19.6\n19.7\n19.8\n19.9\n20.0\n\n\n\n\n70.0\n0.000006\n2.962419e-06\n1.229580e-06\n4.740717e-07\n1.893495e-07\n7.928033e-08\n3.395629e-08\n1.454961e-08\n6.214658e-09\n2.698367e-09\n...\n3.876760e-46\n4.998212e-46\n6.108274e-46\n6.965756e-46\n7.300518e-46\n6.928073e-46\n5.866310e-46\n4.367574e-46\n2.817087e-46\n1.550490e-46\n\n\n70.1\n0.000007\n3.369929e-06\n1.401926e-06\n5.423176e-07\n2.172465e-07\n9.118694e-08\n3.914667e-08\n1.681357e-08\n7.199311e-09\n3.133161e-09\n...\n4.821662e-46\n6.212999e-46\n7.589544e-46\n8.652519e-46\n9.067461e-46\n8.605908e-46\n7.289698e-46\n5.430839e-46\n3.506265e-46\n1.932327e-46\n\n\n70.2\n0.000008\n3.830459e-06\n1.597215e-06\n6.199308e-07\n2.490751e-07\n1.048086e-07\n4.509972e-08\n1.941687e-08\n8.334521e-09\n3.635676e-09\n...\n5.995467e-46\n7.721230e-46\n9.427830e-46\n1.074523e-45\n1.125944e-45\n1.068759e-45\n9.056344e-46\n6.751373e-46\n4.363019e-46\n2.407630e-46\n\n\n70.3\n0.000009\n4.350475e-06\n1.818328e-06\n7.081296e-07\n2.853621e-07\n1.203810e-07\n5.192270e-08\n2.240831e-08\n9.642428e-09\n4.216078e-09\n...\n7.453283e-46\n9.593350e-46\n1.170864e-45\n1.334099e-45\n1.397806e-45\n1.326973e-45\n1.124851e-45\n8.391039e-46\n5.427845e-46\n2.999137e-46\n\n\n70.4\n0.000010\n4.937172e-06\n2.068480e-06\n8.082806e-07\n3.267014e-07\n1.381707e-07\n5.973725e-08\n2.584341e-08\n1.114829e-08\n4.885994e-09\n...\n9.263403e-46\n1.191661e-45\n1.453785e-45\n1.655996e-45\n1.734906e-45\n1.647188e-45\n1.396806e-45\n1.042648e-45\n6.750965e-46\n3.735083e-46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n219.5\n0.000000\n5.214425e-307\n1.377605e-289\n3.568527e-277\n6.457994e-266\n2.232144e-255\n6.340272e-246\n9.969867e-238\n1.389324e-230\n5.441854e-224\n...\n5.200570e-18\n5.741874e-18\n6.194642e-18\n6.495054e-18\n6.583545e-18\n6.417949e-18\n5.986319e-18\n5.315101e-18\n4.468701e-18\n3.538724e-18\n\n\n219.6\n0.000000\n1.813597e-307\n5.050074e-290\n1.356408e-277\n2.537010e-266\n9.046507e-256\n2.642444e-246\n4.256155e-238\n6.055129e-231\n2.417510e-224\n...\n4.558798e-18\n5.035058e-18\n5.433557e-18\n5.698034e-18\n5.775970e-18\n5.630212e-18\n5.250299e-18\n4.659675e-18\n3.915265e-18\n3.097919e-18\n\n\n219.7\n0.000000\n6.302763e-308\n1.849870e-290\n5.151948e-278\n9.959447e-267\n3.663840e-256\n1.100546e-246\n1.815751e-238\n2.637298e-231\n1.073274e-224\n...\n3.995288e-18\n4.414220e-18\n4.764871e-18\n4.997654e-18\n5.066279e-18\n4.938013e-18\n4.603699e-18\n4.084117e-18\n3.429566e-18\n2.711382e-18\n\n\n219.8\n0.000000\n2.188653e-308\n6.771033e-291\n1.955386e-278\n3.906942e-267\n1.482823e-256\n4.580523e-247\n7.741154e-239\n1.147918e-231\n4.761829e-225\n...\n3.500614e-18\n3.869030e-18\n4.177503e-18\n4.382343e-18\n4.442754e-18\n4.329907e-18\n4.035791e-18\n3.578814e-18\n3.003413e-18\n2.372514e-18\n\n\n219.9\n0.000000\n7.594139e-309\n2.476504e-291\n7.416066e-279\n1.531537e-267\n5.997065e-257\n1.905138e-247\n3.298116e-239\n4.993198e-232\n2.111339e-225\n...\n3.066470e-18\n3.390384e-18\n3.661688e-18\n3.841895e-18\n3.895062e-18\n3.795805e-18\n3.537115e-18\n3.135297e-18\n2.629596e-18\n2.075507e-18\n\n\n\n\n1500 rows × 181 columns\n\n\n\n\n\n\nplotly widget\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.renderers.default = 'notebook'\n\n# create figure\nfig = go.Figure()\n\n# assume both dataframes have the same columns (ages) and index (height)\nages = df_pdf_boys.columns\nx_vals = df_pdf_boys.index\n\n# add traces: 2 per age (boys and girls), all hidden except the first pair\nfor i, age in enumerate(ages):\n    fig.add_trace(go.Scatter(x=x_vals, y=df_pdf_boys[age], name=f'Boys {age}', \n                             line=dict(color='#1f77b4'), visible=(i == 0)))\n    fig.add_trace(go.Scatter(x=x_vals, y=df_pdf_girls[age], name=f'Girls {age}', \n                             line=dict(color='#ff7f0e'), visible=(i == 0)))\n\n# create slider steps\nsteps = []\nfor i, age in enumerate(ages):\n    vis = [False] * (2 * len(ages))\n    vis[2*i] = True      # boys trace\n    vis[2*i + 1] = True  # girls trace\n\n    steps.append(dict(\n        method='update',\n        args=[{'visible': vis},\n              {'title': f'Height Distribution - Age: {age}'}],\n        label=str(age)\n    ))\n\n# define slider\nsliders = [dict(\n    active=0,\n    currentvalue={\"prefix\": \"Age: \"},\n    pad={\"t\": 50},\n    steps=steps\n)]\n\n# update layout\nfig.update_layout(\n    sliders=sliders,\n    title='Height Distribution by Age',\n    xaxis_title='Height (cm)',\n    yaxis_title='Density',\n    yaxis=dict(range=[0, 0.12]),\n    showlegend=True,\n    height=600,\n    width=800\n)\n\nfig.show()\n\n\n                                                \n\n\nA few notes about what we can learn from the analysis above.\n\nMy impression that 12-year-old girls are taller than boys is indeed true.\nBoys and girls have very similar distributions up to age 11.\nFrom age 11 to 13 girls are on average taller than boys.\nFrom age 13 boys become taller than girls, on average.\nThe graph showing the coefficient of variation is interesting. CV for girls peaks roughtly at age 12, and for boys it peaks around age 14. These local maxima may be explained by the wide variability in the age ofpuberty onset.\nThe height distribution for each sex, across all ages, is indeed extremely well described by the normal distribution. What biological factors may account for such a fact?\n\nI’ll plot one last graph from now, let’s see what we can learn from it. Let’s see the pdf for boys and girls across three age groups: 8, 12, and 15 year olds.\n\n\ncomparison across three ages\nfig, ax = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\nfig.subplots_adjust(hspace=0.1)\nages_to_plot = [8.0, 12.0, 15.0]\n\nfor i, age in enumerate(ages_to_plot):\n    pdf_boys = norm.pdf(height_list, loc=df_stats_boys.loc[age]['mu'], scale=df_stats_boys.loc[age]['sigma'])\n    pdf_girls = norm.pdf(height_list, loc=df_stats_girls.loc[age]['mu'], scale=df_stats_girls.loc[age]['sigma'])\n    ax[i].plot(height_list, pdf_boys, label='boys', color='tab:blue')\n    ax[i].plot(height_list, pdf_girls, label='girls', color='tab:orange')\n    ax[i].text(0.98, 0.98, f'age: {age} years', transform=ax[i].transAxes, verticalalignment='top', horizontalalignment='right')\n    ax[i].set(ylabel='pdf',\n              ylim=(0, 0.07),\n            )\nax[2].legend(frameon=False)\nax[2].set(xlabel='height (cm)',\n          xlim=(100, 200),);\n\n\n\n\n\n\n\n\n\n\nIndeed, boys and girls age 8 have the exact same height distribution.\n12-year-old girls are indeed taller than boys, on average. This difference is relatiely small, though.\nBy age 15 boys have long surpassed girls in height, and the difference is quite large. Boys still have some growing to do, but girls are mostly done growing.",
    "crumbs": [
      "data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>height data</span>"
    ]
  },
  {
    "objectID": "data/weight.html",
    "href": "data/weight.html",
    "title": "2  weight data",
    "section": "",
    "text": "2.1 example\nLet’s see the weight probability density for boys at age 10 and 15.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import powernorm\nload weight data and compute pdf\nweight_list = np.arange(10, 130, 0.1)\ndef power_normal_pdf(w, age, sex):\n    \"\"\"\n    Calculates the PDF of the Power Normal distribution from the derived formula.\n    This function correctly handles negative L values.\n    \"\"\"\n    # This function is only valid for w &gt; 0\n    w = np.asarray(w)\n    pdf = np.full(w.shape, np.nan)\n    positive_w = w[w &gt; 0]\n    df = pd.read_csv('../archive/data/weight/wtage.csv')\n    agemos = age * 12 + 0.5\n    df = df[(df['Agemos'] == agemos) & (df['Sex'] == sex)]\n    L = df['L'].values[0]\n    M = df['M'].values[0]\n    S = df['S'].values[0]\n    \n    # Calculate the z-score\n    z = ((positive_w / M)**L - 1) / (L * S)\n    \n    # Calculate the two main parts of the formula\n    pre_factor = positive_w**(L - 1) / (S * M**L * np.sqrt(2 * np.pi))\n    exp_term = np.exp(-0.5 * z**2)\n    \n    # Store the results only for the valid (positive w) indices\n    pdf[w &gt; 0] = pre_factor * exp_term\n    return pdf\n\npdf_boys10 = power_normal_pdf(weight_list, 10, 1)\npdf_boys15 = power_normal_pdf(weight_list, 15, 1)\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(weight_list, pdf_boys10, color='tab:blue', lw=2, alpha=0.5, label='10 years')\nax.plot(weight_list, pdf_boys15, color='tab:blue', lw=2, label='15 years')\nax.set(xlabel='weight (kg)',\n       ylabel='pdf',\n       title=\"weight distribution for boys\")\nax.legend(loc=\"upper right\", frameon=False)\nIn future chapters, when we want to talk about weight, it will be more convenient to leverage scipy’s capabilities, both to compute the pdf and to generate random samples.\nleveraging scipy.stats\ndef scipy_power_normal_pdf(w, age, sex):\n    \n    # Load LMS parameters from the CSV file\n    df = pd.read_csv('../archive/data/weight/wtage.csv')\n    agemos = age * 12 + 0.5\n    df = df[(df['Agemos'] == agemos) & (df['Sex'] == sex)]\n    L = df['L'].values[0]\n    M = df['M'].values[0]\n    S = df['S'].values[0]\n    \n    # 1. Transform weight (w) to the standard normal z-score\n    z = ((w / M)**L - 1) / (L * S)\n    \n    # 2. Calculate the derivative of the transformation (dz/dw)\n    # This is the Jacobian factor for the change of variables\n    dz_dw = (w**(L - 1)) / (S * M**L)\n    \n    # 3. Apply the change of variables formula: pdf(w) = pdf(z) * |dz/dw|\n    pdf = stats.norm.pdf(z) * dz_dw\n    \n    return pdf\n\ndef scipy_power_normal_draw_random(N, age, sex):   \n    # Load LMS parameters from the CSV file\n    df = pd.read_csv('../archive/data/weight/wtage.csv')\n    agemos = age * 12 + 0.5\n    df = df[(df['Agemos'] == agemos) & (df['Sex'] == sex)]\n    L = df['L'].values[0]\n    M = df['M'].values[0]\n    S = df['S'].values[0]\n\n    # draw random z from standard normal distribution\n    z = np.random.normal(0, 1, N)\n    # transform z to w\n    w = M * (1 + L * S * z)**(1 / L)\n    \n    return w\n\n# Calculate the PDFs for 10 and 15-year-old boys using the SciPy-based function\nscipy_pdf_boys10 = scipy_power_normal_pdf(weight_list, 10, 1)\ndraw10 = scipy_power_normal_draw_random(1000, 10, 1)\nscipy_pdf_boys15 = scipy_power_normal_pdf(weight_list, 15, 1)\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(weight_list, pdf_boys10, color='tab:blue', lw=2, alpha=0.5, label='10 years')\n# histogram of draw10\nax.hist(draw10, bins=30, density=True, color='tab:blue', alpha=0.3, label='random 10 years')\n# ax.plot(weight_list, scipy_pdf_boys10, color='tab:blue', ls=\":\", label='scipy 10')\nax.set(xlabel='weight (kg)',\n       ylabel='pdf',\n       title=\"using scipy's components to draw random samples\")\nax.legend(loc=\"upper right\", frameon=False)",
    "crumbs": [
      "data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>weight data</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html",
    "href": "t_test/t_test_one_sample.html",
    "title": "3  one-sample t-test",
    "section": "",
    "text": "3.1 Question\nI measured the height of 10 adult men. Were they sampled from the general population of men?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#hypotheses",
    "href": "t_test/t_test_one_sample.html#hypotheses",
    "title": "3  one-sample t-test",
    "section": "3.2 Hypotheses",
    "text": "3.2 Hypotheses\n\nNull hypothesis: The sample mean is equal to the population mean. In this case, the answer would be “yes”\nAlternative hypothesis: The sample mean is not equal to the population mean. Answer would be “no”.\nSignificance level: 0.05\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_1samp, t\n%matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[20.0, 'mu']\nsigma_boys = df_boys.loc[20.0, 'sigma']\n\n\nLet’s start with a sample of 10.\n\n\ngenerate data\nN = 10\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample10 = norm.rvs(size=N, loc=mu_boys+2, scale=sigma_boys)\n\n\n\n\nplot sample against population pdf\nheight_list = np.arange(140, 220, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nax.eventplot(sample10, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='gray', label='sample')\n\nax.text(190, 0.04, \n       f\"sample mean: {sample10.mean():.2f} cm\\nsample std: {sample10.std(ddof=1):.2f} cm\", \n       ha='left', va='top', color='gray')\n\nax.text(190, 0.02, \n       f\"pop. mean: {mu_boys:.2f} cm\\npop. std: {sigma_boys:.2f} cm\", \n       ha='left', va='top', color='tab:blue')\n\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n       title=\"men (age 20)\",\n       xlim=(140, 220),\n       );\n\n\n\n\n\nThe t value is calculated as follows: \nt = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}\n\nwhere\n\n\\bar{x}: sample mean\n\\mu: population mean\ns: sample standard deviation\nn: sample size\n\nLet’s try the formula above and compare it with scipy’s ttest_1samp function.\n\n\ncalculate t-value\nt_value_formula = (sample10.mean() - mu_boys) / (sample10.std(ddof=1) / np.sqrt(N))\nt_value_scipy = ttest_1samp(sample10, popmean=mu_boys)\nprint(f\"t-value (formula): {t_value_formula:.3f}\")\nprint(f\"t-value (scipy): {t_value_scipy.statistic:.3f}\")\n\n\nt-value (formula): 1.759\nt-value (scipy): 1.759\n\n\nLet’s convert this t value to a p value. It is easy to visualize the p value by ploting the pdf for the t distribution. The p value is the area under the curve for t greater than the t value and smaller than the negative t value.\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.10),\n                        xytext=(t_value_scipy.statistic, 0.30),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\nax.annotate(f\"-t value = -{t_value_scipy.statistic:.3f}\",\n                        xy=(-t_value_scipy.statistic, 0.10),\n                        xytext=(-t_value_scipy.statistic, 0.30),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(np.abs(t_array) &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=10)\",\n       );\n\n\n\n\n\n\n\n\n\nThe p value is the fraction of the t distribution that is more extreme than the observed t value. If the p value is less than the significance level, we reject the null hypothesis. In this case, the p value is larger than the significance level, so we fail to reject the null hypothesis. This means that we do not have enough evidence to say that the sample mean is different from the population mean. In other words, we cannot conclude that the 10 men samples were drawn from a distribution different than the general population.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#increase-the-sample-size",
    "href": "t_test/t_test_one_sample.html#increase-the-sample-size",
    "title": "3  one-sample t-test",
    "section": "3.3 increase the sample size",
    "text": "3.3 increase the sample size\nLet’s see what happens when we increase the sample size to 100.\n\n\ngenerate data\nN = 100\n# set scipy seed for reproducibility\nnp.random.seed(628)\nsample100 = norm.rvs(size=N, loc=mu_boys+2, scale=sigma_boys)\n\n\n\n\nplot sample against population pdf\nheight_list = np.arange(140, 220, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nax.eventplot(sample100, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='gray', label='sample')\n\nax.text(190, 0.04, \n       f\"sample mean: {sample100.mean():.2f} cm\\nsample std: {sample100.std(ddof=1):.2f} cm\", \n       ha='left', va='top', color='gray')\n\nax.text(190, 0.02, \n       f\"pop. mean: {mu_boys:.2f} cm\\npop. std: {sigma_boys:.2f} cm\", \n       ha='left', va='top', color='tab:blue')\n\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n       title=\"men (age 20)\",\n       xlim=(140, 220),\n       );\n\n\n\n\n\n\n\n\n\n\n\ncalculate t-value\nt_value_scipy = ttest_1samp(sample100, popmean=mu_boys)\nprint(f\"t-value: {t_value_scipy.statistic:.3f}\")\nprint(f\"p-value: {t_value_scipy.pvalue:.3f}\")\n\n\nt-value: 2.675\np-value: 0.009\n\n\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.03),\n                        xytext=(t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\nax.annotate(f\"-t value = -{t_value_scipy.statistic:.3f}\",\n                        xy=(-t_value_scipy.statistic, 0.03),\n                        xytext=(-t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(np.abs(t_array) &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=100)\",\n       );",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#question-2",
    "href": "t_test/t_test_one_sample.html#question-2",
    "title": "3  one-sample t-test",
    "section": "3.4 Question 2",
    "text": "3.4 Question 2\nCan we say that the sampled men are taller than the general population?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_one_sample.html#hypotheses-1",
    "href": "t_test/t_test_one_sample.html#hypotheses-1",
    "title": "3  one-sample t-test",
    "section": "3.5 Hypotheses",
    "text": "3.5 Hypotheses\n\nNull hypothesis: The sample mean is equal to the population mean.\nAlternative hypothesis: The sample mean is higher the population mean.\nSignificance level: 0.05\n\nThe analysis is the same as before, but we will use a one-tailed test. The t statistic is the same, but the p value is smaller, since we account for a smaller portion of the total area of the pdf.\n\n\ncalculate t-value and p-value\nt_value_scipy = ttest_1samp(sample100, popmean=mu_boys, alternative='greater')\nprint(f\"t-value: {t_value_scipy.statistic:.3f}\")\nprint(f\"p-value: {t_value_scipy.pvalue:.3f}\")\n\n\nt-value: 2.675\np-value: 0.004\n\n\n\n\nvisualize t-distribution\n# degrees of freedom\ndof = N - 1\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.03),\n                        xytext=(t_value_scipy.statistic, 0.20),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(t_array &gt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (N=100)\",\n       );\n\n\n\n\n\n\n\n\n\nThe answer is yes: the sampled men are significantly taller than the general population, since the p value is smaller than the significance level.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>one-sample t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_independent_samples.html",
    "href": "t_test/t_test_independent_samples.html",
    "title": "4  independent samples t-test",
    "section": "",
    "text": "4.1 Question\nAre 12-year old girls significantly taller than 12-year old boys?",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_independent_samples.html#hypotheses",
    "href": "t_test/t_test_independent_samples.html#hypotheses",
    "title": "4  independent samples t-test",
    "section": "4.2 Hypotheses",
    "text": "4.2 Hypotheses\n\nNull hypothesis: Girls and boys have the same mean height.\nAlternative hypothesis: Girls are significantly taller.\nSignificance level: 0.05\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\n\nIn this example, we sampled 10 boys and 14 girls. See below the samples data and their underlying distributions.\n\n\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nShow the code\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    )\n\n\n\n\n\n\n\n\n\nTo answer the question, we will use an independent samples t-test.\n\\begin{align}\nt &= \\frac{\\bar{X}_1 - \\bar{X}_2}{\\Theta} \\\\\n\\Theta &= \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\end{align}\nThis is a generalization of the one-sample t-test. If we take one of the samples to be infinite, we get the one-sample t-test.\nWe can compute the t-statistic by ourselves, and compare the results with those of scipy.stats.ttest_ind. Because we are interested in the difference between the means, we will use the equal_var=False option to compute Welch’s t-test. Also, because we are testing the alternative hypothesis that girls are taller, we will use the one sided test.\n\n\nShow the code\nTheta = np.sqrt(sample_boys.std(ddof=1)**2/sample_boys.size + \\\n                sample_girls.std(ddof=1)**2/sample_girls.size)\nt_stat = (sample_boys.mean() - sample_girls.mean()) / Theta\ndof = N_boys + N_girls - 2\np_val = t.cdf(t_stat, dof)\n\n# the option alternative=\"less\" is used because we are testing whether the first sample (boys) is less than the second sample (girls)\nt_value_scipy = ttest_ind(sample_boys, sample_girls, equal_var=False, alternative=\"less\")\n\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\nprint(f\"t-statistic (scipy): {t_value_scipy.statistic:.3f}, p-value (scipy): {t_value_scipy.pvalue:.3f}\")\n\n\nt-statistic: -0.999, p-value: 0.164\nt-statistic (scipy): -0.999, p-value (scipy): 0.165\n\n\nWe got the exact same results :)\nNow let’s visualize what the p-value means.\n\n\nvisualize t-distribution\n# degrees of freedom\nfig, ax = plt.subplots(figsize=(10, 6))\n\nt_array_min = np.round(t.ppf(0.001, dof),3)\nt_array_max = np.round(t.ppf(0.999, dof),3)\nt_array = np.arange(t_array_min, t_array_max, 0.001)\n\n# annotate vertical array at t_value_scipy\nax.annotate(f\"t value = {t_value_scipy.statistic:.3f}\",\n                        xy=(t_value_scipy.statistic, 0.25),\n                        xytext=(t_value_scipy.statistic, 0.35),\n                        fontsize=14,\n                        arrowprops=dict(arrowstyle=\"-&gt;\", lw=2, color='black'),\n                        ha='center')\n# fill between t-distribution and normal distribution\nax.fill_between(t_array, t.pdf(t_array, dof),\n                 where=(t_array &lt; t_value_scipy.statistic),\n                 color='tab:blue', alpha=0.5,\n                 label='rejection region')\n\n# write t_value_scipy.pvalue on the plot\nax.text(0, 0.05,\n        f\"p value = {t_value_scipy.pvalue:.3f}\", \n        ha='center', va='bottom',\n        bbox=dict(facecolor='tab:blue', alpha=0.5, boxstyle=\"round\"))\n\nax.plot(t_array, t.pdf(t_array, dof),\n       color='black', lw=2)\n\nax.set(xlabel='t',\n       ylabel='probability density',\n       title=\"t-distribution (dof=22)\",\n       );\n\n\n\n\n\n\n\n\n\nBecause the p-value is higher than the significance level, we fail to reject the null hypothesis. This means that, based on the data, we cannot conclude that girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "t_test/t_test_independent_samples.html#increasing-sample-size",
    "href": "t_test/t_test_independent_samples.html#increasing-sample-size",
    "title": "4  independent samples t-test",
    "section": "4.3 increasing sample size",
    "text": "4.3 increasing sample size\nLet’s increase the sample size to see how it affects the p-value. We’ll sample 250 boys and 200 girls now.\n\n\ngenerate data\nN_boys = 250\nN_girls = 200\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nShow the code\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nTheta = np.sqrt(sample_boys.std(ddof=1)**2/sample_boys.size + \\\n                sample_girls.std(ddof=1)**2/sample_girls.size)\nt_stat = (sample_boys.mean() - sample_girls.mean()) / Theta\ndof = N_boys + N_girls - 2\np_val = t.cdf(t_stat, dof)\n\n# the option alternative=\"less\" is used because we are testing whether the first sample (boys) is less than the second sample (girls)\nt_value_scipy = ttest_ind(sample_boys, sample_girls, equal_var=False, alternative=\"less\")\n\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\nprint(f\"t-statistic (scipy): {t_value_scipy.statistic:.3f}, p-value (scipy): {t_value_scipy.pvalue:.3f}\")\n\n\nt-statistic: -2.639, p-value: 0.004\nt-statistic (scipy): -2.639, p-value (scipy): 0.004\n\n\nWe found now a p-value lower than the significance level, so we reject the null hypothesis. This means that, based on the data, we can conclude that girls are significantly taller than boys.",
    "crumbs": [
      "hypothesis testing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>independent samples t-test</span>"
    ]
  },
  {
    "objectID": "confidence_interval/basic_concepts.html",
    "href": "confidence_interval/basic_concepts.html",
    "title": "5  basic concepts",
    "section": "",
    "text": "Suppose we randomly select 30 seven-year-old boys from schools around the country and measure their heights (this is our sample). We’d like to use their average height to estimate the true average height of all seven-year-old boys nationwide (the population). Because different samples of 30 boys would yield slightly different averages, we need a way to quantify that uncertainty. A confidence interval gives us a range—based on our sample data—that expresses what we would expect to find if we were to repeat this sampling process many times.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\nimport scipy\nfrom matplotlib.lines import Line2D\nimport matplotlib.gridspec as gridspec\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nage = 7.0\nmu_boys = df_boys.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\n\n\nSee the height distribution for seven-year-old boys. Below it we see the means for 20 samples of groups of 30 boys. The 95% confidence interval is the range of values that, on average, 95% of the samples CI contain the true population mean. In this case, this amounts to one out of the 20 samples.\n\n\nplot samples against population pdf\nnp.random.seed(628)\nheight_list = np.arange(90, 150, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\n\nfig = plt.figure(figsize=(8, 6))\ngs = gridspec.GridSpec(2, 1, height_ratios=[0.1, 0.9])\ngs.update(left=0.09, right=0.86,top=0.98, bottom=0.06, hspace=0.30, wspace=0.05)\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[1, 0])\n\nax0.plot(height_list, pdf_boys, lw=2, color='tab:blue', label='population')\n\nN_samples = 20\nN = 30\n\nfor i in range(N_samples):\n    sample = norm.rvs(loc=mu_boys, scale=sigma_boys, size=N)\n    sample_mean = sample.mean()\n    # confidence interval\n    alpha = 0.05\n    z_crit = scipy.stats.t.isf(alpha/2, N-1)\n    CI = z_crit * sample.std(ddof=1) / np.sqrt(N)\n    ax1.errorbar(sample_mean, i, xerr=CI, fmt='o', color='tab:blue', \n                 label=f'sample {i+1}' if i == 0 else \"\", capsize=0)\n\n\nfrom matplotlib.patches import ConnectionPatch\nline = ConnectionPatch(xyA=(mu_boys, pdf_boys.max()), xyB=(mu_boys, -1), coordsA=\"data\", coordsB=\"data\",\n                      axesA=ax0, axesB=ax1, color=\"gray\", linestyle='--', linewidth=1.5, alpha=0.5)\nax1.add_artist(line)\n\nax1.annotate(\n        '',\n        xy=(mu_boys + 5, 13),  # tip of the arrow (first error bar, y=0)\n        xytext=(mu_boys + 5 + 13, 13),  # text location\n        arrowprops=dict(arrowstyle='-&gt;', lw=2, color='black'),\n        fontsize=13,\n        color='tab:blue',\n        ha='left',\n        va='center'\n)\n\nax1.text(mu_boys + 5 + 2, 12, \"on average, the CI\\nof 1 out of 20 samples\\n\"\n         r\"($\\alpha=5$% significance level)\"\n          \"\\nwill not contain\\nthe population mean\",\n          va=\"top\", fontsize=12)\n\n# write \"sample i\" for each error bar\nfor i in range(N_samples):\n    ax1.text(mu_boys -10, i, f'sample {N_samples-i:02d}', \n             fontsize=13, color='black',\n             ha='right', va='center')\n\n# ax.legend(frameon=False)\nax0.spines['top'].set_visible(False)\nax0.spines['right'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\n\nax0.set(xticks=np.arange(90, 151, 10),\n        xlim=(90, 150),\n        xlabel='height (cm)',\n        # xticklabels=[],\n        yticks=[],\n        ylabel='pdf',\n        )\nax1.set(xticks=[],\n        xlim=(90, 150),\n        ylim=(-1, N_samples),\n        yticks=[],\n       );",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>basic concepts</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html",
    "href": "confidence_interval/analytical_confidence_interval.html",
    "title": "6  analytical confidence interval",
    "section": "",
    "text": "6.1 CLT\nThe Central Limit Theorem states that the sampling distribution of the sample mean\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i\napproaches a normal distribution as the sample size N increases, regardless of the shape of the population distribution. This normal distribution can be expressed as:\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{N}\\right),\nwhere \\mu and \\sigma^2 are the population mean and variance, respectively. When talking about samples, we use \\bar{x} and s^2 to denote the sample mean and variance.\nLet’s visualize this. The graph below shows how the sample size N affects the sampling distribution of the sample mean \\bar{X}. The higher the sample size, the more concentrated the distribution becomes around the population mean \\mu. If we take N to be infinity, the sampling distribution of the sample mean becomes a delta function at \\mu, and we will know the exact value of the population mean.\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[7.0, 'mu']\nsigma_boys = df_boys.loc[7.0, 'sigma']\nplot pdfs as function of sample size\nfig, ax = plt.subplots(1,2, figsize=(10, 6), sharex=True, sharey=True)\n\nheight_list = np.arange(mu_boys-12, mu_boys+12, 0.01)\nN_list = [10, 30, 100]\nalpha_list = [0.4, 0.6, 1.0]\n\ncolors = plt.cm.hot([0.6, 0.3, 0.1])\n\nN_samples = 1000\nnp.random.seed(628)\nmean_list_10 = []\nmean_list_30 = []\nmean_list_100 = []\nfor i in range(N_samples):\n    mean_list_10.append(np.mean(norm.rvs(size=10, loc=mu_boys, scale=sigma_boys)))\n    mean_list_30.append(np.mean(norm.rvs(size=30, loc=mu_boys, scale=sigma_boys)))\n    mean_list_100.append(np.mean(norm.rvs(size=100, loc=mu_boys, scale=sigma_boys)))\n\nalpha = 0.05\n\n# z_alpha_over_two = norm(loc=mu_boys, scale=SE).ppf(1 - alpha / 2)\n# z_alpha_over_two = np.round(z_alpha_over_two, 2)\n\nfor i,N in enumerate(N_list):\n    SE = sigma_boys / np.sqrt(N)\n    ax[0].plot(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list),\n            color=colors[i], label=f\"N={N}\")\n    \nax[1].hist(mean_list_10, bins=30, density=True, color=colors[0], label=\"N=10\", align='mid', histtype='step')\nax[1].hist(mean_list_30, bins=30, density=True, color=colors[1], label=\"N=10\", align='mid', histtype='step')\nax[1].hist(mean_list_100, bins=30, density=True, color=colors[2], label=\"N=10\", align='mid', histtype='step')\n\nax[1].text(0.99, 0.98, \"number of samples\\n1000\", ha='right', va='top', transform=ax[1].transAxes, fontsize=14)\n\nax[0].legend(frameon=False)\nax[0].set(xlabel=\"height (cm)\",\n       ylabel=\"pdf\",\n       title=\"analytical\"\n       )\nax[1].set(xlabel=\"height (cm)\",\n          title=\"numerical\"\n          )\n# title that hovers over both subplots\nfig.suptitle(f\"Distribution of the sample means for 3 different sample sizes\");",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#confidence-interval-1",
    "href": "confidence_interval/analytical_confidence_interval.html#confidence-interval-1",
    "title": "6  analytical confidence interval",
    "section": "6.2 confidence interval 1",
    "text": "6.2 confidence interval 1\nLet’s use now the sample size N=30. The confidence interval for a significance level \\alpha=0.05 is the interval that leaves \\alpha/2 of the pdf area in each tail of the distribution.\n\n\nShow the code\nfig, ax = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.0, hspace=0.1)\nN = 30\nSE = sigma_boys / np.sqrt(N)\n\nh_min = np.round(norm(loc=mu_boys, scale=SE).ppf(0.001), 2)\nh_max = np.round(norm(loc=mu_boys, scale=SE).ppf(0.999), 2)\nheight_list = np.arange(h_min, h_max, 0.01)\n\nalpha = 0.05\nz_alpha_over_two_hi = np.round(norm(loc=mu_boys, scale=SE).ppf(1 - alpha / 2), 2)\nz_alpha_over_two_lo = np.round(norm(loc=mu_boys, scale=SE).ppf(alpha / 2), 2)\n\n\nax[0].plot(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list))\nax[1].plot(height_list, norm(loc=mu_boys, scale=SE).cdf(height_list))\n\nax[0].fill_between(height_list, norm(loc=mu_boys, scale=SE).pdf(height_list),\n                   where=((height_list &gt; z_alpha_over_two_hi) | (height_list &lt; z_alpha_over_two_lo)),\n                   color='tab:blue', alpha=0.5,\n                   label='rejection region')\n\nax[0].annotate(f\"\",\n               xy=(z_alpha_over_two_hi, 0.02),\n               xytext=(z_alpha_over_two_lo, 0.02),\n               arrowprops=dict(arrowstyle=\"&lt;-&gt;\", lw=1.5, color='black', shrinkA=0.0, shrinkB=0.0),\n               )\nax[1].text(h_max+0.15, norm(loc=mu_boys, scale=SE).cdf(z_alpha_over_two_lo), r\"$\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].text(h_max+0.15, norm(loc=mu_boys, scale=SE).cdf(z_alpha_over_two_hi), r\"$1-\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].axhline(alpha/2, color='gray', linestyle=':')\nax[1].axhline(1-alpha/2, color='gray', linestyle=':')\nax[0].text(mu_boys, 0.03, \"95% confidence interval\", ha=\"center\")\nax[0].set(ylim=(0, 0.42),\n          ylabel=\"pdf\",\n          title=r\"significance level $\\alpha$ = 0.05\",\n          )\nax[1].set(ylim=(-0.1, 1.1),\n          xlim=(h_min, h_max),\n          ylabel=\"cdf\",\n          xlabel=\"height (cm)\",\n          );\n\n\n\n\n\n\n\n\n\nThat’s it. That’s the whole story.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#confidence-interval-2",
    "href": "confidence_interval/analytical_confidence_interval.html#confidence-interval-2",
    "title": "6  analytical confidence interval",
    "section": "6.3 confidence interval 2",
    "text": "6.3 confidence interval 2\nThe rest is repackaging the above in a slightly different way. Instead of finding the top and bottom of the confidence interval according to the cdf of a normal distribution of mean \\mu and variance \\sigma^2/N, we first standardize this distribution to a standard normal distribution Z \\sim N(0,1), compute the confidence interval for Z, and then transform it back to the original distribution.\nIf the distribution of the sample mean \\bar{X}\n\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{N}\\right),\n\nthen the standardized variable Z is defined as:\n\nZ = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{N}} \\sim N(0,1).\n\nWhy is this useful? Because we usually use the same significance level \\alpha for all confidence intervals, and we can compute the confidence interval for Z once and use it for all confidence intervals. For Z \\sim N(0,1) and \\alpha=0.05, the top and bottom of the confidence interval are Z_{\\alpha/2}=\\pm 1.96. Now we only have to invert the expression above to get the confidence interval for \\bar{X}:\n\nX_{1,2} = \\mu \\pm Z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{N}}.\n\nThe very last thing we have to account for is the fact that we don’t know the population statistics \\mu and \\sigma^2. Instead, we have to use the sample statistics \\bar{x} and s^2. Furthermore, we have to use the t-distribution instead of the normal distribution, because we are estimating the population variance from the sample variance. The t-distribution has a shape similar to the normal distribution, but it has heavier tails, which accounts for the additional uncertainty introduced by estimating the population variance. Thus, we replace \\mu with \\bar{x} and \\sigma^2 with s^2, and we use the t-distribution with N-1 degrees of freedom. This gives us the final expression for the confidence interval:\n\nX_{1,2} = \\bar{x} \\pm t^*_{N-1} \\cdot \\frac{s}{\\sqrt{N}},\n\nwhere t^*_{N-1} is the critical value from the t-distribution with N-1 degrees of freedom.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#the-solution",
    "href": "confidence_interval/analytical_confidence_interval.html#the-solution",
    "title": "6  analytical confidence interval",
    "section": "6.4 the solution",
    "text": "6.4 the solution\nLet’s say I measured the heights of 30 7-year-old boys, and this is the data I got:\n\n\nShow the code\nN = 30\nnp.random.seed(271)\nsample = norm.rvs(size=N, loc=mu_boys, scale=sigma_boys)\nprint(f\"Sample mean: {np.mean(sample):.2f} cm\")\nprint(sample)\n\n\nSample mean: 122.60 cm\n[114.15972134 128.21581493 122.9864136  117.94247325 132.11013925\n 118.69131645 123.67695468 112.03152008 121.59853424 114.8629358\n 121.90458112 115.68839748 127.18043069 118.33193499 125.28525617\n 124.5287395  120.72706375 113.10575734 132.229147   129.16820684\n 125.94682095 126.08299475 125.95056303 125.6858065  115.07854075\n 124.93539918 125.12886271 126.91366971 120.88030405 127.04777082]\n\n\nUsing the formula for the confidence interval we get:\n\n\nShow the code\nalpha = 0.05\nz_crit = scipy.stats.t.isf(alpha/2, N-1)\nCI = z_crit * sample.std(ddof=1) / np.sqrt(N)\nCI_low = np.round(sample.mean() - CI, 2)\nCI_high = np.round(sample.mean() + CI, 2)\nprint(f\"Sample mean: {np.mean(sample):.2f} cm\")\nprint(\"The 95% confidence interval is [{}, {}] cm\".format(CI_low, CI_high))\nprint(f\"The true population mean is {mu_boys:.2f} cm\")\n\n\nSample mean: 122.60 cm\nThe 95% confidence interval is [120.54, 124.67] cm\nThe true population mean is 121.74 cm",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/analytical_confidence_interval.html#a-few-points-to-stress",
    "href": "confidence_interval/analytical_confidence_interval.html#a-few-points-to-stress",
    "title": "6  analytical confidence interval",
    "section": "6.5 a few points to stress",
    "text": "6.5 a few points to stress\nIt is worth commenting on a few points:\n\nIf we were to sample a great many number of samples of size N=30, and compute the confidence interval for each sample, then approximately 95% of these intervals would contain the true population mean \\mu.\nIt is not true that the probability that the true population mean \\mu is in the confidence interval is 95%. The true population mean is either in the interval or not, and it does not have a probability associated with it. The 95% confidence level refers to the long-run frequency of intervals containing the true population mean if we were to repeat the sampling process many times. This is the common frequentist interpretation of confidence intervals.\nIf you want to talk about confidence interval in the Bayesian framework, then first we would have to assign a prior distribution to the population mean \\mu, and then we would compute the posterior distribution of \\mu given the data. The credible interval is then the interval that contains 95% of the posterior distribution of \\mu.\nTo sum up the difference between the frequentist and Bayesian interpretations of confidence intervals:\n\nFrequentist CI: “I am 95% confident in the method” (long-run frequency).\nBayesian credible interval: “There is a 95% probability that μ lies in this interval” (degree of belief).",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>analytical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html",
    "href": "confidence_interval/empirical_confidence_interval.html",
    "title": "7  empirical confidence interval",
    "section": "",
    "text": "7.1 bootstrap confidence interval\nThat’s it. Now let’s do it in code.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html#bootstrap-confidence-interval",
    "href": "confidence_interval/empirical_confidence_interval.html#bootstrap-confidence-interval",
    "title": "7  empirical confidence interval",
    "section": "",
    "text": "Draw a sample of size N from the population. Let’s assume you made an experiment and you could only afford to collect N samples. You will not have the opportunity to collect more samples, and that’s all you have available.\nAssume that the sample is representative of the population. This is a strong assumption, but we will use it to compute the confidence interval.\nFrom this original sample, draw B bootstrap samples of size N with replacement. This means that you will randomly select N samples from the original sample, allowing for duplicates. This is like drawing pieces of paper from a hat, where you can put the paper back after drawing it.\nFor each bootstrap sample, compute the statistic of interest (e.g., median, standard deviation, maximum).\nCompute the cdf of the bootstrap statistics. This will give you the empirical distribution of the statistic.\nCompute the confidence interval using the empirical distribution. For a 95% confidence interval, you can take the 2.5th and 97.5th percentiles of the bootstrap statistics.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "confidence_interval/empirical_confidence_interval.html#question",
    "href": "confidence_interval/empirical_confidence_interval.html#question",
    "title": "7  empirical confidence interval",
    "section": "7.2 question",
    "text": "7.2 question\nWe have a sample of 30 7-year-old boys. What can we say about the maximum height of 7-year-olds in the general population?\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\nimport scipy\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\nmu_boys = df_boys.loc[7.0, 'mu']\nsigma_boys = df_boys.loc[7.0, 'sigma']\n\n\n\nN = 30     # bootstrap sample size equal to original sample size\nB = 10000  # number of bootstrap samples\nnp.random.seed(1)\nsample = norm.rvs(size=N, loc=mu_boys, scale=sigma_boys)\nmedian_list = []\nfor i in range(B):\n    sample_bootstrap = np.random.choice(sample, size=N, replace=True)\n    median_list.append(np.median(sample_bootstrap))\nmedian_list = np.array(median_list)\n\nalpha = 0.05\nci_bottom = np.quantile(median_list,alpha/2)\nci_top = np.quantile(median_list, 1-alpha/2)\nprint(f\"Bootstrap CI for median: {ci_bottom:.2f} - {ci_top:.2f} cm\")\n\nBootstrap CI for median: 118.65 - 124.53 cm\n\n\n\n\nShow the code\nfig, ax = plt.subplots(2,1, figsize=(8, 6), sharex=True)\nax[0].hist(median_list, bins=30, density=True, align='mid')\nax[1].hist(median_list, bins=30, density=True, cumulative=True, align='mid')\n\nax[1].axhline(alpha/2, color='gray', linestyle=':')\nax[1].axhline(1-alpha/2, color='gray', linestyle=':')\n\nxlim = ax[1].get_xlim()\nax[1].text(xlim[1]+0.15, alpha/2, r\"$\\alpha/2$\",\n           ha=\"left\", va=\"center\")\nax[1].text(xlim[1]+0.15, 1-alpha/2, r\"$1-\\alpha/2$\",\n           ha=\"left\", va=\"center\")\n\nax[1].annotate(\n     'bottom CI',\n     xy=(ci_bottom, alpha/2), xycoords='data',\n     xytext=(-100, 30), textcoords='offset points',\n     color='black',\n     arrowprops=dict(arrowstyle=\"-&gt;\", color='black',\n                     connectionstyle=\"angle,angleA=0,angleB=90,rad=10\"))\nax[1].annotate(\n     'top CI',\n     xy=(ci_top, 1-alpha/2), xycoords='data',\n     xytext=(-100, 15), textcoords='offset points',\n     color='black',\n     arrowprops=dict(arrowstyle=\"-&gt;\", color='black',\n                     connectionstyle=\"angle,angleA=0,angleB=90,rad=10\"))\n\nax[0].set(ylabel=\"pdf\",\n          title=\"empirical distribution of median heights from bootstrap samples\")\nax[1].set(ylabel=\"cdf\",\n          xlabel=\"height (cm)\")\n\n\n\n\n\n\n\n\n\nClearly, the distribution of median height is not normal. The bootstrap method gives us a way to compute the confidence interval of the median height (or any other statistic of your choosing) without assuming normality.",
    "crumbs": [
      "confidence interval",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>empirical confidence interval</span>"
    ]
  },
  {
    "objectID": "permutation/problem-with-t-test.html",
    "href": "permutation/problem-with-t-test.html",
    "title": "8  the problem with t-test",
    "section": "",
    "text": "8.1 the normality assumption\nTwo possible interpretations come to mind.\nIn the context of the t-test, the above is a distinction without a difference. Even if the population is not normally distributed, the means of the samples will be normally distributed as long as the sample size is large enough. We then use the t-test and go on with our lives.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "permutation/problem-with-t-test.html#the-normality-assumption",
    "href": "permutation/problem-with-t-test.html#the-normality-assumption",
    "title": "8  the problem with t-test",
    "section": "",
    "text": "The assumption is that the height of men and women in the population is normally distributed. From these idealized populations we draw samples.\nThe t-test effectively compares the difference between the means of the two samples, and the variability within each sample. Because of the Central Limit Theorem, the means of the samples will approach a normal distribution as the sample size increases. In this interpretation, the normality assumption is about the distribution of the means of the samples, and not the distribution of the population.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "permutation/problem-with-t-test.html#other-statistical-tests",
    "href": "permutation/problem-with-t-test.html#other-statistical-tests",
    "title": "8  the problem with t-test",
    "section": "8.2 other statistical tests",
    "text": "8.2 other statistical tests\nThe Central Limit Theorem dictates that the means will be normally distributed, but it does not apply to other statistics, such as:\n\nthe median\nthe variance\nthe skewness\nthe maximum\nthe Interquartile Range (IQR)\netc.\n\nIn this case, the t-test can’t be relied upon, and we need another solution.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>the problem with t-test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html",
    "href": "permutation/permutation.html",
    "title": "9  permutation test",
    "section": "",
    "text": "9.1 hypotheses\nThe basic idea behind the permutation test is that, if the null hypothesis is correct, then it wouldn’t matter if we relabelled the samples. If we randomly permute the labels “girls” and “boys” of the two samples, the statistic of interest should not change significantly. However, if by permuting the labels we get a significantly different statistic, then we can reject the null hypothesis.\nThat’s beautiful, right?",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#hypotheses",
    "href": "permutation/permutation.html#hypotheses",
    "title": "9  permutation test",
    "section": "",
    "text": "Null hypothesis (H0): The two samples come from the same distribution.\nAlternative hypothesis (H1): Girls are taller than boys.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#steps",
    "href": "permutation/permutation.html#steps",
    "title": "9  permutation test",
    "section": "9.2 steps",
    "text": "9.2 steps\n\nCompute the statistic of interest (e.g., the difference in medians) for the original samples.\nRandomly permute the labels of the two samples.\nCompute the statistic of interest for the permuted samples.\nRepeat steps 2 and 3 many times (e.g., 1000 times) to create a distribution of the statistic under the null hypothesis.\nCompare the original statistic to the distribution of permuted statistics to see if it is significantly different (e.g., by checking if it falls in the top 5% of the distribution). From this, we can numerically compute a p-value.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#example",
    "href": "permutation/permutation.html#example",
    "title": "9  permutation test",
    "section": "9.3 example",
    "text": "9.3 example\nLet’s use the very same example as in the independent samples t-test.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\n\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\n\n\n\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n\n\n\nplot population and sample data\nheight_list = np.arange(120, 180, 0.1)\npdf_boys = norm.pdf(height_list, loc=mu_boys, scale=sigma_boys)\npdf_girls = norm.pdf(height_list, loc=mu_girls, scale=sigma_girls)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(height_list, pdf_boys, lw=4, alpha=0.3, color='tab:blue', label='boys population')\nax.plot(height_list, pdf_girls, lw=4, alpha=0.3, color='tab:orange', label='girls population')\n\nax.eventplot(sample_boys, orientation=\"horizontal\", lineoffsets=0.03,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:blue', label=f'boys sample, n={N_boys}')\nax.eventplot(sample_girls, orientation=\"horizontal\", lineoffsets=0.023,\n             linewidth=1, linelengths= 0.005,\n             colors='tab:orange', label=f'girls sample, n={N_girls}')\nax.legend(frameon=False)\nax.set(xlabel='height (cm)',\n       ylabel='probability density',\n    );\n\n\n\n\n\n\n\n\n\nThe statistic of interest now is the difference in medians between the two samples.\n\n\ncompute the observed difference in medians\n# define the desired statistic.\n# in can be anything you want, you can even write your own function.\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\nprint(f\"median height for girls: {median_girls:.2f} cm\")\nprint(f\"median height for boys: {median_boys:.2f} cm\")\nprint(f\"median difference (girls minus boys): {observed_diff:.2f} cm\")\n\n\nmedian height for girls: 150.88 cm\nmedian height for boys: 151.19 cm\nmedian difference (girls minus boys): -0.31 cm\n\n\n\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\nNow let’s see the empirical cdf of the permuted statistics, and where the original statistic falls in that distribution.\n\n\nplot empirical distribution of differences\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# compute the empirical CDF\nrank = np.arange(len(diffs)) + 1\ncdf = rank / (len(diffs) + 1)\nsorted_diffs = np.sort(diffs)\n\nax.plot(sorted_diffs, cdf, lw=2, color='tab:blue', label='empirical CDF')\nax.axvline(observed_diff, color='tab:orange', lw=2, ls='--',\n           label=f'obs. median diff. = {observed_diff:.2f} cm')\nax.text(observed_diff + 0.5, 0.3, f'observed difference in\\nmedian height: {observed_diff:.2f} cm',\n        color='tab:orange', fontsize=14, ha='left', va='bottom')\n\nalpha = 0.05\n# for a one-tailed test\nax.axhline(1-alpha, color='k', lw=1, ls='--')\nax.annotate(r\"$1-\\alpha$\", xy=(1.01, 1-alpha), xycoords=('axes fraction', 'data'),\n            ha=\"left\", va=\"center\")\n# for a two-tailed test\n# ax.axhline(alpha/2, color='k', lw=1, ls='--')\n# ax.axhline(1-alpha/2, color='k', lw=1, ls='--')\n# ax.annotate(r\"$\\alpha/2$\", xy=(1.01, alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n# ax.annotate(r\"$1-\\alpha/2$\", xy=(1.01, 1-alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\nax.set(xlabel='difference in median height (cm)',\n         ylabel='empirical CDF',\n         title=f'empirical distribution of differences in median height');\n\n\n\n\n\n\n\n\n\nThe observed statistic is well within the boundaries set by the significance level of 5%. Therefore, we cannot reject the null hypothesis. We conclude that, based on this data, the most probable interpretation is that girls and boys have the same underlying distribution.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#increase-sample-size",
    "href": "permutation/permutation.html#increase-sample-size",
    "title": "9  permutation test",
    "section": "9.4 increase sample size",
    "text": "9.4 increase sample size\nLet’s increase the sample size to 200 girls and 300 boys.\n\n\nthe whole process in one cell\n# take samples\nN_boys = 300\nN_girls = 200\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n\n# compute the observed difference in medians\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\n\n# permutation algorithm\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 200 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\n\n\n\nplot empirical distribution of differences\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# compute the empirical CDF\nrank = np.arange(len(diffs)) + 1\ncdf = rank / (len(diffs) + 1)\nsorted_diffs = np.sort(diffs)\n\nax.plot(sorted_diffs, cdf, lw=2, color='tab:blue', label='empirical CDF')\nax.axvline(observed_diff, color='tab:orange', lw=2, ls='--',\n           label=f'obs. median diff. = {observed_diff:.2f} cm')\nax.text(observed_diff - 0.05, 0.3, f'observed difference in\\nmedian height: {observed_diff:.2f} cm',\n        color='tab:orange', fontsize=14, ha='right', va='bottom')\n\nalpha = 0.05\n# for a one-tailed test\nax.axhline(1-alpha, color='k', lw=1, ls='--')\nax.annotate(r\"$1-\\alpha$\", xy=(1.01, 1-alpha), xycoords=('axes fraction', 'data'),\n            ha=\"left\", va=\"center\")\n# for a two-tailed test\n# ax.axhline(alpha/2, color='k', lw=1, ls='--')\n# ax.axhline(1-alpha/2, color='k', lw=1, ls='--')\n# ax.annotate(r\"$\\alpha/2$\", xy=(1.01, alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n# ax.annotate(r\"$1-\\alpha/2$\", xy=(1.01, 1-alpha/2), xycoords=('axes fraction', 'data'),\n#             ha=\"left\", va=\"center\")\n\nax.set(xlabel='difference in median height (cm)',\n         ylabel='empirical CDF',\n         title=f'empirical distribution of differences in median height');\n\n\n\n\n\n\n\n\n\nNow the observed statistic is well outside the right boundary set by the significance level of 5%. Therefore, we can reject the null hypothesis. We conclude that, based on this data, girls are significantly taller than boys.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/permutation.html#p-value",
    "href": "permutation/permutation.html#p-value",
    "title": "9  permutation test",
    "section": "9.5 p-value",
    "text": "9.5 p-value\nIt is quite easy to compute the p-value from the permutation test. It is simply the fraction of permuted statistics that are more extreme than the observed statistic. In this case, since we are testing whether girls are taller than boys, we have a one-tailed test, and we only consider the right tail of the distribution. If we were testing whether girls are significantly different from boys in their height, we would have a two-tailed test, and we would consider both tails of the distribution.\n\n# one-tailed p-value\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\nobserved difference: 2.004\np-value (one-tailed): 0.0050\n\n\nWe can now address the fact that we ran only 999 permutations, although I intended to run 1000. See in the code that after the permutation algorithm, I inserted the original statistic in the list of permuted statistics. This is because I want to compute the p-value as the fraction of permuted statistics that are more extreme than the original statistic, and I want to include the original statistic in the distribution. If I had not done this, for a truly extreme observed statistic, we would get that the p-value equals 0, that is, the fraction of permuted statistics that are more extreme than the observed statistic is zero. To avoid this behavior, we include the original statistic in the distribution of permuted statistics.\nA corollary of this is that the smallest p-value we can get is 0.001 (for our example with 1000 permutations).",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>permutation test</span>"
    ]
  },
  {
    "objectID": "permutation/numpy-vs-pandas.html",
    "href": "permutation/numpy-vs-pandas.html",
    "title": "10  numpy vs pandas",
    "section": "",
    "text": "10.1 numpy\nIn the previous chapter, we computed the permutation test using numpy. We had two samples of different sizes, and before the permutation test we concatenated the two samples into one array. Then we shuffled the concatenated array and split it back into two samples, according to the original sizes. See a sketch of the code below:\nStore the two samples in numpy arrays:\nDefine the statistic and compute the observed difference:\nRun the permutation test:\nAll this works great if this is how your data looks like. Sometimes, however, you have structured data with more information, such as a DataFrame with multiple columns. In this case, you can leverage the capabilities of pandas.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "permutation/numpy-vs-pandas.html#numpy",
    "href": "permutation/numpy-vs-pandas.html#numpy",
    "title": "10  numpy vs pandas",
    "section": "",
    "text": "boys = np.array([121, 123, 124, 125])\ngirls = np.array([120, 121, 121, 122, 123, 123, 128, 129])\nN_boys = len(boys)\nN_girls = len(girls)\n\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\n\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations - 1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first N_girls values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "permutation/numpy-vs-pandas.html#pandas",
    "href": "permutation/numpy-vs-pandas.html#pandas",
    "title": "10  numpy vs pandas",
    "section": "10.2 pandas",
    "text": "10.2 pandas\nLet’s give an example of structured data. Suppose we have a DataFrame with the following columns: sex, height, and weight.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\n\n\n\nN_total = 20\nnp.random.seed(3)\nheight_list = norm.rvs(size=N_total, loc=150, scale=7)\nweight_list = norm.rvs(size=N_total, loc=42, scale=5)\nsex_list = np.random.choice(['M', 'F'], size=N_total, replace=True)\ndf = pd.DataFrame({\n    'sex': sex_list,\n    'height (cm)': height_list,\n    'weight (kg)': weight_list\n})\ndf\n\n\n\n\n\n\n\n\n\nsex\nheight (cm)\nweight (kg)\n\n\n\n\n0\nM\n162.520399\n36.074767\n\n\n1\nM\n153.055569\n40.971751\n\n\n2\nM\n150.675482\n49.430742\n\n\n3\nF\n136.955551\n43.183581\n\n\n4\nF\n148.058283\n36.881074\n\n\n5\nM\n147.516687\n38.435034\n\n\n6\nF\n149.420810\n45.126225\n\n\n7\nF\n145.610995\n41.197433\n\n\n8\nF\n149.693273\n38.155818\n\n\n9\nM\n146.659474\n40.849846\n\n\n10\nM\n140.802947\n45.725281\n\n\n11\nF\n156.192357\n51.880554\n\n\n12\nF\n156.169226\n35.779383\n\n\n13\nM\n161.967011\n38.867915\n\n\n14\nF\n150.350235\n37.981170\n\n\n15\nM\n147.167258\n29.904584\n\n\n16\nM\n146.182480\n37.381040\n\n\n17\nM\n139.174659\n36.880621\n\n\n18\nM\n156.876572\n47.619890\n\n\n19\nM\n142.292527\n41.340429\n\n\n\n\n\n\n\n\nCalculate sample statistics using groupby:\n\nsample_stats = df.groupby('sex')['height (cm)'].median()\nobserved_diff = sample_stats['F'] - sample_stats['M']\n\nWe can now leverage the pandas.DataFrame.sample method to sample from the DataFrame. Here, we use the following options:\n\nfrac=1 means we want to sample 100% of rows, but shuffled.\nreplace=False means we want to sample without replacement, that is, no duplicate rows.\n\nWe will shuffle the sex column and store the result in a new column called sex_shuffled. Then we can use groupby to compute the median.\n\nN_permutations = 1000\ndiffs = np.empty(N_permutations - 1)\nfor i in range(N_permutations - 1):\n    # shuffle dataframe 'sex' colunn, store it in 'sex_shuffled'\n    df['sex_shuffled'] = df['sex'].sample(frac=1, replace=False).reset_index(drop=True)\n    shuffled_stats = df.groupby('sex_shuffled')['height (cm)'].median()\n    diffs[i] = shuffled_stats['F'] - shuffled_stats['M']  # median(F) - median(M)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>numpy vs pandas</span>"
    ]
  },
  {
    "objectID": "permutation/exact-vs-montecarlo.html",
    "href": "permutation/exact-vs-montecarlo.html",
    "title": "11  exact vs. Monte Carlo permutation tests",
    "section": "",
    "text": "11.1 Monte Carlo permutation tests\nMonte Carlo methods are a class of computational algorithms that rely on repeated random sampling to obtain numerical results. In the context of permutation tests, Monte Carlo methods do not compute the test statistic for every possible permutation of the data. In the examples from before, we computed 1000 permutations only, and from that we estimated the p-value of the test statistic. If we had run the test more than once, we would have obtained a different p-value each time, as the test statistic is computed from a random sample of permutations.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm, ttest_ind, t\n# %matplotlib widget\nload data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 12.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\ngenerate data\nN_boys = 10\nN_girls = 14\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\ncompute the observed difference in medians\n# define the desired statistic.\n# in can be anything you want, you can even write your own function.\nstatistic = np.median\n# compute the median for each sample and the difference\nmedian_girls = statistic(sample_girls)\nmedian_boys = statistic(sample_boys)\nobserved_diff = median_girls - median_boys\nMonte Carlo permutation test 1\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(\"Monte Carlo permutation test 1\")\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\n\nMonte Carlo permutation test 1\nobserved difference: -0.314\np-value (one-tailed): 0.5450\nMonte Carlo permutation test 2\nN_permutations = 1000\n# combine all values in one array\nall_data = np.concatenate([sample_girls, sample_boys])\n# create an array to store the differences\ndiffs = np.empty(N_permutations-1)\n\nfor i in range(N_permutations - 1):    # this \"minus 1\" will be explained later\n    # permute the labels\n    permuted = np.random.permutation(all_data)\n    new_girls = permuted[:N_girls]  # first 14 values are girls\n    new_boys = permuted[N_girls:]   # remaining values are boys\n    diffs[i] = statistic(new_girls) - statistic(new_boys)\n# add the observed difference to the array of differences\ndiffs = np.append(diffs, observed_diff)\n\np_value = np.mean(diffs &gt;= observed_diff)\n# two-tailed p-value\n# p_value = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\nprint(\"Monte Carlo permutation test 2\")\nprint(f\"observed difference: {observed_diff:.3f}\")\nprint(f\"p-value (one-tailed): {p_value:.4f}\")\n\n\nMonte Carlo permutation test 2\nobserved difference: -0.314\np-value (one-tailed): 0.5340\nAs you can see, the p-value in not exactly the same, but the difference is negligible. This is because both times we sampled 1000 permutations that are representative of the full distribution of the test statistic under the null hypothesis.\nOne more thing. The example above with 10 boys and 14 girls is usually considered small. It is often the case that one has a lot more samples, and the number of permutations can be astronomically large, much much larger than two million.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>exact vs. Monte Carlo permutation tests</span>"
    ]
  },
  {
    "objectID": "permutation/exact-vs-montecarlo.html#exact-permutation-test",
    "href": "permutation/exact-vs-montecarlo.html#exact-permutation-test",
    "title": "11  exact vs. Monte Carlo permutation tests",
    "section": "11.2 exact permutation test",
    "text": "11.2 exact permutation test\nIf the total number of permutations is small, we can compute the exact p-value by sampling from the full distribution of the test statistic under the null hypothesis. That is to say, we compute the test statistic for every possible permutation of the data.\nIf we had height measurements of 7 boys and 6 girls, the total number of permutations is:\n\n\\binom{13}{7} = 1716\n\nAny computer can easily handle this number of permutations. How to do it in practice? We will use the itertools.combinations function.\n\n\nShow the code\nimport numpy as np\nfrom itertools import combinations\n\n#| code-summary: \"generate data\"\nN_girls = 6\nN_boys = 7\n# set scipy seed for reproducibility\nnp.random.seed(314)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\n\ncombined = np.concatenate([sample_girls, sample_boys])\nn_total = len(combined)\n\n# observed difference in means\nobserved_diff = np.median(sample_girls) - np.median(sample_boys)\n\n# generate all combinations of indices for group \"girls\"\nindices = np.arange(n_total)\nall_combos = list(combinations(indices, N_girls))\n\n# compute all permutations\ndiffs = []\nfor idx_a in all_combos:\n    mask = np.zeros(n_total, dtype=bool)\n    mask[list(idx_a)] = True\n    sample_g = combined[mask]\n    sample_b = combined[~mask]\n    diffs.append(np.median(sample_g) - np.median(sample_b))\n\ndiffs = np.array(diffs)\n\n# exact one-tailed p-value\np_value = np.mean(diffs &gt;= observed_diff)\nprint(f\"Observed difference: {observed_diff:.3f} cm\")\nprint(f\"Exact p-value (one-tailed): {p_value:.4f}\")\nprint(f\"Total permutations: {len(diffs)}\")\n\n\nObserved difference: 7.620 cm\nExact p-value (one-tailed): 0.0944\nTotal permutations: 1716\n\n\nAttention!\nIf you read the documentation of the itertools library, you might be tempted to use itertools.permutations instead of itertools.combinations.\nDon’t do that.\nAlthough we are conductiong a permutation test, we are not interested in the order of the samples, and that is what the permutations cares about. For instance, if we have 10 people called\n[Alice, Bob, Charlie, David, Eve, Frank, Grace, Heidi, Ivan, Judy]\nand we want to randomly assign the label “girl” to 4 of them, we do not care about the order in which we assign the labels. We just want to know which 4 people are assigned the label “girl”. The permutation function does care about the order, and that is why we should not use it. Instead, we use the combinations function, which return all possible combinations of the data, without regard to the order of selection.",
    "crumbs": [
      "permutation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>exact vs. Monte Carlo permutation tests</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html",
    "href": "regression/geometry-of-regression.html",
    "title": "12  the geometry of regression",
    "section": "",
    "text": "12.1 a very simple example\nIt’s almost always best to start with a simple and concrete example.\nGoal: We wish to find the best straight line that describes the following data points:\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nimport scipy\n\n# %matplotlib widget\ndefine and plot the simple problem\nx = np.array([1, 2, 3])\ny = np.array([2, 2, 6])\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.scatter(x, y, label='data', facecolors='black', edgecolors='black')\n# linear regression\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\nx_domain = np.linspace(0, 4, 101)\nax.plot(x_domain, intercept + slope * x_domain, color='black', label='best line')\n\nax.legend(loc='upper left', fontsize=14, frameon=False)\nax.set(xlim=(0, 4),\n       ylim=(0, 7),\n       xticks=np.arange(0, 5, 1),\n       yticks=np.arange(0, 9, 1),\n       xlabel='X-axis',\n       ylabel='Y-axis');",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#formalizing-the-problem",
    "href": "regression/geometry-of-regression.html#formalizing-the-problem",
    "title": "12  the geometry of regression",
    "section": "12.2 formalizing the problem",
    "text": "12.2 formalizing the problem\nLet’s translate this problem into the language of linear algebra.\nThe independent variable x is the column vector\n\nx=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}\n\nand the dependent variable y is the column vector\n\ny=\n\\begin{pmatrix}\n2 \\\\ 2 \\\\ 6\n\\end{pmatrix}.\n\nBecause we are looking for a straight line, we can express the relationship between x and y as\n\n\\tilde{y} = \\beta_0 + \\beta_1 x.\n\nHere we introduced the notation \\tilde{y} to denote the predicted values of y based on the linear model. It is different from the actual values of y because the straight line usually does not pass exactly on top of y.\nThe parameter \\beta_0 is the intercept and \\beta_1 is the slope of the line.\nWhich values of \\beta_0,\\beta_1 will give us the very best line?",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#higher-dimensions",
    "href": "regression/geometry-of-regression.html#higher-dimensions",
    "title": "12  the geometry of regression",
    "section": "12.3 higher dimensions",
    "text": "12.3 higher dimensions\nIt is very informative to think about this problem not as a scatter plot in the X-Y plane, but as taking place in a higher-dimensional space. Because we have three data points, we can think of the problem in a three-dimensional space. We want to explain the vector y as a linear combination of the vector x and a constant vector (this is what our linear model states).\nIn three dimensions, our building blocks are the vectors c, the intercept, and x, the data points.\n\nc=\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\qquad\nx=\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{pmatrix}.\n\nWe can combine these c and x as column vectors in a matrix called design matrix:\n\nX=\n\\begin{pmatrix}\n1 & x_0 \\\\\n| & | \\\\\n1 & x_i \\\\\n| & | \\\\\n1 & x_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n| & | \\\\\n1 & x \\\\\n| & |\n\\end{pmatrix}\n\nWhy is this convenient? Because now the linear combination of \\vec{1} and x can be expressed as a matrix multiplication:\n\n\\begin{pmatrix}\n\\hat{y}_0 \\\\\n\\hat{y}_1 \\\\\n\\hat{y}_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & x_0 \\\\\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1\\cdot\\beta_0 + x_0\\cdot\\beta_1 \\\\\n1\\cdot\\beta_0 + x_1\\cdot\\beta_1 \\\\\n1\\cdot\\beta_0 + x_2\\cdot\\beta_1\n\\end{pmatrix}\n\nIn short, the linear combination of our two building blocks yields a prediction vector \\hat{y}:\n\n\\hat{y} = X \\beta,\n\nwhere \\beta is the column vector (\\beta_0, \\beta_1)^T.\nThis prediction vector \\hat{y} lies on a plane in the 3d space, it cannot be anywhere in this 3d space. Mathematically, we say that the vector \\hat{y} is in the subspace spanned by the columns of the design matrix X.\nIt will be extremely improbable that the vector y will also lie on this plane, so we will have to find the best prediction \\hat{y} that lies on this plane. Geometrically, our goal is to find the point \\hat{y} on the plane that is closest to the point y in the 3d space.\n\nWhen the distance r=y-\\hat{y} is minimized, the vector r is orthogonal to the plane spanned by the columns of the design matrix X.\nWe call this vector r the residual vector.\nThe residual is orthogonal to each of the columns of X, that is, \\vec{1}\\cdot r=0 and x\\cdot r=0.\n\nI tried to summarize all the above in the 3d image below. This is, for me, the geometry of regression. If you have that in your head, you’ll never forget it.\n\nAnother angle of the image above. This time, because the view direction is within the plane, we see that the residual vector r is orthogonal to the plane spanned by the columns of the design matrix X. \nFor a fully interactive version, see this Geogebra applet.\n\n\nTaking advantage of the matrix notation, we can express the orthogonality condition as follows:\n\n\\begin{pmatrix}\n- & 1 & - \\\\\n- & x & -\n\\end{pmatrix} r =\nX^T r =\n0\n\nLet’s substitute r = y - \\hat{y} = y - X\\beta into the equation above.\n\nX^T(y - X\\beta) = 0\n\nDistributing yields\n\nX^Ty - X^TX\\beta = 0,\n\nand then\n\nX^TX\\beta = X^Ty.\n\nWe need to solve this equation for \\beta, so we left-multiply both sides by the inverse of X^TX,\n\n\\beta = (X^TX)^{-1}X^Ty.\n\nThat’s it. We did it. Given the data points x and y, we can compute the parameters \\beta_0 and \\beta_1 that bring \\hat{y} as close as possible to y. These parameters are the best fit of the straight line to the data points.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#overdetermined-system",
    "href": "regression/geometry-of-regression.html#overdetermined-system",
    "title": "12  the geometry of regression",
    "section": "12.4 overdetermined system",
    "text": "12.4 overdetermined system\nThe design matrix X is a tall and skinny matrix, meaning that it has more rows (n) than columns (m). This is called an overdetermined system, because we have more equations (rows) than unknowns (columns), so we have no hope in finding an exact solution \\beta.\nThis is to say that, almost certainly, the vector y does not lie on the plane spanned by the columns of the design matrix X. No combination of the parameters \\beta will yield a vector \\hat{y} that is exactly equal to y.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#least-squares",
    "href": "regression/geometry-of-regression.html#least-squares",
    "title": "12  the geometry of regression",
    "section": "12.5 least squares",
    "text": "12.5 least squares\nThe method above for finding the best parameters \\beta is called least squares. The name comes from the fact that we are trying to minimize the length of the residual vector\n\nr = y - \\hat{y}.\n\nThe length of the residual is given by the Euclidean norm (or L^2 norm), which is a direct generalization of the Pythagorean theorem for many dimensions.\n\\begin{align}\n\\Vert r\\Vert^2 &= \\Vert y - \\hat{y}\\Vert^2  \\\\\n&= (y_0 - \\hat{y}_0)^2 + (y_1 - \\hat{y}_1)^2 + \\cdots +  (y_{n-1} - \\hat{y}_{n-1})^2 \\\\\n&= r_0^2 + r_1^2 + \\cdots + r_{n-1}^2\n\\end{align}\nThe length (squared) of the residual vector is the sum of the squares of all residuals. The best parameters \\beta are those that yield the least squares, thus the name.\n\n\ndefine and plot the simple problem\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.scatter(x, y, label='data', facecolors='black', edgecolors='black')\nx_domain = np.linspace(0, 4, 101)\nax.plot(x_domain, intercept + slope * x_domain, color='black', label='best line')\n\ndef linear(x, slope, intercept):\n    return intercept + slope * x\n\nfor i, xi in enumerate(x):\n    ax.plot([xi, xi],\n            [y[i], linear(xi, slope, intercept)],\n            color='black', linestyle='--', linewidth=0.5,\n            label='residuals' if i == 0 else None)\n    \nax.legend(loc='upper left', fontsize=14, frameon=False) \nax.set(xlim=(0, 4),\n       ylim=(0, 7),\n       xticks=np.arange(0, 5, 1),\n       yticks=np.arange(0, 9, 1),\n       xlabel='X-axis',\n       ylabel='Y-axis');",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/geometry-of-regression.html#many-more-dimensions",
    "href": "regression/geometry-of-regression.html#many-more-dimensions",
    "title": "12  the geometry of regression",
    "section": "12.6 many more dimensions",
    "text": "12.6 many more dimensions\nThe concrete example here dealt with only three data points, therefore we could visualize the problem in a three-dimensional space. However, the same reasoning applies to any number of data points and any number of independent variables.\n\nany number of data points: we call the number of data points n, and that makes y be a vector in an n-dimensional space.\nany number of independent variables: we calculated a regression for a straight line, and thus we had only two building blocks, the intercept \\vec{1} and the independent variable x. However, we can have any number of independent variables, say m of them. For example, we might want to predict the data using a polynomial of degree m, or we might have any arbitrary m functions that we wish to use: \\exp(x), \\tanh(x^2), whatever we want. All this will work as long as the parameters \\beta multiply these building blocks. That’s the topic of the next chapter.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>the geometry of regression</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html",
    "href": "regression/least-squares.html",
    "title": "13  least squares",
    "section": "",
    "text": "13.1 ordinary least squares (OLS) regression\nLet’s go over a few things that appear in this notebook, statsmodels, Ordinary Least Squares\nimport libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nnp.random.seed(9876789)",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#polynomial-regression",
    "href": "regression/least-squares.html#polynomial-regression",
    "title": "13  least squares",
    "section": "13.2 polynomial regression",
    "text": "13.2 polynomial regression\nLet’s start with a simple polynomial regression example. We will start by generating synthetic data for a quadratic equation plus some noise.\n\n# number of points\nnsample = 100\n# create independent variable x\nx = np.linspace(0, 10, 100)\n# create design matrix with linear and quadratic terms\nX = np.column_stack((x, x ** 2))\n# create coefficients array\nbeta = np.array([5, -2, 0.5])\n# create random error term\ne = np.random.normal(size=nsample)\n\nx and e can be understood as column vectors of length n, while X and \\beta are:\n\nX =\n\\begin{pmatrix}\nx_0 & x_0^2 \\\\\n| & | \\\\\nx_i & x_i^2 \\\\\n| & | \\\\\nx_n & x_n^2 \\\\\n\\end{pmatrix}, \\qquad\n\\beta =\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{pmatrix}.\n\nOops, there is no intercept column \\vec{1} in the design matrix X. Let’s add it:\n\nX = sm.add_constant(X)\nprint(X[:5, :])  # print first 5 rows of design matrix\n\n[[1.         0.         0.        ]\n [1.         0.1010101  0.01020304]\n [1.         0.2020202  0.04081216]\n [1.         0.3030303  0.09182736]\n [1.         0.4040404  0.16324865]]\n\n\nThis add_constant function is smart, it has as default a prepend=True argument, meaning that the intercept is added as the first column, and a has_constant='skip' argument, meaning that it will not add a constant if one is already present in the matrix.\nThe matrix X is now a design matrix for a polynomial regression of degree 2. \nX =\n\\begin{pmatrix}\n1 & x_0 & x_0^2 \\\\\n| & | & | \\\\\n1 & x_i & x_i^2 \\\\\n| & | & | \\\\\n1 & x_n & x_n^2 \\\\\n\\end{pmatrix}\n\nWe now put everything together in the following equation:\n\ny = X \\beta + e\n\nThis creates the dependend variable y as a linear combination of the independent variables in X and the coefficients in \\beta, plus an error term e.\n\ny = np.dot(X, beta) + e\n\nLet’s visualize this:\n\n\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.set(xlabel='x',\n       ylabel='y',\n       title='Simulated Data with Linear and Quadratic Terms'\n       );",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#solving-the-hard-way",
    "href": "regression/least-squares.html#solving-the-hard-way",
    "title": "13  least squares",
    "section": "13.3 solving the “hard way”",
    "text": "13.3 solving the “hard way”\nI’m going to do something that nobody does. I will use the formula we derived in the previous chapter to find the coefficients \\beta of the polynomial regression.\n\n\\beta = (X^TX)^{-1}X^Ty.\n\nTranslating this into code, and keeping in mind that matrix multiplication in Python is done with the @ operator, we get:\n\nbeta_opt = np.linalg.inv(X.T@X)@X.T@y\nprint(f\"beta = {beta_opt}\")\n\nbeta = [ 5.34233516 -2.14024948  0.51025357]\n\n\nThat’s it. We did it (again).\nLet’s take a look at the matrix X^TX. Because X is a tall and skinny matrix of shape (n, 3), the matrix X^T is a wide and short matrix of shape (3, n). This is because we have many more data points n than the number of predictors (\\vec{1},x,x^2), which is of course equal to the number of coefficients (\\beta_0,\\beta_1,\\beta_2).\n\nprint(X.T@X)\n\n[[1.00000000e+02 5.00000000e+02 3.35016835e+03]\n [5.00000000e+02 3.35016835e+03 2.52525253e+04]\n [3.35016835e+03 2.52525253e+04 2.03033670e+05]]\n\n\nWhen we multiply the matrices X^T_{3\\times n} and X_{n\\times 3}, we get a square matrix of shape (3, 3), because the inner dimensions match (the number of columns in X^T is equal to the number of rows in X). The product X^TX is a square matrix of shape (3, 3), which is quite easy to invert. If it were the other way around (X\\,X^T), we would have a matrix of shape (n, n), which is much harder to invert, especially if n is large. Lucky us.\nNow let’s see if the parameters we found are any good.\n\nprint(\"beta parameters used to generate data:\")\nprint(beta)\nprint(\"beta parameters estimated from data:\")\nprint(beta_opt)\n\nbeta parameters used to generate data:\n[ 5.  -2.   0.5]\nbeta parameters estimated from data:\n[ 5.34233516 -2.14024948  0.51025357]\n\n\nPretty good, right? Now let’s see the best fit polynomial on the graph.\n\n\nplot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.plot(x, np.dot(X, beta_opt), color='red', label='fitted line')\nax.legend(frameon=False)\nax.set(xlabel='x',\n       ylabel='y',\n       title='Simulated Data with Linear and Quadratic Terms'\n       );\n\n\n\n\n\n\n\n\n\nWhy did I call it the “hard way”? Because these operations are so common that of course there are libraries that do this for us. We don’t need to remember the equation, we can just use, for example, statsmodels library’s OLS function, which does exactly this. Let’s see how it works.\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nNow we can compare the results of our manual calculation with the results from statsmodels. We should get the same coefficients, and we do.\n\nprint(\"beta parameters used to generate data:\")\nprint(beta)\nprint(\"beta parameters from our calculation:\")\nprint(beta_opt)\nprint(\"beta parameters from statsmodels:\")\nprint(results.params)\n\nbeta parameters used to generate data:\n[ 5.  -2.   0.5]\nbeta parameters from our calculation:\n[ 5.34233516 -2.14024948  0.51025357]\nbeta parameters from statsmodels:\n[ 5.34233516 -2.14024948  0.51025357]",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#statmodels.ols-and-the-summary",
    "href": "regression/least-squares.html#statmodels.ols-and-the-summary",
    "title": "13  least squares",
    "section": "13.4 statmodels.OLS and the summary",
    "text": "13.4 statmodels.OLS and the summary\nStatmodels provides us a lot more information than just the coefficients. Let’s take a look at the summary of the OLS regression.\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.988\nModel:                            OLS   Adj. R-squared:                  0.988\nMethod:                 Least Squares   F-statistic:                     3965.\nDate:                Mon, 23 Jun 2025   Prob (F-statistic):           9.77e-94\nTime:                        12:50:31   Log-Likelihood:                -146.51\nNo. Observations:                 100   AIC:                             299.0\nDf Residuals:                      97   BIC:                             306.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.3423      0.313     17.083      0.000       4.722       5.963\nx1            -2.1402      0.145    -14.808      0.000      -2.427      -1.853\nx2             0.5103      0.014     36.484      0.000       0.482       0.538\n==============================================================================\nOmnibus:                        2.042   Durbin-Watson:                   2.274\nProb(Omnibus):                  0.360   Jarque-Bera (JB):                1.875\nSkew:                           0.234   Prob(JB):                        0.392\nKurtosis:                       2.519   Cond. No.                         144.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nI won’t go into the details of the summary, but I encourage you to take a look at it and see if you can make sense of it.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#r-squared",
    "href": "regression/least-squares.html#r-squared",
    "title": "13  least squares",
    "section": "13.5 R-squared",
    "text": "13.5 R-squared\nR-squared is a measure of how well the model fits the data. It is defined as the proportion of the variance in the dependent variable that is predictable from the independent variables. It can be computed as follows:\n\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n\nwhere SS_{res} and SS_{tot} are defined as follows:\n\\begin{align*}\nSS_{res} &= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\nSS_{tot} &= \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\end{align*}\nThe letters SS mean “sum of squares”, and \\bar{y} is the mean of the dependent variable. Let’s compute it manually, and then compare it with the value from the statsmodels summary.\n\ny_hat = np.dot(X, beta_opt)\nSS_res = np.sum((y - y_hat) ** 2)\nSS_tot = np.sum((y - np.mean(y)) ** 2)\nR2 = 1 - (SS_res / SS_tot)\nprint(\"R-squared (manual calculation): \", R2)\nprint(\"R-squared (from statsmodels): \", results.rsquared)\n\nR-squared (manual calculation):  0.9879144521349076\nR-squared (from statsmodels):  0.9879144521349076\n\n\nThis high R^2 value tells us that the model explains a very large proportion of the variance in the dependent variable.\nHow can we know that the variance has anything to do with the R^2? If we divide both the SS_{res} and SS_{tot} by n-1, we get the sample variances of the residuals and the dependent variable, respectively.\n\\begin{align*}\ns^2_{res} = \\frac{SS_{res}}{n-1} &= \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-1} \\\\\ns^2_{tot} = \\frac{SS_{tot}}{n-1} &= \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n-1}\n\\end{align*}\nThen the R^2 can then be expressed as: \nR^2 = 1 - \\frac{s^2_{res}}{s^2_{tot}}.\n\nI prefer this equation over the first, because it makes it clear that R^2 is the ratio of the variances, which is a more intuitive way to think about it.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/least-squares.html#any-function-will-do",
    "href": "regression/least-squares.html#any-function-will-do",
    "title": "13  least squares",
    "section": "13.6 any function will do",
    "text": "13.6 any function will do\nThe formula we derived the the previous chapter works for predictors (independent variables) of any kind, not only polynomials. The formula will work as long as the parameters \\beta are linear in the predictors. For exammple, we could have a nonlinear function like this:\n\ny = \\beta_0 + \\beta_1 e^x + \\beta_2 \\sin(x^2)\n\nbecause each beta multiplies a predictor. On the other hand, the following function would not work, because the parameters are not linear in the predictors:\n\ny = \\beta_0 + e^{\\beta_1 x} + \\sin(\\beta_2 x^2)\n\nLet’s this this in action, I’ll use the same example provided by statsmodels documentation, which is a nonlinear function of the form:\n\ny = \\beta_0 x + \\beta_1 \\sin(x) + \\beta_2(x - 5)^2 + \\beta_3\n\n\nnsample = 50\nsig = 0.5\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((\n    x,\n    np.sin(x),\n    (x - 5) ** 2,\n    np.ones(nsample)\n    ))\nbeta = [0.5, 0.5, -0.02, 5.0]\ny_true = np.dot(X, beta)\ny = y_true + sig * np.random.normal(size=nsample)\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.set(xlabel='x',\n       ylabel='y',\n       )\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nresult = sm.OLS(y, X).fit()\nprint(result.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.933\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     211.8\nDate:                Mon, 23 Jun 2025   Prob (F-statistic):           6.30e-27\nTime:                        12:51:08   Log-Likelihood:                -34.438\nNo. Observations:                  50   AIC:                             76.88\nDf Residuals:                      46   BIC:                             84.52\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.4687      0.026     17.751      0.000       0.416       0.522\nx2             0.4836      0.104      4.659      0.000       0.275       0.693\nx3            -0.0174      0.002     -7.507      0.000      -0.022      -0.013\nconst          5.2058      0.171     30.405      0.000       4.861       5.550\n==============================================================================\nOmnibus:                        0.655   Durbin-Watson:                   2.896\nProb(Omnibus):                  0.721   Jarque-Bera (JB):                0.360\nSkew:                           0.207   Prob(JB):                        0.835\nKurtosis:                       3.026   Cond. No.                         221.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote something interesting: in our design matrix X, we encoded the intercept column as the last column, there is no reason why it should be the first column (although first column is a common choice). The function ‘statsmodels.OLS’ sees this, and when we print the summary, it will show the intercept as the last coefficient. Nice!\nLet’s see a graph of the data and the fitted model.\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data', facecolors='none', edgecolors='black', alpha=0.5)\nax.plot(x, np.dot(X, result.params), color='red', label='fitted line')\nax.legend(frameon=False)\nax.set(xlabel='x',\n       ylabel='y',\n       )",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>least squares</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html",
    "href": "regression/equivalence.html",
    "title": "14  equivalence",
    "section": "",
    "text": "14.1 orthogonality\nIn the context of linear regression, the orthogonality condition states that the residual vector r is orthogonal to the column space of the design matrix X:\nX^T r = 0\nLet’s substitute r = y - \\hat{y} = y - X\\beta into the equation above.\nX^T(y - X\\beta) = 0\nDistributing yields\nX^Ty - X^TX\\beta = 0,\nand then\nX^TX\\beta = X^Ty.\nWe need to solve this equation for \\beta, so we left-multiply both sides by the inverse of X^TX,\n\\beta = (X^TX)^{-1}X^Ty.\nWe already did that in a previous chapter. Now let’s get exactly the same result using another approach.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#optimization",
    "href": "regression/equivalence.html#optimization",
    "title": "14  equivalence",
    "section": "14.2 optimization",
    "text": "14.2 optimization\nWe wish to minimize the sum of squared errors, which is given by\n\nL = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - X_{ij}\\beta_j)^2.\n\nI’m not exactly sure, but I think we call this L because of the lagrangian function. Later on when we talk about regularization, we will see that the lagrangian function can be constrained by lagrange multipliers. In any case, let’s keep going with the optimization.\nIt is useful to express L in matrix notation. The sum of squared errors can be thought as the dot product of the residual vector with itself:\n\\begin{align*}\nL &= (y - X\\beta)^T(y - X\\beta) \\\\\n&= y^Ty - y^TX\\beta - \\beta^TX^Ty + \\beta^TX^TX\\beta,\n\\end{align*}\nwhere we used the following properties of matrix algebra: 1. The dot product of a vector with itself is the sum of the squares of its components, i.e., a^Ta = \\sum_{i=1}^n a_i^2. We used this to express the sum of squared errors in matrix notation. 2. The dot product is bilinear, i.e., (a - b)^T(c - d) = a^Tc - a^Td - b^Tc + b^Td. We used this to expand the expression for the sum of squared errors. 3. The transpose of a product of matrices is the product of their transposes in reverse order, i.e., (AB)^T = B^TA^T. We used this to compute (X\\beta)^T.\nLet’s use one more property to join the two middle terms, - y^TX\\beta - \\beta^TX^Ty:\n\nThe dot product is symmetric, i.e., a^Tb = b^Ta. This is evident we express the dot product as a summation:\n\n\n  a^Tb = \\sum_{i=1}^n a_i b_i = \\sum_{i=1}^n b_i a_i = b^Ta.\n  \nJoining the two middle terms results in the following L:\n\nL = y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta,\n\n\nThe set of parameters \\beta that minimizes L is that which satisfies the extreme condition of the function L (either maximum or minimum). This means that the gradient of L with respect to \\beta must be zero:\n\n\\frac{\\partial L}{\\partial \\beta} = 0.\n\nLet’s plug in the expression for L:\n\n\\frac{\\partial}{\\partial \\beta} \\left( y^Ty - 2\\beta^TX^Ty + \\beta^TX^TX\\beta \\right) = 0\n\nThe quantity L is a scalar, and also each of the three terms that we are differentiating is a scalar. Let’s differentiate them one by one.\n\nThe first term, y^Ty, is a constant with respect to \\beta, so its derivative is zero.\n\n\nThe second term\n\n\\frac{\\partial}{\\partial \\beta} \\left( - 2\\beta^TX^Ty \\right) = - 2\\frac{\\partial}{\\partial \\beta} \\left( \\beta^TX^Ty \\right).\n\nThe quantity being differentiated is a scalar, it’s the product of the row vector \\beta^T and the column vector X^Ty. Right now we don’t care much about X^Ty, it could be any column vector, so let’s call it c. The derivative of dot product \\beta^T c with respect to a specific element \\beta_k can be written explicitly as\n\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\beta_i c_i \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\beta_1 c_1 + \\beta_2 c_2 + \\ldots + \\beta_p c_p \\right) =  c_k.\n\nWhatever value for the index k we choose, the derivative will be zero for all indices except for k, and that explain the result above.\nSince the gradient \\nabla_{\\beta}(\\beta^T c) is a vector,\n\n\\nabla_{\\beta}(\\beta^T c) = \\frac{\\partial (\\beta^T c)}{\\partial \\beta} =\n\\begin{pmatrix}\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_1} \\\\\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_2} \\\\\n\\vdots \\\\\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta_p}\n\\end{pmatrix}\n\nand we have just figured out what each component is, we can write the solution as\n\n\\frac{\\partial (\\beta^T c)}{\\partial \\beta} =\n\\begin{pmatrix}\nc_1 \\\\ c_2 \\\\ \\vdots \\\\ c_p\n\\end{pmatrix}\n= c\n= X^Ty.\n\nSo the derivative of the second term is simply -2 X^Ty.\n\n\nTo solve the derivatie of the third term, \\beta^TX^TX\\beta, we use the following property:\n\n\\frac{\\partial}{\\partial \\beta} \\left( \\beta^T A \\beta \\right) = 2A\\beta,\n\nwhen A is a symmetric matrix. In our case, A = X^TX, which is symmetric because (X^TX)^T = X^T(X^T)^T = X^TX. Therefore we have:\n\n\\frac{\\partial}{\\partial \\beta} \\left( \\beta^T X^TX \\beta \\right) = 2X^TX\\beta,\n\nand that is the derivative of the third term.\nWe still have to prove why the derivative of \\beta^T A \\beta is 2A\\beta. First, let’s use the summation notation to express the term \\beta^T A \\beta:\n\n\\beta^T A \\beta = \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j.\n\nNow, let’s differentiate this expression with respect to \\beta_k, using the chain rule:\n\n\\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j \\right) = \\sum_{i=1}^p A_{ik} \\beta_i + \\sum_{j=1}^p \\beta_j A_{kj}.\n\nUsing the symmetry of A (A_{ij} = A_{ji}):\n\n\\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^p \\sum_{j=1}^p \\beta_i A_{ij} \\beta_j \\right) = 2 \\sum_{i=1}^p A_{ik} \\beta_i = 2(A\\beta)_k.\n\nThis is the element k of the vector 2A\\beta. Since this is true for any index k, we can write the gradient as\n\n\\nabla_{\\beta}(\\beta^T A \\beta) = 2A\\beta.\n\n\nNow that we have the derivatives of all three terms, we can write the gradient of L:\n\n\\frac{\\partial L}{\\partial \\beta} = 0 - 2X^Ty + 2X^TX\\beta = 0.\n\nRearranging…\n\nX^TX\\beta = X^Ty\n\n…and solving for \\beta:\n\n\\beta = (X^TX)^{-1}X^Ty",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#discussion",
    "href": "regression/equivalence.html#discussion",
    "title": "14  equivalence",
    "section": "14.3 discussion",
    "text": "14.3 discussion\nUsing two completely different approaches, we arrived at the same result for the least squares solution:\n\n\\beta = (X^TX)^{-1}X^Ty\n.\n\nApproach 1: We used the orthogonality condition, which states that the residual vector is orthogonal to the column space of the design matrix.\nApproach 2: We applied the optimization method, minimizing the sum of squared errors—which corresponds to minimizing the squared length of the residual vector.\n\nThere is a deep connection here. The requirement that the residual vector is orthogonal to the column space of the design matrix is equivalent to minimizing the sum of squared errors. We can see this visually: if the projection of the response vector y onto the column space of X were anywhere else, the residual vector would be not only not orthogonal, but also longer!\n\n\n\\text{orthogonality} \\iff \\text{optimization}\n\n\nThis result even tranfers to other contexts, as long as there is a vector space with a well defined inner product (dot product) and an orthogonal basis. In these cases, the least squares solution can be interpreted as finding the projection of a vector onto a subspace spanned by an orthogonal basis. Some examples include:\n\nFourier series: the Fourier coefficients are the least squares solution to the problem of approximating a function by a sum of sines and cosines, where these functions are an orthogonal basis.\nSVD (Singular Value Decomposition): A matrix can be decomposed into orthogonal matrices, and the singular values can be interpreted as the least squares solution to the problem of approximating a matrix by a sum of outer products of orthogonal vectors.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/equivalence.html#other-properties",
    "href": "regression/equivalence.html#other-properties",
    "title": "14  equivalence",
    "section": "14.4 other properties",
    "text": "14.4 other properties\n\n14.4.1 the sum of residuals is zero\nAs long as the model includes an intercept term, the sum of the residuals is zero. The model could be anything, not necessarily linear regression. Let’s prove this property.\n\n\\hat{y}_i = \\beta_0 + f(x_i; \\beta_1, \\beta_2, \\ldots, \\beta_p)\n\nThe Ordinary Least Squares (OLS) estimates of the parameters \\beta minimize the sum of squared residuals L. The equation for \\beta_0 reads:\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_0} &=\n\\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right)^2 \\\\\n&=\n\\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^n \\left[y_i - \\beta_0 - f(x_i; \\beta_1, \\beta_2, \\ldots, \\beta_p) \\right]^2 \\\\\n&= -2 \\sum_{i=1}^n \\left[y_i - \\beta_0 - f(x_i; \\beta_1, \\beta_2, \\ldots, \\beta_p) \\right] \\\\\n&= -2 \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right) = 0\n\\end{align*}\nFrom the last line it follows that the sum of the residuals is zero.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>equivalence</span>"
    ]
  },
  {
    "objectID": "regression/partitioning.html",
    "href": "regression/partitioning.html",
    "title": "15  partitioning of the sum of squares",
    "section": "",
    "text": "15.1 high-dimensional Pythagorean theorem\nWhy is this equation true? It’s easiest to understand this equation by thinking of it as a high-dimensional version of the Pythagorean theorem:\n\\lVert T \\rVert ^2 = \\lVert M \\rVert ^2 + \\lVert E \\rVert ^2,\nwhere:\nI omitted the subscript i to emphasize that these are vectors, not scalars.\nMake sure you understand the figure below, already presented in a previous chapter.\nThe vector r=E in black is the residual or error vector. It is orthogonal to the subspace spanned by the column vector of the design matrix, represented in this image by the blue plane.\nThe vector M is not shown, but it necessarily lies in the blue plane. How do we know that? Because the predicted values \\hat{y} are a linear combination of the columns of the design matrix, and therefore \\hat{y} lies in the column space of the design matrix. Since \\bar{y} is a constant vector (a multiple of the vector of ones), it also lies in the column space of the design matrix. Therefore, M = \\hat{y} - \\bar{y} lies in the column space of the design matrix.\nFrom the above, we can already conclude that E is orthogonal to M. These are the two legs of a right triangle. We now need a hypotenuse. The hypotenuse is the total vector T = y - \\bar{y}, which is the sum of the other two vectors:\nT = M + E.\nThis is easy to see by substituting the definitions of M and E:\nT = y - \\bar{y} = (\\hat{y} - \\bar{y}) + (y - \\hat{y}) = M + E.\nShow the code\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_aspect('equal', adjustable='box')\nax.annotate('', xy=(2, 0), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', width=2, headwidth=10, headlength=10))\nax.annotate('', xy=(2, 1), xytext=(2, 0), arrowprops=dict(facecolor='blue', edgecolor='blue', width=2, headwidth=10, headlength=10))\nax.annotate('', xy=(2, 1), xytext=(0, 0), arrowprops=dict(facecolor='red', edgecolor='red', width=2, headwidth=10, headlength=10))\n\nax.text(1.0, 0.0, 'M', fontsize=20, ha='center', va='bottom')\nax.text(2.03, 0.5, 'E', fontsize=20, ha='left', va='center')\nax.text(1.0, 0.55, 'T', fontsize=20, ha='center', va='bottom')\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\nax.set_xlim(0, 2)\nax.set_ylim(0, 1);",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>partitioning of the sum of squares</span>"
    ]
  },
  {
    "objectID": "regression/partitioning.html#high-dimensional-pythagorean-theorem",
    "href": "regression/partitioning.html#high-dimensional-pythagorean-theorem",
    "title": "15  partitioning of the sum of squares",
    "section": "",
    "text": "T = y - \\bar{y} is the total vector, the vector of deviations of the observed values from their mean;\nM = \\hat{y} - \\bar{y} is the model vector, the vector of deviations of the predicted values from the mean of the observed values;\nE = y - \\hat{y} is the error vector, the vector of residuals, or deviations of the observed values from the predicted values.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>partitioning of the sum of squares</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html",
    "href": "regression/mixed-model.html",
    "title": "16  linear mixed effect model",
    "section": "",
    "text": "16.1 practical example\nWe are given a dataset of annual income (independent variable) and years of education (independent variable) for individuals that studied different majors in university (categorical variable). We want to predict the annual income based on years of education and the major studied, including an interaction term between years of education and major. One more thing: each individual appears more than once in the dataset, so we can assume that there is a random effect associated with each individual.\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.formula.api as smf\ngenerate synthetic data\n# set seed for reproducibility\nnp.random.seed(42)\n# define parameters\nmajors = ['Juggling', 'Magic', 'Dragon Taming']\nn_individuals = 90  # 30 per major\nyears_per_person = np.random.randint(1, 5, size=n_individuals)  # 1 to 4 time points\n\n# assign majors and person IDs\nperson_ids = [f'P{i+1:03d}' for i in range(n_individuals)]\nmajor_assignment = np.repeat(majors, n_individuals // len(majors))\n\n# simulate data\nrecords = []\nfor i, pid in enumerate(person_ids):\n    major = major_assignment[i]\n    n_years = years_per_person[i]\n    years = np.sort(np.random.choice(np.arange(1, 21), size=n_years, replace=False))\n    \n    # base intercept and slope by major\n    if major == 'Juggling':\n        base_income = 25_000\n        growth = 800\n    elif major == 'Magic':\n        base_income = 20_000\n        growth = 1500\n    elif major == 'Dragon Taming':\n        base_income = 30_000\n        growth = 400  # slower growth\n    \n    # add person-specific deviation\n    personal_offset = np.random.normal(0, 5000)\n    slope_offset = np.random.normal(0, 200)\n    \n    for y in years:\n        income = base_income + personal_offset + (growth + slope_offset) * y + np.random.normal(0, 3000)\n        records.append({\n            'person': pid,\n            'major': major,\n            'years_after_grad': y,\n            'income': income\n        })\n\ndf = pd.DataFrame(records)\nLet’s take a look at the dataset. There are many data points, so we will only see 15 points in three different places.\nShow the code\nprint(df[:5])\nprint(df[90:95])\nprint(df[190:195])\n\n\n  person     major  years_after_grad        income\n0   P001  Juggling                 3  37183.719609\n1   P001  Juggling                 5  35238.112407\n2   P001  Juggling                11  37905.435001\n3   P002  Juggling                 2  27432.186391\n4   P002  Juggling                 4  30617.926804\n   person  major  years_after_grad        income\n90   P034  Magic                 1  14151.072305\n91   P034  Magic                 7  19716.656861\n92   P035  Magic                12  41056.576643\n93   P035  Magic                14  46339.987229\n94   P036  Magic                16  41981.131518\n    person          major  years_after_grad        income\n190   P072  Dragon Taming                 7  36173.437735\n191   P073  Dragon Taming                 8  33450.564557\n192   P074  Dragon Taming                 9  35276.927416\n193   P074  Dragon Taming                17  37271.203018\n194   P075  Dragon Taming                 2  31819.051946\nNow let’s see the data in a plot.\nplot income by major\nfig, ax = plt.subplots(figsize=(8, 6))\n\ngb = df.groupby('major')\nfor major, group in gb:\n    ax.scatter(group['years_after_grad'], group['income'], label=major, alpha=0.6)\n\nax.legend(title='Major', frameon=False)\nax.set(xlabel='years after graduation',\n       ylabel='income',\n       xticks=np.arange(0, 21, 5)\n       );\nThe model we will use is\ny = \\underbrace{X \\beta}_{\\text{fixed effects}} + \\underbrace{Z b}_{\\text{random effects}} + \\underbrace{\\varepsilon}_{\\text{residuals}}\nThe only new term here is Zb, the random effects, where Z is the design matrix for the random effects and b is the vector of random effects coefficients. We will discuss that a bit later. Let’s start with the fixed effects part:\nX \\beta = \\beta_0 + \\beta_1 \\cdot \\text{years} + \\beta_2 \\cdot \\text{major} + \\beta_3 \\cdot (\\text{years} \\cdot \\text{major})\nThe “years” variable is continuous, while the “major” variable is categorical. How to include categorical variables in a linear regression model? We can use dummy coding, where we create binary variables for each category of the categorical variable (except one category, which serves as the reference group). In our case, we have three majors: Juggling, Magic, and Dragon Taming. Let’s use “Juggling” as the reference group. We can create two dummy variables that function as toggles.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#practical-example",
    "href": "regression/mixed-model.html#practical-example",
    "title": "16  linear mixed effect model",
    "section": "",
    "text": "major_Magic: 1 if the major is Magic, 0 otherwise\nmajor_DragonTaming: 1 if the major is Dragon Taming, 0 otherwise",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#visualizing-categories-as-toggles",
    "href": "regression/mixed-model.html#visualizing-categories-as-toggles",
    "title": "16  linear mixed effect model",
    "section": "16.2 visualizing categories as toggles",
    "text": "16.2 visualizing categories as toggles\nIn the equation above, we have only one parameter for “major” (\\beta_2), and only one parameter for the interaction terms (\\beta_3). In reality we have more, see:\n\\begin{align*}\n\\text{income} &= \\beta_0 + \\beta_1 \\cdot \\text{years} \\\\\n&+ \\beta_2 \\cdot \\text{major\\_Magic} + \\beta_3 \\cdot \\text{major\\_DragonTaming} \\\\\n&+ \\beta_4 \\cdot (\\text{years} \\cdot \\text{major\\_Magic}) + \\beta_5 \\cdot (\\text{years} \\cdot \\text{major\\_DragonTaming}) \\\\\n&+ \\epsilon\n\\end{align*}\nThe first line represents the linear relationship between income and education of the reference group (Juggling). The second line adds the effects on the intercept of having studied Magic or Dragon Taming instead, and the third line adds the the effects on the slope of these two majors.\nLet’s see for a few data points how this works. Below, dummy variables represent the pair (major_Magic, major_DragonTaming).\n\n\n\nyears_after_grad\nmajor\nDummy variables\nincome\n\n\n\n\n3\nJuggling\n(0, 0)\n37183.72\n\n\n5\nMagic\n(1, 0)\n35101.07\n\n\n7\nDragon Taming\n(0, 1)\n27179.77\n\n\n10\nJuggling\n(0, 0)\n26366.80\n\n\n12\nMagic\n(1, 0)\n26101.53\n\n\n16\nDragon Taming\n(0, 1)\n39252.76\n\n\n\nThe design matrix X would look like this:\n\nX =\n\\begin{array}{c}\n  \\begin{array}{cccccc}\n    \\beta_0 & \\beta_1 & \\beta_2 & \\beta_3 & \\beta_4 & \\beta_5\n  \\end{array} \\\\\n  \\begin{pmatrix}\n    1 & 3 & 0 & 0 & 0 & 0 \\\\\n    1 & 5 & 1 & 0 & 5 & 0 \\\\\n    1 & 7 & 0 & 1 & 0 & 7 \\\\\n    1 & 10 & 0 & 0 & 0 & 0 \\\\\n    1 & 12 & 1 & 0 & 12 & 0 \\\\\n    1 & 16 & 0 & 1 & 0 & 16\n  \\end{pmatrix}\n\\end{array}.\n\nThe betas above the matrix are there just to label the columns, they are not really part of the matrix. The 3rd and 4th columns are the dummy variables for the majors, and the 5th and 6th columns are the interaction terms between education and the majors.\nIf we were not interested in the random effects, we could stop here, and just use the ordinary least squares (OLS) method already discussed to estimate the coefficients \\beta.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#random-effects",
    "href": "regression/mixed-model.html#random-effects",
    "title": "16  linear mixed effect model",
    "section": "16.3 random effects",
    "text": "16.3 random effects\nThe name “mixed effects” comes from the fact that we have both fixed effects and random effects.\nConceptually, the random effects function in a very similar way to the fixed effects. Instead of a small number of categories, now each person in the dataset is a category. In our example we have 90 different people represented in the dataset, so the quantity Z in Zb is the design matrix for the random effects, which is a matrix with 90 columns, one for each person, and as many rows as there are data points in the dataset. Each row has a 1 in the column corresponding to the person, and 0s elsewhere. The vector b is a vector of random effects coefficients, one for each person.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#implementation",
    "href": "regression/mixed-model.html#implementation",
    "title": "16  linear mixed effect model",
    "section": "16.4 implementation",
    "text": "16.4 implementation\nWe can use statsmodels function smf.mixedlm to do everything for us. We just need to specify the formula, which includes the interaction term, and the data.\nIf you don’t mind which category is the reference group, you can skip the cell below. If you want to make sure a give one is the reference group (Juggling in our case), then you should run it.\n\n\nchoose Juggling as reference major\nfrom pandas.api.types import CategoricalDtype\n# define the desired order: Juggling as reference\nmajor_order = CategoricalDtype(categories=[\"Juggling\", \"Magic\", \"Dragon Taming\"], ordered=True)\ndf[\"major\"] = df[\"major\"].astype(major_order)\n\n\nThe syntax is fairly economic. The formula\nincome ~ years_after_grad * major\nspecifies a linear model where both the baseline income (intercept) and the effect of time since graduation (slope) can vary by major. The * operator includes both the main effects (years after graduation and major) and their interaction, allowing the model to fit a different intercept and slope for each major.\nIn the line\nmodel = smf.mixedlm(formula, data=df, groups=df[\"person\"])\nthe groups argument specifies that the random effects are associated with the “person” variable, meaning that each person can have their own random intercept.\n\n# formula with interaction\nformula = \"income ~ years_after_grad * major\"\n\n# fit mixed model with random intercept for person\nmodel = smf.mixedlm(formula, data=df, groups=df[\"person\"])\nresult = model.fit()\n\nLet’s see the results\n\nprint(result.summary())\n\n                            Mixed Linear Model Regression Results\n==============================================================================================\nModel:                         MixedLM            Dependent Variable:            income       \nNo. Observations:              239                Method:                        REML         \nNo. Groups:                    90                 Scale:                         10690821.7105\nMin. group size:               1                  Log-Likelihood:                -2327.5068   \nMax. group size:               4                  Converged:                     Yes          \nMean group size:               2.7                                                            \n----------------------------------------------------------------------------------------------\n                                           Coef.     Std.Err.   z    P&gt;|z|   [0.025    0.975] \n----------------------------------------------------------------------------------------------\nIntercept                                  25206.095 1349.760 18.675 0.000 22560.615 27851.575\nmajor[T.Magic]                             -2999.754 1995.748 -1.503 0.133 -6911.348   911.840\nmajor[T.Dragon Taming]                      5579.198 1954.661  2.854 0.004  1748.133  9410.263\nyears_after_grad                             723.745   72.028 10.048 0.000   582.573   864.917\nyears_after_grad:major[T.Magic]              635.180  109.599  5.795 0.000   420.370   849.989\nyears_after_grad:major[T.Dragon Taming]     -295.862  106.315 -2.783 0.005  -504.235   -87.488\nGroup Var                               33814137.626 2268.953                                 \n==============================================================================================",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#interpreting-the-results",
    "href": "regression/mixed-model.html#interpreting-the-results",
    "title": "16  linear mixed effect model",
    "section": "16.5 interpreting the results",
    "text": "16.5 interpreting the results\nTo interpret the coefficients, start with the reference group, which in this model is someone who studied Juggling. Their predicted income is:\n\n\\text{income} = 25206.10 + 723.75 \\times \\text{years}\n\nNow, for a person who studied Magic, the model adjusts both the intercept and the slope:\nIntercept shift: -2999.75 Slope shift: +635.18 So for Magic, the predicted income becomes:\n\\begin{align*}\n\\text{income} &= (25206.10 - 2999.75) + (723.75 + 635.18) \\times \\text{years} \\\\\n       &= 22206.35 + 1358.93 \\times \\text{years}\n\\end{align*}\nThis means that compared to Jugglers, Magicians start with a lower baseline salary, but their income grows much faster with each year after graduation.\nThe Coef. column shows the estimated value of each parameter (e.g., intercepts, slopes, interactions). The Std.Err. column reports the standard error of the estimate, reflecting its uncertainty. The z column is the test statistic (estimate divided by standard error), and P&gt;|z| gives the p-value, which helps assess whether the effect is statistically significant. The final two columns, [0.025 and 0.975], show the 95% confidence interval for the coefficient — if this interval does not include zero, the effect is likely meaningful.\nThe line labeled Group Var shows the estimated variance of the random intercepts — in this case, variation in baseline income between individuals. The second number reported is the standard error associated with this estimate, which indicates how much uncertainty there is in the estimate of the variance.\nIf you like, you can print out all the variances for the random effects. They are not explicity shown in the summary, but you can access them through the model’s random_effects attribute:\nresult.random_effects\nFinally, the model as is does not include random slopes, meaning that the effect of years after graduation is assumed to be the same for all individuals. If you want to allow for different slopes for each individual, you can modify the model to include random slopes as well. This would require changing the formula and the groups argument accordingly. Also, result.random_effects will then contain not only the random intercepts, but also the random slopes for each individual.\n\nmodel = smf.mixedlm(\n    \"income ~ years_after_grad * major\",\n    data=df,\n    groups=df[\"person\"],\n    re_formula=\"~years_after_grad\"\n)\nresult = model.fit()\nprint(result.summary())\n\n/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n  warnings.warn(\n\n\n                            Mixed Linear Model Regression Results\n==============================================================================================\nModel:                         MixedLM            Dependent Variable:            income       \nNo. Observations:              239                Method:                        REML         \nNo. Groups:                    90                 Scale:                         10125672.1682\nMin. group size:               1                  Log-Likelihood:                -2323.7559   \nMax. group size:               4                  Converged:                     Yes          \nMean group size:               2.7                                                            \n----------------------------------------------------------------------------------------------\n                                           Coef.     Std.Err.   z    P&gt;|z|   [0.025    0.975] \n----------------------------------------------------------------------------------------------\nIntercept                                  25133.841 1208.050 20.805 0.000 22766.106 27501.576\nmajor[T.Magic]                             -2805.540 1811.051 -1.549 0.121 -6355.135   744.055\nmajor[T.Dragon Taming]                      5980.367 1767.166  3.384 0.001  2516.786  9443.949\nyears_after_grad                             731.399   84.211  8.685 0.000   566.349   896.450\nyears_after_grad:major[T.Magic]              611.065  126.072  4.847 0.000   363.969   858.161\nyears_after_grad:major[T.Dragon Taming]     -329.530  122.977 -2.680 0.007  -570.561   -88.498\nGroup Var                               22392488.656 1835.422                                 \nGroup x years_after_grad Cov               90328.607   75.664                                 \nyears_after_grad Var                       39074.487    7.401                                 \n==============================================================================================",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/mixed-model.html#back-to-ols",
    "href": "regression/mixed-model.html#back-to-ols",
    "title": "16  linear mixed effect model",
    "section": "16.6 back to OLS",
    "text": "16.6 back to OLS\nIf you went this far, and now realized you don’t care about random effects, you can just use the statsmodels function smf.ols to fit an ordinary least squares regression model. The syntax is similar, but without the groups argument.\n\nimport statsmodels.formula.api as smf\n\n# formula with main effects and interaction\nformula = \"income ~ years_after_grad * major\"\n\n# fit the model with OLS (no random effects)\nols_model = smf.ols(formula, data=df)\nols_result = ols_model.fit()\n\n# print summary\nprint(ols_result.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 income   R-squared:                       0.455\nModel:                            OLS   Adj. R-squared:                  0.443\nMethod:                 Least Squares   F-statistic:                     38.85\nDate:                Tue, 24 Jun 2025   Prob (F-statistic):           6.27e-29\nTime:                        16:16:38   Log-Likelihood:                -2437.0\nNo. Observations:                 239   AIC:                             4886.\nDf Residuals:                     233   BIC:                             4907.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================================\n                                              coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------\nIntercept                                2.486e+04   1450.267     17.141      0.000     2.2e+04    2.77e+04\nmajor[T.Magic]                          -4402.0846   2281.475     -1.929      0.055   -8897.041      92.872\nmajor[T.Dragon Taming]                   7696.8705   2167.061      3.552      0.000    3427.332     1.2e+04\nyears_after_grad                          778.4674    123.280      6.315      0.000     535.582    1021.352\nyears_after_grad:major[T.Magic]           758.4393    185.397      4.091      0.000     393.170    1123.708\nyears_after_grad:major[T.Dragon Taming]  -510.1096    183.456     -2.781      0.006    -871.553    -148.666\n==============================================================================\nOmnibus:                        2.143   Durbin-Watson:                   1.088\nProb(Omnibus):                  0.343   Jarque-Bera (JB):                2.132\nSkew:                           0.176   Prob(JB):                        0.344\nKurtosis:                       2.699   Cond. No.                         93.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>linear mixed effect model</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html",
    "href": "regression/logistic-regression.html",
    "title": "17  logistic regression",
    "section": "",
    "text": "17.1 question\nWe are given a list of heights for men and women. Given one more data point (180 cm), could we assign a probability that it belongs to either class?",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#discriminative-model",
    "href": "regression/logistic-regression.html#discriminative-model",
    "title": "17  logistic regression",
    "section": "17.2 discriminative model",
    "text": "17.2 discriminative model\nThe idea behind the logistic regression is to find a boundary between our two classes (here men and women). The logistic regression models the probability of a class given a data point, i.e. P(y|x). We can use the logistic function to model this probability:\nP(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\nwhere y is the class (0=women, 1=men), x is the data point (height), and \\beta_0 and \\beta_1 are the parameters of the model.\nOur goal is to find the best s-shaped curve that describes the data. This is done by finding the parameters \\beta_0 and \\beta_1 that maximize the likelihood of the data.\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import norm\n\n\n\n\ngenerate data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 20.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\nN_boys = 150\nN_girls = 200\nnp.random.seed(314)  # set scipy seed for reproducibility\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n# pandas dataframe with the two samples in it\ndf = pd.DataFrame({\n    'height (cm)': np.concatenate([sample_boys, sample_girls]),\n    'sex': ['M'] * N_boys + ['F'] * N_girls\n})\ndf = df.sample(frac=1, random_state=314).reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\n\n\nheight (cm)\nsex\n\n\n\n\n0\n178.558416\nM\n\n\n1\n173.334306\nM\n\n\n2\n183.084154\nM\n\n\n3\n178.236047\nF\n\n\n4\n175.868642\nM\n\n\n...\n...\n...\n\n\n345\n177.387837\nM\n\n\n346\n157.122325\nF\n\n\n347\n166.891746\nF\n\n\n348\n181.090312\nM\n\n\n349\n171.479631\nM\n\n\n\n\n350 rows × 2 columns\n\n\n\n\n\nX = df['height (cm)'].values.reshape(-1, 1)\ny = df['sex'].map({'M': 1, 'F': 0}).values\nlog_reg_2 = LogisticRegression(penalty=None, solver = 'newton-cg', max_iter= 150).fit(X,y)\nbeta1 = log_reg_2.coef_[0][0]\nbeta0 = log_reg_2.intercept_[0]\ndef logistic_function(x, beta0, beta1):\n    return 1 / (1 + np.exp(-(beta0 + beta1 * x)))\n\n\n\nplot\nfig, ax = plt.subplots()\nax.plot(sample_girls, np.zeros_like(sample_girls),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:orange')\nax.plot(sample_boys, np.ones_like(sample_boys),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:blue')\n\nx_array = np.linspace(140, 200, 300).reshape(-1, 1)\ny_proba = log_reg_2.predict_proba(x_array)[:, 1]\n# ax.plot(x_array, y_proba, color='black')\nax.plot(x_array, logistic_function(x_array, beta0, beta1), color='black', label=\"best fit\")\nax.plot(x_array, logistic_function(x_array, beta0+2, beta1), color='gray', linestyle='--', label=\"worse 1\")\nax.plot(x_array, logistic_function(x_array, beta0, beta1-0.02), color='gray', linestyle=':', label=\"worse 2\")\nax.legend(frameon=False)",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#likelihood",
    "href": "regression/logistic-regression.html#likelihood",
    "title": "17  logistic regression",
    "section": "17.3 likelihood",
    "text": "17.3 likelihood\nHow do we know the parameters of the best s-shaped curve? Let’s pretend we have only three data points:\n\nMan, 180 cm. Data point (180,1).\nMan, 170 cm. Data point (170,1).\nWoman, 165 cm. Data point (165,0).\n\n\n\nrecalculate logistic regression with 3 points\nx3 = np.array([180.0, 170.0, 165.0])\ny3 = np.array([1, 1, 0])\nlog_reg_3 = LogisticRegression(penalty=None, solver = 'newton-cg', max_iter= 150).fit(x3.reshape(-1, 1),y3)\nbeta1 = log_reg_3.coef_[0][0]\nbeta0 = log_reg_3.intercept_[0]\n\n\n\n\nplot\nfig, ax = plt.subplots(1, 3, figsize=(10, 5), sharex=True, sharey=True)\nx_array = np.linspace(140, 200, 300)\n\nya = logistic_function(x_array, beta0, beta1)\nyb = logistic_function(x_array, beta0+344, beta1*0.28)\nyc = logistic_function(x_array, beta0+450, beta1*0.06)\nyhat3a = logistic_function(x3, beta0, beta1)\nyhat3b = logistic_function(x3, beta0+344, beta1*0.28)\nyhat3c = logistic_function(x3, beta0+450, beta1*0.06)\n\nfor axi in ax:\n    axi.plot(x3, y3,\n            linestyle='None', marker='o',\n            markerfacecolor='none', markeredgecolor='black')\n\nax[0].plot(x_array, ya, color='black')\nax[1].plot(x_array, yb, color='gray', linestyle='--')\nax[2].plot(x_array, yc, color='gray', linestyle=':')\n\nfor i in range(len(x3)):\n    ax[0].plot([x3[i], x3[i]], [y3[i], yhat3a[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[1].plot([x3[i], x3[i]], [y3[i], yhat3b[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[2].plot([x3[i], x3[i]], [y3[i], yhat3c[i]], color='tab:red', linestyle='-', lw=0.5)\n\nax[0].set(xlabel=\"height (cm)\",\n          ylabel=\"P\",\n          title=\"best fit\")\nax[1].set(xlabel=\"height (cm)\",\n          title=\"worse 1\")\nax[2].set(xlabel=\"height (cm)\",\n            title=\"worse 2\")\n\n\n\n\n\n\n\n\n\nAs usual, our task in performing the regression is to find the parameters \\beta_0 and \\beta_1 that minimize the distance between the model and the data (the residual). See the figure above, we plotted the same three data points, and in each panel we see a different s-shaped curve (black) and the distance between the model and the data (red lines).\n[Note: this time, because we have only three data points, the best fit gave us an extremely sharp logistic function, that neatly discriminates between the data points. In the first example, the function was much more “shallow”, because of the overlap between the Men and Women datasets.]\nIn the logistic regression, instead of minimizing the residual, we maximize the likelihood of the data given the model parameters. The likelihood is the complement of the residual, see the thick red bars in the figure below.\n\n\nplot\nfig, ax = plt.subplots(1, 3, figsize=(10, 5), sharex=True, sharey=True)\n\nfor axi in ax:\n    axi.plot(x3, y3,\n            linestyle='None', marker='o',\n            markerfacecolor='none', markeredgecolor='black')\n\nax[0].plot(x_array, ya, color='black')\nax[1].plot(x_array, yb, color='gray', linestyle='--')\nax[2].plot(x_array, yc, color='gray', linestyle=':')\n\nfor i in range(len(x3)):\n    ax[0].plot([x3[i], x3[i]], [y3[i], yhat3a[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[0].plot([x3[i], x3[i]], [not bool(y3[i]), yhat3a[i]], color='tab:red', linestyle='-', lw=2)\n    ax[1].plot([x3[i], x3[i]], [y3[i], yhat3b[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[1].plot([x3[i], x3[i]], [not bool(y3[i]), yhat3b[i]], color='tab:red', linestyle='-', lw=2)\n    ax[2].plot([x3[i], x3[i]], [y3[i], yhat3c[i]], color='tab:red', linestyle='-', lw=0.5)\n    ax[2].plot([x3[i], x3[i]], [not bool(y3[i]), yhat3c[i]], color='tab:red', linestyle='-', lw=2)\n\nax[2].plot([], [], color='tab:red', linestyle='-', lw=0.5, label=\"residual\")\nax[2].plot([], [], color='tab:red', linestyle='-', lw=2, label=\"likelihood\")\n\nax[0].set(xlabel=\"height (cm)\",\n          ylabel=\"P\",\n          title=\"best fit\")\nax[1].set(xlabel=\"height (cm)\",\n          title=\"worse 1\")\nax[2].set(xlabel=\"height (cm)\",\n            title=\"worse 2\")\nax[2].legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)\n\n\n\n\n\n\n\n\n\nWhat is the probability that we would measure the observed data, given the model parameters? For a single data point (height, class), the length of the red bars can be described by the formula below:\n\nL(y_i|P_i) = P_i^{y_i} (1-P_i)^{1-y_i}.\n\nFor instance, if the data point corresponds to a man (y_i=1), we have L=P_i. The likelihood (thick red bars) for men is just the value of the logistic function for that value of x. For women (y_i=0), we have L=1-P_i. The likelihood for women is just the complement (one minus) of the logistic function.\nAssuming that each data point is independent, the likelihood of the entire dataset is the product of the likelihoods of each data point:\n\nL(\\beta_0, \\beta_1) = \\prod_{i=1}^{N} P_i^{y_i} (1-P_i)^{1-y_i}.\n\nIn the example below, the likelihood of the entire dataset for each panel is as follows:\n\n\ncompute likelihoods\nLa = 1.0\nLb = 1.0\nLc = 1.0\nfor i in range(len(x3)):\n    La = La * yhat3a[i]**y3[i] * (1-yhat3a[i])**(1-y3[i])\n    Lb = Lb * yhat3b[i]**y3[i] * (1-yhat3b[i])**(1-y3[i])\n    Lc = Lc * yhat3c[i]**y3[i] * (1-yhat3c[i])**(1-y3[i])\nprint(\"Likelihood for best fit: \", La)\nprint(\"Likelihood for worse 1: \", Lb)\nprint(\"Likelihood for worse 2: \", Lc)\n\n\nLikelihood for best fit:  0.9984099891897481\nLikelihood for worse 1:  0.7711068593851899\nLikelihood for worse 2:  0.3173593316343797\n\n\nAs we increase the number of data points, the likelihood becomes very small (because we are multiplying many numbers between 0 and 1). To avoid numerical issues, we usually work with the log-likelihood.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#log-likelihood",
    "href": "regression/logistic-regression.html#log-likelihood",
    "title": "17  logistic regression",
    "section": "17.4 log-likelihood",
    "text": "17.4 log-likelihood\nThe log-likelihood is the logarithm of the likelihood:\n\n\\ell(\\beta_0, \\beta_1) = \\log L(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} P_i^{y_i} (1-P_i)^{1-y_i}\n\nUsing the properties of logarithms, we can rewrite the log-likelihood as follows:\n\n\\ell(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} \\left( y_i \\log P_i + (1-y_i) \\log (1-P_i) \\right).",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#binary-cross-entropy-or-log-loss",
    "href": "regression/logistic-regression.html#binary-cross-entropy-or-log-loss",
    "title": "17  logistic regression",
    "section": "17.5 binary cross-entropy, or log loss",
    "text": "17.5 binary cross-entropy, or log loss\nWe can use gradient descent to find the parameters that maximize the log-likelihood. Most implementations of gradient descent are designed to minimize a cost function. Therefore, instead of maximizing the log-likelihood, we can minimize the negative log-likelihood:\n\nJ(\\beta_0, \\beta_1) = -\\ell(\\beta_0, \\beta_1) = -\\sum_{i=1}^{N} \\left( y_i \\log P_i + (1-y_i) \\log (1-P_i) \\right).\n This cost function is also known as binary cross-entropy or log loss.\nIt turns out that taking the log of the likelihood is very convenient. What was before only a trick to avoid numerical issues, now has a nice interpretation. The cross-entropy can be thought of as a measure of “surprise”. The more the model is surprised by the data, the higher the cross-entropy, and the poorer the fit. The less surprised the model is by the data, the lower the cross-entropy, and the better the fit.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#wrapping-up",
    "href": "regression/logistic-regression.html#wrapping-up",
    "title": "17  logistic regression",
    "section": "17.6 wrapping up",
    "text": "17.6 wrapping up\nFrom the provided data:\n\nWhat is the probability that a person whose height is 180 cm is a man?\nIf we had to choose one height to discriminate between men and women, what would it be?\n\nLet’s run the code for the logistic regression again:\n\n\ncompute logistic regression and plot\nX = df['height (cm)'].values.reshape(-1, 1)\ny = df['sex'].map({'M': 1, 'F': 0}).values\nlog_reg_2 = LogisticRegression(penalty=None, solver = 'newton-cg', max_iter= 150).fit(X,y)\nbeta1 = log_reg_2.coef_[0][0]\nbeta0 = log_reg_2.intercept_[0]\n\n#| code-summary: \"plot \" \nfig, ax = plt.subplots()\nax.plot(sample_girls, np.zeros_like(sample_girls),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:orange')\nax.plot(sample_boys, np.ones_like(sample_boys),\n        linestyle='None', marker='o',\n        markerfacecolor='none', markeredgecolor='tab:blue')\n\nx_array = np.linspace(140, 200, 300).reshape(-1, 1)\ny_proba = log_reg_2.predict_proba(x_array)[:, 1]\n# ax.plot(x_array, y_proba, color='black')\nax.plot(x_array, logistic_function(x_array, beta0, beta1), color='black', label=\"best fit\")\n\nh = 180.0\np180 = log_reg_2.predict_proba(np.array([[h]]))[0, 1]\nax.plot([h, h], [0, p180], color='gray', linestyle=':')\nax.plot([np.min(x_array), h], [p180, p180], color='gray', linestyle=':')\nax.text(h+1, p180-0.05, f\"P({h} cm)={p180:.2f}\", color='gray', fontsize=12)\n\np_50percent = -beta0 / beta1\nax.plot([p_50percent, p_50percent], [0, 0.5], color='gray', linestyle=':')\nax.plot([np.min(x_array), p_50percent], [0.5, 0.5], color='gray', linestyle=':')\nax.text(p_50percent-1, 0.5+0.05, f\"P({p_50percent:.0f} cm)=0.5\", color='gray', fontsize=12, ha='right')\n\n\nax.set(xlim=(140, 200),\n       xlabel=\"height (cm)\",\n       ylabel=\"P\",)\n\n\n\n\n\n\n\n\n\nAnswers:\n\nThe probability that a person 180 cm tall is a man is 92%.\nThe height that best discriminates between men (above) and women (below) is 171 cm.\n\nThis last result follows directly from:\n\\begin{align*}\nP(x) = \\frac{1}{1+\\exp[-(\\beta_0+\\beta_1 x)]} &= \\frac{1}{2} \\\\\n& \\text{therefore} \\\\\n1+\\exp[-(\\beta_0+\\beta_1 x)] &= 2 \\\\\n\\exp[-(\\beta_0+\\beta_1 x)] &= 1 \\\\\n-(\\beta_0+\\beta_1 x) &= 0 \\\\\nx &= -\\frac{\\beta_0}{\\beta_1}\n\\end{align*}\nCompare this result with the one we obtained with the parametric generative model discussed in the Bayes’ theorem section.\nIf you want to see a nice tutorial, see Dr. Roi Yehoshua’s “Mastering Logistic Regression”.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic-regression.html#connection-to-neural-networks",
    "href": "regression/logistic-regression.html#connection-to-neural-networks",
    "title": "17  logistic regression",
    "section": "17.7 connection to neural networks",
    "text": "17.7 connection to neural networks\nThe logistic regression can be understood as a single-layer perceptron neural network model. This is to say, a neural network with no hidden layers, and a single output neuron that uses the logistic (sigmoid) activation function.\n\n\n\n“source: https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae/”",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>logistic regression</span>"
    ]
  },
  {
    "objectID": "regression/logistic_2d.html",
    "href": "regression/logistic_2d.html",
    "title": "18  logistic 2d",
    "section": "",
    "text": "18.1 statistics\nWe call our two features, height and weight, x_1 and x_2. We can write the logistic regression model as\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\nwhere p is the probability of being 13 years old. The left hand side is called the log-odds or logit. The right hand side is a linear combination of the features.\nThis is, of course, equivalent to the expression with the sigmoid function:\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}}\nSpeaking a “statistics language”, this linear relationship is expressed by writing everything in matrix form: \nz = X\\beta,\n where X is the design matrix, \\beta is the vector of coefficients, and z is the linear predictor.\n\\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\end{pmatrix} = \\begin{pmatrix} 1 & x_{11} & x_{12} \\\\ 1 & x_{21} & x_{22} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_{n1} & x_{n2} \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} \\beta_0 + \\beta_1x_{11} + \\beta_2x_{12} \\\\ \\beta_0 + \\beta_1x_{21} + \\beta_2x_{22} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1x_{n1} + \\beta_2x_{n2} \\end{pmatrix}",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic 2d</span>"
    ]
  },
  {
    "objectID": "regression/logistic_2d.html#machine-learning",
    "href": "regression/logistic_2d.html#machine-learning",
    "title": "18  logistic 2d",
    "section": "18.2 machine learning",
    "text": "18.2 machine learning\nIn machine learning, we often call the intercept term the bias, and we call the coefficients weights. We can write the linear predictor as \nz = w_1 x_1 + w_2 x_2 + b\n where w_1 and w_2 are the weights, and b is the bias. In matrix form: \nz = w^T x + b,\n which expands to \n\\begin{pmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_n \\end{pmatrix} = \\begin{pmatrix} w_1 & w_2 \\end{pmatrix} \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\\\ \\vdots & \\vdots \\\\ x_{n1} & x_{n2} \\end{pmatrix} + \\begin{pmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{pmatrix} = \\begin{pmatrix} w_1x_{11} + w_2x_{12} + b \\\\ w_1x_{21} + w_2x_{22} + b \\\\ \\vdots \\\\ w_1x_{n1} + w_2x_{n2} + b \\end{pmatrix}\n\nThis is the same as the statistics formulation, just with different names for the parameters.",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic 2d</span>"
    ]
  },
  {
    "objectID": "regression/logistic_2d.html#solving",
    "href": "regression/logistic_2d.html#solving",
    "title": "18  logistic 2d",
    "section": "18.3 solving",
    "text": "18.3 solving\nI boy from either 7th grade (13 years old) or 5th grade (10 years old) is randomly selected. Given his height (150 cm) and weight (45 kg), we want to predict his age group.\n\n\nfinding the decision boundary\nfig, ax = plt.subplots(figsize=(8, 6))\nX = df[['weight', 'height']]\ny = df['age']\nmodel = LogisticRegression()\nmodel.fit(X, y)\nxx, yy = np.meshgrid(np.linspace(10, 80, 100), np.linspace(100, 200, 100))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\npredict_df = pd.DataFrame(grid_points, columns=['weight', 'height'])\nZ = model.predict(predict_df)\nZ = Z.reshape(xx.shape)\nZ_prob = model.predict_proba(predict_df)[:, 1]\nZ = Z_prob.reshape(xx.shape)\n# contour_fill = ax.contourf(xx, yy, Z, levels=np.arange(0, 1.1, 0.2), cmap='RdBu_r', alpha=0.8)\ncont = ax.contour(xx, yy, Z, levels=[0.1, 0.5, 0.9], colors=['gray', 'black', 'gray'], linestyles=['--', '-', '--'], linewidths=[0.8, 2, 0.8])\nax.clabel(cont, fmt='p=%1.1f', inline=True, fontsize=10)\n\nsns.scatterplot(data=df, x='weight', y='height', hue='age', ax=ax)\nax.set(xlabel='weight (kg)',\n       ylabel='height (cm)');\n\nh1 = 150\nw1 = 45\ndf1 = pd.DataFrame([[w1, h1]], columns=['weight', 'height'])\nax.plot([w1], [h1], ls='None', marker='o', markersize=10, markerfacecolor='None', markeredgecolor='red', label=\"new data point\")\nax.legend(title='age', loc='upper right', frameon=False)\np1 = model.predict_proba(df1)[0, 1]\nprint(f\"Predicted probability of being 13 years old: {p1:.3f}\")\n\n\nPredicted probability of being 13 years old: 0.848\n\n\n\n\n\n\n\n\n\nThe thick line in the figure above is the decision boundary, where the probability of being 13 years old is 0.5. The equation of the line is\n\n0 =  w_1 x_1 + w_2 x_2 + b,\n\nwere feature x_1 is weight and feature x_2 is height. The weights w_1,w_2 and bias b are\n\nmodel.coef_, model.intercept_\n\n(array([[0.19047627, 0.37131303]]), array([-62.54777199]))",
    "crumbs": [
      "regression",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>logistic 2d</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html",
    "href": "correlation/correlation.html",
    "title": "19  correlation",
    "section": "",
    "text": "19.1 variance\nTo understand correlation, we need to start with variance. Let’s say X is a random variable with mean \\mu. The variance of X, denoted \\text{var}(X)=\\sigma^2, is defined as the expected value of the squared deviation from the mean:\n\\sigma^2 = \\text{var}(X) = E[(X - \\mu)^2] = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu)^2.\nI should point out that the formula above is for the population variance. If we are working with a sample, we would use N-1 in the denominator instead of N to get an unbiased estimate of the population variance. Also, the mean and variance for the sample are denoted \\bar{X} and s^2 respectively. In any case, let’s continue with the population variance for simplicity.\nIn simple words, the variance measures how much the values of X deviate from the mean \\mu. A high variance indicates that the data points are spread out over a wider range of values, while a low variance indicates that they are closer to the mean.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#covariance",
    "href": "correlation/correlation.html#covariance",
    "title": "19  correlation",
    "section": "19.2 covariance",
    "text": "19.2 covariance\nNow, let’s consider two random variables, X and Y, with means \\mu_X and \\mu_Y. The covariance between X and Y, denoted \\text{cov}(X, Y), is defined as:\n\n\\text{cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu_X)(Y_i - \\mu_Y).\n\nA high covariance indicates that when X is above its mean, Y tends to be above its mean as well (and vice versa). A low (or negative) covariance indicates that when X is above its mean, Y tends to be below its mean.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#correlation",
    "href": "correlation/correlation.html#correlation",
    "title": "19  correlation",
    "section": "19.3 correlation",
    "text": "19.3 correlation\nThe covariance can be any value, making it difficult to interpret. To standardize the measure, we use the correlation coefficient, denoted \\rho (for population) or r (for sample). The correlation coefficient is defined as:\n\\begin{align*}\n\\rho_{X,Y} &= \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} \\\\\n           &= E\\left[\\frac{(X - \\mu_X)}{\\sigma_X}\\frac{(Y - \\mu_Y)}{\\sigma_Y}\\right] \\\\\n           &= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{X_i - \\mu_X}{\\sigma_x}\\frac{Y_i - \\mu_Y}{\\sigma_Y}.\n\\end{align*}\nwhere \\sigma_X and \\sigma_Y are the standard deviations of X and Y, respectively.\nSomething becomes clear now. If we calculate the correlation of X with itself, we get:\n\n\\rho_{X,X} = \\frac{\\text{cov}(X, X)}{\\sigma_X \\sigma_X} = \\frac{\\text{var}(X)}{\\sigma_X^2} = 1.\n\nThe highest possible correlation is 1, which indicates a perfect positive linear relationship between the two variables. The lowest possible correlation is -1, which indicates a perfect negative linear relationship. A correlation of 0 indicates no linear relationship between the variables.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#pearson-correlation-coefficient",
    "href": "correlation/correlation.html#pearson-correlation-coefficient",
    "title": "19  correlation",
    "section": "19.4 Pearson correlation coefficient",
    "text": "19.4 Pearson correlation coefficient\nWhen we say “correlation”, we usually mean the Pearson correlation coefficient, which is the formula given above. Pearson invented this, so it’s named after him. There are other types of correlation coefficients, such as Spearman’s rank correlation coefficient and Kendall’s tau coefficient, which are used for non-parametric data or ordinal data.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#covariance-of-z-scored-variables",
    "href": "correlation/correlation.html#covariance-of-z-scored-variables",
    "title": "19  correlation",
    "section": "19.5 covariance of z-scored variables",
    "text": "19.5 covariance of z-scored variables\nNotice that the correlation formula can be interpreted as the covariance of the z-scored variables. The z-score of a variable X is defined as:\n\nZ_X = \\frac{X - \\mu_X}{\\sigma_X}\n\nThus, the correlation can be rewritten as:\n\n\\rho_{X,Y} = \\text{cov}(Z_X, Z_Y) = \\frac{1}{N} \\sum_{i=1}^{N} Z_{X_i} Z_{Y_i}\n\nIt is quite easy to compute the correlation on the computer. If X and Y are two arrays, first we standardize (z-score) them, then we compute their dot product (sum of piecewise multiplication), and finally we divide by N (or N-1 for sample correlation).",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/correlation.html#linearity",
    "href": "correlation/correlation.html#linearity",
    "title": "19  correlation",
    "section": "19.6 linearity",
    "text": "19.6 linearity\nThe Pearson correlation coefficient measures the strength and direction of a linear relationship between two variables. It does not capture non-linear relationships. For example, if Y = X^2, the correlation between X and Y may be low or even zero, despite a clear non-linear relationship. When we say “correlation”, it is usually implicit that we are referring to linear correlation.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>correlation</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html",
    "href": "correlation/linear_regression.html",
    "title": "20  correlation and linear regression",
    "section": "",
    "text": "20.1 prelude: finding the intercept and slope\nLet’s derive the formulas for the intercept and slope of the regression line. We want to minimize the sum of squared residuals L:\nL = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2,\n\\tag{1}\nwhere\n\\hat{y}_i = \\beta_0 + \\beta_1 x_i.\n\\tag{2}\nTo find the optimal values of \\beta_0 and \\beta_1, we take the partial derivatives of L with respect to \\beta_0 and \\beta_1, set them to zero, and solve the resulting equations.\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_0} &= 0 \\tag{3a}\\\\\n\\frac{\\partial L}{\\partial \\beta_1} &= 0 \\tag{3b}\n\\end{align*}\nWe already did that for a general case, but the calculation had the variables in vector/matrix form. Here we will do it for the simple case of one predictor variable, so that we can see the relationship with correlation more clearly.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html#prelude-finding-the-intercept-and-slope",
    "href": "correlation/linear_regression.html#prelude-finding-the-intercept-and-slope",
    "title": "20  correlation and linear regression",
    "section": "",
    "text": "20.1.1 intercept\nLet’s start with Eq. (3a), and substitute into it Eq. (2):\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_0} &= \\frac{\\partial}{\\partial \\beta_0} (y_i - \\hat{y}_i)^2 \\tag{4a} \\\\\n&= \\frac{\\partial}{\\partial \\beta_0} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\tag{4b} \\\\\n&= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\tag{4c}\n\\end{align*}\nEliminating the constant factor -2 and expanding the summation, we get:\n\n\\sum_{i=1}^n y_i - n \\beta_0 - \\beta_1 \\sum_{i=1}^n x_i = 0\n\\tag{5}\n\nWe now divide by n and rearrange to isolate \\beta_0:\n\n\n\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n\\tag{6}\n\n\nNote: we can rewrite equation (4c) as\n\n\\sum_{i=1}^n (y_i - \\hat{y}_i) = \\sum_{i=1}^n \\text{residuals} = 0,\n\nwhich is a nice thing to know.\n\n\n20.1.2 slope\nNow let’s move on to Eq. (3b), and substitute the result of Eq. (6) into it:\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1} (y_i - \\hat{y}_i)^2 \\tag{7a} \\\\\n&= \\frac{\\partial}{\\partial \\beta_1} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\tag{7b} \\\\\n&= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) x_i = 0 \\tag{7c} \\\\\n&= -2 \\sum_{i=1}^n (y_i - \\bar{y} + \\beta_1 \\bar{x} - \\beta_1 x_i) x_i = 0 \\tag{7d}\n\\end{align*}\nEliminating the constant factor 2 and expanding the summation, we get:\n\n-\\sum_{i=1}^n x_i y_i + \\sum_{i=1}^n x_i \\bar{y} - \\beta_1 \\sum_{i=1}^n x_i \\bar{x} + \\beta_1 \\sum_{i=1}^n x_i^2 = 0\n\\tag{8}\n\nLet’s group the terms involving \\beta_1 on one side and the rest on the other side:\n\n\\beta_1 \\left( \\sum_{i=1}^n x_i^2 - \\sum_{i=1}^n x_i \\bar{x} \\right) = \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\bar{y}\n\\tag{9}\n\nIsolating \\beta_1, we have:\n\n\\beta_1 = \\frac{\\sum x_i y_i - \\sum x_i \\bar{y}}{\\sum x_i^2 - \\sum x_i \\bar{x}} = \\frac{\\text{numerator}}{\\text{denominator}}\n\\tag{10}\n\nIt’s easier to interpret the numerator and denominator separately. To each we will add and subtract a term that will allow us to express them in simpler forms.\nNumerator:\n\n\\text{numerator} = \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\bar{y} + \\sum_{i=1}^n \\bar{x} y_i - \\sum_{i=1}^n \\bar{x} y_i\n\\tag{11}\n\nWe express the third term thus:\n\n\\text{third term} = \\sum_{i=1}^n \\bar{x} y_i = \\bar{x} \\sum_{i=1}^n y_i = n \\bar{x} \\bar{y} = \\sum_{i=1}^n \\bar{x} \\bar{y}\n\\tag{12}\n\nThe numerator now becomes:\n\\begin{align*}\n\\text{numerator} &= \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\bar{y} + \\sum_{i=1}^n \\bar{x} \\bar{y} - \\sum_{i=1}^n \\bar{x} y_i \\tag{13a} \\\\\n&= \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\tag{13b} \\\\\n\\end{align*}\nNow the denominator:\n\n\\text{denominator} = \\sum_{i=1}^n x_i^2 - \\sum_{i=1}^n x_i \\bar{x} + \\sum_{i=1}^n x_i\\bar{x} - \\sum_{i=1}^n x_i\\bar{x}\n\\tag{14}\n\nWe group the second and fourth terms, and express the third term thus:\n\n\\text{third term} = \\sum_{i=1}^n x_i \\bar{x} = \\bar{x} \\sum_{i=1}^n x_i = n \\bar{x}^2 = \\sum_{i=1}^n \\bar{x}^2\n\\tag{15}\n\nThe denominator now becomes:\n\\begin{align*}\n\\text{denominator} &= \\sum_{i=1}^n x_i^2 - 2 \\sum_{i=1}^n x_i \\bar{x} + \\sum_{i=1}^n \\bar{x}^2 \\tag{16a} \\\\\n&= \\sum_{i=1}^n (x_i - \\bar{x})^2 \\tag{16b}\n\\end{align*}\nPutting it all together, we have:\n\n\n\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\tag{17}",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html#slope-and-correlation",
    "href": "correlation/linear_regression.html#slope-and-correlation",
    "title": "20  correlation and linear regression",
    "section": "20.2 slope and correlation",
    "text": "20.2 slope and correlation\nLet’s divide both the numerator and denominator of Eq. (17) by n-1:\n\n\\beta_1 = \\frac{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\tag{18}\n\nThe numerator is the sample covariance \\text{cov}(X, Y), and the denominator is the sample variance \\text{var}(X):\n\n\\beta_1 = \\frac{\\text{cov}(X, Y)}{\\text{var}(X)}\n\\tag{19}\n\nNow, we can express the covariance in terms of the correlation coefficient \\rho_{X,Y} and the standard deviations \\sigma_X and \\sigma_Y (see here):\n\n\\text{cov}(X, Y) = \\rho_{X,Y} \\sigma_X \\sigma_Y\n\\tag{20}\n Substituting Eq. (20) into Eq. (19), we get:\n\n\\beta_1 = \\frac{\\rho_{X,Y} \\sigma_X \\sigma_Y}{\\sigma_X^2}\n\\tag{21}\n\nAnd finally, we have:\n\n\n\\beta_1 = \\rho_{X,Y} \\frac{\\sigma_Y}{\\sigma_X}\n\\tag{22}\n\n\nThis shows that the slope of the regression line is directly proportional to the correlation coefficient. A higher absolute value of the correlation coefficient indicates a steeper slope, while a lower absolute value indicates a flatter slope.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/linear_regression.html#r2-square-of-the-correlation-coefficient-r",
    "href": "correlation/linear_regression.html#r2-square-of-the-correlation-coefficient-r",
    "title": "20  correlation and linear regression",
    "section": "20.3 R^2= square of the correlation coefficient r",
    "text": "20.3 R^2= square of the correlation coefficient r\nLet’s start by saying that the correlation coefficient is called \\rho when referring to the population, and r when referring to a sample. I’m playing loose with this convention here, but I hope it’s clear from the context.\nLet’s show now that the coefficient of determination R^2 is equal to the square of the correlation coefficient r.\nWe start with the definition of R^2:\n\nR^2 = 1 - \\frac{\\text{SS}_{\\text{Error}}}{\\text{SS}_{\\text{Total}}} = \\frac{\\text{SS}_{\\text{Total}}+\\text{SS}_{\\text{Error}}}{\\text{SS}_{\\text{Total}}} =  \\frac{\\text{SS}_{\\text{Model}}}{\\text{SS}_{\\text{Total}}}\n\\tag{23}\n\nwhere we used the fact that \\text{SS}_{\\text{Total}} = \\text{SS}_{\\text{Model}} + \\text{SS}_{\\text{Error}}, already seen before.\nWe now substitute into Eq. (23) the definitions of \\text{SS}_{\\text{Model}} and \\text{SS}_{\\text{Total}}:\n\nR^2 = \\frac{\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{24}\n\nWe now substitute into Eq. (24) the expression of \\hat{y}_i from Eq. (2):\n\nR^2 = \\frac{\\sum_{i=1}^n (\\beta_0 + \\beta_1 x_i - \\bar{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{25}\n\nNow we substitute into Eq. (25) the expression of \\beta_0 from Eq. (6):\n\nR^2 = \\frac{\\sum_{i=1}^n \\left( \\bar{y} - \\beta_1 \\bar{x} + \\beta_1 x_i - \\bar{y} \\right)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} = \\frac{\\sum_{i=1}^n \\left( \\beta_1 (x_i - \\bar{x}) \\right)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{26}\n\n\\beta_1 is a number, so we can take it out of the summation in the numerator:\n\nR^2 = \\frac{\\beta_1^2 \\sum_{i=1}^n (x_i - \\bar{x})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{26b}\n\nNow, let’s substitute into Eq. (26) the expression of \\beta_1 from Eq. (17):\n\nR^2 = \\frac{\\left( \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)^2 \\sum_{i=1}^n (x_i - \\bar{x})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{27}\n\nWe can simplify Eq. (27) by canceling one instance of the denominator in the squared term with the factor outside the squared term:\n\nR^2 = \\frac{\\left[ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\right]^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}\n\\tag{28}\n\nNow, let’s multiply both the numerator and denominator of Eq. (28) by \\frac{1}{(n-1)^2}:\n  \nR^2 = \\frac{\\left[ \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) \\right]^2}{\\left[ \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\right] \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar{y})^2 \\right]}\n\\tag{29}\n\nThe numerator is the square of the sample covariance \\text{cov}(X, Y), and the denominator is the product of the sample variances \\text{var}(X)=\\sigma_X^2 and \\text{var}(Y)=\\sigma_Y^2:\n\nR^2 = \\frac{\\text{cov}(X, Y)^2}{\\sigma_X^2 \\sigma_Y^2} = \\left( \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} \\right)^2\n\\tag{30}\n\nThis is exactly the square of the correlation coefficient r (see here):\n\n\nR^2 = r^2\n\\tag{31}",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>correlation and linear regression</span>"
    ]
  },
  {
    "objectID": "correlation/cosine.html",
    "href": "correlation/cosine.html",
    "title": "21  cosine",
    "section": "",
    "text": "21.1 cosine similarity\nThe cosine similarity is a measure of similarity between two non-zero vectors. It is defined as:\n\\text{cosine\\_similarity}(x,y) = \\frac{x \\cdot y}{\\lVert x \\rVert \\, \\lVert y \\rVert}\n\\tag{8}\nThis measure is common in text analysis, where a non-zero element of a vector represents the presence of a word in a document, and the value of the element represents the frequency of that word. The cosine similarity measures the cosine of the angle between two vectors, which indicates how similar the two vectors are in terms of their direction, regardless of their magnitude. If we were to z-score the vectors, the absence of a word would be represented by a negative value (below average), which is not useful in this context.\nWhen the vectors are z-scored, the cosine similarity is identical to the Pearson correlation coefficient.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>cosine</span>"
    ]
  },
  {
    "objectID": "correlation/cosine.html#cosine-similarity",
    "href": "correlation/cosine.html#cosine-similarity",
    "title": "21  cosine",
    "section": "",
    "text": "cosine similarity: works on any non-zero vectors, does not require z-scoring. There is no need to alter the reference point.\nPearson correlation: works on z-scored vectors, requires centering and scaling. The reference point is the mean of each variable. Useful when comparing variables with different units or scales.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>cosine</span>"
    ]
  },
  {
    "objectID": "correlation/significance.html",
    "href": "correlation/significance.html",
    "title": "22  significance (p-value)",
    "section": "",
    "text": "Given a correlation coefficient r, we can assess its significance using a p-value. Let’s formulate the hypotheses:\n\nNull Hypothesis (H_0): There is no correlation between the two variables (i.e., r = 0).\nAlternative Hypothesis (H_a): There is a correlation between the two variables (i.e., r \\neq 0).\n\nTo calculate the p-value, we can use the following formula for the test statistic t:\n\nt = \\frac{r \\sqrt{n - 2}}{\\sqrt{1 - r^2}}\n\\tag{1}\n where n is the number of data points.\nThis formula follows the fundamental structure of a t-statistic:\n\nt = \\frac{\\text{Signal}}{\\text{Noise}} = \\frac{\\text{Observed Statistic} - \\text{Null Value}}{\\text{Standard Error of the Statistic}}\n\\tag{2}\n\nLet’s rearrange Eq. (1) to match the structure of Eq. (2):\n\nt = \\frac{r - 0}{\\sqrt{\\frac{1 - r^2}{n - 2}}}\n\\tag{3}\n\nThe numerator is clear enough. Let’s discuss the denominator, which represents the standard error of the correlation coefficient.\nThe term 1 - r^2 is the proportion of unexplained variance in the data. As the correlation r gets stronger (closer to 1 or -1), the unexplained variance gets smaller. This makes intuitive sense: a very strong correlation is less likely to be a result of random chance, so the standard error (noise) should be smaller.\nThe term n−2 is the degrees of freedom. As your sample size n increases, the denominator gets larger, which makes the overall standard error smaller. This also makes sense: a correlation found in a large sample is more reliable and less likely to be a fluke than the same correlation found in a small sample.\nLet’s try a concrete example.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\n\n\ngenerate x and y data with some noise\n# set seed for reproducibility\nnp.random.seed(1)\nN = 100\nx = np.linspace(0, 10, N)\ny = 0.2 * x + 7*np.random.normal(size=x.size)\n\n\n\n\ncalculate a bunch of stuff\n# compute sample z-scores of x, y\nzx = (x - np.mean(x)) / np.std(x, ddof=0)\nzy = (y - np.mean(y)) / np.std(y, ddof=0)\n\n# compute Pearson correlation coefficient\nrho = np.sum(zx * zy) / N\n# compute t-statistic\nt = rho * np.sqrt((N-2) / (1-rho**2))\n# compute two-sided p-value\np = 2 * (1 - stats.t.cdf(np.abs(t), df=N-2))\n\n\n\n\nplot\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.scatter(x, y, label='data', alpha=0.5)\n\n# linear fit and R2 with scipy\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\nax.plot(x, slope*x + intercept, color='red', label=f'linear regression, R²={r_value**2:.2f}')\nax.legend(frameon=False)\n\n# print p-value\nprint(f'our p-value:   {p:.4f}')\nprint(f'scipy p-value: {p_value:.4f}')\n\n# compute correlation coefficients and their p-values\nr = np.corrcoef(x, y)[0,1]\nax.set(xlabel='x',\n       ylabel='y',\n       title=f\"Pearson's r = {rho:.3f}, p-value = {p_value:.4f}\");\n\n\nour p-value:   0.0458\nscipy p-value: 0.0458\n\n\n\n\n\n\n\n\n\nThe linear regression accounts for 4% of the variance in the data, which corresponds to a correlation coefficient of r = 0.2. This correlation is statistically significant at p=0.0458&lt;0.05.",
    "crumbs": [
      "correlation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>significance (p-value)</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html",
    "href": "bayes/from-the-ground-up.html",
    "title": "23  from the ground up",
    "section": "",
    "text": "23.1 the scenario\nImagine we’re in a room with a large group of people. We know the group consists of men and women, and we have height measurements for everyone. Someone walks in, we measure their height, but we don’t know if they are a man or a woman. Our goal is to figure out the probability that this person is a man, given their height. For simplicity, let’s say that heights are categorized into three groups: short, medium, and tall. The breakdown of the group is as follows:",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#the-scenario",
    "href": "bayes/from-the-ground-up.html#the-scenario",
    "title": "23  from the ground up",
    "section": "",
    "text": "Short\nMedium\nTall\n\n\n\n\nMan\n15\n30\n20\n\n\nWoman\n25\n35\n10",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#joint-and-conditional-probabilities",
    "href": "bayes/from-the-ground-up.html#joint-and-conditional-probabilities",
    "title": "23  from the ground up",
    "section": "23.2 joint and conditional probabilities",
    "text": "23.2 joint and conditional probabilities\nWhat is the probability that a person is both a man and tall?\nThis is the same as asking: what fraction does the rectangle on the bottom left have with respect to the whole area?\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\n\n\n\n\nvisualize\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_aspect('equal')\n\nm1, m2, m3 = 15, 30, 20\nf1, f2, f3 = 25, 35, 10\n\nm = m1 + m2 + m3\nf = f1 + f2 + f3\ntotal = m + f\nh1 = m1 + f1\nh2 = m2 + f2\nh3 = m3 + f3\nh = h1 + h2 + h3\n\np_3 = h3 / total\np_2 = h2 / total\np_1 = h1 / total\np_m = m / total\np_f = f / total\n\np_m_given_1 = m1 / h1\np_f_given_1 = f1 / h1\np_m_given_2 = m2 / h2\np_f_given_2 = f2 / h2\np_m_given_3 = m3 / h3\np_f_given_3 = f3 / h3\n\np_1_given_m = m1 / m\np_2_given_m = m2 / m\np_3_given_m = m3 / m\np_1_given_f = f1 / f\np_2_given_f = f2 / f\np_3_given_f = f3 / f\n\n# tall shaded area\nax.fill_between([0, 1], 0, p_3, color=\"blue\", alpha=0.5, edgecolor=\"none\")\n# medium shaded area\nax.fill_between([0, 1], p_3, p_3 + p_2, color=\"blue\", alpha=0.2, edgecolor=\"none\")\n# man given tall shaded area\nax.fill_between([0, p_m_given_3], 0, p_3, color=\"red\", alpha=0.5, edgecolor=\"none\")\n# man given medium shaded area\nax.fill_between([0, p_m_given_2], p_3, p_3+p_2, color=\"red\", alpha=0.5, edgecolor=\"none\")\n# man given short shaded area\nax.fill_between([0, p_m_given_1], p_3+p_2, 1.0, color=\"red\", alpha=0.5, edgecolor=\"none\")\n\n\nsns.despine(ax=ax, top=True, right=True)\n\n# tall arrow\nax.annotate(\"\",\n            (1.05, 0),\n            (1.05, p_3),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.05, p_3 / 2, f\" tall, {p_3:.2f}\", va=\"center\", rotation=0)\n# medium arrow\nax.annotate(\"\",\n            (1.05, p_3),\n            (1.05, p_2+p_3),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.05, p_3 + (p_2) / 2, f\" medium, {p_2:.2f}\", va=\"center\", rotation=0)\n# short arrow\nax.annotate(\"\",\n            (1.05, p_2+p_3),\n            (1.05, 1.0),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.05, 1.0 - (1.0 - p_2 - p_3) / 2, f\" short, {1.0-p_2-p_3:.2f}\", va=\"center\", rotation=0)\n# man given short arrow\nax.annotate(\"\",\n            (0, 0.90),\n            (p_m_given_1, 0.90),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m_given_1 / 2, 0.92, f\"man | short, {p_m_given_1:.2f}\", ha=\"center\", fontsize=12)\n\n# woman given short arrow\nax.annotate(\"\",\n            (1.0 - p_f_given_1, 0.90),\n            (1.0, 0.90),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f_given_1) / 2, 0.92, f\"woman | short, {p_f_given_1:.2f}\", ha=\"center\", fontsize=12)\n\n# man given medium arrow\nax.annotate(\"\",\n            (0, 0.50),\n            (p_m_given_2, 0.50),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m_given_2 / 2, 0.52, f\"man | medium, {p_m_given_2:.2f}\", ha=\"center\", fontsize=12)\n\n# woman given medium arrow\nax.annotate(\"\",\n            (1.0 - p_f_given_2, 0.50),\n            (1.0, 0.50),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f_given_2) / 2, 0.52, f\"woman | medium, {p_f_given_2:.2f}\", ha=\"center\", fontsize=12)\n\n# man given tall arrow\nax.annotate(\"\",\n            (0, 0.15),\n            (p_m_given_3, 0.15),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m_given_3 / 2, 0.17, f\"man | tall, {p_m_given_3:.2f}\", ha=\"center\", fontsize=12)\n\n# woman given tall arrow\nax.annotate(\"\",\n            (1.0 - p_f_given_3, 0.15),\n            (1.0, 0.15),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f_given_3) / 2, 0.17, f\"woman | tall, {p_f_given_3:.2f}\", ha=\"center\", fontsize=12)\n\n\nax.set(xticks=[],\n       yticks=[],\n       xlabel=\"sex\",\n       ylabel=\"height\",);\n\n\n\n\n\n\n\n\n\n\nThe numbers next to each category denote the proportions. That makes sense: according to the table, most of tall people are men, and most of the short people are women.\n“men | tall” is a short way to write “men given tall”. In simple words, it is the fraction of men, given that we know the person is tall.\n\nThe answer to the question is obvious now. The probability that a person is both man and tall the product of 0.22 with 0.67.\n\n22% of people are tall.\n67% of those are men.\n\nThe answer is 0.22 * 0.67 = 0.1474, or about 15%.\nIn mathematical notation, we write this as:\n\nP(\\text{man } \\cap \\text{ tall}) = P(\\text{tall}) \\cdot P(\\text{man|tall}),\n\\tag{1}\n\nwhere the symbol \\cap means “and”.\n\nP( ) is called the joint probability, because it describes the probability of two events happening together.\nP() is called the conditional probability, because it describes the probability of one event happening, given that another event is already known to have occurred.\n\nWhen Eq. (1) is rewritten in terms of P(\\text{man|tall}), it is called the equation for conditional probability:\n\nP(\\text{man|tall}) = \\frac{P(\\text{man } \\cap \\text{ tall})}{P(\\text{tall})}.\n\n\nOf course, “man” and “tall” are only labels, the general formula we should remember is:\n\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#different-perspective",
    "href": "bayes/from-the-ground-up.html#different-perspective",
    "title": "23  from the ground up",
    "section": "23.3 different perspective",
    "text": "23.3 different perspective\nWe could have made sense of the data in a different way. Above, we first categorized people by their height, and only then by sex. Let’s try the opposite.\n\n\nvisualize the other way\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_aspect('equal')\n\n# tall given man shaded area\nax.fill_between([0, p_m], 0, p_3_given_m, color=\"blue\", alpha=0.5, edgecolor=\"none\")\n# medium given man shaded area\nax.fill_between([0, p_m], p_3_given_m, p_3_given_m+p_2_given_m, color=\"blue\", alpha=0.2, edgecolor=\"none\")\n\n# man shaded area\nax.fill_between([0, p_m], 0, 1.0, color=\"red\", alpha=0.5, edgecolor=\"none\")\n\n# tall given woman shaded area\nax.fill_between([p_m, 1.0], 0, p_3_given_f, color=\"blue\", alpha=0.5, edgecolor=\"none\")\n# medium given man shaded area\nax.fill_between([p_m, 1.0], p_3_given_f, p_3_given_f+p_2_given_f, color=\"blue\", alpha=0.2, edgecolor=\"none\")\n\nsns.despine(ax=ax, top=True, right=True)\n\n# tall given man arrow\nax.annotate(\"\",\n            (p_m-0.03, 0),\n            (p_m-0.03, p_3_given_m),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m-0.03, p_3_given_m / 2, f\"tall | man, {p_3_given_m:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# medium given man arrow\nax.annotate(\"\",\n            (p_m-0.03, p_3_given_m),\n            (p_m-0.03, p_3_given_m+p_2_given_m),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m-0.03, p_3_given_m + p_2_given_m / 2, f\"medium | man, {p_2_given_m:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# short given man arrow\nax.annotate(\"\",\n            (p_m-0.03, 1.0),\n            (p_m-0.03, 1.0-p_1_given_m),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m-0.03, 1.0 - p_1_given_m / 2, f\"short | man, {p_1_given_m:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n\n# tall given woman arrow\nax.annotate(\"\",\n            (1.0-0.03, 0),\n            (1.0-0.03, p_3_given_f),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0-0.03, p_3_given_f / 2, f\"tall | woman, {p_3_given_f:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# medium given woman arrow\nax.annotate(\"\",\n            (1.0-0.03, p_3_given_f),\n            (1.0-0.03, p_3_given_f+p_2_given_f),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0-0.03, p_3_given_f + p_2_given_f / 2, f\"medium | woman, {p_2_given_f:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n# short given woman arrow\nax.annotate(\"\",\n            (1.0-0.03, 1.0),\n            (1.0-0.03, 1.0-p_1_given_f),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0-0.03, 1.0 - p_1_given_f / 2, f\"short | woman, {p_1_given_f:.2f}  \", ha=\"right\", va=\"center\", fontsize=12)\n\n\n# man arrow\nax.annotate(\"\",\n            (0, 1.05),\n            (p_m, 1.05),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(p_m / 2, 1.07, f\"man, {p_m:.2f}\", ha=\"center\", fontsize=12)\n# woman arrow\nax.annotate(\"\",\n            (1.0 - p_f, 1.05),\n            (1.0, 1.05),\n            ha=\"right\", va=\"center\",\n            size=14,\n            arrowprops=dict(arrowstyle='&lt;-&gt;',\n                            color=\"black\",\n                            ),\n)\nax.text(1.0 - (p_f) / 2, 1.07, f\"woman, {p_f:.2f}\", ha=\"center\", fontsize=12)\n\nax.set(xticks=[],\n       yticks=[],\n       # xlim=(0, 1),\n       # ylim=(0, 1),\n       xlabel=\"sex\",\n       ylabel=\"height\",);\n\n\n\n\n\n\n\n\n\nThe probability that a person is both man and tall is still the area of the purple rectangle on the bottom left. The rectangle has a different shape, but it has to have the same area. The answer to our question now can be understood thus:\n\n48% of people are men.\n31% of those are tall.\n\nThe answer is 0.48 * 0.31 = 15%, exactly the same result as before.\nIn mathematical notation, we write this as:\n\nP(\\text{tall } \\cap \\text{ man}) = P(\\text{man}) \\cdot P(\\text{tall|man}).\n\\tag{2}\n\n\nOne thing should become clear from the images above. The probability that a person is a man, given that they are tall, is not the same as the probability that a person is tall, given that they are a man. In mathematical notation:\n\nP(\\text{man|tall}) \\neq P(\\text{tall|man}).\n\n\nP(\\text{man|tall}): of all tall people, 67% are men.\nP(\\text{tall|man}): of all men, 31% are tall.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#bayes-theorem",
    "href": "bayes/from-the-ground-up.html#bayes-theorem",
    "title": "23  from the ground up",
    "section": "23.4 Bayes’ theorem",
    "text": "23.4 Bayes’ theorem\nWhen two things are true at the same time, it doesn’t matter the order we choose to write them. In mathematical notation:\n\nP(A \\cap B) = P(B \\cap A).\n\nBecause of this, we equate the right-hand sides of Eqs. (1) and (2), and we get Bayes’ theorem:\n\n\nP(\\text{man|tall}) \\cdot P(\\text{tall}) = P(\\text{tall|man}) \\cdot P(\\text{man})\n\n\nPersonally, I choose to remember this equation, because it is symmetric. But the more common way to write Bayes’ theorem is to solve for P(\\text{man|tall}):\n\nP(\\text{man|tall}) = \\frac{P(\\text{tall|man}) \\cdot P(\\text{man})}{P(\\text{tall})},\n\nor in general terms:\n\n\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}.\n\n\nEach term has a name:\n\nP(A|B) is the posterior. This is what we are trying to find out: the probability of A given that we know B.\nP(B|A) is the likelihood. This is the probability of B given that we know A.\nP(A) is the prior. This is what we know about A before we know anything about B.\nP(B) is the evidence. This is what we know about B before we know anything about A.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/from-the-ground-up.html#the-law-of-total-probability",
    "href": "bayes/from-the-ground-up.html#the-law-of-total-probability",
    "title": "23  from the ground up",
    "section": "23.5 the law of total probability",
    "text": "23.5 the law of total probability\nIn the example above, we had enough information to plug all the numbers into Bayes’ theorem. But this is not always the case. Sometimes, we don’t know P(B), the evidence. In this case, we can compute it using the law of total probability. It is best to give a concrete example.\nA famous use of Bayes’ theorem is in desease testing.\n\nA given desease affects 1% of the population.\nA test for the desease is 95% accurate. This means that:\n\nIf a person has the desease, the test will be positive 95% of the time.\nIf a person does not have the desease, the test will be negative 95% of the time.\n\nA person is randomly selected from the population, and they test positive. Should they be worried? What is the probability that they actually have the desease?\n\nLet’s translate this into the language of Bayes’ theorem:\n\nA is the event “the person has the desease”.\nB is the event “the test is positive”.\nWe need to find P(A|B), the probability that the person has the desease, given that they tested positive.\nP(A) = 0.01 is the prior, the probability that a random person has the desease.\nP(B|A) = 0.95 is the likelihood, the probability that the test is positive, given that the person has the desease.\n\nWe don’t know P(B), the evidence, the probability that a random person tests positive. But we can compute it using the law of total probability:\n\\begin{align*}\nP(\\text{test is positive}) &= P(\\text{test is positive } \\textbf{and} \\text{ person has the desease}) \\\\\n                           &+ P(\\text{test is positive } \\textbf{and} \\text{ person doesn't have the desease}).\n\\end{align*}\nThis has to be true, because a person can either have the desease or not. In a more compact form:\n\nP(B) = P(B \\cap A) + P(B \\cap A^c),\n\nwhere A^c is the complement of A, i.e., “the person does not have the desease”.\nUsing the definition of conditional probability, we can rewrite this as: \nP(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c).\n We know all the terms on the right-hand side:\n\nP(B|A) = 0.95, the probability that the test is positive, given that the person has the desease.\nP(A) = 0.01, the probability that a random person has the desease.\nP(A^c) = 0.99, the probability that a random person does not have the desease.\nP(B|A^c) = 0.05, the probability that the test is positive, given that the person does not have the desease. This is 1 minus the accuracy of the test for healthy people.\n\nPlugging in the numbers, we get:\n\nP(B) = 0.95 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.059.\n\nNow we have everything we need to plug the numbers into Bayes’ theorem:\n\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{0.95 \\cdot 0.01}{0.059} = 0.161.\n\nThis means that, even if the person tested positive, there is only a 16.1% chance that they actually have the desease. This is counter-intuitive, but it makes sense when we think about it. The desease is very rare, so even if the test is accurate, most of the positive results will be false positives.\nIn the case that there are several mutually exclusive ways for B to happen, we can generalize the law of total probability:\n\nP(B) = \\sum_i P(B \\cap A_i) = \\sum_i P(B|A_i) \\cdot P(A_i),\n\nwhere the A_i are all the possible ways for B to happen.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>from the ground up</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html",
    "href": "bayes/parametric-generative-classification.html",
    "title": "24  parametric generative classification",
    "section": "",
    "text": "24.1 question\nWe are given a list of heights for men and women. Given one more data point (180 cm), could we assign a probability that it belongs to either class?\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nfrom scipy.stats import norm\ngenerate data\ndf_boys = pd.read_csv('../archive/data/height/boys_height_stats.csv', index_col=0)\ndf_girls = pd.read_csv('../archive/data/height/girls_height_stats.csv', index_col=0)\nage = 20.0\nmu_boys = df_boys.loc[age, 'mu']\nmu_girls = df_girls.loc[age, 'mu']\nsigma_boys = df_boys.loc[age, 'sigma']\nsigma_girls = df_girls.loc[age, 'sigma']\n\nN_boys = 150\nN_girls = 200\nnp.random.seed(314)  # set scipy seed for reproducibility\nsample_boys = norm.rvs(size=N_boys, loc=mu_boys, scale=sigma_boys)\nsample_girls = norm.rvs(size=N_girls, loc=mu_girls, scale=sigma_girls)\n# pandas dataframe with the two samples in it\ndf = pd.DataFrame({\n    'height (cm)': np.concatenate([sample_boys, sample_girls]),\n    'sex': ['M'] * N_boys + ['F'] * N_girls\n})\ndf = df.sample(frac=1, random_state=314).reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\n\n\nheight (cm)\nsex\n\n\n\n\n0\n178.558416\nM\n\n\n1\n173.334306\nM\n\n\n2\n183.084154\nM\n\n\n3\n178.236047\nF\n\n\n4\n175.868642\nM\n\n\n...\n...\n...\n\n\n345\n177.387837\nM\n\n\n346\n157.122325\nF\n\n\n347\n166.891746\nF\n\n\n348\n181.090312\nM\n\n\n349\n171.479631\nM\n\n\n\n\n350 rows × 2 columns",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#explaining-parametric-generative-classification",
    "href": "bayes/parametric-generative-classification.html#explaining-parametric-generative-classification",
    "title": "24  parametric generative classification",
    "section": "24.2 explaining “parametric generative classification”",
    "text": "24.2 explaining “parametric generative classification”\n\nParametric: we assume a specific distribution for the data. In this case, we’ll assume a Gaussian distribution. We call this parametric because the distribution can be fully described by a finite set of parameters (mean and variance for Gaussian).\nGenerative: we model the distribution of each class separately. This would allow us to generate new data points from the learned distributions. “Learned” means estimating the parameters of the distributions from the sample data.\nClassification: we classify a new data point by comparing the likelihoods of it belonging to each class, given the learned distributions.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#visualizing-the-problem",
    "href": "bayes/parametric-generative-classification.html#visualizing-the-problem",
    "title": "24  parametric generative classification",
    "section": "24.3 visualizing the problem",
    "text": "24.3 visualizing the problem\n\n\nShow the code\nare_male = df['sex']=='M'\nboys_sample = df[are_male]['height (cm)'].to_numpy()\ngirls_sample = df[~are_male]['height (cm)'].to_numpy()\n\nxbar_boys = boys_sample.mean()\nxbar_girls = girls_sample.mean()\ns_boys = boys_sample.std(ddof=1)\ns_girls = girls_sample.std(ddof=1)\n\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# plot histogram\nbins = np.arange(135, 210, 5)\nax.hist(boys_sample, bins=bins, alpha=0.3, density=True, label='men', color='tab:blue', histtype='stepfilled')\nax.hist(girls_sample, bins=bins, alpha=0.3, density=True, label='women', color='tab:orange', histtype='stepfilled')\n# plot gaussian pdf based on sample mean and std\nx = np.arange(135, 210, 0.5)\npdf_boys = norm.pdf(x, loc=xbar_boys, scale=s_boys)\npdf_girls = norm.pdf(x, loc=xbar_girls, scale=s_girls)\nax.plot(x, pdf_boys, color='tab:blue')\nax.plot(x, pdf_girls, color='tab:orange')\nh0 = 180\n# plot vertical line at h0\nax.axvline(h0, color='gray', linestyle='--')\n# plot circles where each pdf intersects h0\nlikelihood_boys = norm.pdf(h0, loc=xbar_boys, scale=s_boys)\nlikelihood_girls = norm.pdf(h0, loc=xbar_girls, scale=s_girls)\nax.plot(h0, likelihood_boys, marker='o', color='tab:blue')\nax.plot(h0, likelihood_girls, marker='o', color='tab:orange')\n\nprint(f\"men:   mean={xbar_boys:.1f} cm, std={s_boys:.1f} cm\")\nprint(f\"women: mean={xbar_girls:.1f} cm, std={s_girls:.1f} cm\")\n\nax.legend(frameon=False)\nax.set_xlabel('height (cm)')\nax.set_ylabel('pdf');\n\n\nmen:   mean=177.4 cm, std=6.7 cm\nwomen: mean=163.5 cm, std=7.3 cm\n\n\n\n\n\n\n\n\n\nFrom the sample data, we can compute the mean and standard deviation for each sex (the generative part). We printed these values above. We can then use these parameters to compute the likelihood of the new data point (180 cm) belonging to each class using the Gaussian probability density function (the parametric part). These are plotted as circles in the graph.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#bayes-theorem",
    "href": "bayes/parametric-generative-classification.html#bayes-theorem",
    "title": "24  parametric generative classification",
    "section": "24.4 Bayes’ theorem",
    "text": "24.4 Bayes’ theorem\nWe can then use Bayes’ theorem to compute the posterior probabilities of the new data point belonging to each class (the classification part). Bayes’ theorem states that:\n\nP(\\text{man} | x) = \\frac{P(x | \\text{man})}{P(x)} P(\\text{man})\n\n\nposterior, P(\\text{man} | x). This is what we are looking for, the probability of a new data point corresponding to a man, given that its height is x.\nlikelihood, P(x | \\text{man}). This is the likelihood of observing a height x given that we know it is a man.\nevidence, P(x). This is the total probability of observing a height x across all classes (regardless of sex). It acts as a normalization factor. We calculate the evidence using the law of total probability: \\begin{align*}\nP(x) &= P(x \\cap \\text{man}) + P(x \\cap \\text{woman}) \\\\\n     &=P(x|\\text{man})\\cdot P(\\text{man}) + P(x|\\text{woman})\\cdot P(\\text{woman})\n\\end{align*}\nprior, P(\\text{man}). This is the overall probability of a person being a man in my dataset (regardless of their height).\n\nThink of this as a “battle of likelihoods,” adjusted for the group sizes. You have two groups, men and women, and you’ve modeled their typical heights. When you get a new height, x, you ask two main questions:\n\nLikelihood Question: How “typical” is height x for a man compared to how typical it is for a woman? If men in your data are generally tall and x is a tall height, it’s more likely to be a man. We measure this “typicalness” using a probability distribution.\nPrior Belief Question: In your dataset, are men or women more common? If your dataset contains 90 women and 10 men, any new person is, initially, more likely to be a woman, regardless of their height.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/parametric-generative-classification.html#step-by-step-calculation",
    "href": "bayes/parametric-generative-classification.html#step-by-step-calculation",
    "title": "24  parametric generative classification",
    "section": "24.5 Step-by-Step Calculation",
    "text": "24.5 Step-by-Step Calculation\nStep 1: Model Your Data\nWe assume a Gaussian distribution, and calculate the sample mean and standard deviation for each sex.\n\n\nmen:   mean=177.4 cm, std=6.7 cm\nwomen: mean=163.5 cm, std=7.3 cm\n\n\nStep 2: Calculate the Priors\nThe priors are simply the proportion of each group in the total dataset.\nP(\\text{Man}) = \\frac{N_\\text{men}}{N_\\text{men} + N_\\text{women}}\nP(\\text{Woman}) = \\frac{N_\\text{women}}{N_\\text{men} + N_\\text{women}}\nStep 3: Calculate the Likelihoods\nUsing the normal distribution’s probability density function (PDF), find the likelihood of the new height h for each model. The PDF formula is:\nf(x | \\bar{x}, s) = \\frac{1}{s\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\bar{x}}{s}\\right)^2}\n\nLikelihood for Men: Plug x into the PDF for the men’s model.\nP(x | \\text{Man}) = f(x | \\bar{x}_\\text{men}, s_\\text{men})\nLikelihood for Women: Plug x into the PDF for the women’s model.\nP(x | \\text{Woman}) = f(x | \\bar{x}_\\text{women}, s_\\text{women})\n\nStep 4: Put It All Together\nNow, apply Bayes’ Theorem. The “evidence” term P(x) in the denominator is the sum of all ways you could observe height x:\n\nP(x) = P(x | \\text{Man}) \\cdot P(\\text{Man}) + P(x | \\text{Woman}) \\cdot P(\\text{Woman})\n\nSo, the final calculation for the probability of being a man is:\n\nP(\\text{Man} | x) = \\frac{P(x | \\text{Man}) \\cdot P(\\text{Man})}{P(x | \\text{Man}) \\cdot P(\\text{Man}) + P(x | \\text{Woman}) \\cdot P(\\text{Woman})}\n.\nCrunching the number gives:\n\n\nShow the code\nh0 = 180.0\nlikelihood_boys = norm.pdf(h0, loc=xbar_boys, scale=s_boys)\nlikelihood_girls = norm.pdf(h0, loc=xbar_girls, scale=s_girls)\nprior_boys = N_boys / (N_boys + N_girls)\nprior_girls = N_girls / (N_boys + N_girls)\nevidence = likelihood_boys * prior_boys + likelihood_girls * prior_girls\np_man_given_180 = likelihood_boys * prior_boys / evidence\nprint(f\"Answer: {p_man_given_180 * 100:.2f}%.\")\n\n\nAnswer: 90.92%.\n\n\n\nThe probability that the person is a man, given that their height is 180 cm, is 90.92%",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>parametric generative classification</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html",
    "href": "bayes/odds.html",
    "title": "25  odds and log likelihood",
    "section": "",
    "text": "25.1 the scenario\nImagine we are researchers studying a potential link between a specific mutated gene and a certain disease. We have collected data from a sample of 356 people.\nHere’s our data:\nOur goal is to figure out how finding this mutated gene in a person should change our belief about whether they have the disease.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#the-scenario",
    "href": "bayes/odds.html#the-scenario",
    "title": "25  odds and log likelihood",
    "section": "",
    "text": "Has Disease\nNo Disease\n\n\n\n\nHas Mutated Gene\n23\n117\n\n\nNo Mutated Gene\n6\n210",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#prior",
    "href": "bayes/odds.html#prior",
    "title": "25  odds and log likelihood",
    "section": "25.2 prior",
    "text": "25.2 prior\nThe prior probability of someone having the disease is the chance of having the disease before we know anything about their gene status. We can calculate this from our data.\n\nP(\\text{Disease}) = \\frac{\\text{Number of people with the disease}}{\\text{Total number of people}}\n\nFrom our table: \nP(\\text{Disease}) = \\frac{23 + 6}{23 + 117 + 6 + 210} = \\frac{29}{356} \\approx 0.081",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#odds",
    "href": "bayes/odds.html#odds",
    "title": "25  odds and log likelihood",
    "section": "25.3 odds",
    "text": "25.3 odds\nOdds are a different way to express the same information. Odds compare the chance of an event happening to the chance of it not happening.\n\n\\text{Odds} = \\frac{P(\\text{event})}{P(\\text{not event})}\n\nIn our case, “event” is having the disease. Because we have only two choices, the probability of not having the disease is simply 1 - P(\\text{Disease}), therefore:\n\n\\text{Odds(Disease)} = \\frac{P(\\text{Disease})}{1 - P(\\text{Disease})}\n\nPlugging in the numbers:\n\n\\text{Odds(Disease)} = \\frac{29/356}{1 - 29/356} \\approx 0.0887 \\approx \\frac{1}{11}\n\nThis means that for every person with the disease, about 11 do not have it.\n\n25.3.1 log odds\nThe log odds is simply the natural logarithm of the odds.\n\n\\text{Log Odds} = \\ln\\left(\\frac{P}{1-P}\\right)\n\nWe will see soon enough why this is useful. For now, let’s point out that:\n\nif the odds are 1 (meaning a 50/50 chance), the log odds is 0.\nif the odds are greater than 1 (more likely than not), the log odds is positive.\nif the odds are less than 1 (less likely than not), the log odds is negative.\nthe log odds have a symmetric shape around p=1/2, see figure below.\n\nFor our example, the log odds of having the disease is:\n\n\\text{Log Odds(Disease)} = \\ln(\\text{Odds(Disease)}) = \\ln(0.0887) \\approx -2.42\n\n\n\nimport libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\n\n\n\n\nvisualize\nfig, ax = plt.subplots(1, 2, figsize=(8, 6))\n\np = np.linspace(0, 1, 100)\nodds = p / (1 - p)\nax[0].plot(p, odds, label=\"odds\", color=\"tab:blue\");\nax[0].axhline(1, ls=\"--\", color=\"gray\")\nax[0].set(ylabel=\"odds\",\n          xlabel=\"probability\",\n          title=r\"f(p) = $\\frac{p}{1-p}$\",\n          ylim=(-1, 10),\n          xlim=(0, 1),\n          xticks=[0,0.5,1]);\nax[0].annotate(\"more likely\\nthan not\", xy=(0.25, 1), xytext=(0.25, 3),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"))\nax[0].annotate(\"less likely\", xy=(0.7, 1), xytext=(0.7, -0.1),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"))\n\nax[1].plot(p, np.log(odds), label=\"odds\", color=\"tab:blue\")\nax[1].axhline(0, ls=\"--\", color=\"gray\")\nax[1].yaxis.set_ticks_position('right')\nax[1].yaxis.set_label_position('right')\nax[1].set(ylabel=\"log odds\",\n          xlabel=\"probability\",\n          title=r\"f(p) = log$\\frac{p}{1-p}$\",\n          xlim=(0, 1),\n          xticks=[0,0.5,1]);\nax[1].annotate(\"more likely\\nthan not\", xy=(0.25, 0), xytext=(0.25, 1),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"))\nax[1].annotate(\"less likely\", xy=(0.75, 0), xytext=(0.75, -1),\n               ha=\"center\", arrowprops=dict(arrowstyle=\"&lt;-\", color=\"gray\"));\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10104/1581004145.py:5: RuntimeWarning: divide by zero encountered in divide\n  odds = p / (1 - p)\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10104/1581004145.py:19: RuntimeWarning: divide by zero encountered in log\n  ax[1].plot(p, np.log(odds), label=\"odds\", color=\"tab:blue\")",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#bayes-theorem-in-odds-form",
    "href": "bayes/odds.html#bayes-theorem-in-odds-form",
    "title": "25  odds and log likelihood",
    "section": "25.4 Bayes’ theorem in odds form",
    "text": "25.4 Bayes’ theorem in odds form\nThe standard form of Bayes’ theorem is:\n\nP(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}.\n\\tag{1}\n\nIn our example, the hypothesis H is “having the disease”, and the evidence E is detecting the mutated gene.\nLet’s write Bayes’ theorem for the alternative hypothesis \\neg H (“not having the disease”):\n\nP(\\neg H|E) = \\frac{P(E|\\neg H) \\cdot P(\\neg H)}{P(E)}.\n\\tag{2}\n\nThe odds form of Bayes’ theorem is the ratio of these two equations:\n\n\\underbrace{\\frac{P(H|E)}{P(\\neg H|E)}}_ {\\text{posterior odds}} = \\underbrace{\\frac{P(E|H)}{P(E|\\neg H)}}_{\\text{likelihood ratio}} \\cdot \\underbrace{\\frac{P(H)}{P(\\neg H)}}_{\\text{prior odds}}\n\\tag{3}\n\nWe already discussed the prior odds. The posterior odds represent the odds of having the desease after we have seen the evidence (the mutated gene). The new piece we need is the likelihood ratio.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#likelihood-ratio",
    "href": "bayes/odds.html#likelihood-ratio",
    "title": "25  odds and log likelihood",
    "section": "25.5 likelihood ratio",
    "text": "25.5 likelihood ratio\nThe likelihood ratio (LR) tells us how much more likely we are to see the evidence if the hypothesis is true compared to if it is false.\n\n\\text{LR} = \\frac{P(E|H)}{P(E|\\neg H)}\n\nWe can compute this from our data:\n\nP(E|H): the probability of having the mutated gene given that the person has the disease. From the left column in our table, this is \\frac{23}{23+6} \\approx 0.793.\nP(E|\\neg H): the probability of having the mutated gene given that the person does not have the disease. From the right column in our table, this is \\frac{117}{117+210} \\approx 0.358.\n\nFinally:\n\n\\text{LR} = \\frac{23/29}{117/327} \\approx 2.22\n\nThe interpretation is that seeing the mutated gene is about 2.22 times more likely if the person has the disease than if they do not.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#log-likelihood-ratio",
    "href": "bayes/odds.html#log-likelihood-ratio",
    "title": "25  odds and log likelihood",
    "section": "25.6 log likelihood ratio",
    "text": "25.6 log likelihood ratio\nHere too, taking the logarithm transforms a quantity between 0 and infinity into a number between negative infinity and positive infinity. The log likelihood ratio is: \n\\text{Log-LR} = \\ln(\\text{LR}) = \\ln(2.22) \\approx 0.797\n\nThe fact that this is a positive number says that seeing the mutated gene increases our belief that the person has the disease.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/odds.html#bayes-theorem-in-log-odds-form",
    "href": "bayes/odds.html#bayes-theorem-in-log-odds-form",
    "title": "25  odds and log likelihood",
    "section": "25.7 Bayes’ theorem in log odds form",
    "text": "25.7 Bayes’ theorem in log odds form\nTaking the logarith of Eq. (3) gives us the log odds form of Bayes’ theorem:\n\n\\underbrace{\\ln\\left(\\frac{P(H|E)}{P(\\neg H|E)}\\right)}_{\\text{posterior log odds}} = \\underbrace{\\ln\\left(\\frac{P(E|H)}{P(E|\\neg H)}\\right)}_{\\text{log-likelihood ratio}} + \\underbrace{\\ln\\left(\\frac{P(H)}{P(\\neg H)}\\right)}_{\\text{prior log odds}}\n\\tag{4}\n\nPlugging in our numbers, we can see how our belief about the person having the disease changes after seeing the evidence (the mutated gene).\n\\begin{align*}\n\\text{Posterior Log Odds} &= \\text{Log-LR} + \\text{Prior Log Odds} \\\\\n&= 0.797 + (-2.42) \\\\\n&\\approx -1.623\n\\end{align*}\nFrom the posterior log odds, we can get back to the posterior odds by exponentiating:\n\n\\text{Posterior Odds} = e^{\\text{Posterior Log Odds}} = e^{-1.623} \\approx 0.197\n\nFinally, we can convert the posterior odds back to a probability: \nP(H|E) = \\frac{\\text{Posterior Odds}}{1 + \\text{Posterior Odds}} = \\frac{0.197}{1 + 0.197} \\approx 0.164\n This means that after seeing the mutated gene, our estimate of the probability that the person has the disease has increased from about 8.1% to about 16.4%.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>odds and log likelihood</span>"
    ]
  },
  {
    "objectID": "bayes/logistic-connection.html",
    "href": "bayes/logistic-connection.html",
    "title": "26  logistic connection",
    "section": "",
    "text": "26.1 from Bayes the logistic\nThe arguments below follow those in subsection 12.2 of “Introduction to Environmental Data Science” by William W. Hsieh.\nWe start with Bayes’ theorem for two classes C_1 and C_2:\nP(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}\n\\tag{1}\nUsing the law of total probability in the denominator, we get:\nP(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}\n\\tag{2}\nWe now divide the numerator and denominator by P(x|C_1)P(C_1):\nP(C_1|x) = \\frac{1}{1 + \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}\n\\tag{3}\nWe now note that the ratio P(C_2|x)/P(C_1|x) can be expressed as:\n\\frac{P(C_2|x)}{P(C_1|x)} = \\frac{\\frac{P(x|C_2)P(C_2)}{P(x)}}{\\frac{P(x|C_1)P(C_1)}{P(x)}} = \\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}\n\\tag{4}\nIn the expression above, we used the Bayes’ theorem in (1) to express P(C_2|x) and P(C_1|x) in terms of P(x|C_2) and P(x|C_1). We can now rewrite (3) as: \nP(C_1|x) = \\frac{1}{1 + \\frac{P(C_2|x)}{P(C_1|x)}} = \\frac{1}{1 + \\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right)^{-1}}\n\\tag{5}\nThe posterior probability P(C_1|x) is a function of the ratio P(C_1|x)/P(C_2|x). This ratio is called the posterior odds, or simply odds. We can make this function look like a sigmoid function by taking the logarithm of the posterior odds. The logarithm of the posterior odds is called the log-odds or logit: \n\\text{logit} = u = \\ln\\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right)\n\\tag{6}\nWe can now rewrite (5) in terms of the logit: \nP(C_1|x) = \\frac{1}{1 + e^{-u}}\n\\tag{7}\nFinally, we assume that there is a linear relationship between u and the features x:\nu = \\sum_j w_j x_j + w_0 = \\mathbf{w}^T \\mathbf{x} + w_0\n\\tag{8}\nWe now have the logistic function that connects the features x to the posterior probability P(C_1|x): \nP(C_1|x) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + w_0)}}\n\\tag{9}\nThis seems a rather arbitrary assumption. Why does this make sense?",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>logistic connection</span>"
    ]
  },
  {
    "objectID": "bayes/logistic-connection.html#from-bayes-the-logistic",
    "href": "bayes/logistic-connection.html#from-bayes-the-logistic",
    "title": "26  logistic connection",
    "section": "",
    "text": "The one assumption that is needed to make the connection from Bayes’ theorem to the logistic function is that there is a linear relationship between the log-odds and the features x:\n\n\\ln\\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right) = \\mathbf{w}^T \\mathbf{x} + w_0\n\\tag{10}\n\n\n\n\nA linear relationship between the log odds and the features is simple and easy to interpret.\nLinear models are easy to implement and computationally efficient.\nIn a few specific cases (see below) the linearity doesn’t have to be assumed, it emerges naturally from the model.",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>logistic connection</span>"
    ]
  },
  {
    "objectID": "bayes/logistic-connection.html#emergent-linearity",
    "href": "bayes/logistic-connection.html#emergent-linearity",
    "title": "26  logistic connection",
    "section": "26.2 emergent linearity",
    "text": "26.2 emergent linearity\nLet’s start from the log odds definition in (6):\n\nu = \\ln\\left(\\frac{P(C_1|x)}{P(C_2|x)}\\right) = \\ln\\left(\\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}\\right)\n\\tag{11}\n\nWe rewrite this as:\n\\begin{align*}\nu &= \\ln \\frac{P(x|C_1)}{P(x|C_2)} + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\ln P(x|C_1) - \\ln P(x|C_2) + \\ln \\frac{P(C_1)}{P(C_2)}. \\tag{12}\n\\end{align*}\nWe now make the assumption that the likelihoods P(x|C_k) are Gaussian distributions. For simplicity, let’s assume that x is a single feature (univariate case).\n\nP(x|C_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma_k^2}\\right),\n\\tag{13}\n\nwhere C_k are the two classes we have, C_1 and C_2.\nWe now calculate the log of the likelihoods:\n\n\\ln P(x|C_k) = -\\ln \\sqrt{2\\pi \\sigma_k^2} - \\frac{(x-\\mu_k)^2}{2\\sigma_k^2}.\n\\tag{14}\n\nWe now substitute this into Eq. (12) for the log odds:\n\\begin{align*}\nu &= -\\ln \\sqrt{2\\pi \\sigma_1^2} - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\ln \\sqrt{2\\pi \\sigma_2^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\ln \\frac{\\sigma_2}{\\sigma_1} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\ln \\frac{P(C_1)}{P(C_2)}.\n  \\tag{15}\n\\end{align*}\nKEY ASSUMPTION: if we assume that the two classes have the same variance, \\sigma_1 = \\sigma_2 = \\sigma, the expression simplifies to:\n\\begin{align*}\nu &= \\frac{1}{2\\sigma^2} \\left( (x-\\mu_2)^2 - (x-\\mu_1)^2 \\right) + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\frac{1}{2\\sigma^2} \\left( x^2 - 2x\\mu_2 + \\mu_2^2 - x^2 + 2x\\mu_1 - \\mu_1^2 \\right) + \\ln \\frac{P(C_1)}{P(C_2)} \\\\\n  &= \\frac{\\mu_1 - \\mu_2}{\\sigma^2} x + \\frac{\\mu_2^2 - \\mu_1^2}{2\\sigma^2} + \\ln \\frac{P(C_1)}{P(C_2)}.\n  \\tag{16}\n\\end{align*}\nThe first term depends on x linearly, and the other two terms are constants. We can thus rewrite the log odds u as:\n\nu = wx + w_0,\n\\tag{17}\n\nwhere \nw = \\frac{\\mu_1 - \\mu_2}{\\sigma^2}, \\quad w_0 = \\frac{\\mu_2^2 - \\mu_1^2}{2\\sigma^2} + \\ln \\frac{P(C_1)}{P(C_2)}.\n\\tag{18}\n\nUnder the assumption that the distributions have equal variance, the posterior probability can be expressed as a logistic function of a linear combination of the input feature.\nThis is probably the simplest example of a connection between a generative model (Gaussian distributions for each class) and a discriminative model (logistic regression). It would work for other distributions from the exponential family, e.g., Poisson, Bernoulli, Exponential, etc. The one condition they all need to satisfy is that the non-linear part of the log-likelihoods cancels out when we compute the log-odds, leaving a linear function of x.\nWhen we have real data in our hands, we usually don’t know the underlying distributions. The calculation above showed us that a linear relationship between the log odds and the features naturally emerges in a few cases, and this is the motivation for the wider assumption in (10).",
    "crumbs": [
      "bayes",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>logistic connection</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html",
    "href": "svd_and_pca/svd_image_compression.html",
    "title": "27  SVD for image compression",
    "section": "",
    "text": "27.1 the image\nI will use a black-and-white version of the photo below as the matrix to decompose. There are two reasons to use this image:\nimport libraries\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.decomposition import TruncatedSVD\nload jpg into numpy array, convert to grayscale, and display it\nimage = Image.open('../archive/images/splat.jpg')\ngray_image = image.convert('L')  # convert to grayscale\nimage_array = np.array(gray_image)  # make it a numpy array\n# display the image\nfig, ax = plt.subplots()\nax.imshow(image_array, cmap='gray')\nax.axis('off')  # Hide axis",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#the-image",
    "href": "svd_and_pca/svd_image_compression.html#the-image",
    "title": "27  SVD for image compression",
    "section": "",
    "text": "it is a tall and skinny matrix (width 1600 px, height 2600 px). Tall and skinny matrices are usually used in overdetermined systems, which are common in data science.\nthis is the image of the juice of a tomato I ate, as it fell on the concrete floor. I found this splat pattern so beautiful that I took a picture, and I wanted to immortalize it in this tutorial. You’re welcome.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#decomposition",
    "href": "svd_and_pca/svd_image_compression.html#decomposition",
    "title": "27  SVD for image compression",
    "section": "27.2 decomposition",
    "text": "27.2 decomposition\n\n\ndecompose with numpy in one line of code\nU, S, Vt = np.linalg.svd(image_array)\n\n\nLet’s see how the magnitude of the singular values decreases as we go from the first to the last (k goes from zero to 900-1). Also, let’s see how much of the total energy accumulated up to the k-th singular value squared.\n\n\nplot singular values and cumulative energy\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nlenS = len(S)\nax[0].plot(np.arange(lenS), S, marker='o', markeredgecolor=\"black\", markerfacecolor='None', markersize=5, linestyle='None', alpha=0.5)\nax[0].set_yscale('log')  # Set y-axis to log scale\nax[0].set(xlabel='k',\n          ylabel='singular value'\n         )\nax[0].grid(True)  # Enable grid on the first panel\n\ncumS2 = np.cumsum(S**2) / np.sum(S**2)\nax[1].plot(np.arange(lenS), cumS2, marker='o', markeredgecolor=\"black\", markerfacecolor='None', markersize=5, linestyle='None', alpha=0.5)\nklist = [5, 20, 100, 400]\nfor k in klist:\n    ax[1].plot([k-1], [cumS2[k-1]], marker='o', markeredgecolor=\"tab:red\", markerfacecolor='tab:red', markersize=5, linestyle='None')\n    ax[1].text(k+20, cumS2[k-1]-0.001, f'k={k}', color='tab:red', fontsize=12, ha='left')\n\nax[1].set(xlabel='k',\n          ylabel='cumulative energy'\n         )\nax[1].grid(True)  # Enable grid on the second panel\nax[1].yaxis.set_label_position(\"right\")  # Move ylabel to the right\nax[1].yaxis.tick_right()  # Move yticks to the right\n\n\n\n\n\n\n\n\n\nThe square of the Frobenius norm of the matrix X is equal to the sum of the squares of all its singular values.\n\n\\lVert X \\rVert_F^2 = \\sum_{i=1}^{r} \\sigma_i^2\n\nThe Frobenius norm is a measure of the “magnitude” or “size” of the matrix, which can be interpreted as the total amount of “information” in the data. By taking the cumulative sum of the squared singular values, we are effectively measuring how much of this total information is retained with each successive truncation. The term “energy” is an analogy from physics and signal processing. In these fields, the total energy of a signal is often defined as the integral of its squared magnitude over time. This concept carries over to data analysis where the squared singular values are a direct measure of the variance in the data along each singular vector, and the sum of these squares represents the total variance.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#truncation-and-reconstruction",
    "href": "svd_and_pca/svd_image_compression.html#truncation-and-reconstruction",
    "title": "27  SVD for image compression",
    "section": "27.3 truncation and reconstruction",
    "text": "27.3 truncation and reconstruction\nOur original matrix X has dimensions (1600, 2600) and rank 1600. Therefore, it has 1600 non-zero singular values. We can truncate the SVD to a lower rank k &lt; 1600 and reconstruct an approximation of the original matrix using only the first k singular values and their corresponding singular vectors:\n\nX_k = U_k \\Sigma_k V_k^T = \\sum_{i=1}^{k} \\sigma_i \\cdot \\text{outer}(u_i, v_i^T)\n\nwhere U_k is the matrix of the first k left singular vectors, \\Sigma_k is the diagonal matrix of the first k singular values, and V_k^T is the transpose of the matrix of the first k right singular vectors. The outer product \\text{outer}(u_i, v_i^T) creates a rank-1 matrix from the i-th left and right singular vectors.\nUsing the same truncation values k shown in red in the plot above, we can reconstruct approximations of the original image. As k increases, the reconstructed image becomes more detailed and closer to the original image.\n\n\nreconstruct image from first k singular values/vectors\ndef reconstruct_image(U, S, Vt, k):\n    X_reconstructed = np.zeros_like(image_array, dtype=np.float64)\n    for i in range(k): \n        X_reconstructed += S[i] * np.outer(U[:, i], Vt[i, :])   \n    X_reconstructed = np.clip(X_reconstructed, 0, 255)  # ensure values are in byte range\n    X_reconstructed = X_reconstructed.astype(np.uint8)  # convert to uint8\n    return X_reconstructed\n\nreconstructed_images = []\nfor k in klist:\n    X_reconstructed = reconstruct_image(U, S, Vt, k)\n    reconstructed_images.append(X_reconstructed)\n\n\n\n\nvisualize reconstructed images\nfig = plt.figure(figsize=(6, 8))\nfor i, k in enumerate(klist):\n    ax = fig.add_subplot(2, 2, i+1)\n    X_k = reconstructed_images[i]\n    ax.imshow(X_k, cmap='gray')\n    ax.set_title(f'k={k}')\n    ax.axis('off')  # Hide axis\n\n\n\n\n\n\n\n\n\nThe truncation for k=5 gives a blurry image, but for k=20 it is recognizably a tomato splat. The reconstructions for k=100 and k=400 seem indistinguishable at this resolution. Let’s zoom in on a small section of the image to see the differences more clearly.\n\n\nvisualize zoomed in reconstructed images\nfig = plt.figure(figsize=(6, 6))\nfor i, k in enumerate(klist):\n    ax = fig.add_subplot(2, 2, i+1)\n    X_k = reconstructed_images[i][700:1000, 700:1000]  # zoom in\n    ax.imshow(X_k, cmap='gray')\n    ax.set_title(f'k={k}')\n    ax.axis('off')  # Hide axis\n\n\n\n\n\n\n\n\n\nTo capture all the details in the concrete floor we need more than 100 singular values. If we’re interested in the overall shape of the splat, 100 singular values are more than enough. This justifies the name of this chapter: SVD for image compression. We can compress images by storing only the first k singular values and their corresponding singular vectors, instead of the entire image matrix.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_image_compression.html#computational-efficiency",
    "href": "svd_and_pca/svd_image_compression.html#computational-efficiency",
    "title": "27  SVD for image compression",
    "section": "27.4 computational efficiency",
    "text": "27.4 computational efficiency\nWe calculated the reconstruction “the hard way”, by explicitly forming the outer products and summing them. However, we can also use matrix multiplication to achieve the same result more efficiently.\n\nX_k = U_k \\Sigma_k V_k^T\n where:\n\nU_k is the matrix formed by the first k columns of U.\n\\Sigma_k is the k \\times k diagonal matrix formed by the first k singular values, \\Sigma_{11}=\\sigma_1, \\Sigma_{22}=\\sigma_2, etc.\nV_k^T is the matrix formed by the first k rows of V^T.\n\nLet’s leverage matrix multiplication to compute the reconstruction: X_k = U_k (\\Sigma_k V_k^T).\nFirst, let’s look at the product of the diagonal singular value matrix \\Sigma_k and the truncated V^T matrix, V_k^T. \\Sigma_k is a k \\times k diagonal matrix with singular values \\sigma_1, \\sigma_2, ..., \\sigma_k on the diagonal. V_k^T is a k \\times n matrix where each row is a singular vector v_i^T.\n\n\\Sigma_k V_k^T = \\begin{bmatrix}\n\\sigma_1 & 0 & \\dots & 0 \\\\\n0 & \\sigma_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma_k\n\\end{bmatrix}\n\\begin{bmatrix}\n— & v_1^T & — \\\\\n— & v_2^T & — \\\\\n& \\vdots & \\\\\n— & v_k^T & —\n\\end{bmatrix}\n\nMultiplying a diagonal matrix by another matrix from the left scales each row of the second matrix by the corresponding diagonal element of the first matrix. \n\\Sigma_k V_k^T = \\begin{bmatrix}\n— & \\sigma_1 v_1^T & — \\\\\n— & \\sigma_2 v_2^T & — \\\\\n& \\vdots & \\\\\n— & \\sigma_k v_k^T & —\n\\end{bmatrix}\n\nThis matrix contains the scaled singular vectors as its rows.\nNow, we multiply the truncated U matrix, U_k, with the result from the first step. U_k is an m \\times k matrix whose columns are the singular vectors u_1, u_2, ..., u_k. Let’s call the result from the first step, the matrix A. The product is U_k A.\n\nX_k = U_k A = \\begin{bmatrix}\n| & | & & | \\\\\nu_1 & u_2 & \\dots & u_k \\\\\n| & | & & |\n\\end{bmatrix}\n\\begin{bmatrix}\n— & \\sigma_1 v_1^T & — \\\\\n— & \\sigma_2 v_2^T & — \\\\\n& \\vdots & \\\\\n— & \\sigma_k v_k^T & —\n\\end{bmatrix}\n\nMatrix multiplication can be seen as a sum of outer products of the columns of the first matrix and the rows of the second matrix.\n\nX_k = \\sum_{i=1}^{k} (\\text{column } i \\text{ of } U_k) \\cdot (\\text{row } i \\text{ of } A)\n\n\nX_k = \\sum_{i=1}^{k} u_i (\\sigma_i v_i^T)\n\n\nX_k = \\sum_{i=1}^{k} \\sigma_i (u_i v_i^T)\n\nLet’s time the two methods of reconstruction to see the efficiency gain from using matrix multiplication.\n\nk = 100\n\nstart_time1 = time.time()\nX1 = reconstruct_image(U, S, Vt, k)\nend_time1 = time.time()\n\nstart_time2 = time.time()\nX2 = np.dot(U[:, :k], np.dot(np.diag(S[:k]), Vt[:k, :])) \nend_time2 = time.time()\n\nT1 = end_time1 - start_time1\nT2 = end_time2 - start_time2\n\nprint(f\"explicitly computing the outer products: {T1:.6f} seconds\")\nprint(f\"leveraging matrix multiplication: {T2:.6f} seconds\")\nprint(f\"speedup: {T1/T2:.2f}x\")\n\nexplicitly computing the outer products: 1.508656 seconds\nleveraging matrix multiplication: 0.011750 seconds\nspeedup: 128.40x\n\n\nThere are two equivalent ways of leveraging matrix multiplication. Because of the associativity of matrix multiplication, we can compute the product in two different orders:\n\nFirst compute B = \\Sigma_k V_k^T, then compute X_k = U_k B.\nFirst compute C = U_k \\Sigma_k, then compute X_k = C V_k^T.\n\nFor a tall-and-skinny matrix like our image (m&gt;n), the first method is more efficient because it involves multiplying a smaller intermediate matrix B (of size k \\times n) with U_k (of size m \\times k). The second method would involve multiplying a larger intermediate matrix C (of size m \\times k) with V_k^T (of size k \\times n), which is less efficient. For our 2600x1600 image, the difference is tiny, but for larger datasets it can be significant.\nSVD is such a common operation that most numerical computing libraries have highly optimized implementations. See below sklearn’s TruncatedSVD, which uses scipy.sparse.linalg.svds under the hood. It is designed to compute only the first k singular values and vectors, making it more efficient for large datasets where only a few singular values are needed.\n\n\nShow the code\nstart_time2b = time.time()\nX2b = np.dot(np.dot(U[:, :k], np.diag(S[:k])), Vt[:k, :])\nend_time2b = time.time()\n\nsvd = TruncatedSVD(n_components=100)\ntruncated_image = svd.fit_transform(image_array)\nstart_time3 = time.time()\nX3 = svd.inverse_transform(truncated_image)\nend_time3 = time.time()\n\nT2b = end_time2b - start_time2b\nT3 = end_time3 - start_time3\n\nprint(f\"matrix multiplication, option 2: {T2b:.6f} seconds\")\nprint(f\"using sklearn's TruncatedSVD: {T3:.6f} seconds\")\n\n\nmatrix multiplication, option 2: 0.068557 seconds\nusing sklearn's TruncatedSVD: 0.011218 seconds",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>SVD for image compression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html",
    "href": "svd_and_pca/svd_regression.html",
    "title": "28  SVD for regression",
    "section": "",
    "text": "28.1 the problem with Ordinary Least Squares\nLet’s say I want to predict the weight of a person based on their height. I have the following data:\nimport libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nvisualize data\nh = np.array([150 ,160, 170, 180, 190])\nw = np.array([53,  65,  68,  82,  88])\nfig, ax = plt.subplots()\nax.scatter(h, w)\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"Weight (kg)\")\n\n\nText(0, 0.5, 'Weight (kg)')\nI can try to predict the weight using a linear model:\n\\text{weight} = \\beta_0 + \\beta_1 \\cdot \\text{height}.\n\\tag{1}\nIn a general form, we can write this as:\nX\\beta = y,\n\\tag{2}\nwhere X is the design matrix, \\beta is the vector of coefficients, and y is the vector of outputs (weights). This problem probably has no exact solution for \\beta, because the design matrix X is not square (there are more data points than parameters). So we want to find the best approximation \\hat{\\beta} that minimizes the error:\n\\hat{\\beta} = \\arg\\min_\\beta \\|y - X\\beta\\|^2.\n\\tag{3}\nWe know how to solve this, we use the equation we derived in the chapter “the geometry of regression”:\n\\hat{\\beta} = (X^TX)^{-1}X^Ty,\n\\tag{4}\nFor a linear model, the design matrix is:\nX =\n\\begin{bmatrix}\n| & | \\\\\n\\mathbf{1} & h\\\\\n| & |\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 150 \\\\\n1 & 160 \\\\\n1 & 170 \\\\\n1 & 180 \\\\\n1 & 190\n\\end{bmatrix}\n.\n\\tag{5}\nWhat does the matrix X^T X look like?\n\\begin{align*}\nX^TX &=\n\\begin{bmatrix}\n- & \\mathbf{1}^T & - \\\\\n- & h^T & -\n\\end{bmatrix}\n\\begin{bmatrix}\n| & | \\\\\n\\mathbf{1} & h\\\\\n| & |\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n\\mathbf{1}^T\\mathbf{1} & \\mathbf{1}^Th \\\\\nh^T\\mathbf{1} & h^Th\n\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}\n5 & 850 \\\\\n850 & 153000\n\\end{bmatrix}\n\\tag{6}\n\\end{align*}\nThere’s no problem inverting this matrix, so we can find the coefficient estimates \\hat{\\beta} using the formula above.\nSuppose now that we have a new predictor, the height of the person in inches. The design matrix now looks like this:\nX =\n\\begin{bmatrix}\n| & | & | \\\\\n\\mathbf{1} & h_{cm} & h_{inch}\\\\\n| & | & |\n\\end{bmatrix}\n\\tag{7}\nObviously, the columns h_{cm} and h_{inch} are linearly dependent (h_{cm}=ah_{inch}). This means that the matrix X^TX also has linearly dependent columns:\n\\begin{align*}\nX^TX &=\n\\begin{bmatrix}\n- & \\mathbf{1}^T & - \\\\\n- & h_{cm}^T & - \\\\\n- & h_{inch}^T & -\n\\end{bmatrix}\n\\begin{bmatrix}\n| & | & | \\\\\n\\mathbf{1} & h_{cm} & h_{inch}\\\\\n| & | & |\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\n\\mathbf{1}^T\\mathbf{1} & \\mathbf{1}^Th_{cm} & \\mathbf{1}^Th_{inch} \\\\\nh_{cm}^T\\mathbf{1} & h_{cm}^Th_{cm} & h_{cm}^Th_{inch} \\\\\nh_{inch}^T\\mathbf{1} & h_{inch}^Th_{cm} & h_{inch}^Th_{inch}\n\\end{bmatrix}\n\\tag{8}\n\\end{align*}\nUsing the fact that h_{cm}=ah_{inch}, we can see that the second and third columns are linearly dependent. This means that the matrix X^TX is not invertible, and we cannot use the formula above to find the coefficient estimates \\hat{\\beta}. What now?\nThis is an extreme case, but problems similar to this can happen in real life. For example, if we have two predictors that are highly correlated, the matrix X^TX will be close to singular (not invertible). In this case, the coefficients \\beta will be very sensitive to small changes in the data.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html#the-problem-with-ordinary-least-squares",
    "href": "svd_and_pca/svd_regression.html#the-problem-with-ordinary-least-squares",
    "title": "28  SVD for regression",
    "section": "",
    "text": "Height (cm)\nWeight (kg)\n\n\n\n\n150\n53\n\n\n160\n65\n\n\n170\n68\n\n\n180\n82\n\n\n190\n88",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html#svd-to-the-rescue",
    "href": "svd_and_pca/svd_regression.html#svd-to-the-rescue",
    "title": "28  SVD for regression",
    "section": "28.2 SVD to the rescue",
    "text": "28.2 SVD to the rescue\nLet’s use Singular Value Decomposition (SVD) to find the coefficients \\beta. SVD is a powerful technique that can handle multicollinearity and other issues in the data.\nThe SVD of a matrix X is given by:\n\nX = U\\Sigma V^T,\n\\tag{9}\n\nwhere U and V are orthogonal matrices and \\Sigma is a diagonal matrix with singular values on the diagonal.\nWe can plug the SVD of X into the least squares problem, which is to find the \\hat{\\beta} that best satisfies X\\hat{\\beta} = \\hat{y}:\n\nU\\Sigma V^T\\hat{\\beta} = \\hat{y}.\n\\tag{10}\n\nWe define now the Moore-Penrose pseudo-inverse of X, which is given by:\n\nX^+ = V\\Sigma^+U^T,\n\\tag{11}\n\nwhere \\Sigma^+ is obtained by taking the reciprocal of the non-zero singular values in \\Sigma and transposing the resulting matrix.\nThis pseudo-inverse has the following properties:\n\nX^+X = I. This means that X^+ is a left-inverse of X.\nXX^+ is a projection matrix onto the column space of X. In other words, left-multiplying XX^+ to any vector gives the projection of that vector onto the column space of X. In particular, XX^+y = \\hat{y}.\nThis is very similar to the property we used in the chapter “the geometry of regression”, where we had P_Xy = \\hat{y}, with P_X being the projection matrix onto the column space of X. There, we found that\n\n\\hat{\\beta} = (X^TX)^{-1}X^Ty,\n\nLeft-multiplying both sides by X gives: \nX\\hat{\\beta} = X(X^TX)^{-1}X^Ty = P_Xy = \\hat{y}.\n So we found that in the OLS case, X(X^TX)^{-1}X^T is the projection matrix onto the column space of X. Here, we have a more general result that works even when X^TX is not invertible: XX^+ is the projection matrix onto the column space of X.\n\nWe left-multiply Eq. (10) by X^+, and also substitute \\hat{y}=XX^+y as we just saw:\n\\begin{align*}\nX^+ (U\\Sigma V^T\\hat{\\beta}) &= X^+XX^+y\\\\\n(V\\Sigma^+U^T)(U\\Sigma V^T)\\hat{\\beta} &= X^+y\n\\tag{12}\n\\end{align*}\nThe term (V\\Sigma^+U^T)(U\\Sigma V^T) simplifies to the identity matrix, so we have:\n\n\\hat{\\beta} = X^+y = V\\Sigma^+U^Ty.\n\\tag{13}\n\nThis is the formula we will use to find the coefficients \\hat{\\beta} using SVD. This formula works even when X^TX is not invertible, and it is more stable than the OLS formula.",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "svd_and_pca/svd_regression.html#in-practice",
    "href": "svd_and_pca/svd_regression.html#in-practice",
    "title": "28  SVD for regression",
    "section": "28.3 in practice",
    "text": "28.3 in practice\nLet’s go back to the problematic example with height in cm and inches. We can use the SVD to find the coefficients \\hat{\\beta}.\n\n# design matrix with intercept, height in cm and height in inches\nconversion_factor = 2.54\nX = np.vstack([np.ones(len(h)), h, h/conversion_factor]).T\n# compute SVD of X\nU, S, VT = np.linalg.svd(X, full_matrices=False)\nSigma = np.diag(S)\nV = VT.T\n# compute coefficients using SVD\ny = w\nSigma_plus = np.zeros(Sigma.T.shape)\nfor i in range(len(S)):\n    if S[i] &gt; 1e-10:  # avoid division by zero\n        Sigma_plus[i, i] = 1 / S[i]\nX_plus = V @ Sigma_plus @ U.T\nbeta_hat = X_plus @ y\n# make predictions\ny_hat = X @ beta_hat\n\n\n\nplot results\nfig, ax = plt.subplots()\nax.scatter(h, w, label=\"data\")\n# plot predictions\nax.plot(h, y_hat, color=\"tab:red\", label=\"SVD fit\")\nax.legend()\nax.set_xlabel(\"height (cm)\")\nax.set_ylabel(\"weight (kg)\");",
    "crumbs": [
      "svd and pca",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>SVD for regression</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html",
    "href": "decision_trees/CART_classification.html",
    "title": "29  classification with CART",
    "section": "",
    "text": "29.1 a jungle party\nImagine you’re throwing a party in the jungle. You know you have three types of guests—koalas, foxes, and bonobos—but you don’t know who is who. To make sure you serve the right food, you need to automatically figure out which animal is which. Koalas only eat eucalyptus leaves, foxes prefer meat, and bonobos love fruit, so it’s important to get it right!\nTo solve this problem, you’ll use a decision tree. You’ve gathered some data on your past animal guests, and for each one, you have their height and weight, as well as their species (their label). Your goal is to build a system that can learn from this historical data to correctly classify a new, unlabeled animal based on its height and weight alone.\nThe data is structured as:\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nWe are using the famous Iris dataset structure, but pretending they are animals for this example.\nload iris dataset and prepare data\niris = load_iris()\nX = iris.data[:, [0, 1]]\ny = iris.target\nplot\nfig, ax = plt.subplots(figsize=(6, 6))\nmarkers = ['o', 's', '+']\ncolors = ['black', 'None', 'tab:red']\niris.target_names = ['koala', 'fox', 'bonobo']\niris.feature_names = ['height', 'weight', 'age']\nfor i, marker in enumerate(markers):\n    ax.scatter(X[y == i, 0], X[y == i, 1], \n               c=colors[i], \n               edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\nax.legend()\nax.set_xlabel(iris.feature_names[0])\nax.set_ylabel(iris.feature_names[1])\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_1841/3638754974.py:8: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax.scatter(X[y == i, 0], X[y == i, 1],\n\n\nText(0, 0.5, 'weight')\nWe will use the CART (Classification and Regression Trees) algorithm to build a decision tree. There are many other methods, but CART is one of the most popular, it’s important to know it well.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>classification with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#a-jungle-party",
    "href": "decision_trees/CART_classification.html#a-jungle-party",
    "title": "29  classification with CART",
    "section": "",
    "text": "Features: height and weight, continuous, numerical features.\nCategories (Classes): Koala, Fox, Bonobo",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>classification with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#the-split",
    "href": "decision_trees/CART_classification.html#the-split",
    "title": "29  classification with CART",
    "section": "29.2 the split",
    "text": "29.2 the split\nWe will cut the data into two parts along one of the features. We will try all possible cut thresholds for each of the features and see which one gives us the best split. Ideally, we want to end up with two groups, “left” and “right”, that are as “pure” as possible, meaning that each group contains animals of only one species. It might not be possible to get perfect homogeneity, so we will need to quantify how good a split is. We will use two different metrics to evaluate the quality of a split:\n\nInformation Gain (based on Entropy): This measures how much uncertainty in the data is reduced after the split. A higher information gain means a better split. \n  IG(T, X) = \\underbrace{H(T)}_{\\text{original entropy}} - \\underbrace{\\frac{N_\\text{left}}{N} H(T_\\text{left})}_{\\text{left group}} - \\underbrace{\\frac{N_\\text{right}}{N} H(T_\\text{right})}_{\\text{right group}}\n where T_\\text{left} and T_\\text{right} are the subsets after the split, and N denotes the total number of samples in a (sub) dataset.\nGini Impurity: This measure is often explained as the impurity of a split. A better point of view is to interpret is as the probability of misclassification. If we picked an element at random from the dataset, what is the probability that we would misclassify it if we labeled it according to the distribution of classes in the dataset? The probability of picking an element of class i is P_i, and we would misclassify it with probability 1 - P_i. So the total probability of misclassification is P_i(1 - P_i). If we sum this over all classes, we get the Gini Impurity: \n  G(T) = \\sum_{i=1}^{C} P_i (1 - P_i) = 1 - \\sum_{i=1}^{C} P_i^2\n\n\nNow let’s take our dataset and try to find the best split using both metrics.\n\n29.2.1 entropy\nWe will start with the entropy metric.\n\n\ndefine useful functions\ndef calculate_gini(y_subset):\n    \"\"\"Calculates the Gini Impurity for a given subset of class labels.\"\"\"\n    # If the subset is empty, there's no impurity.\n    if len(y_subset) == 0:\n        return 0.0\n    \n    # Get the counts of each unique class in the subset.\n    unique_classes, counts = np.unique(y_subset, return_counts=True)\n    \n    # Calculate the probability of each class.\n    probabilities = counts / len(y_subset)\n    \n    # Gini Impurity formula: 1 - sum(p^2) for each class\n    return 1.0 - np.sum(probabilities**2)\n\ndef calculate_entropy(y_subset):\n    \"\"\"Calculates the Entropy for a given subset of class labels.\"\"\"\n    if len(y_subset) == 0:\n        return 0.0\n    unique_classes, counts = np.unique(y_subset, return_counts=True)\n    probabilities = counts / len(y_subset)\n    epsilon = 1e-9\n    return -np.sum(probabilities * np.log2(probabilities + epsilon))\n\ndef find_best_split(X_feature, y, criterion='entropy'):\n    \"\"\"\n    Finds the best split point for a single feature based on a specified criterion.\n    Returns the best threshold and the score (Information Gain or Gini Impurity) after the split.\n    \"\"\"\n    # Get the unique values of the feature to consider as split points.\n    unique_values = np.sort(np.unique(X_feature))\n    differences = np.diff(unique_values)\n    threshold_candidates = unique_values[:-1] + differences / 2\n    if criterion == 'entropy':\n        initial_score = calculate_entropy(y)\n        best_score = 0.0  # We want to maximize Information Gain\n        criterion_function = calculate_entropy\n    elif criterion == 'gini':\n        best_score = 1.0  # We want to minimize Gini Impurity\n        criterion_function = calculate_gini\n    else:\n        raise ValueError(\"Criterion must be 'entropy' or 'gini'\")\n\n    best_threshold = None\n\n    # Iterate through each unique value as a potential split point\n    for threshold in threshold_candidates:\n        # Split the data into two groups based on the threshold\n        condition = X_feature &lt;= threshold\n        y_left = y[condition]    # condition is True\n        y_right = y[~condition]  # condition is False\n\n        score_left = criterion_function(y_left)\n        score_right = criterion_function(y_right)\n        fraction_left = len(y_left) / len(y)\n        fraction_right = len(y_right) / len(y)\n        weighted_score = fraction_left * score_left + fraction_right * score_right\n\n        if criterion == 'entropy':\n            information_gain = initial_score - weighted_score\n            # If this split is the best so far, save it!\n            if information_gain &gt; best_score:  # Max Information Gain\n                best_score = information_gain\n                best_threshold = threshold\n        \n        elif criterion == 'gini':\n            # If this split is the best so far, save it!\n            if weighted_score &lt; best_score:  # Min Gini Impurity\n                best_score = weighted_score\n                best_threshold = threshold\n\n    return best_threshold, best_score\n\ndef quantify_all_splits(X_feature, y, criterion='entropy'):\n    \"\"\"\n    Finds the best split point for a single feature based on a specified criterion.\n    Returns the best threshold and the score (Information Gain or Gini Impurity) after the split.\n    \"\"\"\n    # Get the unique values of the feature to consider as split points.\n    unique_values = np.sort(np.unique(X_feature))\n    differences = np.diff(unique_values)\n    threshold_candidates = unique_values[:-1] + differences / 2\n    score_list = []\n    if criterion == 'entropy':\n        initial_score = calculate_entropy(y)\n        best_score = 0.0  # We want to maximize Information Gain\n        criterion_function = calculate_entropy\n    elif criterion == 'gini':\n        best_score = 1.0  # We want to minimize Gini Impurity\n        criterion_function = calculate_gini\n    else:\n        raise ValueError(\"Criterion must be 'entropy' or 'gini'\")\n\n    best_threshold = None\n\n    # Iterate through each unique value as a potential split point\n    for threshold in threshold_candidates:\n        # Split the data into two groups based on the threshold\n        condition = X_feature &lt;= threshold\n        y_left = y[condition]    # condition is True\n        y_right = y[~condition]  # condition is False\n\n        score_left = criterion_function(y_left)\n        score_right = criterion_function(y_right)\n        fraction_left = len(y_left) / len(y)\n        fraction_right = len(y_right) / len(y)\n        weighted_score = fraction_left * score_left + fraction_right * score_right\n\n        if criterion == 'entropy':\n            information_gain = initial_score - weighted_score\n            score_list.append((threshold, information_gain))\n            # If this split is the best so far, save it!\n\n        elif criterion == 'gini':\n            score_list.append((threshold, weighted_score))\n\n    return np.array(score_list)\n\n\n\n\nplot\nfig = plt.figure(1, figsize=(8, 6))\ngs = gridspec.GridSpec(2, 2, width_ratios=[0.2,1], height_ratios=[1,0.2])\ngs.update(left=0.16, right=0.86,top=0.88, bottom=0.13, hspace=0.05, wspace=0.05)\n\nax_main = plt.subplot(gs[0, 1])\nax_height = plt.subplot(gs[1, 1])\nax_weight = plt.subplot(gs[0, 0])\n\nfor i, marker in enumerate(markers):\n    ax_main.scatter(X[y == i, 0], X[y == i, 1], \n                    c=colors[i], \n                    edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nsplit_feature_height = quantify_all_splits(X[:, 0], y, criterion='entropy')\nsplit_feature_weight = quantify_all_splits(X[:, 1], y, criterion='entropy')\n\nax_height.plot(split_feature_height[:, 0], split_feature_height[:, 1], marker='o')\nax_weight.plot(split_feature_weight[:, 1], split_feature_weight[:, 0], marker='o')  \n\nbest_height_split = np.argmax(split_feature_height[:, 1])\nbest_weight_split = np.argmax(split_feature_weight[:, 1])\n\nax_main.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_height.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_main.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\nax_weight.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\n\nax_main.set(xticklabels=[],\n            yticklabels=[],\n            title=\"splits using Entropy\",\n            xlim=(4, 8),\n            ylim=(1.5, 5.0)\n            )\nax_main.legend()\n\nax_height.set(ylim=(0, 0.6),\n              xlim=(4, 8),\n              xlabel=\"height\"\n              )\nax_weight.set(xlim=(0, 0.6),\n              ylim=(1.5, 5.0),\n                ylabel=\"weight\",\n              )\n\nax_weight.spines['top'].set_visible(False)\nax_weight.spines['right'].set_visible(False)\nax_height.spines['top'].set_visible(False)\nax_height.spines['right'].set_visible(False)\n\nax_height.text(-0.15, 0.5, \"Information\\nGain\", rotation=0, va='center', ha='center', transform=ax_height.transAxes)\nplt.tight_layout()\n\nprint(f\"Height: highest Information Gain: {split_feature_height[best_height_split, 1]:.2f} at height={split_feature_height[best_height_split, 0]:.2f}\")\nprint(f\"Weight: highest Information Gain: {split_feature_weight[best_weight_split, 1]:.2f} at weight={split_feature_weight[best_weight_split, 0]:.2f}\")\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_1841/2101311549.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax_main.scatter(X[y == i, 0], X[y == i, 1],\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_1841/2101311549.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\nHeight: highest Information Gain: 0.56 at height=5.55\nWeight: highest Information Gain: 0.28 at weight=3.35\n\n\n\n\n\n\n\n\n\n\nThe best split for the ‘height’ feature is at a height 5.55, with IG=0.56.\nThe best split for the ‘weight’ feature is at a weight 3.35, with IG=0.28.\n\nUsing the Entropy metric, the first split will be on the ‘height’ feature at a height of 5.55, since it has the highest information gain.\n\n\n29.2.2 Gini impurity\nNow let’s try the Gini impurity metric.\n\n\nplot\nfig = plt.figure(1, figsize=(8, 6))\ngs = gridspec.GridSpec(2, 2, width_ratios=[0.2,1], height_ratios=[1,0.2])\ngs.update(left=0.16, right=0.86,top=0.88, bottom=0.13, hspace=0.05, wspace=0.05)\n\nax_main = plt.subplot(gs[0, 1])\nax_height = plt.subplot(gs[1, 1])\nax_weight = plt.subplot(gs[0, 0])\n\nfor i, marker in enumerate(markers):\n    ax_main.scatter(X[y == i, 0], X[y == i, 1], \n                    c=colors[i], \n                    edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nsplit_feature_height = quantify_all_splits(X[:, 0], y, criterion='gini')\nsplit_feature_weight = quantify_all_splits(X[:, 1], y, criterion='gini')\n\nax_height.plot(split_feature_height[:, 0], split_feature_height[:, 1], marker='o')\nax_weight.plot(split_feature_weight[:, 1], split_feature_weight[:, 0], marker='o')  \n\nbest_height_split = np.argmin(split_feature_height[:, 1])\nbest_weight_split = np.argmin(split_feature_weight[:, 1])\n\nax_main.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_height.axvline(split_feature_height[best_height_split, 0], color='gray', linestyle='--')\nax_main.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\nax_weight.axhline(split_feature_weight[best_weight_split, 0], color='gray', linestyle=':')\n\nax_main.set(xticklabels=[],\n            yticklabels=[],\n            title=\"splits using Gini Impurity\",\n            xlim=(4, 8),\n            ylim=(1.5, 5.0)\n            )\nax_main.legend()\n\nax_height.set(ylim=(0.4, 0.70),\n              xlim=(4, 8),\n              xlabel=\"height\"\n              )\nax_weight.set(xlim=(0.4, 0.70),\n              ylim=(1.5, 5.0),\n                ylabel=\"weight\",\n              )\n\nax_weight.spines['top'].set_visible(False)\nax_weight.spines['right'].set_visible(False)\nax_height.spines['top'].set_visible(False)\nax_height.spines['right'].set_visible(False)\n\nax_height.text(-0.15, 0.5, \"Gini\\nImpurity\", rotation=0, va='center', ha='center', transform=ax_height.transAxes)\nplt.tight_layout()\n\nprint(f\"Height: lowest Gini Impurity: {split_feature_height[best_height_split, 1]:.2f} at height={split_feature_height[best_height_split, 0]:.2f}\")\nprint(f\"Weight: lowest Gini Impurity: {split_feature_weight[best_weight_split, 1]:.2f} at weight={split_feature_weight[best_weight_split, 0]:.2f}\")\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_1841/3659548085.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax_main.scatter(X[y == i, 0], X[y == i, 1],\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_1841/3659548085.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\nHeight: lowest Gini Impurity: 0.44 at height=5.45\nWeight: lowest Gini Impurity: 0.54 at weight=3.35\n\n\n\n\n\n\n\n\n\n\nThe best split for the ‘height’ feature is at a height 5.45, with Gini=0.44.\nThe best split for the ‘weight’ feature is at a weight 3.35, with Gini=0.54.\n\nUsing the Gini Impurity metric, the first split will be on the ‘height’ feature at a height of 5.45, since it has the lowest Gini impurity.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>classification with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#successive-splits",
    "href": "decision_trees/CART_classification.html#successive-splits",
    "title": "29  classification with CART",
    "section": "29.3 successive splits",
    "text": "29.3 successive splits\nThe same idea used for the first split is then applied to each of the subsets, recursively. The process continues until one of the stopping criteria is met, such as:\n\nAll samples in a node belong to the same class.\nThe maximum depth of the tree is reached.\nThe number of samples in a node is less than a predefined minimum.\n\nI find it useful to visualize the decision tree as a series of splits in the feature space: \nSource: “Predicting University Students’ Academic Success and Major Using Random Forests”, by Cédric Beaulac and Jeffrey S. Rosenthal\nSplitting the feature space with vertical and horizontal lines reminds me of a classic 1990’s computer game, JazzBall. Check out a video of the game here, and see it reminds you of the basic algorithm discussed so far.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>classification with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#sklearn-tree-decisiontreeclassifier",
    "href": "decision_trees/CART_classification.html#sklearn-tree-decisiontreeclassifier",
    "title": "29  classification with CART",
    "section": "29.4 sklearn tree DecisionTreeClassifier",
    "text": "29.4 sklearn tree DecisionTreeClassifier\nWe will now use the DecisionTreeClassifier from sklearn.tree to build a decision tree classifier. Let’s restrict the maximum depth of the tree to 3, so we can visualize it easily. All the hard work was already coded for use, it’ll take us only two lines of code to create and fit the model.\n\n\nbuild and fit decision tree classifiers\n# using gini impurity\nclassifier_gini = DecisionTreeClassifier(criterion='gini', max_depth=3)#, random_state=42)\nclassifier_gini.fit(X, y)\n\n# using entropy\nclassifier_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3)#, random_state=42)\nclassifier_entropy.fit(X, y)\n\n\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'entropy'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \n3\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\nNow let’s visualize the results.\n\n\ndefine useful functions\n# Helper function to plot decision boundaries\ndef plot_decision_boundaries(ax, model, X, y, title):\n    \"\"\"\n    Plots the decision boundaries for a given classifier.\n    \"\"\"\n    # Define a mesh grid to color the background based on predictions\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                         np.arange(y_min, y_max, 0.02))\n\n    # Predict the class for each point in the mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the colored regions\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n\n    # Set labels and title\n    ax.set_title(title)\n    ax.set_xlabel(iris.feature_names[0])\n    ax.set_ylabel(iris.feature_names[1])\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\n\n\nplot boundaries for each criterion\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplot_decision_boundaries(ax[0], classifier_entropy, X, y, \"Entropy Criterion\")\nfor i, marker in enumerate(markers):\n    ax[0].scatter(X[y == i, 0], X[y == i, 1], \n                  c=colors[i], \n                  edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nplot_decision_boundaries(ax[1], classifier_gini, X, y, \"Gini Criterion\")\nfor i, marker in enumerate(markers):\n    ax[1].scatter(X[y == i, 0], X[y == i, 1], \n                  c=colors[i], \n                  edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_5801/2896127227.py:5: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax[0].scatter(X[y == i, 0], X[y == i, 1],\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_5801/2896127227.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax[1].scatter(X[y == i, 0], X[y == i, 1],\n\n\n\n\n\n\n\n\n\nJust by using different criteria, we get different boundaries! We can now predict the species of a new animal based on its height and weight.\nFor example, an animal with height 6.3 and weight 4.0 would be classified as:\n\n\npredict\n# Predict the species of a new animal with height 5.7 and weight 4.0 using the entropy-based classifier\nsample = np.array([[6.3, 4.0]])\npredicted_class_entropy = classifier_entropy.predict(sample)\npredicted_class_gini = classifier_gini.predict(sample)\nprint(f\"Predicted species (Entropy): {iris.target_names[predicted_class_entropy[0]]}\")\nprint(f\"Predicted species (Gini): {iris.target_names[predicted_class_gini   [0]]}\")\n\n\nPredicted species (Entropy): koala\nPredicted species (Gini): bonobo\n\n\nSee also the decision trees for each criterion.\n\n\nplot decision tree for entropy criterion\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplot_tree(classifier_entropy,\n          filled=True,\n          rounded=True,\n          class_names=iris.target_names,\n          feature_names=[iris.feature_names[0], iris.feature_names[1]],\n          ax=ax);\n\n\n\n\n\n\n\n\n\n\n\nplot decision tree for gini criterion\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplot_tree(classifier_gini,\n          filled=True,\n          rounded=True,\n          class_names=iris.target_names,\n          feature_names=[iris.feature_names[0], iris.feature_names[1]],\n          ax=ax);",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>classification with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_classification.html#overfitting",
    "href": "decision_trees/CART_classification.html#overfitting",
    "title": "29  classification with CART",
    "section": "29.5 overfitting",
    "text": "29.5 overfitting\nWhat would happen if we chose to grow our decision tree until all leaves are pure? This would lead to a very complex tree that perfectly classifies the training data, but might not generalize well to new, unseen data. This is known as overfitting.\n\n\nno max_depth, grow until pure\nclassifier_gini = DecisionTreeClassifier(criterion='gini')\nclassifier_gini.fit(X, y)\n\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\nplot boundaries and decision tree\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplot_decision_boundaries(ax[0], classifier_gini, X, y, \"Decision Boundaries in feature space (Gini Criterion)\")\nfor i, marker in enumerate(markers):\n    ax[0].scatter(X[y == i, 0], X[y == i, 1], \n                  c=colors[i], \n                  edgecolor='k', s=30, marker=marker, label=iris.target_names[i])\n\nplot_tree(classifier_gini,\n          filled=True,\n          rounded=True,\n          class_names=iris.target_names,\n          feature_names=[iris.feature_names[0], iris.feature_names[1]],\n          ax=ax[1]);\nax[1].set_title(\"Decision Tree (Gini Criterion)\");\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_5801/205532632.py:5: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  ax[0].scatter(X[y == i, 0], X[y == i, 1],\n\n\n\n\n\n\n\n\n\nWe can see that some of the regions are very small and specific to the training data points. This means that the model has learned not only the underlying patterns in the data but also the noise and outliers, which can lead to poor performance on new data.\nTo avoid overfitting, we can use techniques such as:\n\nSetting a maximum depth. We limit how deep the tree can grow. Read about max_depth.\nSetting a minimum number of samples required to split a node. Read about min_samples_split.\nPost-pruning: Cutting back the tree after it has been grown to remove branches that do not provide significant predictive power. Read about cost_complexity_pruning_path.\n\nOther machine learning algorithms, such as Random Forests and Gradient Boosted Trees, use ensemble methods that combine multiple decision trees to improve performance and reduce overfitting.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>classification with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_regression.html",
    "href": "decision_trees/CART_regression.html",
    "title": "30  regression with CART",
    "section": "",
    "text": "30.1 the split\nWe will follow a similar procedure to split the data along the features, but this time our target variable is continuous (age) instead of categorical (animal type). In classification we wanted to have leaves as pure as possible, and we quantified that either with Gini impurity or entropy. In regression, we want to minimize the variance of the target variable within each leaf. In our example, this means that we want to split the data in a way that the ages of the animals in each leaf are as similar as possible.\nThe cost function we will use to evaluate the quality of a split is the Weighted Mean Squared Error (MSE):\nJ(j,s) = \\frac{N_\\text{left}}{N_\\text{total}} \\cdot \\text{MSE}_\\text{left} + \\frac{N_\\text{right}}{N_\\text{total}} \\cdot \\text{MSE}_\\text{right},\n where N_\\text{left} and N_\\text{right} are the number of samples in the left and right child nodes, respectively, and N_\\text{total} is the total number of samples in the parent node. \\text{MSE}_\\text{left} and \\text{MSE}_\\text{right} are the mean squared errors of the target variable in the left and right child nodes:\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\bar{y})^2,\n where y_i are the target values in the node, \\bar{y} is the mean target value in the node, and N is the number of samples in the node.\nThe cost function is weighted by the number of samples in each child node to account for the fact that larger nodes have a greater impact on the overall variance.\nSo how do we know which split is the best? We will evaluate all possible split thresholds s along all features j and choose the one that minimizes the Weighted MSE. In a mathematical language:\n(j^*, s^*) = \\arg\\min_{j,s} J(j,s).",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>regression with CART</span>"
    ]
  },
  {
    "objectID": "decision_trees/CART_regression.html#sklearn-tree-decisiontreeregressor",
    "href": "decision_trees/CART_regression.html#sklearn-tree-decisiontreeregressor",
    "title": "30  regression with CART",
    "section": "30.2 sklearn tree DecisionTreeRegressor",
    "text": "30.2 sklearn tree DecisionTreeRegressor\nWe will use the DecisionTreeRegressor class from sklearn.tree to build our regression tree.\n\n\nShow the code\nregressor = DecisionTreeRegressor(max_depth=3)\nregressor.fit(X, y)\n\n\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'squared_error'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \n3\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\nShow the code\n# Helper function to plot decision boundaries\ndef plot_decision_boundaries(ax, model, X, y, title):\n    \"\"\"\n    Plots the decision boundaries for a given classifier.\n    \"\"\"\n    # Define a mesh grid to color the background based on predictions\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                         np.arange(y_min, y_max, 0.02))\n\n    # Predict the class for each point in the mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the colored regions\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap='plasma', vmin=1, vmax=7)\n\n    # Set labels and title\n    ax.set_title(title)\n    ax.set_xlabel(iris.feature_names[0])\n    ax.set_ylabel(iris.feature_names[1])\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Helper function to plot decision boundaries\ndef plot_decision_boundaries_3d(ax, model, X, y, title):\n    # Define a mesh grid\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    \n    # Predict the values for each point in the mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the surface\n    surf = ax.plot_surface(xx, yy, Z, alpha=0.5, cmap='plasma', vmin=1, vmax=7)\n    \n    # Plot the actual data points\n    scatter = ax.scatter(X[:, 0], X[:, 1], y, c=y, cmap='plasma', edgecolor='k', s=50, vmin=1, vmax=7)\n    \n    # Set labels and title\n    ax.set_xlabel(iris.feature_names[0])\n    ax.set_ylabel(iris.feature_names[1])\n    ax.set_zlabel(iris.feature_names[2])\n    ax.set_title(title)\n    \n    # fig.colorbar(scatter, ax=ax, label='Age (years)', shrink=0.5)\n    \n    return fig, ax\n\n\n\n\nShow the code\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_decision_boundaries(ax, regressor, X, y, \"Regression Tree Predictions\")\nscatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='plasma', edgecolor='k', s=50, vmin=1, vmax=7)\nfig.colorbar(scatter, label='Age (years)')\n\n\n\n\n\n\n\n\n\nEach region (leaf) of the tree will predict the mean age of the training samples that fall into that region. Visualizing this in 3d shows us steps, because the regression tree creates a piecewise constant approximation of the target variable (age) over the feature space (height and weight).\n\n\nShow the code\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(121, projection='3d')\nplot_decision_boundaries_3d(ax, regressor, X, y, \"3d plot\")\nax.view_init(elev=20, azim=110)\n\n\n\n\n\n\n\n\n\nThe same consideration regarding overfitting applies here as in classification.",
    "crumbs": [
      "decision trees",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>regression with CART</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html",
    "href": "misc/trend_test.html",
    "title": "31  trend test",
    "section": "",
    "text": "31.1 linear regression\nOne of the simplest ways to determine if there is a trend between two variables is to use linear regression.\nfit linear model and plot\nslope, intercept, _, _, _ = linregress(x, y)\ny_hat = slope * x + intercept\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, label='data')\nax.plot(x, y_hat, color='red', label=f'model: y = {slope:.2f}x + {intercept:.2f}')\nax.set(xlabel='X')\nax.set_ylabel('Y', rotation=0, labelpad=20)\nax.legend()\nGreat, we found that there is a positive slope, its value is 0.13. It this enough to say that there is a trend?\nWe need to determine if the slope is significantly different from zero. For that we can use a t-test.\nNull Hypothesis: The slope is equal to zero (no trend).\nAlternative Hypothesis: The slope is not equal to zero (there is a trend).\nThe t-statistic is calculated as:\nt = \\frac{\\text{slope} - 0}{SE_\\text{slope}},\nwhere SE_\\text{slope} is the standard error of the slope, and it is given by:\nSE_\\text{slope} = \\frac{SD_\\text{residuals}}{\\sqrt{\\sum (x_i - \\bar{x})^2}}.\nwhere SD_\\text{residuals} is the standard deviation of the residuals, x_i are the individual x values, and \\bar{x} is the mean of the x values.\nThe standard deviation of the residuals is calculated as:\nSD_\\text{residuals} = \\sqrt{\\frac{\\sum (y_i - \\hat{y}_i)^2}{n - 2}},\nwhere y_i are the observed y values, \\hat{y}_i are the predicted y values from the regression, and n is the number of data points. The number of the degrees of freedom is n - 2 because we are estimating two parameters (the slope and the intercept) from the data.\nLet’s compute SE_\\text{slope}, the t-statistic, and the p-value for our example.\nmanual computation vs scipy\n# calculate the residuals\nresiduals = y - y_hat\n# calculate the sum of squared residuals (SSR)\nsum_squared_residuals = np.sum(residuals**2)\n# calculate the Residual Standard Error (s_e)\nn = len(x)\ndegrees_of_freedom = n - 2\nresidual_std_error = np.sqrt(sum_squared_residuals / degrees_of_freedom)\n# calculate the sum of squared deviations of x from its mean\nx_mean = np.mean(x)\nsum_squared_x_deviations = np.sum((x - x_mean)**2)\n# put it all together to get SE_slope\n# SE_slope = (typical error) / (spread of x)\nSE_slope = residual_std_error / np.sqrt(sum_squared_x_deviations)\n# verify the result against the value directly from scipy\nscipy_slope, scipy_intercept, scipy_r, scipy_p, scipy_se = linregress(x, y)\nprint(f\"manually calculated SE_slope:          {SE_slope:.6f}\")\nprint(f\"SE_slope from scipy.stats.linregress:  {scipy_se:.6f}\")\n\n\nmanually calculated SE_slope:          0.057695\nSE_slope from scipy.stats.linregress:  0.057695\ncalculate p-value manually\nt_statistic = (slope-0) / SE_slope\np_value = 2 * (1 - scipy.stats.t.cdf(np.abs(t_statistic), df=degrees_of_freedom))\nprint(f\"manually calculated p-value: {p_value:.6f}\")\nprint(f'scipy p-value:               {scipy_p:.6f}')\n\n\nmanually calculated p-value: 0.028344\nscipy p-value:               0.028344\nIf we choose a significance level \\alpha=0.05, the p-value we found indicates that we can reject the null hypothesis and conclude that there is a significant trend between x and y.\nIf instead of testing if the slope is different from zero, but rather if it is greater than zero (i.e., a one-sided test), we would divide the p-value by 2.\none-sided p-value\np_value = (1 - scipy.stats.t.cdf(np.abs(t_statistic), df=degrees_of_freedom))\nscipy_slope, scipy_intercept, scipy_r, scipy_p, scipy_se = linregress(x, y, alternative='greater')\nprint(f\"manually calculated p-value: {p_value:.6f}\")\nprint(f'scipy p-value:               {scipy_p:.6f}')\n\n\nmanually calculated p-value: 0.014172\nscipy p-value:               0.014172\nOne last remark. What does the formula for the standard error of the slope mean?",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#linear-regression",
    "href": "misc/trend_test.html#linear-regression",
    "title": "31  trend test",
    "section": "",
    "text": "inside the square root, we have a quantity dependent on y squared divided by a quantity dependent on x squared. Dimensionally this makes sense, because the standard error of the slope should have the same dimension as the slope \\Delta y/\\Delta x.\nthe larger the variability of the residuals (i.e., the more scattered the data points are around the regression line), the larger the standard error of the slope, and thus the less precise our estimate of the slope is.\nWe can manipulate the formula a little bit to get more intuition: \n  SE_\\text{slope} = \\sqrt{\\frac{1}{n-2}}\\frac{SD_y}{SD_x}\\sqrt{1-r^2},\n where SD_y and SD_x are the standard deviations of y and x, respectively, and r is the correlation coefficient between x and y. From this formula we can see that:\n\nthe standard error of the slope decreases with increasing sample size n (more data points lead to a more precise estimate of the slope);\nimagine all the data points in a rectangular box, and all the possible slopes that can be drawn within that box. If you change the dimensions of the box, you need to account for that, and that is the second term.\nThe last term acounts for the spread of the points about the line.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#mann-kendall-trend-test",
    "href": "misc/trend_test.html#mann-kendall-trend-test",
    "title": "31  trend test",
    "section": "31.2 Mann-Kendall Trend Test",
    "text": "31.2 Mann-Kendall Trend Test\nThe method above assumed that the relationship between x and y is linear, and that the residuals are normally distributed. If these assumptions are not met, we can use a non-parametric test like the Mann-Kendall trend test. The intuition behind this test works like a voting system. You go through your data and compare every data point to all points that come after it.\n\nif a later points is higher, you give a +1 vote\nif a later point is lower, you give a -1 vote\nif they are equal, you give a 0 vote\n\nAll these votes are summed up. A large positive sum indicates an increasing trend, a large negative sum indicates a decreasing trend, and a sum close to zero indicates no trend. We can then calculate a test statistic Z based on the sum of votes, and use it to determine the p-value. If the p-value is less than our chosen significance level (e.g., 0.05), we can reject the null hypothesis of no trend. We can use the package pymannkendall to perform the Mann-Kendall trend test.\n\n\napplying the Mann-Kendall test\nfrom pymannkendall import original_test\nmk_result = original_test(y)\nprint(mk_result)\n\n\nMann_Kendall_Test(trend='increasing', h=True, p=0.04970274086760851, z=1.9625134103851736, Tau=0.25517241379310346, s=111.0, var_s=3141.6666666666665, slope=0.14036402565970577, intercept=11.836643578083883)\n\n\nThe test concluded that there is an increasing trend, with a p-value of 0.0497.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#spearmans-rank-correlation",
    "href": "misc/trend_test.html#spearmans-rank-correlation",
    "title": "31  trend test",
    "section": "31.3 Spearman’s Rank Correlation",
    "text": "31.3 Spearman’s Rank Correlation\nThis is another non-parametric test. It assesses how well the relationship between two variables can be described using a monotonic function. It does this by converting the data to ranks and then calculating the Pearson correlation coefficient on the ranks. The Spearman’s rank correlation coefficient, denoted by \\rho (rho), ranges from -1 to 1, where:\n\n1 indicates a perfect positive monotonic relationship,\n-1 indicates a perfect negative monotonic relationship,\n0 indicates no monotonic relationship.\n\nThis test is robust to outliers and does not assume a linear relationship between the variables.\nWe can use the scipy.stats.spearmanr function to calculate Spearman’s rank correlation coefficient and the associated p-value.\n\n\napplying the Spearman’s Rank Correlation test\nspearman_corr, spearman_p = scipy.stats.spearmanr(x, y)\nprint(f\"Spearman's correlation: {spearman_corr:.6f}, p-value: {spearman_p:.6f}\")\n\n\nSpearman's correlation: 0.361958, p-value: 0.049356\n\n\nWe found that there is a positive monotonic relationship between x and y, with a p-value of 0.0494, indicating that the relationship is statistically significant at the 0.05 significance level.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/trend_test.html#theil-sen-estimator",
    "href": "misc/trend_test.html#theil-sen-estimator",
    "title": "31  trend test",
    "section": "31.4 Theil-Sen Estimator",
    "text": "31.4 Theil-Sen Estimator\nThe Theil-Sen estimator is a robust method for estimating the slope of a linear trend. It is particularly useful when the data contains outliers or is not normally distributed. The Theil-Sen estimator calculates the slope as the median of all possible pairwise slopes between data points. We can use the scipy.stats.theilslopes function to calculate the Theil-Sen estimator and the associated confidence intervals.\n\n\napplying the Theil-Sen Estimator\nfrom scipy.stats import theilslopes\ntheil_slope, theil_intercept, theil_lower, theil_upper = theilslopes(y, x, 0.95)\nprint(f\"Theil-Sen slope: {theil_slope:.6f}, intercept: {theil_intercept:.6f}\")\n\n\nTheil-Sen slope: 0.140364, intercept: 11.836644",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>trend test</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html",
    "href": "misc/entropy.html",
    "title": "32  entropy",
    "section": "",
    "text": "32.1 image-generating machines\nWe’re given machines that generate images of cats (😺) and dogs (🐶). See the following outputs from the machines:\nmachine 1:\n😺 😺 😺 😺 😺 🐶, 5 cats and 1 dog\nmachine 2:\n😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺 😺, 14 cats and 0 dogs\nmachine 3:\n🐶 😺 🐶 😺 🐶 🐶 🐶 😺 😺 🐶, 4 cats and 6 dogs\nmachine 4:\n🐶 😺 🐶 🐶 🐶 😺 😺 😺 🐶 😺 🐶 😺, 6 cats and 6 dogs",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#surprise",
    "href": "misc/entropy.html#surprise",
    "title": "32  entropy",
    "section": "32.2 surprise",
    "text": "32.2 surprise\nHow surprised are we by each output?\nFor machine 2, we wouldn’t be surprised at all if the next image it generates is a cat. It’s been generating only cats so far. However, if it generates a dog, that would be quite surprising.\nMachine 4 produced equal numbers of cats and dogs. So we would be equally surprised if the next image is a cat or a dog.\nMachines 1 and 3 are somewhere in between.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#information",
    "href": "misc/entropy.html#information",
    "title": "32  entropy",
    "section": "32.3 information",
    "text": "32.3 information\nWe could say similar things about information. If machine 2 generates another cat, we don’t learn anything new, it’s the same machine as ever. But if it generates a dog, we learn a lot about this machine’s behavior. In contrast, machine 4 generates equal amounts of information whether it produces a cat or a dog, since both have had the same number so far.\nTo say that we are surprised by an event is the same as saying that we learn some information from it. If an event is not surprising at all, then it doesn’t provide us with any new information.\n\nThe sun will rise tomorrow.\n\nThis is neither surprising nor informative.\n\nI have a dragon living in my garage.\n\nThis is both surprising and informative.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#quantifying-surprise-and-information",
    "href": "misc/entropy.html#quantifying-surprise-and-information",
    "title": "32  entropy",
    "section": "32.4 quantifying surprise and information",
    "text": "32.4 quantifying surprise and information\nAn event that is very likely to happen is not surprising, and doesn’t provide us with much information. An event that is unlikely to happen is surprising, and provides us with a lot of information. It seems reasonable to say that the amount of information I we gain from an event x, whose probability is P(x), is inversely proportional to the probability:\n\nI(P(x)) = \\frac{1}{P(x)}\n\\tag{1}",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#problems-with-the-inverse-formula",
    "href": "misc/entropy.html#problems-with-the-inverse-formula",
    "title": "32  entropy",
    "section": "32.5 problems with the inverse formula",
    "text": "32.5 problems with the inverse formula\nThere are at least two problems with this inverse formula.\nProblem one\nAn event with zero probability would provide us with infinite information. Maybe that’s okay, since I could be infinitely surprised if the sun didn’t rise tomorrow. However, a certain event (probability 1) would provide us with only 1 unit of information. That doesn’t seem right. A certain event should provide us with no information at all, zero.\nProblem two\nLet’s say that we learn about two completely independent events that happened yesterday, x and y:\n\nx: My sister flipped a coin and got “heads”.\ny: My cousin has won the lottery.\n\nSince the events are independent, the probability that both have happened is the product of their probabilities:\n\nP(x \\text{ and } y) = P(x)P(y)\n\\tag{2}\n\nNow let’s calculate the information we gained from learning that both events happened:\n\\begin{align*}\nI(P(x \\text{ and } y)) &= I(P(x)P(y)) \\\\\n                       &= \\frac{1}{P(x)P(y)} \\\\\n                       &= \\frac{1}{P(x)} \\cdot \\frac{1}{P(y)} \\\\\n                       &= I(P(x)) \\cdot I(P(y))\n                       \\tag{3}\n\\end{align*}\nThe total information I gained is the product of the information I gained from each event. There’s something wrong with that. Assume that my sister flipped a fair coin, so the probability of heads is 1/2. From the inverse formula, this means that the “heads” outcome doubled the information I got from yesterday’s events. My surprise from learning that my cousin won the lottery was multiplied by 2 by a mere coin toss. That’s obviously not ok.\nWhat would make sense is if the total information I gained from both events was the sum of the information I gained from each independent event. Then, if my sister flipped a fair coin, the information I gained from learning that my cousin won the lottery would be increased by a fixed amount, regardless of how surprising the lottery win was.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#a-new-and-better-formula",
    "href": "misc/entropy.html#a-new-and-better-formula",
    "title": "32  entropy",
    "section": "32.6 a new and better formula",
    "text": "32.6 a new and better formula\nWe are looking for a function I(P(x)) such that:\n\nI(1) = 0\nA sure event provides no information.\nI(P(x)P(y)) = I(P(x)) + I(P(y))\nThe information from two independent events is the sum of the information from each event.\n\nWe can guess another formula that satisfies these two properties:\n\nI(P(x)) = \\log\\left(\\frac{1}{P(x)}\\right)\n\\tag{4}\n\n\nThis formula satisfies the first property, since \\log(1) = 0.\nIt also satisfies the second property, since \\log(ab) = \\log(a) + \\log(b).\n\nClaude Shannon, the father of information theory, proposed this formula in his seminal 1948 paper “A Mathematical Theory of Communication”. There, he proved (see Appendix 2) that this is the only function that satisfies three properties, related to the two stated above, but more restrictive. See Shannon’s three properties, stated in Section 6 of his paper, titled “Choice, Uncertainty, and Entropy”.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#entropy",
    "href": "misc/entropy.html#entropy",
    "title": "32  entropy",
    "section": "32.7 entropy",
    "text": "32.7 entropy\nShannon writes:\n\nSuppose we have a set of possible events whose probabilities of occurrence are p_1, p_2, \\ldots, p_n. These probabilities are known but that is all we know concerning which event will occur. Can we find a measure of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?\n\nAfter introducing the three properties that a measure H must satisfy to answer the question above, Shannon presents the formula for the entropy:\n\nWe shall call \nH = - \\sum p_i \\log(p_i)\n\\tag{5}\n the entropy of the set of probabilities p_1, \\ldots, p_n.\n\nThis is the standard formula for entropy, but a better rendition is:\n\nH = \\sum P_i \\log\\left(\\frac{1}{P_i}\\right)\n\\tag{6}\n\n(Shannon uses p_i, but I’ll go back to using capital P for probabilities.)\nThis formula is better because it clearly answers the following question:\n\nWhat would be the expected amount of information (or surprise) we would gain from a probabilistic event?\n\nWe don’t know what image our machines will produce next. But we do know the probabilities of each outcome. So we can calculate the expected information from the next image:\n\nH = \\sum_i P(x_i) I(P(x_i)) = \\sum_i P(x_i) \\log\\left(\\frac{1}{P(x_i)}\\right)\n\\tag{7}\n\nThe information from each possible outcome x_i is weighted by the probability of that outcome, and we sum over all possible outcomes.\nFor instance, machine 1 produces dogs with a probability of 1/6, and cats with a probability of 5/6. The expected information from the next image is the information from a dog times the probability of getting a dog, plus the information from a cat times the probability of getting a cat. If our machine produced N possible images, we would do the same, summing over all N possible images with their respective probabilities as weights.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#binary-machines",
    "href": "misc/entropy.html#binary-machines",
    "title": "32  entropy",
    "section": "32.8 binary machines",
    "text": "32.8 binary machines\nIn the example of the machines that produce only two outcomes, we have that one probability is P (say, for getting cats) and the other is Q=1-P. So we can write the entropy as a function of a single probability:\n\nH(P) = - P \\log(P) - (1-P) \\log(1-P)\n\n\n\nimport libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", font_scale=1.5)\nimport pandas as pd\nimport matplotlib.gridspec as gridspec\n\n\n\n\nplot\nfig, ax = plt.subplots()\np = np.linspace(0, 1, 100)\neps = 1e-10\ndef entropy_H(p):\n    return -p*np.log2(p+eps)-(1-p)*np.log2(1-p+eps)\nax.plot(p, entropy_H(p), label=r\"$  H(p)$\")\n\np_machine1 = 5/6\nax.plot([p_machine1], [entropy_H(p_machine1)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine1-0.05, entropy_H(p_machine1), \"M1\", ha='center', color='tab:orange')\n\np_machine2 = 1.0\nax.plot([p_machine2], [entropy_H(p_machine2)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine2-0.05, entropy_H(p_machine2), \"M2\", ha='center', color='tab:orange')\n\np_machine3 = 4/10\nax.plot([p_machine3], [entropy_H(p_machine3)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine3-0.05, entropy_H(p_machine3), \"M3\", ha='center', color='tab:orange')\n\np_machine4 = 6/12\nax.plot([p_machine4], [entropy_H(p_machine4)], marker='o', ls='none', color='tab:orange')\nax.text(p_machine4, entropy_H(p_machine4)-0.05, \"M4\", ha='center', va='top', color='tab:orange')\n\nax.set_xlabel(\"P\")\nax.set_ylabel(\"bits\")\nax.set_title(\"Binary Entropy Function\")\n\n\nText(0.5, 1.0, 'Binary Entropy Function')\n\n\n\n\n\n\n\n\n\nThe expected information is zero for machine 2, which certainly produces only cats (P=1). The expected information is maximal for machine 4, which produces cats and dogs with equal probabilities (P=1/2).\nIn Equations (5) and (6), we didn’t explicitly say what the base of the logarithm is. It doesn’t matter, since changing the base only changes the units of information. In the example above, we use base 2 because we have a binary choice, and the units for entropy are called bits, suggested by JW Tukey, meaning binary digits.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/entropy.html#one-last-example",
    "href": "misc/entropy.html#one-last-example",
    "title": "32  entropy",
    "section": "32.9 one last example",
    "text": "32.9 one last example\nConsider the frequency of the letters in the Hebrew alphabet (there are 22 letters). The most common letter is “י”, which appears with a frequency of about 11.06%. The least common letter is “ט”, which appears with a frequency of about 1.24%. If we opened a book written in Hebrew to a random page, and picked a random letter on that page, what would be the expected information from that letter?\n\n\nimport data and plot\ndf = pd.read_csv(\"../archive/data/letter_frequency_hebrew.csv\", sep=\"\\t\")\n\nfig= plt.figure(1, figsize=(8, 8))\n\ngs = gridspec.GridSpec(3, 2, width_ratios=[1,0.2], height_ratios=[1,1,1])\ngs.update(left=0.16, right=0.86,top=0.88, bottom=0.13, hspace=0.05, wspace=0.05)\n\nax0 = plt.subplot(gs[0, 0])\nax1 = plt.subplot(gs[1, 0], sharex=ax0)\nax2 = plt.subplot(gs[2, 0], sharex=ax0)\nax3 = plt.subplot(gs[:, 1])\n\nbar_width = 0.8\n\nax0.bar(df['Letter'], df['Frequency(%)']/100, width=bar_width)\nfor i, letter in enumerate(df['Letter']):\n    ax0.text(i, df['Frequency(%)'][i]/100 + 0.002, letter, ha='center', va='bottom', fontsize=10)\nax0.set_xticklabels(df['Letter'], rotation=0);\nax0.set_ylabel('prob. mass function', fontsize=12)\nax0.set(xlim=(-0.5, len(df['Letter'])-0.5))\nax0.text(0.4, 0.95, 'letter frequency', transform=ax0.transAxes, ha='center', va='top', fontsize=14)\nax0.text(0.4, 0.85, r'$P_i$', transform=ax0.transAxes, ha='center', va='top', fontsize=16)\n\nax1.bar(df['Letter'], np.log2(1/(df['Frequency(%)']/100)), width=bar_width)\nax1.set_ylabel('self-information\\n(bits)', fontsize=12)\nax1.text(0.4, 0.95, 'letter self-information', transform=ax1.transAxes, ha='center', va='top', fontsize=14)\nax1.text(0.4, 0.85, r'$\\log_2(1/P_i)$', transform=ax1.transAxes, ha='center', va='top', fontsize=16)\n\nHi = (df['Frequency(%)']/100) * np.log2(1/(df['Frequency(%)']/100))\ncolors = sns.color_palette(\"deep\", n_colors=len(df))\nfor i, (letter, h) in enumerate(zip(df['Letter'], Hi)):\n    ax2.bar(letter, h, color=colors[i % len(colors)], width=bar_width)\nax2.set_ylabel('entropy (bits)', fontsize=12)\nax2.set(yticks=[0,1,2])\nax2.set_ylim(0,4.5/3)\nax2.text(0.4, 0.95, 'letter entropy contribution', transform=ax2.transAxes, ha='center', va='top', fontsize=14)\nax2.text(0.4, 0.85, r'$P_i\\cdot\\log_2(1/P_i)$', transform=ax2.transAxes, ha='center', va='top', fontsize=16)\n\n# Annotate arrow from x=10 to x=20 on ax2\nax2.annotate(\n    '', \n    xy=(len(df['Letter'])-1, 0.3), \n    xytext=(10, 0.3), \n    arrowprops=dict(arrowstyle='-&gt;', lw=2, color='black')\n)\nax2.text(15.5, 0.35, r'$H=\\sum P_i \\log_2(1/P_i)$', ha='center', va='bottom', fontsize=16)\nfor spine in ['top', 'right']:\n    ax0.spines[spine].set_visible(False)\n    ax1.spines[spine].set_visible(False)\n    ax2.spines[spine].set_visible(False)\n\n# Plot stacked bars of the entropy contributions in ax3\nbottom = 0\nfor i, H in enumerate(Hi):\n    p = ax3.bar(0, H, width=bar_width, label=df['Letter'][i], bottom=bottom)\n    ax3.text(0, bottom + H/2, f\"{df['Letter'][i]}\", ha='center', va='center', color='black', fontsize=10)\n    bottom += H\nax3.set_xlim(-len(df['Letter'])/2*0.2, len(df['Letter'])/2*0.2)\nax3.set_ylim(0,4.5)\nfor spine in ['left', 'top', 'right']:\n    ax3.spines[spine].set_visible(False)\nax3.yaxis.tick_right()\n\ntotal_H = np.sum(Hi)\nax3.axhline(total_H, color='gray', linestyle=':', linewidth=1)\nax3.set(\n    xticks=[],\n    yticks=[0,1,2,3,4,total_H],\n    yticklabels=[0,1,2,3,4, f'H = {total_H:.2f} bits']\n)\nax3.tick_params(axis='y', which='major', length=0)\n\nfig.tight_layout();\n\n\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_99634/300921586.py:19: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax0.set_xticklabels(df['Letter'], rotation=0);\n/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_99634/300921586.py:74: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  fig.tight_layout();\n\n\n\n\n\n\n\n\n\nThe expected information would be 4.14 bits. Because Hebrew has structure (like any other language), this is less than the maximum possible information. If all 22 letters were equally likely, the expected information would be:\n\\begin{align*}\nH_\\text{max} &= \\sum_{i=1}^{22} \\frac{1}{22} \\log_2\\left(\\frac{1}{\\frac{1}{22}}\\right) \\\\\n             &= \\log_2(22) \\\\\n             &\\approx 4.46 \\text{ bits}\n\\end{align*}\nFrom this exercise, we learned that in the case that all outcomes are equally likely, the entropy is simply the logarithm of the number of possible outcomes. This is a useful fact to remember.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>entropy</span>"
    ]
  },
  {
    "objectID": "misc/cross-entropy.html",
    "href": "misc/cross-entropy.html",
    "title": "33  cross-entropy and KL divergence",
    "section": "",
    "text": "33.1 wrong model\nWhat happens when my friend visits me in city A and, not knowing any better, assumes that it rains 10% of the days?\nWe have two probability distributions, the true weather distribution P, and the assumed weather distribution Q:\nWhat will be the expected surprise of my friend, when he visits me in city A? Now that we have discussed surprise (information) and entropy, we can calculate the following quantity, called cross-entropy:\nH(P, Q) = - \\sum_x P(x) \\log Q(x)\nMy friend will evaluate his surprise using the mental model that he has, i.e., the assumed distribution Q. For example, because he comes from a dry city, every time it rains he is surprised a lot more than when it does not rain.\nHowever, since my friend is visiting me in city A, he will actually experience the weather according to the true distribution P. He will not weigh the big surprise of rain with the probability of rain in city B (10%), but with the probability of rain in city A (50%).\nThis reasoning explains the asymetry of the cross-entropy: the first argument is the true distribution, which determines how often each event happens, while the second argument is the assumed distribution, which determines how surprised my friend will be when each event happens.\nLet’s compute my friend’s expected surprise when he visits me in city A:\n\\begin{align*}\nH(P, Q) &= - \\sum_x P(x) \\log Q(x) \\\\\n        &= - (P(\\text{rain}) \\log Q(\\text{rain}) + P(\\text{no rain}) \\log Q(\\text{no rain})) \\\\\n        &= - (0.5 \\log 0.1 + 0.5 \\log 0.9) \\\\\n        &= - (0.5 \\cdot -1 + 0.5 \\cdot -0.045757) \\\\\n        &= 1.74 \\text{ bits},\n\\end{align*}\nwhere we used the base-2 logarithm, so the result is in bits.\nTo see the asymetry of the cross-entropy, let’s compute my expected surprise when I visit my friend in city B:\n\\begin{align*}\nH(Q, P) &= - \\sum_x Q(x) \\log P(x) \\\\\n        &= - (Q(\\text{rain}) \\log P(\\text{rain}) + Q(\\text{no rain}) \\log P(\\text{no rain})) \\\\\n        &= - (0.1 \\log 0.5 + 0.9 \\log 0.5) \\\\\n        &= - (0.1 \\cdot -1 + 0.9 \\cdot -1) \\\\\n        &= 1 \\text{ bits},\n\\end{align*}\nMy friend’s expected surprise will be higher when he visits me in city A (1.74 bits) than my expected surprise when I visit him in city B (1 bit).",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "misc/cross-entropy.html#wrong-model",
    "href": "misc/cross-entropy.html#wrong-model",
    "title": "33  cross-entropy and KL divergence",
    "section": "",
    "text": "True distribution: P(rain) = 0.5, P(no rain) = 0.5\nAssumed distribution: Q(rain) = 0.1, Q(no rain) = 0.9",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "misc/cross-entropy.html#kullback-leibler-divergence",
    "href": "misc/cross-entropy.html#kullback-leibler-divergence",
    "title": "33  cross-entropy and KL divergence",
    "section": "33.2 Kullback-Leibler divergence",
    "text": "33.2 Kullback-Leibler divergence\nNot all of my friend’s surprise is due to the fact that he has an inaccurate mental model of the weather. Some of his surprise is simply due to the inherent randomness of the weather. This would be the same surprise that I myself experience when I live in city A, and I have the correct mental model of the weather.\nIt would make sense to separate the surprise that is due to the inherent randomness of the weather from the surprise that is due to my friend’s wrong mental model. We can do this by subtracting the entropy of the true distribution P from the cross-entropy H(P, Q):\nD_{KL}(P \\| Q) = H(P, Q) - H(P)\nThis quantity is called the Kullback-Leibler divergence, and it measures the amount of surprise that is only due to my friend’s wrong mental model.\nLet’s use the properties of logarithms to rewrite the KL divergence in a more convenient form:\n\\begin{align*}\nD_{KL}(P \\| Q) &= H(P, Q) - H(P)\\\\\n                &= - \\sum_x P(x) \\log Q(x) + \\sum_x P(x) \\log P(x) \\\\\n                &= \\sum_x P(x) (\\log P(x) - \\log Q(x)) \\\\\n                &= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}.\n\\end{align*}\nThis is the most common form of the KL divergence.\nWhen our model is perfect, i.e., when P = Q, the KL divergence is zero (\\log(P/P) = 0), because there is no extra surprise due to a wrong mental model. When our model is not perfect, the KL divergence is always positive, because we are always more surprised when our mental model is wrong.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  },
  {
    "objectID": "misc/cross-entropy.html#model-training-and-objective-functions",
    "href": "misc/cross-entropy.html#model-training-and-objective-functions",
    "title": "33  cross-entropy and KL divergence",
    "section": "33.3 model training and objective functions",
    "text": "33.3 model training and objective functions\nWe might want to train a model that classifies photos. We have a dataset of photos, and for each photo we know the correct label (cat, dog, elephant, etc.). The goal of the model is to predict the correct label for each photo.\nAt every step of the training process, we need to evaluate how well the model is doing. The true data distribution P is given by the labels in the training dataset, while the model’s predicted distribution Q is given by the model’s output. Ideally, our model’s predicted distribution Q should be as close as possible to the true data distribution P. That’s sounds like a job for the KL divergence!\nWe will adjust the model’s parameters to minimize the KL divergence between the true data distribution P and the model’s predicted distribution Q. In practice, we will minimize the cross-entropy H(P, Q) instead of the KL divergence D_{KL}(P \\| Q), because the entropy H(P) does not depend on the model’s parameters, and therefore does not affect the optimization process. Think about it: no matter what the model’s parameters are, the entropy of the true data distribution P will always be the same. So minimizing the KL divergence is equivalent to minimizing the cross-entropy. We don’t care if they differ by a constant.",
    "crumbs": [
      "miscellaneous",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>cross-entropy and KL divergence</span>"
    ]
  }
]