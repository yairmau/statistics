{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b8fac8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"MLE and information theory\"\n",
    "execute:\n",
    "  # echo: false\n",
    "  freeze: auto  # re-render only when source changes\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-summary: \"Show the code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f663e685",
   "metadata": {},
   "source": [
    "We will show here how the maximizing the log likelihood naturally connects to minimizing the Kullback-Leibler (KL) divergence.\n",
    "\n",
    "We will start from a true data generating distribution $P(x)$, and we will assume we have $n$ independent and identically distributed (i.i.d.) observations $x_1, x_2, \\ldots, x_n$ drawn from it.\n",
    "We model the data using a parametric family of distributions $Q(x | \\theta)$, where $\\theta$ is the parameter we want to estimate. Our goal is to find the parameters $\\theta$ that make the model distribution $Q(x | \\theta)$ as close as possible to the true distribution $P(x)$. This set of parameters is typically found using Maximum Likelihood Estimation (MLE):\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\max_{\\theta} \\ell(\\theta \\mid x_{1:n})\n",
    "$$\n",
    "\n",
    "In English: our best estimation of the parameters $\\theta$ is the one that maximizes the parameters (also called arguments) of the log likelihood of the observed data $x_{1:n} = (x_1, x_2, \\ldots, x_n)$.\n",
    "\n",
    "The log likelihood function for the observed data given the parameters $\\theta$ is defined as:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta \\mid x_{1:n}) = \\sum_{i=1}^{n} \\log Q(x_i | \\theta)\n",
    "$$\n",
    "\n",
    "Here we used $Q$ to denote the model distribution, to distinguish it from the true data generating distribution $P$. We will now divide both sides by $n$ to express the average log likelihood per observation:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\ell(\\theta \\mid x_{1:n}) = \\frac{1}{n} \\sum_{i=1}^{n} \\log Q(x_i | \\theta)\n",
    "$$\n",
    "\n",
    "This step can be justified in a few ways:\n",
    "\n",
    "* Dividing by a constant does not change the location of the maximum.\n",
    "* It is clear that $\\ell$ scales linearly with $n$, that is, the more data we have, the larger the log likelihood will be. Dividing by $n$ normalizes this effect.\n",
    "* I already know where I want to go with this, and this steps helps me :)\n",
    "\n",
    "Note that now the right-hand side is the sample mean of the log probabilities $\\log Q(x | \\theta)$ evaluated at the observed data points $x_i$.\n",
    "This sample mean can also be understood as an empirical expectation over the observed data, where each data point is given equal weight $1/n$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{n} \\ell(\\theta \\mid x_{1:n}) &= \\frac{1}{n} \\sum_{i=1}^{n} \\log Q(x_i | \\theta) \\\\\n",
    "&= \\mathbb{E}_{x \\sim \\hat{P}_n}[\\log Q(x | \\theta)],\n",
    "\\end{align*}\n",
    "\n",
    "where the subscript $x \\sim \\hat{P}_n$ indicates that the expectation $(\\mathbb{E})$ is taken with respect to the empirical distribution $\\hat{P}_n$ defined by the observed data points $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "Now, what happens as we collect more and more data, i.e., as $n$ approaches infinity? Our \"data-driven\" distribution $\\hat{P}_n$ starts to look exactly like the \"true\" distribution $P$, therefore:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lim_{n \\to \\infty} \\mathbb{E}_{x \\sim \\hat{P}_n}[\\log Q(x | \\theta)] &= \\mathbb{E}_{x \\sim P}[\\log Q(x | \\theta)] \\\\\n",
    "&= \\sum_{x} P(x) \\log Q(x | \\theta)\n",
    "\\end{align*}\n",
    "\n",
    "We're almost there. Remember when we defined the cross-entropy in the chapter [cross-entropy and KL divergence](/information_theory/cross-entropy.ipynb)? It was defined as:\n",
    "\n",
    "$$\n",
    "H(P, Q) = - \\sum_x P(x) \\log Q(x),\n",
    "$$\n",
    "\n",
    "which is exactly the negative of what we got before. We have shown so far that maximizing the average log likelihood is equivalent to minimizing the cross-entropy between the true distribution $P$ and the model distribution $Q$. The caveat for this statement is that this equivalence holds in the limit of infinite data. The very last step is to connect this to the KL divergence, which we defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\| Q) = H(P, Q) - H(P).\n",
    "$$\n",
    "\n",
    "The term $H(P)$ is the entropy of the true distribution $P$, which does not depend on the model parameters $\\theta$. Therefore, minimizing the cross-entropy $H(P, Q)$ is equivalent to minimizing the KL divergence $D_{KL}(P \\| Q)$.\n",
    "\n",
    "## implication\n",
    "\n",
    "The minimization of the KL divergence is ubiquitous in machine learning, and it is often used as a loss function to train models. This connection between MLE and KL divergence provides a theoretical foundation for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853094b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
