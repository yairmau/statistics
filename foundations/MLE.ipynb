{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff60fd2",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"maximum likelihood estimation\"\n",
    "execute:\n",
    "  # echo: false\n",
    "  freeze: auto  # re-render only when source changes\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-summary: \"Show the code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0d8ab",
   "metadata": {},
   "source": [
    "![](always-has-been.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca77",
   "metadata": {},
   "source": [
    "The one idea that permeates all of statistics and machine learning is that of maximum likelihood estimation. I cannot overstate its centrality.\n",
    "\n",
    "In the chapter on [probability and likelihood](/foundations/probability_and_likelihood.ipynb), we solved a practical example of estimating the rate parameter of an exponential distribution using maximum likelihood estimation. For the case where we observed many samples, we found that the formula for the mean naturally emerged as the maximum likelihood estimator for the rate parameter. We will dig deeper into this idea, and show deep connections between maximum likelihood estimation and many other statistical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521b6fa",
   "metadata": {},
   "source": [
    "## log likelihood\n",
    "\n",
    "We already saw the formula for the likelihood for multiple independent observations:\n",
    "\n",
    "$$\n",
    "L(\\lambda \\mid t_1, t_2, \\ldots, t_n) = \\prod_{i=1}^{n} L(\\lambda \\mid t_i),\n",
    "$$\n",
    "\n",
    "We justified the product form by assuming that the observations are \"iid\", meaning independent and identically distributed. Instead of working with the product form, it is often more convenient to work with the log likelihood, which converts the product into a sum:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta \\mid x_{1:n}) = \\log L(\\theta \\mid x_{1:n}) = \\sum_{i=1}^{n} \\log L(\\theta \\mid x_i) = \\sum_{i=1}^{n} \\log P( x_i \\mid \\theta).\n",
    "$$\n",
    "\n",
    "Here we changed the notation a bit to use $\\theta$ for the parameters (which could be a vector) and $x$ for the data vector, which is more common in statistics. \n",
    "There are a few reasons why we prefer to work with the log likelihood instead of the likelihood itself:\n",
    "\n",
    "* **Numerical Stability (Underflow):** In [likelihood with multiple observations](/foundations/probability_and_likelihood.html#q3-likelihood-with-multiple-observations), we saw that likelihoods are products: $\\prod P(x_i)$. If you have 100 observations, you are multiplying 100 small numbers, which can lead to numerical underflow where a computer rounds the result to zero. Summing logs, $\\sum \\ln P(x_i​)$, keeps the numbers in a range computers can handle.\n",
    "* **Mathematical Elegance:** Many common distributions (like the Exponential or Normal) use $e$, Euler's number. The natural log cancels the exponent, turning products of complex terms into simple sums that are much easier to differentiate.\n",
    "* **Preservation of the Maximum:** Because ln(x) is a monotonically increasing function, the value of $\\theta$ that maximizes the log-likelihood is identical to the value that maximizes the likelihood.\n",
    "* **Optimization Surface:** Taking the log reshapes the likelihood surface. It stretches out the extremely steep slopes near the peak, creating a well-behaved \"hill\" that optimization algorithms can navigate reliably without the fluctuations caused by nearly-vertical gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25030d48",
   "metadata": {},
   "source": [
    "## where summary statistics come from\n",
    "\n",
    "Let's do the following exercise. Suppose we have $n$ iid observations $x$ from a model with parameters $\\theta$. Using the definition of the log likelihood, derive the maximum likelihood estimator for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e481c3e",
   "metadata": {},
   "source": [
    "### normal distribution\n",
    "\n",
    "The normal distribution is defined by two parameters: the mean $\\mu$ and the standard deviation $\\sigma$. As an example, consider measuring the heights of a group of people. We can assume that the heights are normally distributed with some unknown mean and standard deviation.\n",
    "\n",
    "Its probability density function is given by:\n",
    "$$\n",
    "f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\n",
    "$$\n",
    "\n",
    "The log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\mu, \\sigma \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i \\mid \\mu, \\sigma) \\\\\n",
    "&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^{n} \\left( -\\log(\\sigma \\sqrt{2\\pi}) - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right) \\\\\n",
    "&= -n \\log(\\sigma) -n \\log(\\sqrt{2\\pi}) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2.\n",
    "\\end{align*}\n",
    "\n",
    "Maximizing the log likelihood with respect to $\\mu$ and $\\sigma$ involves taking partial derivatives, setting them to zero, and solving for the parameters.\n",
    "\n",
    "For $\\mu$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial \\mu} &= \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) \\\\\n",
    "&= \\frac{1}{\\sigma^2} \\left( \\sum_{i=1}^{n} x_i - n\\mu \\right) = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $\\mu$ gives:\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i,\n",
    "$$\n",
    "which is the sample mean.\n",
    "\n",
    "Now, for $\\sigma$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma} &= -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\\\\n",
    "&= \\frac{1}{\\sigma^3} \\left( -n\\sigma^2 + \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right) = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $\\sigma$ gives:\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2,\n",
    "$$\n",
    "which is the sample variance (using $n$ in the denominator for MLE).\n",
    "\n",
    "Going back to the height example, our MLEs for the mean and standard deviation of heights are simply the sample mean and sample standard deviation of the observed heights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5058f10",
   "metadata": {},
   "source": [
    "## add graphs for each case, show PDF and log likelihood surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85323182",
   "metadata": {},
   "source": [
    "### binomial distribution\n",
    "\n",
    "The binomial distribution models the number of successes in $n$ independent Bernoulli trials, each with success probability $p$. A very common example is flipping a biased coin, whose probability of yielding heads is $p$, flipping it $n$ times and counting the number of heads $k$.\n",
    "Its probability mass function is given by:\n",
    "$$\n",
    "P(X = k \\mid n, p) = \\binom{n}{k} p^k (1 - p)^{n - k}.\n",
    "$$\n",
    "The log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(p \\mid k, n) &= \\log P(X = k \\mid n, p) \\\\\n",
    "&= \\log \\left( \\binom{n}{k} p^k (1 - p)^{n - k} \\right) \\\\\n",
    "&= \\log \\binom{n}{k} + k \\log p + (n - k) \\log (1 - p).\n",
    "\\end{align*}\n",
    "\n",
    "To find the maximum likelihood estimator for $p$, we take the derivative of the log likelihood with respect to $p$ and set it to zero:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d\\ell}{dp} &= \\frac{k}{p} - \\frac{n - k}{1 - p} = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $p$ gives:\n",
    "$$\n",
    "\\hat{p} = \\frac{k}{n},\n",
    "$$\n",
    "which is the sample proportion of successes.\n",
    "Going back to the coin example, if we flip the coin 100 times and get 60 heads, our MLE for $p$ would be $\\hat{p} = 60/100 = 0.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29ac2f",
   "metadata": {},
   "source": [
    "### Laplace distribution\n",
    "\n",
    "The Laplace distribution, also known as the double exponential distribution, is characterized by its location parameter $\\mu$ and scale parameter $b$. We use the normal distribution for 'well-behaved' data like human heights. We use the Laplace distribution for 'wild' data like stock market data, where outliers aren't just mistakes—they are part of the system. For example, most days the price of a stock barely moves, but rare events (market crashes or surges) happen much more often than a Normal distribution would allow.\n",
    "\n",
    "Its probability density function is given by:\n",
    "$$\n",
    "f(x \\mid \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x - \\mu|}{b}\\right).\n",
    "$$\n",
    "\n",
    "The log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\mu, b \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i \\mid \\mu, b) \\\\\n",
    "&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{2b} \\exp\\left(-\\frac{|x_i - \\mu|}{b}\\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^{n} \\left( -\\log(2b) - \\frac{|x_i - \\mu|}{b} \\right) \\\\\n",
    "&= -n \\log(2b) - \\frac{1}{b} \\sum_{i=1}^{n} |x_i - \\mu|.\n",
    "\\end{align*}\n",
    "\n",
    "Maximizing the log likelihood with respect to $\\mu$ and $b$ involves taking partial derivatives, setting them to zero, and solving for the parameters.\n",
    "\n",
    "For $\\mu$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial \\mu} &= \\frac{1}{b} \\sum_{i=1}^{n} \\text{sgn}(x_i - \\mu) = 0,\n",
    "\\end{align*}\n",
    "where $\\text{sgn}(x)$ is the sign function, which returns -1 for negative values, 1 for positive values, and 0 for zero.\n",
    "\n",
    "Solving for $\\mu$ is essentially a balancing act. It requires us finding a value for $\\mu$ such that the number of positive ones (points above $\\mu$) exactly cancels out the number of negative ones (points below $\\mu$). This happens when $\\mu$ is the median of the data:\n",
    "$$\n",
    "\\hat{\\mu} = \\text{median}(x_{1:n}).\n",
    "$$\n",
    "\n",
    "\n",
    "Now, for $b$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial b} &= -\\frac{n}{b} + \\frac{1}{b^2} \\sum_{i=1}^{n} |x_i - \\mu| = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $b$ gives:\n",
    "$$\n",
    "\\hat{b} = \\frac{1}{n} \\sum_{i=1}^{n} |x_i - \\hat{\\mu}|,\n",
    "$$\n",
    "which is the mean absolute deviation (MAD) from the median. This measure of spread is more robust to outliers than the standard deviation.\n",
    "\n",
    "Going back to the stock market example, our MLEs for the location and scale parameters are simply the sample median and mean absolute deviation from the median of the observed returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922425cc",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "\n",
    "The Poisson distribution models the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence $\\lambda$. A common example is counting the number of emails received in an hour, or the number of radioactive decays detected in a piece of radioactive material over a certain period.\n",
    "Its probability mass function is given by:\n",
    "$$\n",
    "P(X = k \\mid \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}.\n",
    "$$\n",
    "The log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\lambda \\mid k_{1:n}) &= \\sum_{i=1}^{n} \\log P(X = k_i \\mid \\lambda) \\\\\n",
    "&= \\sum_{i=1}^{n} \\left( k_i \\log \\lambda - \\lambda - \\log(k_i!) \\right) \\\\\n",
    "&= \\left(\\sum_{i=1}^{n} k_i \\right) \\log \\lambda - n \\lambda - \\sum_{i=1}^{n} \\log(k_i!).\n",
    "\\end{align*}\n",
    "\n",
    "To find the maximum likelihood estimator for $\\lambda$, we take the derivative of the log likelihood with respect to $\\lambda$ and set it to zero:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d\\ell}{d\\lambda} &= \\frac{\\sum_{i=1}^{n} k_i}{\\lambda} - n = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $\\lambda$ gives:\n",
    "$$\n",
    "\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^{n} k_i,\n",
    "$$\n",
    "which is the sample mean of the observed counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b72a1",
   "metadata": {},
   "source": [
    "### uniform distribution\n",
    "\n",
    "The uniform distribution models a situation where all outcomes in a given range are equally likely. A common example is rolling a fair die, where each face (1 through 6) has an equal probability of landing face up.\n",
    "Its probability density function is given by:\n",
    "$$\n",
    "f(x \\mid a, b) = \\frac{1}{b - a} \\text{ for } a \\leq x \\leq b.\n",
    "$$\n",
    "\n",
    "The log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(a, b \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i \\mid a, b) \\\\\n",
    "&= \\sum_{i=1}^{n} \\log \\left( \\frac{1}{b - a} \\right) \\\\\n",
    "&= -n \\log(b - a).\n",
    "\\end{align*}\n",
    "\n",
    "In this instance, it is much easier to find the maximum likelihood estimators for $a$ and $b$ using the probability density function directly, rather than taking derivatives of the log likelihood. The likelihood reads:\n",
    "$$\n",
    "L(a, b \\mid x_{1:n}) = \\left( \\frac{1}{b - a} \\right)^n.\n",
    "$$\n",
    "\n",
    "The likelihood doesn't have a peak in the usual sense; it increases as the interval $[a, b]$ narrows. In order to maximize the likelihood, the interval must be as small as possible while still containing all observed data points. If at least one data point falls outside the interval, the likelihood becomes zero, because the uniform distribution assigns zero probability to values outside $[a, b]$, and the product of probabilities will include a zero term. The smallest interval that contains all observed data points is defined by the minimum and maximum of the data. Therefore,\n",
    "$$\n",
    "\\hat{a} = \\min(x_{1:n}), \\quad \\hat{b} = \\max(x_{1:n}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130e6bc",
   "metadata": {},
   "source": [
    "### exponential distribution revisited\n",
    "\n",
    "We can now revisit our original example of estimating the rate parameter $\\lambda$ of an exponential distribution using maximum likelihood estimation. Recall that the probability density function of the exponential distribution is given by:\n",
    "$$\n",
    "f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}.\n",
    "$$\n",
    "The log likelihood reads:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\lambda \\mid x_{1:n}) &= \\sum_{i=1}^{n} \\log f(x_i \\mid \\lambda) \\\\\n",
    "&= \\sum_{i=1}^{n} \\left( \\log \\lambda - \\lambda x_i \\right) \\\\\n",
    "&= n \\log \\lambda - \\lambda \\sum_{i=1}^{n} x_i.\n",
    "\\end{align*}\n",
    "To find the maximum likelihood estimator for $\\lambda$, we take the derivative of the log likelihood with respect to $\\lambda$ and set it to zero:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d\\ell}{d\\lambda} &= \\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Solving for $\\lambda$ gives:\n",
    "$$\n",
    "\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} x_i} = \\frac{1}{\\bar{x}},\n",
    "$$\n",
    "where $\\bar{x}$ is the sample mean of the observed data. This result aligns with our previous derivation using the likelihood function directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5c2dc",
   "metadata": {},
   "source": [
    "### summary\n",
    "\n",
    "By appliying the reasoning of maximum likelihood estimation to various probability distributions, we have derived the maximum likelihood estimators for their parameters. Notably, we found that:\n",
    "\n",
    "* For the **normal** distribution, the MLEs are the sample mean and sample standard deviation.\n",
    "* For the **binomial** distribution, the MLE is the sample proportion of successes.\n",
    "* For the **Laplace** distribution, the MLEs are the sample median and mean absolute deviation from the median.\n",
    "* For the **Poisson** distribution, the MLE is the sample mean of the observed counts.\n",
    "* For the **uniform** distribution, the MLEs are the sample minimum and sample maximum.\n",
    "* For the **exponential** distribution, the MLE is the reciprocal of the sample mean.\n",
    "\n",
    "All these estimators are intuitive and align with common summary statistics used in data analysis, highlighting the deep connection between maximum likelihood estimation and descriptive statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8e717",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
