% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod,
  oneside]{scrreprt}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{cancel}
\usepackage{textcomp}
\KOMAoption{captions}{tableheading}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Statistics and Machine Learning},
  pdfauthor={Yair Mau},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Statistics and Machine Learning}
\author{Yair Mau}
\date{}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{home}\label{home}
\addcontentsline{toc}{chapter}{home}

\markboth{home}{home}

I'm teaching myself statistics and machine learning, and the best way to
truly understand is to use the new tools I've acquired. This is what
this website is for. It is mainly a reference guide for my future self.

\section*{books}\label{books}
\addcontentsline{toc}{section}{books}

\markright{books}

These are the books that I've read and recommend.

\subsubsection*{Modern Statistics: Intuition, Math, Python,
R}\label{modern-statistics-intuition-math-python-r}
\addcontentsline{toc}{subsubsection}{Modern Statistics: Intuition, Math,
Python, R}

by Mike X Cohen

\includegraphics[width=\linewidth,height=3.125in,keepaspectratio]{archive/images/modern_statistics_mike_x_cohen.jpg}

\href{https://github.com/mikexcohen/statistics_book}{Github}

This is a really approachable book, the author has a very nice
conversational style, and I enjoyed it a lot. Highly recommended

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Data-Driven Science and Engineering: Machine Learning,
Dynamical Systems, and
Control}\label{data-driven-science-and-engineering-machine-learning-dynamical-systems-and-control}
\addcontentsline{toc}{subsubsection}{Data-Driven Science and
Engineering: Machine Learning, Dynamical Systems, and Control}

by Steven L. Brunton, J. Nathan Kutz

\includegraphics[width=\linewidth,height=3.125in,keepaspectratio]{archive/images/data-driven-brunton-kutz.jpg}

The whole book is available in this
\href{https://www.databookuw.com/}{website}.

This is the sort of books that is suitable for
\href{https://aperiodical.com/wp-content/uploads/2022/05/FTBdMZ5XsAAeq8C.png}{those
who already know the subject}. I would not recommend it as a first read.
In any case, some chapters gave me new intuition on the subject. I do
highly recommend
\href{https://www.youtube.com/@Eigensteve/playlists}{Steve Brunton's
youtube channel}, it's fantastic.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Neural Networks and Deep
Learning}\label{neural-networks-and-deep-learning}
\addcontentsline{toc}{subsubsection}{Neural Networks and Deep Learning}

by Michael Nielsen

\includegraphics[width=\linewidth,height=3.125in,keepaspectratio]{archive/images/nielsen-neuralnetworksanddeeplearning.png}

This is an online book,
\href{http://neuralnetworksanddeeplearning.com/index.html}{freely
available here}. It can be tiring to read a whole book on a computer
screen, so you can find
\href{https://github.com/antonvladyka/neuralnetworksanddeeplearning.com.pdf}{Anton
Vladyka's LaTeX rendition of this book} in his GitHub repository. I
wanted to read the pdf in my tiny kindle reader, so I recompiled Anton's
LaTeX code to make it fit the screen, and on the way changed the font,
and corrected typos here and there.
\href{https://www.overleaf.com/read/hdmntcghxqym\#98d001}{Overleaf
project}. \href{./archive/deep_learning.pdf}{Download pdf}.

Nielsen writes very well, I really enjoyed this book. The part on
backprogation is a bit confusing, I would recommend watching
\href{https://youtu.be/Ilg3gGewQ5U?si=MvCRV8oDI_WvW5EH}{3b1b's youtube
video} on that.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Introduction to Environmental Data
Science}\label{introduction-to-environmental-data-science}
\addcontentsline{toc}{subsubsection}{Introduction to Environmental Data
Science}

by William W. Hsieh

\includegraphics[width=\linewidth,height=3.125in,keepaspectratio]{archive/images/hsieh-intoduction-to-environmental-data-science.jpg}

This book's best quality is that it covers a bunch of topics, methods,
techniques. It is not a good book to learn concepts for the first time,
it's more useful as a menu of what exists, and maybe a brief reminder of
topics you studied in the past but forgot. The ``environmental'' aspect
is completely incidental, in my opinion. Hsieh brings examples from the
Environment, but you don't need to have a background in environmental
science to be able to read it.

\section*{websites}\label{websites}
\addcontentsline{toc}{section}{websites}

\markright{websites}

\subsubsection*{Dr.~Roi Yehoshua's
tutorials}\label{dr.-roi-yehoshuas-tutorials}
\addcontentsline{toc}{subsubsection}{Dr.~Roi Yehoshua's tutorials}

Really good tutorials, you should check this out:\\
\url{https://towardsdatascience.com/author/roiyeho/}

It seems the he wrote a book, I haven't read it, but should be good:\\
\href{https://www.oreilly.com/library/view/machine-learning-foundations/9780135337851/}{Machine
Learning Foundations, Volume 1: Supervised Learning}

\part{data}

\chapter{height data}\label{height-data}

I found growth curves for girls and boys in Israel:

\begin{itemize}
\tightlist
\item
  \href{https://www.gov.il/BlobFolder/reports/kidsandmatures-curves/he/subjects_children-adolescents_g-height-age-5-19.pdf}{url
  girls}, pdf girls
\item
  \href{https://www.gov.il/BlobFolder/reports/kidsandmatures-curves/he/subjects_children-adolescents_B-height-age-5-19.pdf}{url
  boys}, pdf boys
\item
  \href{https://briuton.co.il/4587/\%D7\%A2\%D7\%A7\%D7\%95\%D7\%9E\%D7\%95\%D7\%AA-\%D7\%92\%D7\%93\%D7\%99\%D7\%9C\%D7\%94-\%D7\%91\%D7\%A0\%D7\%99\%D7\%9D-\%D7\%95\%D7\%91\%D7\%A0\%D7\%95\%D7\%AA/}{url
  both}, png boys, png girls.
\end{itemize}

For example, see this:

\pandocbounded{\includegraphics[keepaspectratio]{archive/data/boys_chart3.png}}

I used the great online resource
\href{https://apps.automeris.io/wpd4/}{Web Plot Digitizer v4} to extract
the data from the images files. I captured all the growth curves as best
as I could. The first step now is to get interpolated versions of the
digitized data. For instance, see below the 50th percentile for boys:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ curve\_fit}
\ImportTok{from}\NormalTok{ scipy.special }\ImportTok{import}\NormalTok{ erf}
\ImportTok{from}\NormalTok{ scipy.interpolate }\ImportTok{import}\NormalTok{ UnivariateSpline}
\ImportTok{import}\NormalTok{ matplotlib.animation }\ImportTok{as}\NormalTok{ animation}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\ImportTok{import}\NormalTok{ plotly.graph\_objects }\ImportTok{as}\NormalTok{ go}
\ImportTok{import}\NormalTok{ plotly.io }\ImportTok{as}\NormalTok{ pio}
\NormalTok{pio.renderers.default }\OperatorTok{=} \StringTok{\textquotesingle{}notebook\textquotesingle{}}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age\_list }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(np.arange(}\FloatTok{2.0}\NormalTok{, }\FloatTok{20.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DecValTok{1}\NormalTok{)}
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(np.arange(}\DecValTok{70}\NormalTok{, }\DecValTok{220}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_temp\_boys\_50th }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys{-}p50.csv\textquotesingle{}}\NormalTok{, names}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{])}
\NormalTok{spline }\OperatorTok{=}\NormalTok{ UnivariateSpline(df\_temp\_boys\_50th[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{], df\_temp\_boys\_50th[}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{], s}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{interpolated }\OperatorTok{=}\NormalTok{ spline(age\_list)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(df\_temp\_boys\_50th[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{], df\_temp\_boys\_50th[}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{], label}\OperatorTok{=}\StringTok{\textquotesingle{}digitized data\textquotesingle{}}\NormalTok{,}
\NormalTok{        marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{"black"}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{6}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(age\_list, interpolated, label}\OperatorTok{=}\StringTok{\textquotesingle{}interpolated\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}age (years)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{2}\NormalTok{),}
\NormalTok{       title}\OperatorTok{=}\StringTok{"boys, 50th percentile"}
\NormalTok{       )}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-5-output-1.png}}

Let's do the same for all the other curves, and then save them to a
file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}p05\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}p10\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}p25\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}p50\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}p75\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}p90\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}p95\textquotesingle{}}\NormalTok{]}
\NormalTok{file\_names\_boys }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}boys{-}p05.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}boys{-}p10.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}boys{-}p25.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}boys{-}p50.csv\textquotesingle{}}\NormalTok{,}
                   \StringTok{\textquotesingle{}boys{-}p75.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}boys{-}p90.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}boys{-}p95.csv\textquotesingle{}}\NormalTok{,]}
\NormalTok{file\_names\_girls }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}girls{-}p05.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}girls{-}p10.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}girls{-}p25.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}girls{-}p50.csv\textquotesingle{}}\NormalTok{,}
                   \StringTok{\textquotesingle{}girls{-}p75.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}girls{-}p90.csv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}girls{-}p95.csv\textquotesingle{}}\NormalTok{,]}

\CommentTok{\# create dataframe with age column}
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{: age\_list\})}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{: age\_list\})}
\CommentTok{\# loop over file names and read in data}
\ControlFlowTok{for}\NormalTok{ i, file\_name }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(file\_names\_boys):}
    \CommentTok{\# read in data}
\NormalTok{    df\_temp }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/\textquotesingle{}} \OperatorTok{+}\NormalTok{ file\_name, names}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{])}
\NormalTok{    spline }\OperatorTok{=}\NormalTok{ UnivariateSpline(df\_temp[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{], df\_temp[}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{], s}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    df\_boys[col\_names[i]] }\OperatorTok{=}\NormalTok{ spline(age\_list)}
\ControlFlowTok{for}\NormalTok{ i, file\_name }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(file\_names\_girls):}
    \CommentTok{\# read in data}
\NormalTok{    df\_temp }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/\textquotesingle{}} \OperatorTok{+}\NormalTok{ file\_name, names}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{])}
\NormalTok{    spline }\OperatorTok{=}\NormalTok{ UnivariateSpline(df\_temp[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{], df\_temp[}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{], s}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    df\_girls[col\_names[i]] }\OperatorTok{=}\NormalTok{ spline(age\_list)}

\CommentTok{\# make age index}
\NormalTok{df\_boys.set\_index(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df\_boys.index }\OperatorTok{=}\NormalTok{ df\_boys.index.}\BuiltInTok{round}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{df\_boys.to\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_vs\_age\_combined.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df\_girls.set\_index(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df\_girls.index }\OperatorTok{=}\NormalTok{ df\_girls.index.}\BuiltInTok{round}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{df\_girls.to\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_vs\_age\_combined.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's take a look at what we just did.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_girls}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllll@{}}
\toprule\noalign{}
& p05 & p10 & p25 & p50 & p75 & p90 & p95 \\
age & & & & & & & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2.0 & 79.269087 & 80.794167 & 83.049251 & 85.155597 & 87.475854 &
89.779822 & 90.882059 \\
2.1 & 80.202106 & 81.772053 & 84.052858 & 86.207778 & 88.713405 &
90.883740 & 92.409913 \\
2.2 & 81.130687 & 82.706754 & 85.011591 & 87.211543 & 89.856186 &
91.940642 & 93.416959 \\
2.3 & 82.048325 & 83.601023 & 85.928399 & 88.170313 & 90.914093 &
92.953965 & 94.270653 \\
2.4 & 82.948516 & 84.457612 & 86.806234 & 89.087509 & 91.897022 &
93.927147 & 95.226089 \\
... & ... & ... & ... & ... & ... & ... & ... \\
19.6 & 152.520938 & 154.812286 & 158.775277 & 163.337149 & 167.699533 &
171.531349 & 173.969235 \\
19.7 & 152.534223 & 154.814440 & 158.791925 & 163.310864 & 167.704618 &
171.519600 & 173.980150 \\
19.8 & 152.548001 & 154.827666 & 158.815071 & 163.275852 & 167.708562 &
171.504730 & 173.990964 \\
19.9 & 152.562338 & 154.853760 & 158.845506 & 163.231563 & 167.711342 &
171.486629 & 174.001704 \\
20.0 & 152.577300 & 154.894521 & 158.884019 & 163.177444 & 167.712936 &
171.465189 & 174.012396 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\CommentTok{\# loop over col\_names and plot each column}
\NormalTok{colors }\OperatorTok{=}\NormalTok{ sns.color\_palette(}\StringTok{"Oranges"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(col\_names))}
\ControlFlowTok{for}\NormalTok{ col, color }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(col\_names, colors):}
\NormalTok{    ax.plot(df\_girls.index, df\_girls[col], label}\OperatorTok{=}\NormalTok{col, color}\OperatorTok{=}\NormalTok{color)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}age (years)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{2}\NormalTok{),}
\NormalTok{       title}\OperatorTok{=}\StringTok{"growth curves for girls}\CharTok{\textbackslash{}n}\StringTok{percentile curves: 5, 10, 25, 50, 75, 90, 95"}\NormalTok{,}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-8-output-1.png}}

Let's now see the percentiles for girls age 20.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{percentile\_list }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{95}\NormalTok{])}
\NormalTok{data }\OperatorTok{=}\NormalTok{ df\_girls.loc[}\FloatTok{20.0}\NormalTok{]}
\NormalTok{ax.plot(data, percentile\_list, ls}\OperatorTok{=}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{6}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{         ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}percentile\textquotesingle{}}\NormalTok{,}
\NormalTok{         yticks}\OperatorTok{=}\NormalTok{percentile\_list,}
\NormalTok{         title}\OperatorTok{=}\StringTok{"cdf for girls, age 20"}
\NormalTok{         )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-9-output-1.png}}

I suspect that the heights in the population are normally distributed.
Let's check that. I'll fit the data to the integral of a gaussian,
because the percentiles correspond to a cdf. If a pdf is a gaussian, its
cumulative is given by

\[
\Phi(x) = \frac{1}{2} \left( 1 + \text{erf}\left(\frac{x - \mu}{\sigma \sqrt{2}}\right) \right)
\]

where \(\mu\) is the mean and \(\sigma\) is the standard deviation of
the distribution. The error function \(\text{erf}\) is a sigmoid
function, which is a good approximation for the cdf of the normal
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ erf\_model(x, mu, sigma):}
    \ControlFlowTok{return} \DecValTok{50} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ erf((x }\OperatorTok{{-}}\NormalTok{ mu) }\OperatorTok{/}\NormalTok{ (sigma }\OperatorTok{*}\NormalTok{ np.sqrt(}\DecValTok{2}\NormalTok{))) )}
\CommentTok{\# initial guess for parameters: [mu, sigma]}
\NormalTok{p0 }\OperatorTok{=}\NormalTok{ [}\DecValTok{150}\NormalTok{, }\DecValTok{6}\NormalTok{]}
\CommentTok{\# Calculate R{-}squared}
\KeywordTok{def}\NormalTok{ calculate\_r2(y\_true, y\_pred):}
\NormalTok{    ss\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((y\_true }\OperatorTok{{-}}\NormalTok{ y\_pred) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}
\NormalTok{    ss\_tot }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((y\_true }\OperatorTok{{-}}\NormalTok{ np.mean(y\_true)) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}
    \ControlFlowTok{return} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ (ss\_res }\OperatorTok{/}\NormalTok{ ss\_tot)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ df\_girls.loc[}\FloatTok{20.0}\NormalTok{]}
\NormalTok{params, \_ }\OperatorTok{=}\NormalTok{ curve\_fit(erf\_model, data, percentile\_list, p0}\OperatorTok{=}\NormalTok{p0,}
\NormalTok{                        bounds}\OperatorTok{=}\NormalTok{([}\DecValTok{100}\NormalTok{, }\DecValTok{3}\NormalTok{],   }\CommentTok{\# lower bounds for mu and sigma}
\NormalTok{                                [}\DecValTok{200}\NormalTok{, }\DecValTok{10}\NormalTok{])  }\CommentTok{\# upper bounds for mu and sigma}
\NormalTok{                        )}
\CommentTok{\# store the parameters in the dataframe}
\NormalTok{percentile\_predicted }\OperatorTok{=}\NormalTok{ erf\_model(data, }\OperatorTok{*}\NormalTok{params)}
\CommentTok{\# R{-}squared value}
\NormalTok{r2 }\OperatorTok{=}\NormalTok{ calculate\_r2(percentile\_list, percentile\_predicted)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{percentile\_list }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{95}\NormalTok{])}
\NormalTok{data }\OperatorTok{=}\NormalTok{ df\_girls.loc[}\FloatTok{20.0}\NormalTok{]}
\NormalTok{ax.plot(data, percentile\_list, ls}\OperatorTok{=}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{6}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{)}
\NormalTok{fit }\OperatorTok{=}\NormalTok{ erf\_model(height\_list, }\OperatorTok{*}\NormalTok{params)}
\NormalTok{ax.plot(height\_list, fit, label}\OperatorTok{=}\StringTok{\textquotesingle{}fit\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax.text(}\DecValTok{150}\NormalTok{, }\DecValTok{75}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}$}\ErrorTok{\textbackslash{}}\SpecialStringTok{mu$ = }\SpecialCharTok{\{}\NormalTok{params[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.1f\}}\SpecialStringTok{ cm}\CharTok{\textbackslash{}n}\SpecialStringTok{$}\ErrorTok{\textbackslash{}}\SpecialStringTok{sigma$ = }\SpecialCharTok{\{}\NormalTok{params[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{:.1f\}}\SpecialStringTok{ cm}\CharTok{\textbackslash{}n}\SpecialStringTok{R$\^{}2$ = }\SpecialCharTok{\{}\NormalTok{r2}\SpecialCharTok{:.6f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{,}
\NormalTok{        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{, bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{))}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{140}\NormalTok{, }\DecValTok{190}\NormalTok{),}
\NormalTok{         ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}percentile\textquotesingle{}}\NormalTok{,}
\NormalTok{         yticks}\OperatorTok{=}\NormalTok{percentile\_list,}
\NormalTok{         title}\OperatorTok{=}\StringTok{"the data is very well fitted by a normal distribution"}
\NormalTok{         )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-12-output-1.png}}

Another way of making sure that the model fits the data is to make a QQ
plot. In this plot, the quantiles of the data are plotted against the
quantiles of the normal distribution. If the data is normally
distributed, the points should fall on a straight line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitted\_quantiles }\OperatorTok{=}\NormalTok{ norm.cdf(data, loc}\OperatorTok{=}\NormalTok{params[}\DecValTok{0}\NormalTok{], scale}\OperatorTok{=}\NormalTok{params[}\DecValTok{1}\NormalTok{])}
\NormalTok{experimental\_quantiles }\OperatorTok{=}\NormalTok{ percentile\_list }\OperatorTok{/} \DecValTok{100}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{, adjustable}\OperatorTok{=}\StringTok{\textquotesingle{}box\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(experimental\_quantiles, fitted\_quantiles,}
\NormalTok{        ls}\OperatorTok{=}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{6}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\StringTok{\textquotesingle{}qq points\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{, label}\OperatorTok{=}\StringTok{"1:1 line"}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}empirical quantiles\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}fitted quantiles\textquotesingle{}}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{       title}\OperatorTok{=}\StringTok{"QQ plot"}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-13-output-1.png}}

Great, now we just need to do exactly the same for both sexes, and all
the ages. I chose to divide age from 2 to 20 into 0.1 intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_stats\_boys }\OperatorTok{=}\NormalTok{ pd.DataFrame(index}\OperatorTok{=}\NormalTok{age\_list, columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{])}
\NormalTok{df\_stats\_boys[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{df\_stats\_boys[}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{df\_stats\_boys[}\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{df\_stats\_girls }\OperatorTok{=}\NormalTok{ pd.DataFrame(index}\OperatorTok{=}\NormalTok{age\_list, columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{])}
\NormalTok{df\_stats\_girls[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{df\_stats\_girls[}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{0.0}
\NormalTok{df\_stats\_girls[}\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \FloatTok{0.0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p0 }\OperatorTok{=}\NormalTok{ [}\DecValTok{80}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\CommentTok{\# loop over ages in the index, calculate mu and sigma}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ df\_boys.index:}
    \CommentTok{\# fit the model to the data}
\NormalTok{    data }\OperatorTok{=}\NormalTok{ df\_boys.loc[i]}
\NormalTok{    params, \_ }\OperatorTok{=}\NormalTok{ curve\_fit(erf\_model, data, percentile\_list, p0}\OperatorTok{=}\NormalTok{p0,}
\NormalTok{                          bounds}\OperatorTok{=}\NormalTok{([}\DecValTok{70}\NormalTok{, }\DecValTok{2}\NormalTok{],   }\CommentTok{\# lower bounds for mu and sigma}
\NormalTok{                                  [}\DecValTok{200}\NormalTok{, }\DecValTok{10}\NormalTok{])  }\CommentTok{\# upper bounds for mu and sigma}
\NormalTok{                         )}
    \CommentTok{\# store the parameters in the dataframe}
\NormalTok{    df\_stats\_boys.at[i, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]}
\NormalTok{    df\_stats\_boys.at[i, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]}
\NormalTok{    percentile\_predicted }\OperatorTok{=}\NormalTok{ erf\_model(data, }\OperatorTok{*}\NormalTok{params)}
    \CommentTok{\# R{-}squared value}
\NormalTok{    r2 }\OperatorTok{=}\NormalTok{ calculate\_r2(percentile\_list, percentile\_predicted)}
\NormalTok{    df\_stats\_boys.at[i, }\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ r2}
\NormalTok{    p0 }\OperatorTok{=}\NormalTok{ params}
\CommentTok{\# same for girls}
\NormalTok{p0 }\OperatorTok{=}\NormalTok{ [}\DecValTok{80}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ df\_girls.index:}
    \CommentTok{\# fit the model to the data}
\NormalTok{    data }\OperatorTok{=}\NormalTok{ df\_girls.loc[i]}
\NormalTok{    params, \_ }\OperatorTok{=}\NormalTok{ curve\_fit(erf\_model, data, percentile\_list, p0}\OperatorTok{=}\NormalTok{p0,}
\NormalTok{                          bounds}\OperatorTok{=}\NormalTok{([}\DecValTok{70}\NormalTok{, }\DecValTok{3}\NormalTok{],   }\CommentTok{\# lower bounds for mu and sigma}
\NormalTok{                                  [}\DecValTok{200}\NormalTok{, }\DecValTok{10}\NormalTok{])  }\CommentTok{\# upper bounds for mu and sigma}
\NormalTok{                         )}
    \CommentTok{\# store the parameters in the dataframe}
\NormalTok{    df\_stats\_girls.at[i, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ params[}\DecValTok{0}\NormalTok{]}
\NormalTok{    df\_stats\_girls.at[i, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]}
\NormalTok{    percentile\_predicted }\OperatorTok{=}\NormalTok{ erf\_model(data, }\OperatorTok{*}\NormalTok{params)}
    \CommentTok{\# R{-}squared value}
\NormalTok{    r2 }\OperatorTok{=}\NormalTok{ calculate\_r2(percentile\_list, percentile\_predicted)}
\NormalTok{    df\_stats\_girls.at[i, }\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ r2}
\NormalTok{    p0 }\OperatorTok{=}\NormalTok{ params}

\CommentTok{\# save the dataframes to csv files}
\NormalTok{df\_stats\_boys.to\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df\_stats\_girls.to\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's see what we got. The top panel in the graph shows the average
height for boys and girls, the middle panel shows the coefficient of
variation (\(\sigma/\mu\)), and the bottom panel shows the R2 of the fit
(note that the range is very close to 1).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_stats\_boys}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& mu & sigma & r2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2.0 & 86.463069 & 3.563785 & 0.999511 \\
2.1 & 87.374895 & 3.596583 & 0.999676 \\
2.2 & 88.269676 & 3.627433 & 0.999742 \\
2.3 & 89.148086 & 3.657263 & 0.999752 \\
2.4 & 90.010783 & 3.686764 & 0.999733 \\
... & ... & ... & ... \\
19.6 & 176.802810 & 7.134561 & 0.999991 \\
19.7 & 176.845789 & 7.135786 & 0.999994 \\
19.8 & 176.892196 & 7.137430 & 0.999995 \\
19.9 & 176.942521 & 7.139466 & 0.999990 \\
20.0 & 176.997255 & 7.141858 & 0.999976 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{fig.subplots\_adjust(left}\OperatorTok{=}\FloatTok{0.15}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(df\_stats\_boys[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{], label}\OperatorTok{=}\StringTok{\textquotesingle{}boys\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(df\_stats\_girls[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{], label}\OperatorTok{=}\StringTok{\textquotesingle{}girls\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(df\_stats\_boys[}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{] }\OperatorTok{/}\NormalTok{ df\_stats\_boys[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{], lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(df\_stats\_girls[}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{] }\OperatorTok{/}\NormalTok{ df\_stats\_girls[}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{], lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot(df\_stats\_boys.index, df\_stats\_boys[}\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{], label}\OperatorTok{=}\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\VerbatimStringTok{r2}\DecValTok{$}\VerbatimStringTok{ boys\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot(df\_stats\_girls.index, df\_stats\_girls[}\StringTok{\textquotesingle{}r2\textquotesingle{}}\NormalTok{], label}\OperatorTok{=}\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\VerbatimStringTok{r2}\DecValTok{$}\VerbatimStringTok{ girls\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}average height (cm)\textquotesingle{}}\NormalTok{,)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}CV\textquotesingle{}}\NormalTok{,}
\NormalTok{          ylim}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\FloatTok{0.055}\NormalTok{])}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}age (years)\textquotesingle{}}\NormalTok{,}
\NormalTok{            ylabel}\OperatorTok{=}\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\VerbatimStringTok{R}\DecValTok{\^{}}\VerbatimStringTok{2}\DecValTok{$}\VerbatimStringTok{\textquotesingle{}}\NormalTok{,}
\NormalTok{            xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{2}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{2}\NormalTok{),}
\NormalTok{          )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-17-output-1.png}}

Let's see how the pdfs for boys and girls move and morph as age
increases.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age\_list\_string }\OperatorTok{=}\NormalTok{ age\_list.astype(}\BuiltInTok{str}\NormalTok{).tolist()}
\NormalTok{df\_pdf\_boys }\OperatorTok{=}\NormalTok{ pd.DataFrame(index}\OperatorTok{=}\NormalTok{height\_list, columns}\OperatorTok{=}\NormalTok{age\_list\_string)}
\NormalTok{df\_pdf\_girls }\OperatorTok{=}\NormalTok{ pd.DataFrame(index}\OperatorTok{=}\NormalTok{height\_list, columns}\OperatorTok{=}\NormalTok{age\_list\_string)}

\ControlFlowTok{for}\NormalTok{ age }\KeywordTok{in}\NormalTok{ df\_pdf\_boys.columns:}
\NormalTok{    age\_float }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(}\BuiltInTok{float}\NormalTok{(age), }\DecValTok{1}\NormalTok{)}
\NormalTok{    df\_pdf\_boys[age] }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list,}
\NormalTok{                                loc}\OperatorTok{=}\NormalTok{df\_stats\_boys.loc[age\_float][}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{],}
\NormalTok{                                scale}\OperatorTok{=}\NormalTok{df\_stats\_boys.loc[age\_float][}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{])}
\ControlFlowTok{for}\NormalTok{ age }\KeywordTok{in}\NormalTok{ df\_pdf\_girls.columns:}
\NormalTok{    age\_float }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(}\BuiltInTok{float}\NormalTok{(age), }\DecValTok{1}\NormalTok{)}
\NormalTok{    df\_pdf\_girls[age] }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list,}
\NormalTok{                                loc}\OperatorTok{=}\NormalTok{df\_stats\_girls.loc[age\_float][}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{],}
\NormalTok{                                scale}\OperatorTok{=}\NormalTok{df\_stats\_girls.loc[age\_float][}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_pdf\_girls}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule\noalign{}
& 2.0 & 2.1 & 2.2 & 2.3 & 2.4 & 2.5 & 2.6 & 2.7 & 2.8 & 2.9 & ... & 19.1
& 19.2 & 19.3 & 19.4 & 19.5 & 19.6 & 19.7 & 19.8 & 19.9 & 20.0 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
70.0 & 0.000006 & 2.962419e-06 & 1.229580e-06 & 4.740717e-07 &
1.893495e-07 & 7.928033e-08 & 3.395629e-08 & 1.454961e-08 & 6.214658e-09
& 2.698367e-09 & ... & 3.876760e-46 & 4.998212e-46 & 6.108274e-46 &
6.965756e-46 & 7.300518e-46 & 6.928073e-46 & 5.866310e-46 & 4.367574e-46
& 2.817087e-46 & 1.550490e-46 \\
70.1 & 0.000007 & 3.369929e-06 & 1.401926e-06 & 5.423176e-07 &
2.172465e-07 & 9.118694e-08 & 3.914667e-08 & 1.681357e-08 & 7.199311e-09
& 3.133161e-09 & ... & 4.821662e-46 & 6.212999e-46 & 7.589544e-46 &
8.652519e-46 & 9.067461e-46 & 8.605908e-46 & 7.289698e-46 & 5.430839e-46
& 3.506265e-46 & 1.932327e-46 \\
70.2 & 0.000008 & 3.830459e-06 & 1.597215e-06 & 6.199308e-07 &
2.490751e-07 & 1.048086e-07 & 4.509972e-08 & 1.941687e-08 & 8.334521e-09
& 3.635676e-09 & ... & 5.995467e-46 & 7.721230e-46 & 9.427830e-46 &
1.074523e-45 & 1.125944e-45 & 1.068759e-45 & 9.056344e-46 & 6.751373e-46
& 4.363019e-46 & 2.407630e-46 \\
70.3 & 0.000009 & 4.350475e-06 & 1.818328e-06 & 7.081296e-07 &
2.853621e-07 & 1.203810e-07 & 5.192270e-08 & 2.240831e-08 & 9.642428e-09
& 4.216078e-09 & ... & 7.453283e-46 & 9.593350e-46 & 1.170864e-45 &
1.334099e-45 & 1.397806e-45 & 1.326973e-45 & 1.124851e-45 & 8.391039e-46
& 5.427845e-46 & 2.999137e-46 \\
70.4 & 0.000010 & 4.937172e-06 & 2.068480e-06 & 8.082806e-07 &
3.267014e-07 & 1.381707e-07 & 5.973725e-08 & 2.584341e-08 & 1.114829e-08
& 4.885994e-09 & ... & 9.263403e-46 & 1.191661e-45 & 1.453785e-45 &
1.655996e-45 & 1.734906e-45 & 1.647188e-45 & 1.396806e-45 & 1.042648e-45
& 6.750965e-46 & 3.735083e-46 \\
... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... &
... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
219.5 & 0.000000 & 5.214425e-307 & 1.377605e-289 & 3.568527e-277 &
6.457994e-266 & 2.232144e-255 & 6.340272e-246 & 9.969867e-238 &
1.389324e-230 & 5.441854e-224 & ... & 5.200570e-18 & 5.741874e-18 &
6.194642e-18 & 6.495054e-18 & 6.583545e-18 & 6.417949e-18 & 5.986319e-18
& 5.315101e-18 & 4.468701e-18 & 3.538724e-18 \\
219.6 & 0.000000 & 1.813597e-307 & 5.050074e-290 & 1.356408e-277 &
2.537010e-266 & 9.046507e-256 & 2.642444e-246 & 4.256155e-238 &
6.055129e-231 & 2.417510e-224 & ... & 4.558798e-18 & 5.035058e-18 &
5.433557e-18 & 5.698034e-18 & 5.775970e-18 & 5.630212e-18 & 5.250299e-18
& 4.659675e-18 & 3.915265e-18 & 3.097919e-18 \\
219.7 & 0.000000 & 6.302763e-308 & 1.849870e-290 & 5.151948e-278 &
9.959447e-267 & 3.663840e-256 & 1.100546e-246 & 1.815751e-238 &
2.637298e-231 & 1.073274e-224 & ... & 3.995288e-18 & 4.414220e-18 &
4.764871e-18 & 4.997654e-18 & 5.066279e-18 & 4.938013e-18 & 4.603699e-18
& 4.084117e-18 & 3.429566e-18 & 2.711382e-18 \\
219.8 & 0.000000 & 2.188653e-308 & 6.771033e-291 & 1.955386e-278 &
3.906942e-267 & 1.482823e-256 & 4.580523e-247 & 7.741154e-239 &
1.147918e-231 & 4.761829e-225 & ... & 3.500614e-18 & 3.869030e-18 &
4.177503e-18 & 4.382343e-18 & 4.442754e-18 & 4.329907e-18 & 4.035791e-18
& 3.578814e-18 & 3.003413e-18 & 2.372514e-18 \\
219.9 & 0.000000 & 7.594139e-309 & 2.476504e-291 & 7.416066e-279 &
1.531537e-267 & 5.997065e-257 & 1.905138e-247 & 3.298116e-239 &
4.993198e-232 & 2.111339e-225 & ... & 3.066470e-18 & 3.390384e-18 &
3.661688e-18 & 3.841895e-18 & 3.895062e-18 & 3.795805e-18 & 3.537115e-18
& 3.135297e-18 & 2.629596e-18 & 2.075507e-18 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ plotly.graph\_objects }\ImportTok{as}\NormalTok{ go}
\ImportTok{import}\NormalTok{ plotly.io }\ImportTok{as}\NormalTok{ pio}

\NormalTok{pio.renderers.default }\OperatorTok{=} \StringTok{\textquotesingle{}notebook\textquotesingle{}}

\CommentTok{\# create figure}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ go.Figure()}

\CommentTok{\# assume both dataframes have the same columns (ages) and index (height)}
\NormalTok{ages }\OperatorTok{=}\NormalTok{ df\_pdf\_boys.columns}
\NormalTok{x\_vals }\OperatorTok{=}\NormalTok{ df\_pdf\_boys.index}

\CommentTok{\# add traces: 2 per age (boys and girls), all hidden except the first pair}
\ControlFlowTok{for}\NormalTok{ i, age }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ages):}
\NormalTok{    fig.add\_trace(go.Scatter(x}\OperatorTok{=}\NormalTok{x\_vals, y}\OperatorTok{=}\NormalTok{df\_pdf\_boys[age], name}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Boys }\SpecialCharTok{\{}\NormalTok{age}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{, }
\NormalTok{                             line}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(color}\OperatorTok{=}\StringTok{\textquotesingle{}\#1f77b4\textquotesingle{}}\NormalTok{), visible}\OperatorTok{=}\NormalTok{(i }\OperatorTok{==} \DecValTok{0}\NormalTok{)))}
\NormalTok{    fig.add\_trace(go.Scatter(x}\OperatorTok{=}\NormalTok{x\_vals, y}\OperatorTok{=}\NormalTok{df\_pdf\_girls[age], name}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Girls }\SpecialCharTok{\{}\NormalTok{age}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{, }
\NormalTok{                             line}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(color}\OperatorTok{=}\StringTok{\textquotesingle{}\#ff7f0e\textquotesingle{}}\NormalTok{), visible}\OperatorTok{=}\NormalTok{(i }\OperatorTok{==} \DecValTok{0}\NormalTok{)))}

\CommentTok{\# create slider steps}
\NormalTok{steps }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i, age }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ages):}
\NormalTok{    vis }\OperatorTok{=}\NormalTok{ [}\VariableTok{False}\NormalTok{] }\OperatorTok{*}\NormalTok{ (}\DecValTok{2} \OperatorTok{*} \BuiltInTok{len}\NormalTok{(ages))}
\NormalTok{    vis[}\DecValTok{2}\OperatorTok{*}\NormalTok{i] }\OperatorTok{=} \VariableTok{True}      \CommentTok{\# boys trace}
\NormalTok{    vis[}\DecValTok{2}\OperatorTok{*}\NormalTok{i }\OperatorTok{+} \DecValTok{1}\NormalTok{] }\OperatorTok{=} \VariableTok{True}  \CommentTok{\# girls trace}

\NormalTok{    steps.append(}\BuiltInTok{dict}\NormalTok{(}
\NormalTok{        method}\OperatorTok{=}\StringTok{\textquotesingle{}update\textquotesingle{}}\NormalTok{,}
\NormalTok{        args}\OperatorTok{=}\NormalTok{[\{}\StringTok{\textquotesingle{}visible\textquotesingle{}}\NormalTok{: vis\},}
\NormalTok{              \{}\StringTok{\textquotesingle{}title\textquotesingle{}}\NormalTok{: }\SpecialStringTok{f\textquotesingle{}Height Distribution {-} Age: }\SpecialCharTok{\{}\NormalTok{age}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{\}],}
\NormalTok{        label}\OperatorTok{=}\BuiltInTok{str}\NormalTok{(age)}
\NormalTok{    ))}

\CommentTok{\# define slider}
\NormalTok{sliders }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{dict}\NormalTok{(}
\NormalTok{    active}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    currentvalue}\OperatorTok{=}\NormalTok{\{}\StringTok{"prefix"}\NormalTok{: }\StringTok{"Age: "}\NormalTok{\},}
\NormalTok{    pad}\OperatorTok{=}\NormalTok{\{}\StringTok{"t"}\NormalTok{: }\DecValTok{50}\NormalTok{\},}
\NormalTok{    steps}\OperatorTok{=}\NormalTok{steps}
\NormalTok{)]}

\CommentTok{\# update layout}
\NormalTok{fig.update\_layout(}
\NormalTok{    sliders}\OperatorTok{=}\NormalTok{sliders,}
\NormalTok{    title}\OperatorTok{=}\StringTok{\textquotesingle{}Height Distribution by Age\textquotesingle{}}\NormalTok{,}
\NormalTok{    xaxis\_title}\OperatorTok{=}\StringTok{\textquotesingle{}Height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{    yaxis\_title}\OperatorTok{=}\StringTok{\textquotesingle{}Density\textquotesingle{}}\NormalTok{,}
\NormalTok{    yaxis}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(}\BuiltInTok{range}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{0.12}\NormalTok{]),}
\NormalTok{    showlegend}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    height}\OperatorTok{=}\DecValTok{600}\NormalTok{,}
\NormalTok{    width}\OperatorTok{=}\DecValTok{800}
\NormalTok{)}

\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

A few notes about what we can learn from the analysis above.

\begin{itemize}
\tightlist
\item
  My impression that 12-year-old girls are taller than boys is indeed
  true.
\item
  Boys and girls have very similar distributions up to age 11.
\item
  From age 11 to 13 girls are on average taller than boys.
\item
  From age 13 boys become taller than girls, on average.
\item
  The graph showing the coefficient of variation is interesting. CV for
  girls peaks roughtly at age 12, and for boys it peaks around age 14.
  These local maxima may be explained by the wide variability in the age
  ofpuberty onset.
\item
  The height distribution for each sex, across all ages, is indeed
  extremely well described by the normal distribution. What biological
  factors may account for such a fact?
\end{itemize}

I'll plot one last graph from now, let's see what we can learn from it.
Let's see the pdf for boys and girls across three age groups: 8, 12, and
15 year olds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{fig.subplots\_adjust(hspace}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{ages\_to\_plot }\OperatorTok{=}\NormalTok{ [}\FloatTok{8.0}\NormalTok{, }\FloatTok{12.0}\NormalTok{, }\FloatTok{15.0}\NormalTok{]}

\ControlFlowTok{for}\NormalTok{ i, age }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ages\_to\_plot):}
\NormalTok{    pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{df\_stats\_boys.loc[age][}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{], scale}\OperatorTok{=}\NormalTok{df\_stats\_boys.loc[age][}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{])}
\NormalTok{    pdf\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{df\_stats\_girls.loc[age][}\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{], scale}\OperatorTok{=}\NormalTok{df\_stats\_girls.loc[age][}\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{])}
\NormalTok{    ax[i].plot(height\_list, pdf\_boys, label}\OperatorTok{=}\StringTok{\textquotesingle{}boys\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax[i].plot(height\_list, pdf\_girls, label}\OperatorTok{=}\StringTok{\textquotesingle{}girls\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax[i].text(}\FloatTok{0.98}\NormalTok{, }\FloatTok{0.98}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}age: }\SpecialCharTok{\{}\NormalTok{age}\SpecialCharTok{\}}\SpecialStringTok{ years\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[i].transAxes, verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, horizontalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax[i].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{,}
\NormalTok{              ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.07}\NormalTok{),}
\NormalTok{            )}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{          xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{),)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/height_files/figure-pdf/cell-21-output-1.png}}

\begin{itemize}
\tightlist
\item
  Indeed, boys and girls age 8 have the exact same height distribution.
\item
  12-year-old girls are indeed taller than boys, on average. This
  difference is relatiely small, though.
\item
  By age 15 boys have long surpassed girls in height, and the difference
  is quite large. Boys still have some growing to do, but girls are
  mostly done growing.
\end{itemize}

\chapter{weight data}\label{weight-data}

Now that we have height data covered, it's time we deal with weight
data.

Yes, I am \textbf{VERY WELL AWARE} that weight is a force, and it is not
measured in kg. Nevertheless, I will use the word weight in the
colloquial sense, and for all purposes it is a synonym for mass.

This analysis is based on the
\href{https://www.cdc.gov/growthcharts/cdc-data-files.htm}{CDC Growth
Charts Data Files}. From there I downloaded a csv for the weight of boys
and girls, from age 2 to 20.

For each sex and age, the csv contains three important columns for us:

\begin{itemize}
\tightlist
\item
  \(M\): median
\item
  \(S\): generalized coefficient of variation
\item
  \(L\): Box-Cox power
\end{itemize}

These three variables can be combined to give the weight W at a given
Z-score (number of standard deviations from the mean):

\[
W = M \left(1 + L S Z\right)^{1/L}
\tag{1}
\]

The website contains a different formula for the case \(L=0\), but in
our data set \(L\) is never zero.

It will be useful in a little while to know the inverse of Eq. (1),
which is

\[
Z = \frac{(W/M)^L - 1}{L S}.
\tag{1b}
\]

The formulas above indicate that we're using the
\href{https://en.wikipedia.org/wiki/Box\%E2\%80\%93Cox_distribution}{Box-Cox
distribution} (also called power-normal distribution), and they will
help us compute the probability density function (pdf) for weight.

Given that the pdf of a z-scored variable is \[
f_z(Z) = \frac{1}{\sqrt{2 \pi}} e^{-Z^2/2},
\tag{2}
\]

we need to change variables from Z to W. To do that, we will use

\[
f_w(W)dW = f_z(Z)dZ.
\tag{3}
\]

The rationale behind this is that the probability (area) of being in a
small interval is the same, whether we measure it in terms of W or Z.
See more here:
\href{https://en.wikipedia.org/wiki/Probability_density_function\#Scalar_to_scalar}{Function
of random variables and change of variables in the probability density
function}. Solving Eq. (3) for \(f_w(W)\), we get

\[
f_w(W) = f_z(Z) \left|\frac{dZ}{dW}\right| = f_z(Z) \left|\frac{dW}{dZ}\right|^{-1}.
\tag{4}
\]

Using Eq. (1), the derivative of \(W\) with respect to \(Z\) is

\begin{align*}
\frac{dW}{dZ} &= MS\left(1+LSZ\right)^{\frac{1}{L}-1}\\
              &= \underbrace{M\left(1+LSZ\right)^{\frac{1}{L}}}_{=W \text{ according to Eq. (1)}}S\left(1+LSZ\right)^{-1} \\
              &= WS\frac{1}{1+LSZ} \\
              &= WS\frac{1}{\bcancel{1}+\cancel{LS}\frac{(W/M)^L - \bcancel{1}}{\cancel{L S}}} \\
              &= WS\frac{1}{(W/M)^L} \\
              & = W^{1-L} M^L S
\tag{5}
\end{align*}

Putting everything together {[}remember that we need the reciprocal of
the result in Eq. (5){]}, we get

\begin{align*}
f_w(W) &= f_z(Z) \frac{W^{L-1}}{M^L S} \\
       &= \frac{1}{\sqrt{2 \pi}} e^{-Z^2/2} \frac{W^{L-1}}{M^L S}
\tag{6}
\end{align*}

Of course, we could have substituted \(Z\) from Eq. (1b) into Eq. (6) to
get a formula that depends only on \(W\), it would be too messy. Later
on, we will compute \(f_w(W)\) numerically, so we will not need to do
that.

\section{example}\label{example}

Let's see the weight probability density for boys at age 10 and 15.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ powernorm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weight\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{10}\NormalTok{, }\DecValTok{130}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\KeywordTok{def}\NormalTok{ power\_normal\_pdf(w, age, sex):}
    \CommentTok{"""}
\CommentTok{    Calculates the PDF of the Power Normal distribution from the derived formula.}
\CommentTok{    This function correctly handles negative L values.}
\CommentTok{    """}
    \CommentTok{\# This function is only valid for w \textgreater{} 0}
\NormalTok{    w }\OperatorTok{=}\NormalTok{ np.asarray(w)}
\NormalTok{    pdf }\OperatorTok{=}\NormalTok{ np.full(w.shape, np.nan)}
\NormalTok{    positive\_w }\OperatorTok{=}\NormalTok{ w[w }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{]}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/weight/wtage.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    agemos }\OperatorTok{=}\NormalTok{ age }\OperatorTok{*} \DecValTok{12} \OperatorTok{+} \FloatTok{0.5}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ df[(df[}\StringTok{\textquotesingle{}Agemos\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ agemos) }\OperatorTok{\&}\NormalTok{ (df[}\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ sex)]}
\NormalTok{    L }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}L\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    M }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    S }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}S\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
    
    \CommentTok{\# Calculate the z{-}score}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ ((positive\_w }\OperatorTok{/}\NormalTok{ M)}\OperatorTok{**}\NormalTok{L }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ (L }\OperatorTok{*}\NormalTok{ S)}
    
    \CommentTok{\# Calculate the two main parts of the formula}
\NormalTok{    pre\_factor }\OperatorTok{=}\NormalTok{ positive\_w}\OperatorTok{**}\NormalTok{(L }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ (S }\OperatorTok{*}\NormalTok{ M}\OperatorTok{**}\NormalTok{L }\OperatorTok{*}\NormalTok{ np.sqrt(}\DecValTok{2} \OperatorTok{*}\NormalTok{ np.pi))}
\NormalTok{    exp\_term }\OperatorTok{=}\NormalTok{ np.exp(}\OperatorTok{{-}}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ z}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
    
    \CommentTok{\# Store the results only for the valid (positive w) indices}
\NormalTok{    pdf[w }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{] }\OperatorTok{=}\NormalTok{ pre\_factor }\OperatorTok{*}\NormalTok{ exp\_term}
    \ControlFlowTok{return}\NormalTok{ pdf}

\NormalTok{pdf\_boys10 }\OperatorTok{=}\NormalTok{ power\_normal\_pdf(weight\_list, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{pdf\_boys15 }\OperatorTok{=}\NormalTok{ power\_normal\_pdf(weight\_list, }\DecValTok{15}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(weight\_list, pdf\_boys10, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}10 years\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(weight\_list, pdf\_boys15, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}15 years\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}weight (kg)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"weight distribution for boys"}\NormalTok{)}
\NormalTok{ax.legend(loc}\OperatorTok{=}\StringTok{"upper right"}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/weight_files/figure-pdf/cell-4-output-1.png}}

In future chapters, when we want to talk about weight, it will be more
convenient to leverage scipy's capabilities, both to compute the pdf and
to generate random samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ scipy\_power\_normal\_pdf(w, age, sex):}
    
    \CommentTok{\# Load LMS parameters from the CSV file}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/weight/wtage.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    agemos }\OperatorTok{=}\NormalTok{ age }\OperatorTok{*} \DecValTok{12} \OperatorTok{+} \FloatTok{0.5}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ df[(df[}\StringTok{\textquotesingle{}Agemos\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ agemos) }\OperatorTok{\&}\NormalTok{ (df[}\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ sex)]}
\NormalTok{    L }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}L\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    M }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    S }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}S\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
    
    \CommentTok{\# 1. Transform weight (w) to the standard normal z{-}score}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ ((w }\OperatorTok{/}\NormalTok{ M)}\OperatorTok{**}\NormalTok{L }\OperatorTok{{-}} \DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ (L }\OperatorTok{*}\NormalTok{ S)}
    
    \CommentTok{\# 2. Calculate the derivative of the transformation (dz/dw)}
    \CommentTok{\# This is the Jacobian factor for the change of variables}
\NormalTok{    dz\_dw }\OperatorTok{=}\NormalTok{ (w}\OperatorTok{**}\NormalTok{(L }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)) }\OperatorTok{/}\NormalTok{ (S }\OperatorTok{*}\NormalTok{ M}\OperatorTok{**}\NormalTok{L)}
    
    \CommentTok{\# 3. Apply the change of variables formula: pdf(w) = pdf(z) * |dz/dw|}
\NormalTok{    pdf }\OperatorTok{=}\NormalTok{ stats.norm.pdf(z) }\OperatorTok{*}\NormalTok{ dz\_dw}
    
    \ControlFlowTok{return}\NormalTok{ pdf}

\KeywordTok{def}\NormalTok{ scipy\_power\_normal\_draw\_random(N, age, sex):   }
    \CommentTok{\# Load LMS parameters from the CSV file}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/weight/wtage.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    agemos }\OperatorTok{=}\NormalTok{ age }\OperatorTok{*} \DecValTok{12} \OperatorTok{+} \FloatTok{0.5}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ df[(df[}\StringTok{\textquotesingle{}Agemos\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ agemos) }\OperatorTok{\&}\NormalTok{ (df[}\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ sex)]}
\NormalTok{    L }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}L\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    M }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    S }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}S\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# draw random z from standard normal distribution}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, N)}
    \CommentTok{\# transform z to w}
\NormalTok{    w }\OperatorTok{=}\NormalTok{ M }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ L }\OperatorTok{*}\NormalTok{ S }\OperatorTok{*}\NormalTok{ z)}\OperatorTok{**}\NormalTok{(}\DecValTok{1} \OperatorTok{/}\NormalTok{ L)}
    
    \ControlFlowTok{return}\NormalTok{ w}

\CommentTok{\# Calculate the PDFs for 10 and 15{-}year{-}old boys using the SciPy{-}based function}
\NormalTok{scipy\_pdf\_boys10 }\OperatorTok{=}\NormalTok{ scipy\_power\_normal\_pdf(weight\_list, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{draw10 }\OperatorTok{=}\NormalTok{ scipy\_power\_normal\_draw\_random(}\DecValTok{1000}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{scipy\_pdf\_boys15 }\OperatorTok{=}\NormalTok{ scipy\_power\_normal\_pdf(weight\_list, }\DecValTok{15}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(weight\_list, pdf\_boys10, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}10 years\textquotesingle{}}\NormalTok{)}
\CommentTok{\# histogram of draw10}
\NormalTok{ax.hist(draw10, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}random 10 years\textquotesingle{}}\NormalTok{)}
\CommentTok{\# ax.plot(weight\_list, scipy\_pdf\_boys10, color=\textquotesingle{}tab:blue\textquotesingle{}, ls=":", label=\textquotesingle{}scipy 10\textquotesingle{})}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}weight (kg)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"using scipy\textquotesingle{}s components to draw random samples"}\NormalTok{)}
\NormalTok{ax.legend(loc}\OperatorTok{=}\StringTok{"upper right"}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{data/weight_files/figure-pdf/cell-6-output-1.png}}

\part{hypothesis testing}

\chapter{one-sample t-test}\label{one-sample-t-test}

\section{Question}\label{question}

I measured the height of 10 adult men. Were they sampled from the
general population of men?

\section{Hypotheses}\label{hypotheses}

\begin{itemize}
\tightlist
\item
  Null hypothesis: The sample mean is equal to the population mean. In
  this case, the answer would be ``yes''
\item
  Alternative hypothesis: The sample mean is not equal to the population
  mean. Answer would be ``no''.
\item
  Significance level: 0.05
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_1samp, t}
\OperatorTok{\%}\NormalTok{matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[}\FloatTok{20.0}\NormalTok{, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[}\FloatTok{20.0}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Let's start with a sample of 10.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OperatorTok{=} \DecValTok{10}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample10 }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N, loc}\OperatorTok{=}\NormalTok{mu\_boys}\OperatorTok{+}\DecValTok{2}\NormalTok{, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{140}\NormalTok{, }\DecValTok{220}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(height\_list, pdf\_boys, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.eventplot(sample10, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.03}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}sample\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.text(}\DecValTok{190}\NormalTok{, }\FloatTok{0.04}\NormalTok{, }
       \SpecialStringTok{f"sample mean: }\SpecialCharTok{\{}\NormalTok{sample10}\SpecialCharTok{.}\NormalTok{mean()}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm}\CharTok{\textbackslash{}n}\SpecialStringTok{sample std: }\SpecialCharTok{\{}\NormalTok{sample10}\SpecialCharTok{.}\NormalTok{std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{, }
\NormalTok{       ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.text(}\DecValTok{190}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }
       \SpecialStringTok{f"pop. mean: }\SpecialCharTok{\{}\NormalTok{mu\_boys}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm}\CharTok{\textbackslash{}n}\SpecialStringTok{pop. std: }\SpecialCharTok{\{}\NormalTok{sigma\_boys}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{, }
\NormalTok{       ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"men (age 20)"}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{140}\NormalTok{, }\DecValTok{220}\NormalTok{),}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_one_sample_files/figure-pdf/cell-5-output-1.png}}

The t value is calculated as follows: \[
t = \frac{\bar{x} - \mu}{s / \sqrt{n}}
\]

where

\begin{itemize}
\tightlist
\item
  \(\bar{x}\): sample mean
\item
  \(\mu\): population mean
\item
  \(s\): sample standard deviation
\item
  \(n\): sample size
\end{itemize}

Let's try the formula above and compare it with scipy's ttest\_1samp
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_value\_formula }\OperatorTok{=}\NormalTok{ (sample10.mean() }\OperatorTok{{-}}\NormalTok{ mu\_boys) }\OperatorTok{/}\NormalTok{ (sample10.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ np.sqrt(N))}
\NormalTok{t\_value\_scipy }\OperatorTok{=}\NormalTok{ ttest\_1samp(sample10, popmean}\OperatorTok{=}\NormalTok{mu\_boys)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}value (formula): }\SpecialCharTok{\{}\NormalTok{t\_value\_formula}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}value (scipy): }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
t-value (formula): 1.759
t-value (scipy): 1.759
\end{verbatim}

Let's convert this t value to a p value. It is easy to visualize the p
value by ploting the pdf for the t distribution. The p value is the area
under the curve for t greater than the t value and smaller than the
negative t value.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# degrees of freedom}
\NormalTok{dof }\OperatorTok{=}\NormalTok{ N }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{t\_array\_min }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.001}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array\_max }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.999}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array }\OperatorTok{=}\NormalTok{ np.arange(t\_array\_min, t\_array\_max, }\FloatTok{0.001}\NormalTok{)}

\CommentTok{\# annotate vertical array at t\_value\_scipy}
\NormalTok{ax.annotate(}\SpecialStringTok{f"t value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{                        xy}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.10}\NormalTok{),}
\NormalTok{                        xytext}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.30}\NormalTok{),}
\NormalTok{                        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{                        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.annotate(}\SpecialStringTok{f"{-}t value = {-}}\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{                        xy}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\NormalTok{t\_value\_scipy.statistic, }\FloatTok{0.10}\NormalTok{),}
\NormalTok{                        xytext}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\NormalTok{t\_value\_scipy.statistic, }\FloatTok{0.30}\NormalTok{),}
\NormalTok{                        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{                        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\CommentTok{\# fill between t{-}distribution and normal distribution}
\NormalTok{ax.fill\_between(t\_array, t.pdf(t\_array, dof),}
\NormalTok{                 where}\OperatorTok{=}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(t\_array) }\OperatorTok{\textgreater{}}\NormalTok{ t\_value\_scipy.statistic),}
\NormalTok{                 color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{                 label}\OperatorTok{=}\StringTok{\textquotesingle{}rejection region\textquotesingle{}}\NormalTok{)}

\CommentTok{\# write t\_value\_scipy.pvalue on the plot}
\NormalTok{ax.text(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{,}
        \SpecialStringTok{f"p value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, }
\NormalTok{        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{))}

\NormalTok{ax.plot(t\_array, t.pdf(t\_array, dof),}
\NormalTok{       color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}t\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"t{-}distribution (N=10)"}\NormalTok{,}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_one_sample_files/figure-pdf/cell-7-output-1.png}}

The p value is the fraction of the t distribution that is more extreme
than the observed t value. If the p value is less than the significance
level, we reject the null hypothesis. In this case, the p value is
larger than the significance level, so we fail to reject the null
hypothesis. This means that we do not have enough evidence to say that
the sample mean is different from the population mean. In other words,
we cannot conclude that the 10 men samples were drawn from a
distribution different than the general population.

\section{increase the sample size}\label{increase-the-sample-size}

Let's see what happens when we increase the sample size to 100.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OperatorTok{=} \DecValTok{100}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{628}\NormalTok{)}
\NormalTok{sample100 }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N, loc}\OperatorTok{=}\NormalTok{mu\_boys}\OperatorTok{+}\DecValTok{2}\NormalTok{, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{140}\NormalTok{, }\DecValTok{220}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(height\_list, pdf\_boys, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.eventplot(sample100, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.03}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}sample\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.text(}\DecValTok{190}\NormalTok{, }\FloatTok{0.04}\NormalTok{, }
       \SpecialStringTok{f"sample mean: }\SpecialCharTok{\{}\NormalTok{sample100}\SpecialCharTok{.}\NormalTok{mean()}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm}\CharTok{\textbackslash{}n}\SpecialStringTok{sample std: }\SpecialCharTok{\{}\NormalTok{sample100}\SpecialCharTok{.}\NormalTok{std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{, }
\NormalTok{       ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.text(}\DecValTok{190}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }
       \SpecialStringTok{f"pop. mean: }\SpecialCharTok{\{}\NormalTok{mu\_boys}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm}\CharTok{\textbackslash{}n}\SpecialStringTok{pop. std: }\SpecialCharTok{\{}\NormalTok{sigma\_boys}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{, }
\NormalTok{       ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"men (age 20)"}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{140}\NormalTok{, }\DecValTok{220}\NormalTok{),}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_one_sample_files/figure-pdf/cell-9-output-1.png}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_value\_scipy }\OperatorTok{=}\NormalTok{ ttest\_1samp(sample100, popmean}\OperatorTok{=}\NormalTok{mu\_boys)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}value: }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p{-}value: }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
t-value: 2.675
p-value: 0.009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# degrees of freedom}
\NormalTok{dof }\OperatorTok{=}\NormalTok{ N }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{t\_array\_min }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.001}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array\_max }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.999}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array }\OperatorTok{=}\NormalTok{ np.arange(t\_array\_min, t\_array\_max, }\FloatTok{0.001}\NormalTok{)}

\CommentTok{\# annotate vertical array at t\_value\_scipy}
\NormalTok{ax.annotate(}\SpecialStringTok{f"t value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{                        xy}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.03}\NormalTok{),}
\NormalTok{                        xytext}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.20}\NormalTok{),}
\NormalTok{                        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{                        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.annotate(}\SpecialStringTok{f"{-}t value = {-}}\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{                        xy}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\NormalTok{t\_value\_scipy.statistic, }\FloatTok{0.03}\NormalTok{),}
\NormalTok{                        xytext}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\NormalTok{t\_value\_scipy.statistic, }\FloatTok{0.20}\NormalTok{),}
\NormalTok{                        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{                        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\CommentTok{\# fill between t{-}distribution and normal distribution}
\NormalTok{ax.fill\_between(t\_array, t.pdf(t\_array, dof),}
\NormalTok{                 where}\OperatorTok{=}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(t\_array) }\OperatorTok{\textgreater{}}\NormalTok{ t\_value\_scipy.statistic),}
\NormalTok{                 color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{                 label}\OperatorTok{=}\StringTok{\textquotesingle{}rejection region\textquotesingle{}}\NormalTok{)}

\CommentTok{\# write t\_value\_scipy.pvalue on the plot}
\NormalTok{ax.text(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{,}
        \SpecialStringTok{f"p value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, }
\NormalTok{        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{))}

\NormalTok{ax.plot(t\_array, t.pdf(t\_array, dof),}
\NormalTok{       color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}t\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"t{-}distribution (N=100)"}\NormalTok{,}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_one_sample_files/figure-pdf/cell-11-output-1.png}}

\section{Question 2}\label{question-2}

Can we say that the sampled men are taller than the general population?

\section{Hypotheses}\label{hypotheses-1}

\begin{itemize}
\tightlist
\item
  Null hypothesis: The sample mean is equal to the population mean.
\item
  Alternative hypothesis: The sample mean is higher the population mean.
\item
  Significance level: 0.05
\end{itemize}

The analysis is the same as before, but we will use a one-tailed test.
The t statistic is the same, but the p value is smaller, since we
account for a smaller portion of the total area of the pdf.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_value\_scipy }\OperatorTok{=}\NormalTok{ ttest\_1samp(sample100, popmean}\OperatorTok{=}\NormalTok{mu\_boys, alternative}\OperatorTok{=}\StringTok{\textquotesingle{}greater\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}value: }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p{-}value: }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
t-value: 2.675
p-value: 0.004
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# degrees of freedom}
\NormalTok{dof }\OperatorTok{=}\NormalTok{ N }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{t\_array\_min }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.001}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array\_max }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.999}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array }\OperatorTok{=}\NormalTok{ np.arange(t\_array\_min, t\_array\_max, }\FloatTok{0.001}\NormalTok{)}

\CommentTok{\# annotate vertical array at t\_value\_scipy}
\NormalTok{ax.annotate(}\SpecialStringTok{f"t value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{                        xy}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.03}\NormalTok{),}
\NormalTok{                        xytext}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.20}\NormalTok{),}
\NormalTok{                        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{                        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\CommentTok{\# fill between t{-}distribution and normal distribution}
\NormalTok{ax.fill\_between(t\_array, t.pdf(t\_array, dof),}
\NormalTok{                 where}\OperatorTok{=}\NormalTok{(t\_array }\OperatorTok{\textgreater{}}\NormalTok{ t\_value\_scipy.statistic),}
\NormalTok{                 color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{                 label}\OperatorTok{=}\StringTok{\textquotesingle{}rejection region\textquotesingle{}}\NormalTok{)}

\CommentTok{\# write t\_value\_scipy.pvalue on the plot}
\NormalTok{ax.text(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{,}
        \SpecialStringTok{f"p value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, }
\NormalTok{        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{))}

\NormalTok{ax.plot(t\_array, t.pdf(t\_array, dof),}
\NormalTok{       color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}t\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"t{-}distribution (N=100)"}\NormalTok{,}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_one_sample_files/figure-pdf/cell-13-output-1.png}}

The answer is yes: the sampled men are significantly taller than the
general population, since the p value is smaller than the significance
level.

\chapter{independent samples t-test}\label{independent-samples-t-test}

\section{Question}\label{question-1}

Are 12-year old girls significantly taller than 12-year old boys?

\section{Hypotheses}\label{hypotheses-2}

\begin{itemize}
\tightlist
\item
  Null hypothesis: Girls and boys have the same mean height.
\item
  Alternative hypothesis: Girls are \emph{significantly} taller.
\item
  Significance level: 0.05
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{12.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{mu\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

In this example, we sampled 10 boys and 14 girls. See below the samples
data and their underlying distributions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{10}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{14}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{120}\NormalTok{, }\DecValTok{180}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{pdf\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(height\_list, pdf\_boys, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}boys population\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(height\_list, pdf\_girls, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}girls population\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.eventplot(sample\_boys, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.03}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}boys sample, n=}\SpecialCharTok{\{}\NormalTok{N\_boys}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.eventplot(sample\_girls, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.023}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}girls sample, n=}\SpecialCharTok{\{}\NormalTok{N\_girls}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_independent_samples_files/figure-pdf/cell-5-output-1.png}}

To answer the question, we will use an independent samples t-test.

\begin{align}
t &= \frac{\bar{X}_1 - \bar{X}_2}{\Theta} \\
\Theta &= \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\end{align}

This is a generalization of the one-sample t-test. If we take one of the
samples to be infinite, we get the one-sample t-test.

We can compute the t-statistic by ourselves, and compare the results
with those of \texttt{scipy.stats.ttest\_ind}. Because we are interested
in the difference between the means, we will use the
\texttt{equal\_var=False} option to compute Welch's t-test. Also,
because we are testing the alternative hypothesis that girls are taller,
we will use the one sided test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Theta }\OperatorTok{=}\NormalTok{ np.sqrt(sample\_boys.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sample\_boys.size }\OperatorTok{+} \OperatorTok{\textbackslash{}}
\NormalTok{                sample\_girls.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sample\_girls.size)}
\NormalTok{t\_stat }\OperatorTok{=}\NormalTok{ (sample\_boys.mean() }\OperatorTok{{-}}\NormalTok{ sample\_girls.mean()) }\OperatorTok{/}\NormalTok{ Theta}
\NormalTok{dof }\OperatorTok{=}\NormalTok{ N\_boys }\OperatorTok{+}\NormalTok{ N\_girls }\OperatorTok{{-}} \DecValTok{2}
\NormalTok{p\_val }\OperatorTok{=}\NormalTok{ t.cdf(t\_stat, dof)}

\CommentTok{\# the option alternative="less" is used because we are testing whether the first sample (boys) is less than the second sample (girls)}
\NormalTok{t\_value\_scipy }\OperatorTok{=}\NormalTok{ ttest\_ind(sample\_boys, sample\_girls, equal\_var}\OperatorTok{=}\VariableTok{False}\NormalTok{, alternative}\OperatorTok{=}\StringTok{"less"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}statistic: }\SpecialCharTok{\{}\NormalTok{t\_stat}\SpecialCharTok{:.3f\}}\SpecialStringTok{, p{-}value: }\SpecialCharTok{\{}\NormalTok{p\_val}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}statistic (scipy): }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{, p{-}value (scipy): }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
t-statistic: -0.999, p-value: 0.164
t-statistic (scipy): -0.999, p-value (scipy): 0.165
\end{verbatim}

We got the exact same results :)

Now let's visualize what the p-value means.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# degrees of freedom}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{t\_array\_min }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.001}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array\_max }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(t.ppf(}\FloatTok{0.999}\NormalTok{, dof),}\DecValTok{3}\NormalTok{)}
\NormalTok{t\_array }\OperatorTok{=}\NormalTok{ np.arange(t\_array\_min, t\_array\_max, }\FloatTok{0.001}\NormalTok{)}

\CommentTok{\# annotate vertical array at t\_value\_scipy}
\NormalTok{ax.annotate(}\SpecialStringTok{f"t value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{,}
\NormalTok{                        xy}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.25}\NormalTok{),}
\NormalTok{                        xytext}\OperatorTok{=}\NormalTok{(t\_value\_scipy.statistic, }\FloatTok{0.35}\NormalTok{),}
\NormalTok{                        fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{                        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\CommentTok{\# fill between t{-}distribution and normal distribution}
\NormalTok{ax.fill\_between(t\_array, t.pdf(t\_array, dof),}
\NormalTok{                 where}\OperatorTok{=}\NormalTok{(t\_array }\OperatorTok{\textless{}}\NormalTok{ t\_value\_scipy.statistic),}
\NormalTok{                 color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{                 label}\OperatorTok{=}\StringTok{\textquotesingle{}rejection region\textquotesingle{}}\NormalTok{)}

\CommentTok{\# write t\_value\_scipy.pvalue on the plot}
\NormalTok{ax.text(}\DecValTok{0}\NormalTok{, }\FloatTok{0.05}\NormalTok{,}
        \SpecialStringTok{f"p value = }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{, }
\NormalTok{        ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{))}

\NormalTok{ax.plot(t\_array, t.pdf(t\_array, dof),}
\NormalTok{       color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}t\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"t{-}distribution (dof=22)"}\NormalTok{,}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_independent_samples_files/figure-pdf/cell-7-output-1.png}}

Because the p-value is higher than the significance level, we fail to
reject the null hypothesis. This means that, based on the data, we
cannot conclude that girls are significantly taller than boys.

\section{increasing sample size}\label{increasing-sample-size}

Let's increase the sample size to see how it affects the p-value. We'll
sample 250 boys and 200 girls now.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{250}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{200}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{120}\NormalTok{, }\DecValTok{180}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{pdf\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(height\_list, pdf\_boys, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}boys population\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(height\_list, pdf\_girls, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}girls population\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.eventplot(sample\_boys, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.03}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}boys sample, n=}\SpecialCharTok{\{}\NormalTok{N\_boys}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.eventplot(sample\_girls, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.023}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}girls sample, n=}\SpecialCharTok{\{}\NormalTok{N\_girls}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/t_test_independent_samples_files/figure-pdf/cell-9-output-1.png}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Theta }\OperatorTok{=}\NormalTok{ np.sqrt(sample\_boys.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sample\_boys.size }\OperatorTok{+} \OperatorTok{\textbackslash{}}
\NormalTok{                sample\_girls.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}\OperatorTok{**}\DecValTok{2}\OperatorTok{/}\NormalTok{sample\_girls.size)}
\NormalTok{t\_stat }\OperatorTok{=}\NormalTok{ (sample\_boys.mean() }\OperatorTok{{-}}\NormalTok{ sample\_girls.mean()) }\OperatorTok{/}\NormalTok{ Theta}
\NormalTok{dof }\OperatorTok{=}\NormalTok{ N\_boys }\OperatorTok{+}\NormalTok{ N\_girls }\OperatorTok{{-}} \DecValTok{2}
\NormalTok{p\_val }\OperatorTok{=}\NormalTok{ t.cdf(t\_stat, dof)}

\CommentTok{\# the option alternative="less" is used because we are testing whether the first sample (boys) is less than the second sample (girls)}
\NormalTok{t\_value\_scipy }\OperatorTok{=}\NormalTok{ ttest\_ind(sample\_boys, sample\_girls, equal\_var}\OperatorTok{=}\VariableTok{False}\NormalTok{, alternative}\OperatorTok{=}\StringTok{"less"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}statistic: }\SpecialCharTok{\{}\NormalTok{t\_stat}\SpecialCharTok{:.3f\}}\SpecialStringTok{, p{-}value: }\SpecialCharTok{\{}\NormalTok{p\_val}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"t{-}statistic (scipy): }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{, p{-}value (scipy): }\SpecialCharTok{\{}\NormalTok{t\_value\_scipy}\SpecialCharTok{.}\NormalTok{pvalue}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
t-statistic: -2.639, p-value: 0.004
t-statistic (scipy): -2.639, p-value (scipy): 0.004
\end{verbatim}

We found now a p-value lower than the significance level, so we reject
the null hypothesis. This means that, based on the data, we can conclude
that girls are significantly taller than boys.

\chapter{statistical power}\label{statistical-power}

\section{motivation}\label{motivation}

In the chapter
\href{./hypothesis_testing/t_test_independent_samples.ipynb}{independent
samples t-test}, we asked the question:

\textbf{Are 12-year old girls significantly taller than 12-year old
boys?}

We showed that, when sampling 10 boys and 14 girls, we could not reject
the null hypothesis that boys and girls have the same mean height. This
was so because our p-value was higher that our significance level
\(\alpha=0.05\). We then increased the sample sizes to 250 boys and 200
girls, and now the p-value dropped below \(\alpha\), so we could reject
the null hypothesis, and accept the alternative hypothesis that girls
are significantly taller than boys.

Statistical power is the idea behind this relationship between sample
size and our ability to discern (or not) significant effects in our
data.

\section{the burning house mnemonic}\label{the-burning-house-mnemonic}

In my house I have a fire alarm. Sometimes it goes off without any
apparent reason, or just because it sensed a little smoke from my
toaster. Sure, this is annoying, but no sensor is perfect, and that's
the price I pay for securing my house. Online reviews of this fire alarm
system say that this is a great choice, because the chance that there is
a real fire and that the alarm doesn't notice it is miniscule, but
nonzero nonetheless.

Let's make this scenario more structured. For every hour that passes,
there can either be a fire in the house or not. In that same time
interval, the fire alarm might go off or not.

In trying to secure my house from fire, I have to consider two important
probabilities.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  I really don't want any misses, that is, events when there's a real
  fire and the alarm doesn't notice it. Let's call \(\beta\) the
  probability that, in the case of a real fire, the alarm misses a real
  fire. It will be easy to remember \(\beta\), it stands for
  \textbf{burn}.
\item
  I can take precautions to decrease the chance that I don't notice a
  real fire and my house burns: I can by a more sensitive sensor, or
  install many more sensors around the house. The downside of that is
  that the alarm will sound a lot more often, and almost always it will
  be a false alarm. Let's call \(\alpha\) the probability that there
  wasn't any fire in the house, but the alarm went off. \(\alpha\) stads
  for false \textbf{alarm}.
\end{enumerate}

Nothing is perfect in the real world, and I have a tradeoff between
\(\alpha\) and \(\beta\):

\begin{itemize}
\tightlist
\item
  If I am super risk-averse, I can decrease \(\beta\), the chance that
  my house will \textbf{burn} without the alarm going off, and at the
  same time increase \(\alpha\), the chance of false alarms.
\item
  If I can't stand having the alarm going off all the time without
  reason, I can decrease \(\alpha\) by decreasing the sensitivity of the
  sensor, but that will necessarily increase \(\beta\).
\item
  This is not a binary choice, it's a tradeoff. The only way of getting
  the best of both worlds is by spending a lot of money. For example, I
  could buy 20 super-precise sensors and spread them around the house,
  and only have the alarm go off when 5 of them identify smoke, so my
  toaster alone will not trigger any false alarms.
\end{itemize}

The whole point of this ``burning house'' example is to help me (and
maybe you) remember what \(\alpha\) and \(\beta\) mean. They are also
given the horrible names of Type I and Type II errors, which baffles me:
who would want to remember these names? Not all is lost, in the greek
alphabet \(\alpha\) is the 1st letter and \(\beta\) is the 2nd, so we
can rely on that to remember what are Type I and Type II errors. If I
were king of statistics for one day, I would call the two types of error
simply ``false alarms'' and ``misses''.

\section{statistical power}\label{statistical-power-1}

Statistical power the probability of correctly identifying the fire.
Mathematically, it's \(1-\beta\). In the the language of hypothesis
testing:

\begin{itemize}
\tightlist
\item
  H0, Null Hypothesis: there isn't any fire.
\item
  H1, Alternative Hypothesis: there is a fire.
\item
  Power is the probability that I will correctly reject H0. That's the
  whole point of an alarm system! That's the whole point of any
  experiment designed to test a hypothesis! Sure, if H0 is indeed true,
  then I would want not to reject it.
\end{itemize}

The quantities \(\alpha\) and \(\beta\) are usually visualized in a
matrix:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2308}}
  >{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3846}}
  >{\centering\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3846}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
H0 is TrueThere is no fire
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
H0 is FalseThere is a fire
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reject H0Sound the alarm! & False \textbf{alarm} \(=\alpha\)Type I Error
& Correctly identify the firePower \(= 1-\beta\) \\
Fail to Reject H0Alarm stays quiet & No fire and no alarm\((1-\alpha)\)
& House \textbf{burned} \(=\beta\)Type II Error \\
\end{longtable}

In this table, the probabilities are meant to be read vertically:

\begin{align*}
\underbrace{Pr(\text{alarm}\mid \text{No fire} )}_{\alpha} + 
\underbrace{Pr(\text{no alarm}\mid \text{No fire})}_{1-\alpha} &= 1 \\
\underbrace{Pr(\text{alarm}\mid \text{Fire} )}_{1-\beta} + 
\underbrace{Pr(\text{no alarm}\mid \text{Fire})}_{\beta} &= 1
\end{align*}

\section{power analysis}\label{power-analysis}

Power analysis is a puzzle made of four pieces:

\begin{itemize}
\tightlist
\item
  \textbf{statistical significance:} This is \(\alpha\), the probability
  of false alarms.
\item
  \textbf{statistical power:} This is \(1-\beta\), the probability of
  correctly identifying a real effect.
\item
  \textbf{effect size:} What is the difference between my two hypothesis
  (The null \(H_0\) and the alternative \(H_1\)).
\item
  \textbf{sample size:} Self explanatory.
\end{itemize}

If I know three of the four, I can figure out the one missing. In
general, \(\alpha\) and \(\beta\) are determined by the experimenter,
and we are left with two common cases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Given an expected effect size, I want to figure out the minimum sample
  size that gives me a high probability of rejecting \(H_0\) if the
  effect is real.
\item
  Given a fixed sample size (e.g.~limited resources), what is the
  greatest effect size I can hope to identify?
\end{enumerate}

Let's go back to the question at the very top of this chapter:

\textbf{Are 12-year old girls significantly taller than 12-year old
boys?}

We will perform the simplest power analysis we can, just to demonstrate
the logic involved in the calculation. We will assume the following:

\begin{itemize}
\tightlist
\item
  \(\alpha=0.05\), this is the standard, ``vanilla'' value for
  statistical significance.
\item
  \(1-\beta=0.80\), this is the standard, ``vanilla'' value for
  statistical power.
\item
  We assume that the standard deviation in the population is known for
  each sex, and that it is equal. Here we assume it to be
  \(\sigma=7.39\) cm.
\item
  Assume that we will sample the same number of boys and girls, call it
  \(n\).
\item
  Assume that \(n\) is large enough, such that, according to the Central
  Limit Theorem, the means will be normally distributed.
\item
  Our question asks whether girls are taller than boys, so we will use a
  one-sided test. If instead we were asking if girls are significantly
  different from boys then it would be a two-sided test.
\end{itemize}

Let's call \(\bar h_\text{girls}\) and \(\bar h_\text{boys}\) the sample
mean heights of girls and boys. Then, the estimator for the difference
in mean height is

\[
\hat{\Delta} = \bar h_\text{girls} - \bar h_\text{boys}
\]

Because we assume that \(n\) is large enough, the means for girls and
boys are normally distributed, centered around the true population
means, and with variance \(\sigma^2/n\) (see
\href{https://yairmau.com/statistics/confidence_interval/analytical_confidence_interval.html\#clt}{Central
Limit Theorem}). This means that their difference is also normally
distributed:

\[
\hat{\Delta} \sim \mathcal{N}\left( \Delta, \frac{2\sigma^2}{n}\right),
\]

where \(\Delta\) is the true difference in mean height between girls and
boys. The factor of 2 arises because the difference in sample means
contains uncertainty from both groups; since the two sample means are
independent, their variances add.

For convenience, we can standardize \(\hat{\Delta}\) to get a Z-score:

\[
Z = \frac{\hat{\Delta}}{\sigma\sqrt{2/n}}
\]

Under the Null Hypothesis \(H_0\), there is no difference between the
population mean of girls and boys (\(\Delta=0\)), so

\[
Z_{H_0} \sim \mathcal{N}\left(0, 1\right).
\]

The Alternative Hypothesis \(H_1\) states that girls are on average
taller than boys (\(\Delta>0\)), so

\[
Z_{H_1} \sim \mathcal{N}\left(\frac{\Delta}{\sigma}\sqrt{\frac{n}{2}}, 1\right).
\]

In this example, the effect size will be quantified by the Cohen's
\(d\), given by \(d=\Delta/\sigma\), which is the true difference in
units of standard deviation. Therefore, we can write another expression
for \(Z_{H_1}\) as

\[
Z_{H_1} \sim \mathcal{N}\left(d\sqrt{\frac{n}{2}}, 1\right).
\]

Let's visualize all this in a widget.

\begin{itemize}
\tightlist
\item
  Panel (a) shows the distribution of \(\hat{\Delta}\) for two sampling
  quantities, and you can play with the sliders to choose any values you
  want. It's easy to see how increasing the sample size makes the
  standard error of the mean smaller (\(\sigma/\sqrt{n}\)), which causes
  the distribution to become more concentrated.
\item
  Panel (b) shows the differences after the standardization, that is,
  their Z-scores. The black curve shows the H0 distribution. As the
  sample number increases, the H1 distributions move to the right,
  meaning that it should be easier for us to see the effect we're
  looking for.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ altair }\ImportTok{as}\NormalTok{ alt}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{12.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{mu\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma }\OperatorTok{=}\NormalTok{ (sigma\_boys }\OperatorTok{+}\NormalTok{ sigma\_girls)}\OperatorTok{/}\DecValTok{2}
\NormalTok{Delta }\OperatorTok{=}\NormalTok{ (mu\_girls }\OperatorTok{{-}}\NormalTok{ mu\_boys)}
\NormalTok{cohens\_d }\OperatorTok{=}\NormalTok{ Delta }\OperatorTok{/}\NormalTok{ sigma}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# {-}{-}{-} Parameters {-}{-}{-}}
\NormalTok{seed }\OperatorTok{=} \DecValTok{2}  \CommentTok{\# change this to any int (or set to None for non{-}reproducible draws)}
\NormalTok{delta }\OperatorTok{=}\NormalTok{ mu\_girls }\OperatorTok{{-}}\NormalTok{ mu\_boys}
\NormalTok{num\_samples\_dist }\OperatorTok{=} \DecValTok{5000}
\NormalTok{bin\_step }\OperatorTok{=} \FloatTok{0.2}

\CommentTok{\# colors}
\NormalTok{c\_n1 }\OperatorTok{=} \StringTok{"\#dbb40c"}
\NormalTok{c\_n2 }\OperatorTok{=} \StringTok{"\#86b4a9"}
\NormalTok{c\_null }\OperatorTok{=} \StringTok{"\#000000"}

\CommentTok{\# {-}{-}{-} RNG (seedable) {-}{-}{-}}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed) }\ControlFlowTok{if}\NormalTok{ seed }\KeywordTok{is} \KeywordTok{not} \VariableTok{None} \ControlFlowTok{else}\NormalTok{ np.random.default\_rng()}

\CommentTok{\# {-}{-}{-} Independent random draws {-}{-}{-}}
\NormalTok{data\_dist }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"sample\_id"}\NormalTok{: np.arange(num\_samples\_dist)\})}
\NormalTok{data\_dist[}\StringTok{"z0"}\NormalTok{] }\OperatorTok{=}\NormalTok{ rng.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, num\_samples\_dist)  }\CommentTok{\# null}
\NormalTok{data\_dist[}\StringTok{"z1"}\NormalTok{] }\OperatorTok{=}\NormalTok{ rng.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, num\_samples\_dist)  }\CommentTok{\# N1}
\NormalTok{data\_dist[}\StringTok{"z2"}\NormalTok{] }\OperatorTok{=}\NormalTok{ rng.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, num\_samples\_dist)  }\CommentTok{\# N2}

\CommentTok{\# {-}{-}{-} Interactive sliders {-}{-}{-}}
\NormalTok{n1\_slider }\OperatorTok{=}\NormalTok{ alt.param(name}\OperatorTok{=}\StringTok{"N1"}\NormalTok{, value}\OperatorTok{=}\DecValTok{50}\NormalTok{,  bind}\OperatorTok{=}\NormalTok{alt.binding\_range(}\BuiltInTok{min}\OperatorTok{=}\DecValTok{5}\NormalTok{, }\BuiltInTok{max}\OperatorTok{=}\DecValTok{400}\NormalTok{, step}\OperatorTok{=}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{"n1 "}\NormalTok{))}
\NormalTok{n2\_slider }\OperatorTok{=}\NormalTok{ alt.param(name}\OperatorTok{=}\StringTok{"N2"}\NormalTok{, value}\OperatorTok{=}\DecValTok{400}\NormalTok{, bind}\OperatorTok{=}\NormalTok{alt.binding\_range(}\BuiltInTok{min}\OperatorTok{=}\DecValTok{5}\NormalTok{, }\BuiltInTok{max}\OperatorTok{=}\DecValTok{400}\NormalTok{, step}\OperatorTok{=}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{"n2 "}\NormalTok{))}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# helper: step histogram from a single computed value column}
\CommentTok{\# if label is not None, it creates a "lbl" field for legend/color}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ step\_hist(calc\_expr, x\_domain, y\_domain, x\_title, color}\OperatorTok{=}\VariableTok{None}\NormalTok{, label}\OperatorTok{=}\VariableTok{None}\NormalTok{, show\_legend}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
\NormalTok{    base }\OperatorTok{=}\NormalTok{ (}
\NormalTok{        alt.Chart(data\_dist)}
\NormalTok{        .transform\_calculate(value}\OperatorTok{=}\NormalTok{calc\_expr)}
\NormalTok{        .transform\_joinaggregate(total}\OperatorTok{=}\StringTok{"count()"}\NormalTok{)}
\NormalTok{        .transform\_bin(}
\NormalTok{            [}\StringTok{"bin\_start"}\NormalTok{, }\StringTok{"bin\_end"}\NormalTok{],}
\NormalTok{            field}\OperatorTok{=}\StringTok{"value"}\NormalTok{,}
            \BuiltInTok{bin}\OperatorTok{=}\NormalTok{alt.Bin(extent}\OperatorTok{=}\NormalTok{x\_domain, step}\OperatorTok{=}\NormalTok{bin\_step),}
\NormalTok{        )}
\NormalTok{        .transform\_aggregate(count}\OperatorTok{=}\StringTok{"count()"}\NormalTok{, groupby}\OperatorTok{=}\NormalTok{[}\StringTok{"bin\_start"}\NormalTok{, }\StringTok{"bin\_end"}\NormalTok{, }\StringTok{"total"}\NormalTok{])}
\NormalTok{        .transform\_calculate(density}\OperatorTok{=}\SpecialStringTok{f"datum.count / (datum.total * }\SpecialCharTok{\{}\NormalTok{bin\_step}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{)}
\NormalTok{        .mark\_line(interpolate}\OperatorTok{=}\StringTok{"step"}\NormalTok{, strokeWidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{)}
\NormalTok{        .encode(}
\NormalTok{            x}\OperatorTok{=}\NormalTok{alt.X(}\StringTok{"bin\_start:Q"}\NormalTok{, scale}\OperatorTok{=}\NormalTok{alt.Scale(domain}\OperatorTok{=}\NormalTok{x\_domain), title}\OperatorTok{=}\NormalTok{x\_title),}
\NormalTok{            y}\OperatorTok{=}\NormalTok{alt.Y(}\StringTok{"density:Q"}\NormalTok{, stack}\OperatorTok{=}\VariableTok{None}\NormalTok{, scale}\OperatorTok{=}\NormalTok{alt.Scale(domain}\OperatorTok{=}\NormalTok{y\_domain), title}\OperatorTok{=}\StringTok{"Density"}\NormalTok{),}
\NormalTok{        )}
\NormalTok{        .properties(width}\OperatorTok{=}\DecValTok{620}\NormalTok{, height}\OperatorTok{=}\DecValTok{230}\NormalTok{)}
\NormalTok{    )}

    \ControlFlowTok{if}\NormalTok{ label }\KeywordTok{is} \VariableTok{None}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ base.encode(color}\OperatorTok{=}\NormalTok{alt.value(color))}

\NormalTok{    leg }\OperatorTok{=}\NormalTok{ (}
\NormalTok{        alt.Legend(}
\NormalTok{            title}\OperatorTok{=}\VariableTok{None}\NormalTok{,}
\NormalTok{            orient}\OperatorTok{=}\StringTok{"right"}\NormalTok{,}
\NormalTok{            labelFontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            symbolSize}\OperatorTok{=}\DecValTok{180}\NormalTok{,}
\NormalTok{            symbolStrokeWidth}\OperatorTok{=}\DecValTok{3}\NormalTok{,}
\NormalTok{        )}
        \ControlFlowTok{if}\NormalTok{ show\_legend}
        \ControlFlowTok{else} \VariableTok{None}
\NormalTok{    )}

    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        base}
\NormalTok{        .transform\_calculate(lbl}\OperatorTok{=}\SpecialStringTok{f"\textquotesingle{}}\SpecialCharTok{\{}\NormalTok{label}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}"}\NormalTok{)}
\NormalTok{        .encode(}
\NormalTok{            color}\OperatorTok{=}\NormalTok{alt.Color(}
                \StringTok{"lbl:N"}\NormalTok{,}
\NormalTok{                scale}\OperatorTok{=}\NormalTok{alt.Scale(domain}\OperatorTok{=}\NormalTok{[}\StringTok{"N1"}\NormalTok{, }\StringTok{"N2"}\NormalTok{], }\BuiltInTok{range}\OperatorTok{=}\NormalTok{[c\_n1, c\_n2]),}
\NormalTok{                legend}\OperatorTok{=}\NormalTok{leg,}
\NormalTok{            )}
\NormalTok{        )}
\NormalTok{    )}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} TOP PANEL (only N1 and N2) {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{top\_n1 }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{delta}\SpecialCharTok{\}}\SpecialStringTok{ + datum.z1 * sqrt(2 * }\SpecialCharTok{\{}\NormalTok{sigma}\OperatorTok{**}\DecValTok{2}\SpecialCharTok{\}}\SpecialStringTok{ / N1)"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{1.0}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Observed Difference of Means ()"}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"N1"}\NormalTok{,}
\NormalTok{    show\_legend}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{)}

\NormalTok{top\_n2 }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{delta}\SpecialCharTok{\}}\SpecialStringTok{ + datum.z2 * sqrt(2 * }\SpecialCharTok{\{}\NormalTok{sigma}\OperatorTok{**}\DecValTok{2}\SpecialCharTok{\}}\SpecialStringTok{ / N2)"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{1.0}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Observed Difference of Means ()"}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"N2"}\NormalTok{,}
\NormalTok{    show\_legend}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{)}

\NormalTok{top\_row }\OperatorTok{=}\NormalTok{ alt.layer(top\_n1, top\_n2).properties(}
\NormalTok{    title}\OperatorTok{=}\StringTok{"(a): Distribution of Sample Mean Differences"}
\NormalTok{).resolve\_scale(color}\OperatorTok{=}\StringTok{"shared"}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} BOTTOM PANEL (null + alts, no legend) {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{bottom\_null }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\StringTok{"datum.z0"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Standardized Value (Z)"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\NormalTok{c\_null,}
\NormalTok{)}

\NormalTok{bottom\_n1 }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\SpecialStringTok{f"datum.z1 + (}\SpecialCharTok{\{}\NormalTok{delta}\SpecialCharTok{\}}\SpecialStringTok{ / (}\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{\}}\SpecialStringTok{ * sqrt(2 / N1)))"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Standardized Value (Z)"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\NormalTok{c\_n1,}
\NormalTok{)}

\NormalTok{bottom\_n2 }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\SpecialStringTok{f"datum.z2 + (}\SpecialCharTok{\{}\NormalTok{delta}\SpecialCharTok{\}}\SpecialStringTok{ / (}\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{\}}\SpecialStringTok{ * sqrt(2 / N2)))"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Standardized Value (Z)"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\NormalTok{c\_n2,}
\NormalTok{)}

\NormalTok{null\_text }\OperatorTok{=}\NormalTok{ alt.Chart(pd.DataFrame(\{}\StringTok{"x"}\NormalTok{: [}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{], }\StringTok{"y"}\NormalTok{: [}\FloatTok{0.4}\NormalTok{], }\StringTok{"label"}\NormalTok{: [}\StringTok{"Null"}\NormalTok{]\})).mark\_text(}
\NormalTok{    align}\OperatorTok{=}\StringTok{"left"}\NormalTok{,}
\NormalTok{    baseline}\OperatorTok{=}\StringTok{"middle"}\NormalTok{,}
\NormalTok{    fontSize}\OperatorTok{=}\DecValTok{18}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{).encode(}
\NormalTok{    x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{    y}\OperatorTok{=}\StringTok{"y:Q"}\NormalTok{,}
\NormalTok{    text}\OperatorTok{=}\StringTok{"label:N"}
\NormalTok{)}

\NormalTok{bottom\_row }\OperatorTok{=}\NormalTok{ alt.layer(bottom\_null, bottom\_n1, bottom\_n2, null\_text).properties(}
\NormalTok{    title}\OperatorTok{=}\StringTok{"(b): Distributions of Standardized Differences"}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} FINAL {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{chart }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.vconcat(top\_row, bottom\_row)}
\NormalTok{    .add\_params(n1\_slider, n2\_slider)}
\NormalTok{    .configure\_view(stroke}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\NormalTok{    .configure\_axis(}
\NormalTok{        grid}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        titleFontSize}\OperatorTok{=}\DecValTok{18}\NormalTok{,}
\NormalTok{        labelFontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        titlePadding}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    )}
\NormalTok{    .configure\_title(}
\NormalTok{        fontSize}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{        anchor}\OperatorTok{=}\StringTok{"start"}\NormalTok{,}
\NormalTok{        offset}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    )}
\NormalTok{)}

\NormalTok{chart}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
alt.VConcatChart(...)
\end{verbatim}

Note that in this widget, and in the ones below, we are not really
drawing random samples each time we move the sliders. Your are reading
this on a static website, and I don't have a python kernel to execute
these calculations. I build this widget to give a feel of how the
distributions would look like had we been able to run the code. The
numbers N1 and N2 represent the sizes of the two samples, and we
simulate the statistics of the means of 5000 such samples to build the
distributions.

The two colored curves behave in the exact same way, I just thought it
would be instructive to see how they compare. In the widget below, we
show only one curve, and how it changes as we control either the sample
size or the real difference \(\Delta\) in the population between girls
and boys. I find it really useful to get intuiton about the equation for
\(Z_{H_1}\). See how it responds a lot less in changes of \(n\) because
of the square root.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ altair }\ImportTok{as}\NormalTok{ alt}

\CommentTok{\# {-}{-}{-} Parameters {-}{-}{-}}
\NormalTok{seed }\OperatorTok{=} \DecValTok{2}  \CommentTok{\# change this to any int (or set to None for non{-}reproducible draws)}
\NormalTok{num\_samples\_dist }\OperatorTok{=} \DecValTok{5000}
\NormalTok{bin\_step }\OperatorTok{=} \FloatTok{0.2}

\CommentTok{\# colors}
\NormalTok{c\_alt  }\OperatorTok{=} \StringTok{"\#dbb40c"}
\NormalTok{c\_null }\OperatorTok{=} \StringTok{"\#000000"}

\CommentTok{\# {-}{-}{-} RNG (seedable) {-}{-}{-}}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng(seed) }\ControlFlowTok{if}\NormalTok{ seed }\KeywordTok{is} \KeywordTok{not} \VariableTok{None} \ControlFlowTok{else}\NormalTok{ np.random.default\_rng()}

\CommentTok{\# {-}{-}{-} Independent random draws (fixed once) {-}{-}{-}}
\NormalTok{data\_dist }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"sample\_id"}\NormalTok{: np.arange(num\_samples\_dist)\})}
\NormalTok{data\_dist[}\StringTok{"z0"}\NormalTok{] }\OperatorTok{=}\NormalTok{ rng.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, num\_samples\_dist)  }\CommentTok{\# null}
\NormalTok{data\_dist[}\StringTok{"z1"}\NormalTok{] }\OperatorTok{=}\NormalTok{ rng.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, num\_samples\_dist)  }\CommentTok{\# alt}

\CommentTok{\# {-}{-}{-} Interactive sliders {-}{-}{-}}
\NormalTok{n\_slider }\OperatorTok{=}\NormalTok{ alt.param(}
\NormalTok{    name}\OperatorTok{=}\StringTok{"n"}\NormalTok{, value}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    bind}\OperatorTok{=}\NormalTok{alt.binding\_range(}\BuiltInTok{min}\OperatorTok{=}\DecValTok{5}\NormalTok{, }\BuiltInTok{max}\OperatorTok{=}\DecValTok{400}\NormalTok{, step}\OperatorTok{=}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{"n "}\NormalTok{)}
\NormalTok{)}
\NormalTok{delta\_slider }\OperatorTok{=}\NormalTok{ alt.param(}
\NormalTok{    name}\OperatorTok{=}\StringTok{""}\NormalTok{, value}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,}
\NormalTok{    bind}\OperatorTok{=}\NormalTok{alt.binding\_range(}\BuiltInTok{min}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, }\BuiltInTok{max}\OperatorTok{=}\FloatTok{5.0}\NormalTok{, step}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, name}\OperatorTok{=}\StringTok{" "}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# helper: step histogram from a single computed value column}
\CommentTok{\# if label is not None, it creates a "lbl" field for legend/color}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ step\_hist(calc\_expr, x\_domain, y\_domain, x\_title, color}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        alt.Chart(data\_dist)}
\NormalTok{        .transform\_calculate(value}\OperatorTok{=}\NormalTok{calc\_expr)}
\NormalTok{        .transform\_joinaggregate(total}\OperatorTok{=}\StringTok{"count()"}\NormalTok{)}
\NormalTok{        .transform\_bin(}
\NormalTok{            [}\StringTok{"bin\_start"}\NormalTok{, }\StringTok{"bin\_end"}\NormalTok{],}
\NormalTok{            field}\OperatorTok{=}\StringTok{"value"}\NormalTok{,}
            \BuiltInTok{bin}\OperatorTok{=}\NormalTok{alt.Bin(extent}\OperatorTok{=}\NormalTok{x\_domain, step}\OperatorTok{=}\NormalTok{bin\_step),}
\NormalTok{        )}
\NormalTok{        .transform\_aggregate(count}\OperatorTok{=}\StringTok{"count()"}\NormalTok{, groupby}\OperatorTok{=}\NormalTok{[}\StringTok{"bin\_start"}\NormalTok{, }\StringTok{"bin\_end"}\NormalTok{, }\StringTok{"total"}\NormalTok{])}
\NormalTok{        .transform\_calculate(density}\OperatorTok{=}\SpecialStringTok{f"datum.count / (datum.total * }\SpecialCharTok{\{}\NormalTok{bin\_step}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{)}
\NormalTok{        .mark\_line(interpolate}\OperatorTok{=}\StringTok{"step"}\NormalTok{, strokeWidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{)}
\NormalTok{        .encode(}
\NormalTok{            x}\OperatorTok{=}\NormalTok{alt.X(}\StringTok{"bin\_start:Q"}\NormalTok{, scale}\OperatorTok{=}\NormalTok{alt.Scale(domain}\OperatorTok{=}\NormalTok{x\_domain), title}\OperatorTok{=}\NormalTok{x\_title),}
\NormalTok{            y}\OperatorTok{=}\NormalTok{alt.Y(}\StringTok{"density:Q"}\NormalTok{, stack}\OperatorTok{=}\VariableTok{None}\NormalTok{, scale}\OperatorTok{=}\NormalTok{alt.Scale(domain}\OperatorTok{=}\NormalTok{y\_domain), title}\OperatorTok{=}\StringTok{"Density"}\NormalTok{),}
\NormalTok{            color}\OperatorTok{=}\NormalTok{alt.value(color),}
\NormalTok{        )}
\NormalTok{        .properties(width}\OperatorTok{=}\DecValTok{620}\NormalTok{, height}\OperatorTok{=}\DecValTok{230}\NormalTok{)}
\NormalTok{    )}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} TOP PANEL (single curve: {-}hat under H1) {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# uses delta\_slider () and n\_slider (n)}
\NormalTok{top\_alt }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\SpecialStringTok{f" + datum.z1 * sqrt(2 * }\SpecialCharTok{\{}\NormalTok{sigma}\OperatorTok{**}\DecValTok{2}\SpecialCharTok{\}}\SpecialStringTok{ / n)"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{1.0}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Observed Difference of Means ()"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\NormalTok{c\_alt,}
\NormalTok{)}

\NormalTok{top\_row }\OperatorTok{=}\NormalTok{ top\_alt.properties(title}\OperatorTok{=}\StringTok{"(a): Distribution of Sample Mean Difference (H)"}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} BOTTOM PANEL (null + standardized H1) {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{bottom\_null }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\StringTok{"datum.z0"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Standardized Value (Z)"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\NormalTok{c\_null,}
\NormalTok{)}

\NormalTok{bottom\_alt }\OperatorTok{=}\NormalTok{ step\_hist(}
\NormalTok{    calc\_expr}\OperatorTok{=}\SpecialStringTok{f"datum.z1 + ( / (}\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{\}}\SpecialStringTok{ * sqrt(2 / n)))"}\NormalTok{,}
\NormalTok{    x\_domain}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{],}
\NormalTok{    y\_domain}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{],}
\NormalTok{    x\_title}\OperatorTok{=}\StringTok{"Standardized Value (Z)"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\NormalTok{c\_alt,}
\NormalTok{)}

\NormalTok{null\_text }\OperatorTok{=}\NormalTok{ alt.Chart(pd.DataFrame(\{}\StringTok{"x"}\NormalTok{: [}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{], }\StringTok{"y"}\NormalTok{: [}\FloatTok{0.4}\NormalTok{], }\StringTok{"label"}\NormalTok{: [}\StringTok{"Null"}\NormalTok{]\})).mark\_text(}
\NormalTok{    align}\OperatorTok{=}\StringTok{"left"}\NormalTok{,}
\NormalTok{    baseline}\OperatorTok{=}\StringTok{"middle"}\NormalTok{,}
\NormalTok{    fontSize}\OperatorTok{=}\DecValTok{18}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{).encode(}
\NormalTok{    x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{    y}\OperatorTok{=}\StringTok{"y:Q"}\NormalTok{,}
\NormalTok{    text}\OperatorTok{=}\StringTok{"label:N"}
\NormalTok{)}

\NormalTok{bottom\_row }\OperatorTok{=}\NormalTok{ alt.layer(bottom\_null, bottom\_alt, null\_text).properties(}
\NormalTok{    title}\OperatorTok{=}\StringTok{"(b): Standardized Distributions (H vs H)"}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} FINAL {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{chart }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.vconcat(top\_row, bottom\_row)}
\NormalTok{    .add\_params(n\_slider, delta\_slider)}
\NormalTok{    .configure\_view(stroke}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\NormalTok{    .configure\_axis(}
\NormalTok{        grid}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{        titleFontSize}\OperatorTok{=}\DecValTok{18}\NormalTok{,}
\NormalTok{        labelFontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        titlePadding}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    )}
\NormalTok{    .configure\_title(}
\NormalTok{        fontSize}\OperatorTok{=}\DecValTok{20}\NormalTok{,}
\NormalTok{        anchor}\OperatorTok{=}\StringTok{"start"}\NormalTok{,}
\NormalTok{        offset}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{    )}
\NormalTok{)}

\NormalTok{chart}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
alt.VConcatChart(...)
\end{verbatim}

The statistical significance \(\alpha\) determines where we put the
border between rejecting H0 or not. In this example, \(\alpha=0.05\),
and because we are dealing with a one-sided test, this border is located
at the value of \(Z\) that leaves 0.05 of the area below H0 to its
right. This is the same as leaving \(1-\alpha=0.95\) of the area to its
left, so a common name in the literature is \(z_{1-\alpha}\). The
cumulative distribution function (CDF) is the integral of the
probability density function (PDF) from \(-\infty\) to \(z\), so it
quantifies the area to the \textbf{left} of \(z\). We need the value
\(z\) for which the CDF is equal to \(1-\alpha\). Luckily, we can use
python's \texttt{scipy.stats.norm.ppf} function to get this value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zcrit }\OperatorTok{=}\NormalTok{ norm.ppf(}\FloatTok{0.95}\NormalTok{, loc}\OperatorTok{=}\DecValTok{0}\NormalTok{, scale}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"zcrit (for =0.05) = }\SpecialCharTok{\{}\NormalTok{zcrit}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
zcrit (for =0.05) = 1.6449
\end{verbatim}

The probabilities \(\alpha\) and \(\beta\) are shown as shaded areas in
the widget below. See what happens when you control the slider for \(n\)
and \(\Delta\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ altair }\ImportTok{as}\NormalTok{ alt}

\CommentTok{\# {-}{-}{-} constants {-}{-}{-}}
\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{zcrit }\OperatorTok{=} \FloatTok{1.6448536269514722}  \CommentTok{\# one{-}sided z\_\{1{-}alpha\}}
\NormalTok{sigma }\OperatorTok{=} \FloatTok{7.39}

\CommentTok{\# x{-}grid}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{4.0}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\DecValTok{1601}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"x"}\NormalTok{: x\})}

\CommentTok{\# sliders}
\NormalTok{n\_slider }\OperatorTok{=}\NormalTok{ alt.param(}
\NormalTok{    name}\OperatorTok{=}\StringTok{"n"}\NormalTok{, value}\OperatorTok{=}\DecValTok{50}\NormalTok{,}
\NormalTok{    bind}\OperatorTok{=}\NormalTok{alt.binding\_range(}\BuiltInTok{min}\OperatorTok{=}\DecValTok{5}\NormalTok{, }\BuiltInTok{max}\OperatorTok{=}\DecValTok{400}\NormalTok{, step}\OperatorTok{=}\DecValTok{1}\NormalTok{, name}\OperatorTok{=}\StringTok{"n "}\NormalTok{)}
\NormalTok{)}
\NormalTok{delta\_slider }\OperatorTok{=}\NormalTok{ alt.param(}
\NormalTok{    name}\OperatorTok{=}\StringTok{"Delta"}\NormalTok{, value}\OperatorTok{=}\FloatTok{2.0}\NormalTok{,}
\NormalTok{    bind}\OperatorTok{=}\NormalTok{alt.binding\_range(}\BuiltInTok{min}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, }\BuiltInTok{max}\OperatorTok{=}\FloatTok{5.0}\NormalTok{, step}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, name}\OperatorTok{=}\StringTok{" "}\NormalTok{)}
\NormalTok{)}

\NormalTok{base }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.Chart(df)}
\NormalTok{    .add\_params(n\_slider, delta\_slider)}
\NormalTok{    .transform\_calculate(}
\NormalTok{        mu1}\OperatorTok{=}\SpecialStringTok{f"(Delta / }\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{\}}\SpecialStringTok{) * sqrt(n / 2)"}
\NormalTok{    )}
\NormalTok{    .transform\_calculate(}
\NormalTok{        pdf0}\OperatorTok{=}\StringTok{"(1 / sqrt(2 * PI)) * exp({-}0.5 * datum.x * datum.x)"}\NormalTok{,}
\NormalTok{        pdf1}\OperatorTok{=}\StringTok{"(1 / sqrt(2 * PI)) * exp({-}0.5 * pow(datum.x {-} datum.mu1, 2))"}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} curves {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{h0\_line }\OperatorTok{=}\NormalTok{ base.mark\_line(strokeWidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{, color}\OperatorTok{=}\StringTok{"\#000000"}\NormalTok{).encode(}
\NormalTok{    x}\OperatorTok{=}\NormalTok{alt.X(}\StringTok{"x:Q"}\NormalTok{, title}\OperatorTok{=}\StringTok{"z"}\NormalTok{),}
\NormalTok{    y}\OperatorTok{=}\NormalTok{alt.Y(}\StringTok{"pdf0:Q"}\NormalTok{, title}\OperatorTok{=}\StringTok{"Density"}\NormalTok{),}
\NormalTok{)}

\NormalTok{h1\_line }\OperatorTok{=}\NormalTok{ base.mark\_line(strokeWidth}\OperatorTok{=}\FloatTok{2.5}\NormalTok{, color}\OperatorTok{=}\StringTok{"\#dbb40c"}\NormalTok{).encode(}
\NormalTok{    x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{    y}\OperatorTok{=}\StringTok{"pdf1:Q"}\NormalTok{,}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} shaded regions {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# alpha: H0 right tail}
\NormalTok{h0\_shade }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    base}
\NormalTok{    .transform\_filter(}\SpecialStringTok{f"datum.x \textgreater{}= }\SpecialCharTok{\{}\NormalTok{zcrit}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    .mark\_area(opacity}\OperatorTok{=}\FloatTok{0.20}\NormalTok{, color}\OperatorTok{=}\StringTok{"\#808080"}\NormalTok{)}
\NormalTok{    .encode(}
\NormalTok{        x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{        y}\OperatorTok{=}\StringTok{"pdf0:Q"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# beta: H1 left of cutoff}
\NormalTok{h1\_shade }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    base}
\NormalTok{    .transform\_filter(}\SpecialStringTok{f"datum.x \textless{}= }\SpecialCharTok{\{}\NormalTok{zcrit}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    .mark\_area(opacity}\OperatorTok{=}\FloatTok{0.20}\NormalTok{, color}\OperatorTok{=}\StringTok{"\#dbb40c"}\NormalTok{)}
\NormalTok{    .encode(}
\NormalTok{        x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{        y}\OperatorTok{=}\StringTok{"pdf1:Q"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} decision boundary {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{crit\_line }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.Chart(pd.DataFrame(\{}\StringTok{"z"}\NormalTok{: [zcrit]\}))}
\NormalTok{    .mark\_rule(strokeWidth}\OperatorTok{=}\DecValTok{2}\NormalTok{, strokeDash}\OperatorTok{=}\NormalTok{[}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{], color}\OperatorTok{=}\StringTok{"\#808080"}\NormalTok{)}
\NormalTok{    .encode(x}\OperatorTok{=}\StringTok{"z:Q"}\NormalTok{)}
\NormalTok{)}

\NormalTok{crit\_label }\OperatorTok{=}\NormalTok{ alt.Chart(}
\NormalTok{    pd.DataFrame(\{}\StringTok{"x"}\NormalTok{: [zcrit], }\StringTok{"y"}\NormalTok{: [}\FloatTok{0.39}\NormalTok{], }\StringTok{"t"}\NormalTok{: [}\SpecialStringTok{f"z critical (=}\SpecialCharTok{\{}\NormalTok{alpha}\SpecialCharTok{\}}\SpecialStringTok{)"}\NormalTok{]\})}
\NormalTok{).mark\_text(}
\NormalTok{    align}\OperatorTok{=}\StringTok{"left"}\NormalTok{, baseline}\OperatorTok{=}\StringTok{"top"}\NormalTok{, dx}\OperatorTok{=}\DecValTok{6}\NormalTok{, fontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{, color}\OperatorTok{=}\StringTok{"\#808080"}
\NormalTok{).encode(}
\NormalTok{    x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{    y}\OperatorTok{=}\StringTok{"y:Q"}\NormalTok{,}
\NormalTok{    text}\OperatorTok{=}\StringTok{"t:N"}
\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} power label {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{power\_text\_1 }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.Chart(pd.DataFrame(\{}\StringTok{"x"}\NormalTok{: [}\FloatTok{5.0}\NormalTok{], }\StringTok{"y"}\NormalTok{: [}\FloatTok{0.38}\NormalTok{]\}))}
\NormalTok{    .mark\_text(}
\NormalTok{        align}\OperatorTok{=}\StringTok{"left"}\NormalTok{,}
\NormalTok{        baseline}\OperatorTok{=}\StringTok{"middle"}\NormalTok{,}
\NormalTok{        fontSize}\OperatorTok{=}\DecValTok{16}\NormalTok{,}
\NormalTok{        color}\OperatorTok{=}\StringTok{"\#dbb40c"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{    .encode(}
\NormalTok{        x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{        y}\OperatorTok{=}\StringTok{"y:Q"}\NormalTok{,}
\NormalTok{        text}\OperatorTok{=}\NormalTok{alt.value(}\StringTok{"Statistical Power"}\NormalTok{),}
\NormalTok{    )}
\NormalTok{)}

\NormalTok{power\_text\_2 }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.Chart(pd.DataFrame(\{}\StringTok{"x"}\NormalTok{: [}\FloatTok{5.0}\NormalTok{], }\StringTok{"y"}\NormalTok{: [}\FloatTok{0.35}\NormalTok{]\}))}
\NormalTok{    .transform\_calculate(}
\NormalTok{        t}\OperatorTok{=}\SpecialStringTok{f"1.702 * (}\SpecialCharTok{\{}\NormalTok{zcrit}\SpecialCharTok{\}}\SpecialStringTok{ {-} ((Delta / }\SpecialCharTok{\{}\NormalTok{sigma}\SpecialCharTok{\}}\SpecialStringTok{) * sqrt(n / 2)))"}\NormalTok{,}
\NormalTok{        t\_clamped}\OperatorTok{=}\StringTok{"min(max(datum.t, {-}60), 60)"}\NormalTok{,}
\NormalTok{        power}\OperatorTok{=}\StringTok{"1 / (1 + exp(datum.t\_clamped))"}\NormalTok{,}
\NormalTok{        label}\OperatorTok{=}\StringTok{"\textquotesingle{}1   = \textquotesingle{} + format(datum.power, \textquotesingle{}.3f\textquotesingle{})"}
\NormalTok{    )}
\NormalTok{    .mark\_text(}
\NormalTok{        align}\OperatorTok{=}\StringTok{"left"}\NormalTok{,}
\NormalTok{        baseline}\OperatorTok{=}\StringTok{"middle"}\NormalTok{,}
\NormalTok{        fontSize}\OperatorTok{=}\DecValTok{16}\NormalTok{,}
\NormalTok{        color}\OperatorTok{=}\StringTok{"\#dbb40c"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{    .encode(}
\NormalTok{        x}\OperatorTok{=}\StringTok{"x:Q"}\NormalTok{,}
\NormalTok{        y}\OperatorTok{=}\StringTok{"y:Q"}\NormalTok{,}
\NormalTok{        text}\OperatorTok{=}\StringTok{"label:N"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{)}


\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} legend for shaded areas {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{legend\_df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{"region"}\NormalTok{: [}\StringTok{" (False Alarms)"}\NormalTok{, }\StringTok{" (Misses)"}\NormalTok{]}
\NormalTok{\})}

\NormalTok{shade\_legend }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.Chart(legend\_df)}
\NormalTok{    .mark\_point(size}\OperatorTok{=}\DecValTok{120}\NormalTok{, filled}\OperatorTok{=}\VariableTok{True}\NormalTok{, opacity}\OperatorTok{=}\DecValTok{0}\NormalTok{)  }\CommentTok{\# hide the dummy points}
\NormalTok{    .encode(}
\NormalTok{        color}\OperatorTok{=}\NormalTok{alt.Color(}
            \StringTok{"region:N"}\NormalTok{,}
\NormalTok{            scale}\OperatorTok{=}\NormalTok{alt.Scale(}
\NormalTok{                domain}\OperatorTok{=}\NormalTok{[}\StringTok{" (False Alarms)"}\NormalTok{, }\StringTok{" (Misses)"}\NormalTok{],}
                \BuiltInTok{range}\OperatorTok{=}\NormalTok{[}\StringTok{"\#808080"}\NormalTok{, }\StringTok{"\#dbb40c"}\NormalTok{],}
\NormalTok{            ),}
\NormalTok{            legend}\OperatorTok{=}\NormalTok{alt.Legend(}
\NormalTok{                title}\OperatorTok{=}\StringTok{"Shaded regions"}\NormalTok{,}
\NormalTok{                orient}\OperatorTok{=}\StringTok{"top{-}left"}\NormalTok{,}
\NormalTok{                labelFontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                titleFontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{                symbolSize}\OperatorTok{=}\DecValTok{200}\NormalTok{,}
\NormalTok{                symbolOpacity}\OperatorTok{=}\DecValTok{1}\NormalTok{,          }\CommentTok{\# keep legend symbols visible}
\NormalTok{            ),}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{)}


\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} final chart {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{chart }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    alt.layer(}
\NormalTok{        h0\_shade,}
\NormalTok{        h1\_shade,}
\NormalTok{        h0\_line,}
\NormalTok{        h1\_line,}
\NormalTok{        crit\_line,}
\NormalTok{        crit\_label,}
\NormalTok{        power\_text\_1,}
\NormalTok{        power\_text\_2,}
\NormalTok{        shade\_legend,}
\NormalTok{    )}
\NormalTok{    .properties(width}\OperatorTok{=}\DecValTok{600}\NormalTok{, height}\OperatorTok{=}\DecValTok{360}\NormalTok{, title}\OperatorTok{=}\StringTok{"H0 vs H1 in Z{-}space ( = 0.05)"}\NormalTok{)}
\NormalTok{    .configure\_view(stroke}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\NormalTok{    .configure\_axis(titleFontSize}\OperatorTok{=}\DecValTok{18}\NormalTok{, labelFontSize}\OperatorTok{=}\DecValTok{14}\NormalTok{, grid}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    .configure\_title(fontSize}\OperatorTok{=}\DecValTok{20}\NormalTok{, anchor}\OperatorTok{=}\StringTok{"start"}\NormalTok{, offset}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{)}

\NormalTok{chart}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
alt.LayerChart(...)
\end{verbatim}

So how can we compute the statistical power? If \(\beta\) is the shaded
area in yellow in the widget above, then the remaining area to the right
is the statistical power, \(1-\beta\). Remember that we already figured
out how \(H_1\) is distributed:

\[
Z_{H_1} \sim \mathcal{N}\left(\frac{\Delta}{\sigma}\sqrt{\frac{n}{2}}, 1\right).
\]

Therefore, the statistical power is given by

\[
\text{Power} = 1 - \beta = \int_{z_{1-\alpha}}^{\infty} f_{Z_{H_1}}(z) \, dz.
\]

We can combine the information in the two previous equations to get an
explicit expression for the statistical power: \[
\text{Power} = 1 - \beta = \int_{z_{1-\alpha}}^{\infty} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(z - d\sqrt{n/2})^2}{2}\right) \, dz.
\]

We started this chapter by saying that power analysis is a puzzle made
of four pieces: statistical significance, \(\alpha\); statistical power,
\(1-\beta\); effect size, \(d\); and sample size, \(n\). By fixing any
three of these quantities, we can solve for the fourth. When deriving
these equations, we made several assumptions. A big one was that the
sample size is large enough for the Central Limit Theorem to hold, and
because of that we could use the normal distribution. A more rigorous
treatment, valid for smaller sample sizes, would use the t-distribution
instead of the normal distribution, but the fundamental logic would be
the same. We will use python's \texttt{statsmodels} package to perform
power analysis and solve two common questions that arise when desigining
an experiment.

\section{in action}\label{in-action}

\subsection{how many samples do I
need?}\label{how-many-samples-do-i-need}

How many boys and girls do I need to sample to have an 80\% chance of
detecting a real difference in mean height? We assume that:

\begin{itemize}
\tightlist
\item
  The expected difference in mean height is not so large,
  \(\Delta=\mu_{\text{girls}} - \mu_{\text{boys}}=2.05\) cm.
\item
  The standard deviation in both populations is quite similar, about
  \(\sigma=7.39\) cm.
\item
  We want to use a significance level of \(\alpha=0.05\).
\item
  We want to have a statistical power of \(1-\beta=0.8\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.stats.power }\ImportTok{as}\NormalTok{ smp}
\NormalTok{power\_analysis }\OperatorTok{=}\NormalTok{ smp.TTestIndPower()}
\NormalTok{cohens\_d }\OperatorTok{=}\NormalTok{ Delta }\OperatorTok{/}\NormalTok{ sigma}
\NormalTok{sample\_size }\OperatorTok{=}\NormalTok{ power\_analysis.solve\_power(effect\_size}\OperatorTok{=}\NormalTok{cohens\_d, power}\OperatorTok{=}\FloatTok{0.8}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"sample size: }\SpecialCharTok{\{}\NormalTok{sample\_size}\SpecialCharTok{:.0f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
sample size: 205
\end{verbatim}

We can easily plot power curves for various effect sizes using the
\texttt{plot\_power} method of
\texttt{statsmodels.stats.power.TTestIndPower}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{normalized\_effect\_sizes }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.8}\NormalTok{])}
\NormalTok{sample\_sizes }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{10}\NormalTok{))}

\CommentTok{\# plt.style.use(\textquotesingle{}seaborn\textquotesingle{})}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ power\_analysis.plot\_power(}
\NormalTok{    dep\_var}\OperatorTok{=}\StringTok{\textquotesingle{}nobs\textquotesingle{}}\NormalTok{, nobs}\OperatorTok{=}\NormalTok{sample\_sizes,  }
\NormalTok{    effect\_size}\OperatorTok{=}\NormalTok{normalized\_effect\_sizes, alpha}\OperatorTok{=}\FloatTok{0.05}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax, }
\NormalTok{    title}\OperatorTok{=}\StringTok{\textquotesingle{}Power of Independent Samples t{-}test}\CharTok{\textbackslash{}n}\StringTok{$}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{alpha = 0.05$\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.axhline(}\FloatTok{0.8}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}80\% Power\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend([}\StringTok{\textquotesingle{}d = 0.2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}d = 0.5\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}d = 0.8\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}80\% Power\textquotesingle{}}\NormalTok{])}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Power\textquotesingle{}}\NormalTok{, xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Sample Size per Group\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/statistical_power_files/figure-pdf/cell-9-output-1.png}}

\subsection{what is the minimum effect size I can
detect?}\label{what-is-the-minimum-effect-size-i-can-detect}

Assume that I have limited resources, and I can only sample 50 boys and
50 girls. From the result above, I could not detect a difference of 2.05
cm with high probability. What difference in mean height can I hope to
detect with high probability? We assume that:

\begin{itemize}
\tightlist
\item
  The standard deviation in both populations is quite similar, about
  \(\sigma=7.39\) cm.
\item
  We want to use a significance level of \(\alpha=0.05\).
\item
  We want to have a statistical power of \(1-\beta=0.8\).
\item
  The sample size is fixed to \(n=50\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.stats.power }\ImportTok{as}\NormalTok{ smp}
\NormalTok{power\_analysis }\OperatorTok{=}\NormalTok{ smp.TTestIndPower()}
\NormalTok{sample\_size }\OperatorTok{=} \DecValTok{50}
\NormalTok{effect\_size }\OperatorTok{=}\NormalTok{ power\_analysis.solve\_power(nobs1}\OperatorTok{=}\NormalTok{sample\_size, power}\OperatorTok{=}\FloatTok{0.8}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"minimum detectable effect size: Cohen\textquotesingle{}s d = }\SpecialCharTok{\{}\NormalTok{effect\_size}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"minimum detectable effect size: Delta = }\SpecialCharTok{\{}\NormalTok{effect\_size }\OperatorTok{*}\NormalTok{ sigma}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
minimum detectable effect size: Cohen's d = 0.57
minimum detectable effect size: Delta = 4.18 cm
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#+}
\NormalTok{effect\_sizes }\OperatorTok{=}\NormalTok{ np.arange(}\FloatTok{0.01}\NormalTok{, }\FloatTok{1.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{sample\_sizes }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{20}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{250}\NormalTok{])}

\CommentTok{\# plt.style.use(\textquotesingle{}seaborn\textquotesingle{})}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ power\_analysis.plot\_power(}
\NormalTok{    dep\_var}\OperatorTok{=}\StringTok{\textquotesingle{}effect\_size\textquotesingle{}}\NormalTok{, nobs}\OperatorTok{=}\NormalTok{sample\_sizes,  }
\NormalTok{    effect\_size}\OperatorTok{=}\NormalTok{effect\_sizes, alpha}\OperatorTok{=}\FloatTok{0.05}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax, }
\NormalTok{    title}\OperatorTok{=}\StringTok{\textquotesingle{}Power of Independent Samples t{-}test}\CharTok{\textbackslash{}n}\StringTok{$}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{alpha = 0.05$\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.axhline(}\FloatTok{0.8}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}80\% Power\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend([}\StringTok{\textquotesingle{}n = 20\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}n = 50\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}n = 250\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}80\% Power\textquotesingle{}}\NormalTok{])}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Power\textquotesingle{}}\NormalTok{, xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Effect Size (d)\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/statistical_power_files/figure-pdf/cell-11-output-1.png}}

\chapter{the problem with t-test}\label{the-problem-with-t-test}

Let's go back to the example of the
\href{./t_test/t_test_independent_samples.html}{independent samples
t-test}.

We sampled 10 boys and 14 girls, age 12, and asked:

\begin{quote}
Are 12-year old girls significantly taller than 12-year old boys?
\end{quote}

We then went about answering this question by talking about the means of
each sample, and if the differences between the means were large enough
to be considered significant.

The whole machinery behind the t-test is based on the normality
assumption.

\section{the normality assumption}\label{the-normality-assumption}

Two possible interpretations come to mind.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The assumption is that the height of men and women in the
  \emph{population} is normally distributed. From these idealized
  populations we draw samples.
\item
  The t-test effectively compares the difference between the means of
  the two samples, and the variability within each sample. Because of
  the Central Limit Theorem, the means of the samples will approach a
  normal distribution as the sample size increases. In this
  interpretation, the normality assumption is about the distribution of
  the means of the samples, and not the distribution of the population.
\end{enumerate}

In the context of the t-test, the above is a distinction without a
difference. Even if the population is not normally distributed, the
means of the samples will be normally distributed as long as the sample
size is large enough. We then use the t-test and go on with our lives.

\section{other statistical tests}\label{other-statistical-tests}

The Central Limit Theorem dictates that the means will be normally
distributed, but it does not apply to other statistics, such as:

\begin{itemize}
\tightlist
\item
  the median
\item
  the variance
\item
  the skewness
\item
  the maximum
\item
  the Interquartile Range (IQR)
\item
  etc.
\end{itemize}

In this case, the t-test can't be relied upon, and we need another
solution.

\chapter{permutation test}\label{permutation-test}

We wish to compare two samples, and see if they significantly differ
regarding some statistic of interest (median, mean, etc.). To make
things concrete, let's talk about the heights of 12-year-old boys and
girls. Are girls significantly taller than boys?

\section{hypotheses}\label{hypotheses-3}

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis (H0):} The two samples come from the same
  distribution.
\item
  \textbf{Alternative hypothesis (H1):} Girls are taller than boys.
\end{itemize}

The basic idea behind the permutation test is that, \emph{if the null
hypothesis is correct}, then it wouldn't matter if we relabelled the
samples. If we randomly permute the labels ``girls'' and ``boys'' of the
two samples, the statistic of interest should not change significantly.
However, if by permuting the labels we get a significantly different
statistic, then we can reject the null hypothesis.

That's beautiful, right?

\section{steps}\label{steps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the statistic of interest (e.g., the difference in medians)
  for the original samples.
\item
  Randomly permute the labels of the two samples.
\item
  Compute the statistic of interest for the permuted samples.
\item
  Repeat steps 2 and 3 many times (e.g., 1000 times) to create a
  distribution of the statistic under the null hypothesis.
\item
  Compare the original statistic to the distribution of permuted
  statistics to see if it is significantly different (e.g., by checking
  if it falls in the top 5\% of the distribution). From this, we can
  numerically compute a p-value.
\end{enumerate}

\section{example}\label{example-1}

Let's use the very same example as in the
\href{./t_test/t_test_independent_samples.html}{independent samples
t-test}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{12.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{mu\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{10}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{14}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{120}\NormalTok{, }\DecValTok{180}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{pdf\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.plot(height\_list, pdf\_boys, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}boys population\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(height\_list, pdf\_girls, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}girls population\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.eventplot(sample\_boys, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.03}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}boys sample, n=}\SpecialCharTok{\{}\NormalTok{N\_boys}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.eventplot(sample\_girls, orientation}\OperatorTok{=}\StringTok{"horizontal"}\NormalTok{, lineoffsets}\OperatorTok{=}\FloatTok{0.023}\NormalTok{,}
\NormalTok{             linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{, linelengths}\OperatorTok{=} \FloatTok{0.005}\NormalTok{,}
\NormalTok{             colors}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}girls sample, n=}\SpecialCharTok{\{}\NormalTok{N\_girls}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}probability density\textquotesingle{}}\NormalTok{,}
\NormalTok{    )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/permutation_files/figure-pdf/cell-5-output-1.png}}

The statistic of interest now is the difference in medians between the
two samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define the desired statistic.}
\CommentTok{\# in can be anything you want, you can even write your own function.}
\NormalTok{statistic }\OperatorTok{=}\NormalTok{ np.median}
\CommentTok{\# compute the median for each sample and the difference}
\NormalTok{median\_girls }\OperatorTok{=}\NormalTok{ statistic(sample\_girls)}
\NormalTok{median\_boys }\OperatorTok{=}\NormalTok{ statistic(sample\_boys)}
\NormalTok{observed\_diff }\OperatorTok{=}\NormalTok{ median\_girls }\OperatorTok{{-}}\NormalTok{ median\_boys}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median height for girls: }\SpecialCharTok{\{}\NormalTok{median\_girls}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median height for boys: }\SpecialCharTok{\{}\NormalTok{median\_boys}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"median difference (girls minus boys): }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
median height for girls: 150.88 cm
median height for boys: 151.19 cm
median difference (girls minus boys): -0.31 cm
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_permutations }\OperatorTok{=} \DecValTok{1000}
\CommentTok{\# combine all values in one array}
\NormalTok{all\_data }\OperatorTok{=}\NormalTok{ np.concatenate([sample\_girls, sample\_boys])}
\CommentTok{\# create an array to store the differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.empty(N\_permutations}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):    }\CommentTok{\# this "minus 1" will be explained later}
    \CommentTok{\# permute the labels}
\NormalTok{    permuted }\OperatorTok{=}\NormalTok{ np.random.permutation(all\_data)}
\NormalTok{    new\_girls }\OperatorTok{=}\NormalTok{ permuted[:N\_girls]  }\CommentTok{\# first 14 values are girls}
\NormalTok{    new\_boys }\OperatorTok{=}\NormalTok{ permuted[N\_girls:]   }\CommentTok{\# remaining values are boys}
\NormalTok{    diffs[i] }\OperatorTok{=}\NormalTok{ statistic(new\_girls) }\OperatorTok{{-}}\NormalTok{ statistic(new\_boys)}
\CommentTok{\# add the observed difference to the array of differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.append(diffs, observed\_diff)}
\end{Highlighting}
\end{Shaded}

Now let's see the empirical cdf of the permuted statistics, and where
the original statistic falls in that distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\CommentTok{\# compute the empirical CDF}
\NormalTok{rank }\OperatorTok{=}\NormalTok{ np.arange(}\BuiltInTok{len}\NormalTok{(diffs)) }\OperatorTok{+} \DecValTok{1}
\NormalTok{cdf }\OperatorTok{=}\NormalTok{ rank }\OperatorTok{/}\NormalTok{ (}\BuiltInTok{len}\NormalTok{(diffs) }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{sorted\_diffs }\OperatorTok{=}\NormalTok{ np.sort(diffs)}

\NormalTok{ax.plot(sorted\_diffs, cdf, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}empirical CDF\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.axvline(observed\_diff, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{,}
\NormalTok{           label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}obs. median diff. = }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(observed\_diff }\OperatorTok{+} \FloatTok{0.5}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}observed difference in}\CharTok{\textbackslash{}n}\SpecialStringTok{median height: }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm\textquotesingle{}}\NormalTok{,}
\NormalTok{        color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{)}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}
\CommentTok{\# for a one{-}tailed test}
\NormalTok{ax.axhline(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha, color}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.annotate(}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{1{-}}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\FloatTok{1.01}\NormalTok{, }\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha), xycoords}\OperatorTok{=}\NormalTok{(}\StringTok{\textquotesingle{}axes fraction\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"left"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}
\CommentTok{\# for a two{-}tailed test}
\CommentTok{\# ax.axhline(alpha/2, color=\textquotesingle{}k\textquotesingle{}, lw=1, ls=\textquotesingle{}{-}{-}\textquotesingle{})}
\CommentTok{\# ax.axhline(1{-}alpha/2, color=\textquotesingle{}k\textquotesingle{}, lw=1, ls=\textquotesingle{}{-}{-}\textquotesingle{})}
\CommentTok{\# ax.annotate(r"$\textbackslash{}alpha/2$", xy=(1.01, alpha/2), xycoords=(\textquotesingle{}axes fraction\textquotesingle{}, \textquotesingle{}data\textquotesingle{}),}
\CommentTok{\#             ha="left", va="center")}
\CommentTok{\# ax.annotate(r"$1{-}\textbackslash{}alpha/2$", xy=(1.01, 1{-}alpha/2), xycoords=(\textquotesingle{}axes fraction\textquotesingle{}, \textquotesingle{}data\textquotesingle{}),}
\CommentTok{\#             ha="left", va="center")}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}difference in median height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{         ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}empirical CDF\textquotesingle{}}\NormalTok{,}
\NormalTok{         title}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}empirical distribution of differences in median height\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/permutation_files/figure-pdf/cell-8-output-1.png}}

The observed statistic is well within the boundaries set by the
significance level of 5\%. Therefore, we cannot reject the null
hypothesis. We conclude that, based on this data, the most probable
interpretation is that girls and boys have the same underlying
distribution.

\section{increase sample size}\label{increase-sample-size}

Let's increase the sample size to 200 girls and 300 boys.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# take samples}
\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{300}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{200}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}

\CommentTok{\# compute the observed difference in medians}
\NormalTok{statistic }\OperatorTok{=}\NormalTok{ np.median}
\CommentTok{\# compute the median for each sample and the difference}
\NormalTok{median\_girls }\OperatorTok{=}\NormalTok{ statistic(sample\_girls)}
\NormalTok{median\_boys }\OperatorTok{=}\NormalTok{ statistic(sample\_boys)}
\NormalTok{observed\_diff }\OperatorTok{=}\NormalTok{ median\_girls }\OperatorTok{{-}}\NormalTok{ median\_boys}

\CommentTok{\# permutation algorithm}
\NormalTok{N\_permutations }\OperatorTok{=} \DecValTok{1000}
\CommentTok{\# combine all values in one array}
\NormalTok{all\_data }\OperatorTok{=}\NormalTok{ np.concatenate([sample\_girls, sample\_boys])}
\CommentTok{\# create an array to store the differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.empty(N\_permutations}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):    }\CommentTok{\# this "minus 1" will be explained later}
    \CommentTok{\# permute the labels}
\NormalTok{    permuted }\OperatorTok{=}\NormalTok{ np.random.permutation(all\_data)}
\NormalTok{    new\_girls }\OperatorTok{=}\NormalTok{ permuted[:N\_girls]  }\CommentTok{\# first 200 values are girls}
\NormalTok{    new\_boys }\OperatorTok{=}\NormalTok{ permuted[N\_girls:]   }\CommentTok{\# remaining values are boys}
\NormalTok{    diffs[i] }\OperatorTok{=}\NormalTok{ statistic(new\_girls) }\OperatorTok{{-}}\NormalTok{ statistic(new\_boys)}
\CommentTok{\# add the observed difference to the array of differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.append(diffs, observed\_diff)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\CommentTok{\# compute the empirical CDF}
\NormalTok{rank }\OperatorTok{=}\NormalTok{ np.arange(}\BuiltInTok{len}\NormalTok{(diffs)) }\OperatorTok{+} \DecValTok{1}
\NormalTok{cdf }\OperatorTok{=}\NormalTok{ rank }\OperatorTok{/}\NormalTok{ (}\BuiltInTok{len}\NormalTok{(diffs) }\OperatorTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{sorted\_diffs }\OperatorTok{=}\NormalTok{ np.sort(diffs)}

\NormalTok{ax.plot(sorted\_diffs, cdf, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}empirical CDF\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.axvline(observed\_diff, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{,}
\NormalTok{           label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}obs. median diff. = }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(observed\_diff }\OperatorTok{{-}} \FloatTok{0.05}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}observed difference in}\CharTok{\textbackslash{}n}\SpecialStringTok{median height: }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm\textquotesingle{}}\NormalTok{,}
\NormalTok{        color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{)}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}
\CommentTok{\# for a one{-}tailed test}
\NormalTok{ax.axhline(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha, color}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.annotate(}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{1{-}}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\FloatTok{1.01}\NormalTok{, }\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha), xycoords}\OperatorTok{=}\NormalTok{(}\StringTok{\textquotesingle{}axes fraction\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"left"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}
\CommentTok{\# for a two{-}tailed test}
\CommentTok{\# ax.axhline(alpha/2, color=\textquotesingle{}k\textquotesingle{}, lw=1, ls=\textquotesingle{}{-}{-}\textquotesingle{})}
\CommentTok{\# ax.axhline(1{-}alpha/2, color=\textquotesingle{}k\textquotesingle{}, lw=1, ls=\textquotesingle{}{-}{-}\textquotesingle{})}
\CommentTok{\# ax.annotate(r"$\textbackslash{}alpha/2$", xy=(1.01, alpha/2), xycoords=(\textquotesingle{}axes fraction\textquotesingle{}, \textquotesingle{}data\textquotesingle{}),}
\CommentTok{\#             ha="left", va="center")}
\CommentTok{\# ax.annotate(r"$1{-}\textbackslash{}alpha/2$", xy=(1.01, 1{-}alpha/2), xycoords=(\textquotesingle{}axes fraction\textquotesingle{}, \textquotesingle{}data\textquotesingle{}),}
\CommentTok{\#             ha="left", va="center")}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}difference in median height (cm)\textquotesingle{}}\NormalTok{,}
\NormalTok{         ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}empirical CDF\textquotesingle{}}\NormalTok{,}
\NormalTok{         title}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}empirical distribution of differences in median height\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{hypothesis_testing/permutation_files/figure-pdf/cell-10-output-1.png}}

Now the observed statistic is well outside the right boundary set by the
significance level of 5\%. Therefore, we can reject the null hypothesis.
We conclude that, based on this data, girls are significantly taller
than boys.

\section{p-value}\label{p-value}

It is quite easy to compute the p-value from the permutation test. It is
simply the fraction of permuted statistics that are more extreme than
the observed statistic. In this case, since we are testing whether girls
are taller than boys, we have a one-tailed test, and we only consider
the right tail of the distribution. If we were testing whether girls are
significantly different from boys in their height, we would have a
two-tailed test, and we would consider both tails of the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# one{-}tailed p{-}value}
\NormalTok{p\_value }\OperatorTok{=}\NormalTok{ np.mean(diffs }\OperatorTok{\textgreater{}=}\NormalTok{ observed\_diff)}
\CommentTok{\# two{-}tailed p{-}value}
\CommentTok{\# p\_value = np.mean(np.abs(diffs) \textgreater{}= np.abs(observed\_diff))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"observed difference: }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p{-}value (one{-}tailed): }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
observed difference: 2.004
p-value (one-tailed): 0.0050
\end{verbatim}

We can now address the fact that we ran only 999 permutations, although
I intended to run 1000. See in the code that after the permutation
algorithm, I inserted the original statistic in the list of permuted
statistics. This is because I want to compute the p-value as the
fraction of permuted statistics that are more extreme than the original
statistic, and I want to include the original statistic in the
distribution. If I had not done this, for a truly extreme observed
statistic, we would get that the p-value equals 0, that is, the fraction
of permuted statistics that are more extreme than the observed statistic
is zero. To avoid this behavior, we include the original statistic in
the distribution of permuted statistics.

A corollary of this is that the smallest p-value we can get is 0.001
(for our example with 1000 permutations).

\chapter{numpy vs pandas}\label{numpy-vs-pandas}

\section{numpy}\label{numpy}

In the previous chapter, we computed the permutation test using
\texttt{numpy}. We had two samples of different sizes, and before the
permutation test we concatenated the two samples into one array. Then we
shuffled the concatenated array and split it back into two samples,
according to the original sizes. See a sketch of the code below:

Store the two samples in numpy arrays:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boys }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{121}\NormalTok{, }\DecValTok{123}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{125}\NormalTok{])}
\NormalTok{girls }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{120}\NormalTok{, }\DecValTok{121}\NormalTok{, }\DecValTok{121}\NormalTok{, }\DecValTok{122}\NormalTok{, }\DecValTok{123}\NormalTok{, }\DecValTok{123}\NormalTok{, }\DecValTok{128}\NormalTok{, }\DecValTok{129}\NormalTok{])}
\NormalTok{N\_boys }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(boys)}
\NormalTok{N\_girls }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(girls)}
\end{Highlighting}
\end{Shaded}

Define the statistic and compute the observed difference:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{statistic }\OperatorTok{=}\NormalTok{ np.median}
\CommentTok{\# compute the median for each sample and the difference}
\NormalTok{median\_girls }\OperatorTok{=}\NormalTok{ statistic(sample\_girls)}
\NormalTok{median\_boys }\OperatorTok{=}\NormalTok{ statistic(sample\_boys)}
\NormalTok{observed\_diff }\OperatorTok{=}\NormalTok{ median\_girls }\OperatorTok{{-}}\NormalTok{ median\_boys}
\end{Highlighting}
\end{Shaded}

Run the permutation test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_permutations }\OperatorTok{=} \DecValTok{1000}
\CommentTok{\# combine all values in one array}
\NormalTok{all\_data }\OperatorTok{=}\NormalTok{ np.concatenate([sample\_girls, sample\_boys])}
\CommentTok{\# create an array to store the differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.empty(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):    }\CommentTok{\# this "minus 1" will be explained later}
    \CommentTok{\# permute the labels}
\NormalTok{    permuted }\OperatorTok{=}\NormalTok{ np.random.permutation(all\_data)}
\NormalTok{    new\_girls }\OperatorTok{=}\NormalTok{ permuted[:N\_girls]  }\CommentTok{\# first N\_girls values are girls}
\NormalTok{    new\_boys }\OperatorTok{=}\NormalTok{ permuted[N\_girls:]   }\CommentTok{\# remaining values are boys}
\NormalTok{    diffs[i] }\OperatorTok{=}\NormalTok{ statistic(new\_girls) }\OperatorTok{{-}}\NormalTok{ statistic(new\_boys)}
\CommentTok{\# add the observed difference to the array of differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.append(diffs, observed\_diff)}
\end{Highlighting}
\end{Shaded}

All this works great if this is how your data looks like. Sometimes,
however, you have structured data with more information, such as a
DataFrame with multiple columns. In this case, you can leverage the
capabilities of \texttt{pandas}.

\section{pandas}\label{pandas}

Let's give an example of structured data. Suppose we have a DataFrame
with the following columns: \texttt{sex}, \texttt{height}, and
\texttt{weight}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_total }\OperatorTok{=} \DecValTok{20}
\NormalTok{np.random.seed(}\DecValTok{3}\NormalTok{)}
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_total, loc}\OperatorTok{=}\DecValTok{150}\NormalTok{, scale}\OperatorTok{=}\DecValTok{7}\NormalTok{)}
\NormalTok{weight\_list }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_total, loc}\OperatorTok{=}\DecValTok{42}\NormalTok{, scale}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{sex\_list }\OperatorTok{=}\NormalTok{ np.random.choice([}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{], size}\OperatorTok{=}\NormalTok{N\_total, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{: sex\_list,}
    \StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{: height\_list,}
    \StringTok{\textquotesingle{}weight (kg)\textquotesingle{}}\NormalTok{: weight\_list}
\NormalTok{\})}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& sex & height (cm) & weight (kg) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & M & 162.520399 & 36.074767 \\
1 & M & 153.055569 & 40.971751 \\
2 & M & 150.675482 & 49.430742 \\
3 & F & 136.955551 & 43.183581 \\
4 & F & 148.058283 & 36.881074 \\
5 & M & 147.516687 & 38.435034 \\
6 & F & 149.420810 & 45.126225 \\
7 & F & 145.610995 & 41.197433 \\
8 & F & 149.693273 & 38.155818 \\
9 & M & 146.659474 & 40.849846 \\
10 & M & 140.802947 & 45.725281 \\
11 & F & 156.192357 & 51.880554 \\
12 & F & 156.169226 & 35.779383 \\
13 & M & 161.967011 & 38.867915 \\
14 & F & 150.350235 & 37.981170 \\
15 & M & 147.167258 & 29.904584 \\
16 & M & 146.182480 & 37.381040 \\
17 & M & 139.174659 & 36.880621 \\
18 & M & 156.876572 & 47.619890 \\
19 & M & 142.292527 & 41.340429 \\
\end{longtable}

Calculate sample statistics using \texttt{groupby}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_stats }\OperatorTok{=}\NormalTok{ df.groupby(}\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{].median()}
\NormalTok{observed\_diff }\OperatorTok{=}\NormalTok{ sample\_stats[}\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ sample\_stats[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

We can now leverage the \texttt{pandas.DataFrame.sample} method to
sample from the DataFrame. Here, we use the following options:

\begin{itemize}
\tightlist
\item
  \texttt{frac=1} means we want to sample 100\% of rows, but shuffled.
\item
  \texttt{replace=False} means we want to sample without replacement,
  that is, no duplicate rows.
\end{itemize}

We will shuffle the \texttt{sex} column and store the result in a new
column called \texttt{sex\_shuffled}. Then we can use \texttt{groupby}
to compute the median.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_permutations }\OperatorTok{=} \DecValTok{1000}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.empty(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):}
    \CommentTok{\# shuffle dataframe \textquotesingle{}sex\textquotesingle{} colunn, store it in \textquotesingle{}sex\_shuffled\textquotesingle{}}
\NormalTok{    df[}\StringTok{\textquotesingle{}sex\_shuffled\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{].sample(frac}\OperatorTok{=}\DecValTok{1}\NormalTok{, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    shuffled\_stats }\OperatorTok{=}\NormalTok{ df.groupby(}\StringTok{\textquotesingle{}sex\_shuffled\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{].median()}
\NormalTok{    diffs[i] }\OperatorTok{=}\NormalTok{ shuffled\_stats[}\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ shuffled\_stats[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{]  }\CommentTok{\# median(F) {-} median(M)}
\CommentTok{\# add the observed difference to the array of differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.append(diffs, observed\_diff)}
\end{Highlighting}
\end{Shaded}

\chapter{exact vs.~Monte Carlo permutation
tests}\label{exact-vs.-monte-carlo-permutation-tests}

The permutation tests from before do not sample from the full
distribution of the test statistic under the null hypothesis. This would
be imppractical if the total number of permutations is large, as it
would require computing the test statistic for every possible
permutation of the data.

For example, if we have 10 boys and 14 girls, the total number of
permutations is almost two million:

\[
\binom{24}{14} = \frac{24!}{14!\cdot(24-14)!} = 1961256
\]

The expression above is the binomial coefficient, which counts the
number of ways to choose 14 samples from a total of 24, without regard
to the order of selection. This is why we say ``24 choose 14'' to refer
to the parenthesis above.

There is no preference in ``24 choose 14'' over ``24 choose 10'', as
both expressions yield the same result. You can verify this on your own.

\section{Monte Carlo permutation
tests}\label{monte-carlo-permutation-tests}

Monte Carlo methods are a class of computational algorithms that rely on
repeated random sampling to obtain numerical results. In the context of
permutation tests, Monte Carlo methods do not compute the test statistic
for every possible permutation of the data. In the examples from before,
we computed 1000 permutations only, and from that we estimated the
p-value of the test statistic. If we had run the test more than once, we
would have obtained a different p-value each time, as the test statistic
is computed from a random sample of permutations.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{12.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{mu\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{10}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{14}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define the desired statistic.}
\CommentTok{\# in can be anything you want, you can even write your own function.}
\NormalTok{statistic }\OperatorTok{=}\NormalTok{ np.median}
\CommentTok{\# compute the median for each sample and the difference}
\NormalTok{median\_girls }\OperatorTok{=}\NormalTok{ statistic(sample\_girls)}
\NormalTok{median\_boys }\OperatorTok{=}\NormalTok{ statistic(sample\_boys)}
\NormalTok{observed\_diff }\OperatorTok{=}\NormalTok{ median\_girls }\OperatorTok{{-}}\NormalTok{ median\_boys}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_permutations }\OperatorTok{=} \DecValTok{1000}
\CommentTok{\# combine all values in one array}
\NormalTok{all\_data }\OperatorTok{=}\NormalTok{ np.concatenate([sample\_girls, sample\_boys])}
\CommentTok{\# create an array to store the differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.empty(N\_permutations}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):    }\CommentTok{\# this "minus 1" will be explained later}
    \CommentTok{\# permute the labels}
\NormalTok{    permuted }\OperatorTok{=}\NormalTok{ np.random.permutation(all\_data)}
\NormalTok{    new\_girls }\OperatorTok{=}\NormalTok{ permuted[:N\_girls]  }\CommentTok{\# first 14 values are girls}
\NormalTok{    new\_boys }\OperatorTok{=}\NormalTok{ permuted[N\_girls:]   }\CommentTok{\# remaining values are boys}
\NormalTok{    diffs[i] }\OperatorTok{=}\NormalTok{ statistic(new\_girls) }\OperatorTok{{-}}\NormalTok{ statistic(new\_boys)}
\CommentTok{\# add the observed difference to the array of differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.append(diffs, observed\_diff)}

\NormalTok{p\_value }\OperatorTok{=}\NormalTok{ np.mean(diffs }\OperatorTok{\textgreater{}=}\NormalTok{ observed\_diff)}
\CommentTok{\# two{-}tailed p{-}value}
\CommentTok{\# p\_value = np.mean(np.abs(diffs) \textgreater{}= np.abs(observed\_diff))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Monte Carlo permutation test 1"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"observed difference: }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p{-}value (one{-}tailed): }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Monte Carlo permutation test 1
observed difference: -0.314
p-value (one-tailed): 0.5450
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_permutations }\OperatorTok{=} \DecValTok{1000}
\CommentTok{\# combine all values in one array}
\NormalTok{all\_data }\OperatorTok{=}\NormalTok{ np.concatenate([sample\_girls, sample\_boys])}
\CommentTok{\# create an array to store the differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.empty(N\_permutations}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_permutations }\OperatorTok{{-}} \DecValTok{1}\NormalTok{):    }\CommentTok{\# this "minus 1" will be explained later}
    \CommentTok{\# permute the labels}
\NormalTok{    permuted }\OperatorTok{=}\NormalTok{ np.random.permutation(all\_data)}
\NormalTok{    new\_girls }\OperatorTok{=}\NormalTok{ permuted[:N\_girls]  }\CommentTok{\# first 14 values are girls}
\NormalTok{    new\_boys }\OperatorTok{=}\NormalTok{ permuted[N\_girls:]   }\CommentTok{\# remaining values are boys}
\NormalTok{    diffs[i] }\OperatorTok{=}\NormalTok{ statistic(new\_girls) }\OperatorTok{{-}}\NormalTok{ statistic(new\_boys)}
\CommentTok{\# add the observed difference to the array of differences}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.append(diffs, observed\_diff)}

\NormalTok{p\_value }\OperatorTok{=}\NormalTok{ np.mean(diffs }\OperatorTok{\textgreater{}=}\NormalTok{ observed\_diff)}
\CommentTok{\# two{-}tailed p{-}value}
\CommentTok{\# p\_value = np.mean(np.abs(diffs) \textgreater{}= np.abs(observed\_diff))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Monte Carlo permutation test 2"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"observed difference: }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"p{-}value (one{-}tailed): }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Monte Carlo permutation test 2
observed difference: -0.314
p-value (one-tailed): 0.5340
\end{verbatim}

As you can see, the p-value in not exactly the same, but the difference
is negligible. This is because both times we sampled 1000 permutations
that are representative of the full distribution of the test statistic
under the null hypothesis.

One more thing. The example above with 10 boys and 14 girls is usually
considered small. It is often the case that one has a lot more samples,
and the number of permutations can be astronomically large, much much
larger than two million.

\section{exact permutation test}\label{exact-permutation-test}

If the total number of permutations is small, we can compute the
\textbf{exact} p-value by sampling from the full distribution of the
test statistic under the null hypothesis. That is to say, we compute the
test statistic for every possible permutation of the data.

If we had height measurements of 7 boys and 6 girls, the total number of
permutations is:

\[
\binom{13}{7} = 1716
\]

Any computer can easily handle this number of permutations. How to do it
in practice? We will use the \texttt{itertools.combinations} function.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ itertools }\ImportTok{import}\NormalTok{ combinations}

\CommentTok{\#| code{-}summary: "generate data"}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{6}
\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{7}
\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}

\NormalTok{combined }\OperatorTok{=}\NormalTok{ np.concatenate([sample\_girls, sample\_boys])}
\NormalTok{n\_total }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(combined)}

\CommentTok{\# observed difference in means}
\NormalTok{observed\_diff }\OperatorTok{=}\NormalTok{ np.median(sample\_girls) }\OperatorTok{{-}}\NormalTok{ np.median(sample\_boys)}

\CommentTok{\# generate all combinations of indices for group "girls"}
\NormalTok{indices }\OperatorTok{=}\NormalTok{ np.arange(n\_total)}
\NormalTok{all\_combos }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(combinations(indices, N\_girls))}

\CommentTok{\# compute all permutations}
\NormalTok{diffs }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ idx\_a }\KeywordTok{in}\NormalTok{ all\_combos:}
\NormalTok{    mask }\OperatorTok{=}\NormalTok{ np.zeros(n\_total, dtype}\OperatorTok{=}\BuiltInTok{bool}\NormalTok{)}
\NormalTok{    mask[}\BuiltInTok{list}\NormalTok{(idx\_a)] }\OperatorTok{=} \VariableTok{True}
\NormalTok{    sample\_g }\OperatorTok{=}\NormalTok{ combined[mask]}
\NormalTok{    sample\_b }\OperatorTok{=}\NormalTok{ combined[}\OperatorTok{\textasciitilde{}}\NormalTok{mask]}
\NormalTok{    diffs.append(np.median(sample\_g) }\OperatorTok{{-}}\NormalTok{ np.median(sample\_b))}

\NormalTok{diffs }\OperatorTok{=}\NormalTok{ np.array(diffs)}

\CommentTok{\# exact one{-}tailed p{-}value}
\NormalTok{p\_value }\OperatorTok{=}\NormalTok{ np.mean(diffs }\OperatorTok{\textgreater{}=}\NormalTok{ observed\_diff)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Observed difference: }\SpecialCharTok{\{}\NormalTok{observed\_diff}\SpecialCharTok{:.3f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Exact p{-}value (one{-}tailed): }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total permutations: }\SpecialCharTok{\{}\BuiltInTok{len}\NormalTok{(diffs)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Observed difference: 7.620 cm
Exact p-value (one-tailed): 0.0944
Total permutations: 1716
\end{verbatim}

\textbf{Attention!}

If you read the documentation of the \texttt{itertools} library, you
might be tempted to use \texttt{itertools.permutations} instead of
\texttt{itertools.combinations}.

Don't do that.

Although we are conductiong a permutation test, we are not interested in
the order of the samples, and that is what the \texttt{permutations}
cares about. For instance, if we have 10 people called

{[}Alice, Bob, Charlie, David, Eve, Frank, Grace, Heidi, Ivan, Judy{]}

and we want to randomly assign the label ``girl'' to 4 of them, we do
not care about the order in which we assign the labels. We just want to
know which 4 people are assigned the label ``girl''. The permutation
function does care about the order, and that is why we should not use
it. Instead, we use the \texttt{combinations} function, which return all
possible combinations of the data, without regard to the order of
selection.

\part{confidence interval}

\chapter{basic concepts}\label{basic-concepts}

Suppose we randomly select 30 seven-year-old boys from schools around
the country and measure their heights (this is our sample). We'd like to
use their average height to estimate the true average height of all
seven-year-old boys nationwide (the population). Because different
samples of 30 boys would yield slightly different averages, we need a
way to quantify that uncertainty. A confidence interval gives us a
range---based on our sample data---that expresses what we would expect
to find if we were to repeat this sampling process many times.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\ImportTok{import}\NormalTok{ scipy}
\ImportTok{from}\NormalTok{ matplotlib.lines }\ImportTok{import}\NormalTok{ Line2D}
\ImportTok{import}\NormalTok{ matplotlib.gridspec }\ImportTok{as}\NormalTok{ gridspec}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{7.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

See the height distribution for seven-year-old boys. Below it we see the
means for 20 samples of groups of 30 boys. The 95\% confidence interval
is the range of values that, on average, 95\% of the samples CI contain
the true population mean. In this case, this amounts to one out of the
20 samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{628}\NormalTok{)}
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{90}\NormalTok{, }\DecValTok{150}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(height\_list, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{gs }\OperatorTok{=}\NormalTok{ gridspec.GridSpec(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, height\_ratios}\OperatorTok{=}\NormalTok{[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{])}
\NormalTok{gs.update(left}\OperatorTok{=}\FloatTok{0.09}\NormalTok{, right}\OperatorTok{=}\FloatTok{0.86}\NormalTok{,top}\OperatorTok{=}\FloatTok{0.98}\NormalTok{, bottom}\OperatorTok{=}\FloatTok{0.06}\NormalTok{, hspace}\OperatorTok{=}\FloatTok{0.30}\NormalTok{, wspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}
\NormalTok{ax0 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{ax1 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{])}

\NormalTok{ax0.plot(height\_list, pdf\_boys, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{)}

\NormalTok{N\_samples }\OperatorTok{=} \DecValTok{20}
\NormalTok{N }\OperatorTok{=} \DecValTok{30}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_samples):}
\NormalTok{    sample }\OperatorTok{=}\NormalTok{ norm.rvs(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys, size}\OperatorTok{=}\NormalTok{N)}
\NormalTok{    sample\_mean }\OperatorTok{=}\NormalTok{ sample.mean()}
    \CommentTok{\# confidence interval}
\NormalTok{    alpha }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{    z\_crit }\OperatorTok{=}\NormalTok{ scipy.stats.t.isf(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, N}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{    CI }\OperatorTok{=}\NormalTok{ z\_crit }\OperatorTok{*}\NormalTok{ sample.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ np.sqrt(N)}
\NormalTok{    ax1.errorbar(sample\_mean, i, xerr}\OperatorTok{=}\NormalTok{CI, fmt}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, }
\NormalTok{                 label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}sample }\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}} \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0} \ControlFlowTok{else} \StringTok{""}\NormalTok{, capsize}\OperatorTok{=}\DecValTok{0}\NormalTok{)}


\ImportTok{from}\NormalTok{ matplotlib.patches }\ImportTok{import}\NormalTok{ ConnectionPatch}
\NormalTok{line }\OperatorTok{=}\NormalTok{ ConnectionPatch(xyA}\OperatorTok{=}\NormalTok{(mu\_boys, pdf\_boys.}\BuiltInTok{max}\NormalTok{()), xyB}\OperatorTok{=}\NormalTok{(mu\_boys, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{), coordsA}\OperatorTok{=}\StringTok{"data"}\NormalTok{, coordsB}\OperatorTok{=}\StringTok{"data"}\NormalTok{,}
\NormalTok{                      axesA}\OperatorTok{=}\NormalTok{ax0, axesB}\OperatorTok{=}\NormalTok{ax1, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\FloatTok{1.5}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax1.add\_artist(line)}

\NormalTok{ax1.annotate(}
        \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{,}
\NormalTok{        xy}\OperatorTok{=}\NormalTok{(mu\_boys }\OperatorTok{+} \DecValTok{5}\NormalTok{, }\DecValTok{13}\NormalTok{),  }\CommentTok{\# tip of the arrow (first error bar, y=0)}
\NormalTok{        xytext}\OperatorTok{=}\NormalTok{(mu\_boys }\OperatorTok{+} \DecValTok{5} \OperatorTok{+} \DecValTok{13}\NormalTok{, }\DecValTok{13}\NormalTok{),  }\CommentTok{\# text location}
\NormalTok{        arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textgreater{}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
\NormalTok{        fontsize}\OperatorTok{=}\DecValTok{13}\NormalTok{,}
\NormalTok{        color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}
\NormalTok{)}

\NormalTok{ax1.text(mu\_boys }\OperatorTok{+} \DecValTok{5} \OperatorTok{+} \DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\StringTok{"on average, the CI}\CharTok{\textbackslash{}n}\StringTok{of 1 out of 20 samples}\CharTok{\textbackslash{}n}\StringTok{"}
         \VerbatimStringTok{r"}\KeywordTok{(}\DecValTok{$}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha=5}\DecValTok{$}\VerbatimStringTok{\% significance level}\KeywordTok{)}\VerbatimStringTok{"}
          \StringTok{"}\CharTok{\textbackslash{}n}\StringTok{will not contain}\CharTok{\textbackslash{}n}\StringTok{the population mean"}\NormalTok{,}
\NormalTok{          va}\OperatorTok{=}\StringTok{"top"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# write "sample i" for each error bar}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_samples):}
\NormalTok{    ax1.text(mu\_boys }\OperatorTok{{-}}\DecValTok{10}\NormalTok{, i, }\SpecialStringTok{f\textquotesingle{}sample }\SpecialCharTok{\{}\NormalTok{N\_samples}\OperatorTok{{-}}\NormalTok{i}\SpecialCharTok{:02d\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{, }
\NormalTok{             fontsize}\OperatorTok{=}\DecValTok{13}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{,}
\NormalTok{             ha}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}

\CommentTok{\# ax.legend(frameon=False)}
\NormalTok{ax0.spines[}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax0.spines[}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax1.spines[}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax1.spines[}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax1.spines[}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax1.spines[}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}

\NormalTok{ax0.}\BuiltInTok{set}\NormalTok{(xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{90}\NormalTok{, }\DecValTok{151}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{        xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{150}\NormalTok{),}
\NormalTok{        xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{,}
        \CommentTok{\# xticklabels=[],}
\NormalTok{        yticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{        ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{,}
\NormalTok{        )}
\NormalTok{ax1.}\BuiltInTok{set}\NormalTok{(xticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{        xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{150}\NormalTok{),}
\NormalTok{        ylim}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, N\_samples),}
\NormalTok{        yticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{confidence_interval/basic_concepts_files/figure-pdf/cell-4-output-1.png}}

\chapter{analytical confidence
interval}\label{analytical-confidence-interval}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\ImportTok{import}\NormalTok{ scipy}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

We wish to compute the confidence interval for the mean height of
7-year-old boys, for a sample of size \(N\).

We will start our journey with a refresher of the Central Limit Theorem
(CLT).

\section{CLT}\label{clt}

The Central Limit Theorem states that the sampling distribution of the
sample mean

\[
\bar{X} = \frac{1}{N} \sum_{i=1}^{N} X_i
\]

approaches a normal distribution as the sample size \(N\) increases,
regardless of the shape of the population distribution. This normal
distribution can be expressed as:

\[
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{N}\right),
\]

where \(\mu\) and \(\sigma^2\) are the population mean and variance,
respectively. When talking about samples, we use \(\bar{x}\) and \(s^2\)
to denote the sample mean and variance.

Let's visualize this. The graph below shows how the sample size \(N\)
affects the sampling distribution of the sample mean \(\bar{X}\). The
higher the sample size, the more concentrated the distribution becomes
around the population mean \(\mu\). If we take \(N\) to be infinity, the
sampling distribution of the sample mean becomes a delta function at
\(\mu\), and we will know the exact value of the population mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[}\FloatTok{7.0}\NormalTok{, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[}\FloatTok{7.0}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{, sharey}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(mu\_boys}\OperatorTok{{-}}\DecValTok{12}\NormalTok{, mu\_boys}\OperatorTok{+}\DecValTok{12}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{N\_list }\OperatorTok{=}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{100}\NormalTok{]}
\NormalTok{alpha\_list }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{1.0}\NormalTok{]}

\NormalTok{colors }\OperatorTok{=}\NormalTok{ plt.cm.hot([}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.1}\NormalTok{])}

\NormalTok{N\_samples }\OperatorTok{=} \DecValTok{1000}
\NormalTok{np.random.seed(}\DecValTok{628}\NormalTok{)}
\NormalTok{mean\_list\_10 }\OperatorTok{=}\NormalTok{ []}
\NormalTok{mean\_list\_30 }\OperatorTok{=}\NormalTok{ []}
\NormalTok{mean\_list\_100 }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N\_samples):}
\NormalTok{    mean\_list\_10.append(np.mean(norm.rvs(size}\OperatorTok{=}\DecValTok{10}\NormalTok{, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)))}
\NormalTok{    mean\_list\_30.append(np.mean(norm.rvs(size}\OperatorTok{=}\DecValTok{30}\NormalTok{, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)))}
\NormalTok{    mean\_list\_100.append(np.mean(norm.rvs(size}\OperatorTok{=}\DecValTok{100}\NormalTok{, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)))}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}

\CommentTok{\# z\_alpha\_over\_two = norm(loc=mu\_boys, scale=SE).ppf(1 {-} alpha / 2)}
\CommentTok{\# z\_alpha\_over\_two = np.round(z\_alpha\_over\_two, 2)}

\ControlFlowTok{for}\NormalTok{ i,N }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(N\_list):}
\NormalTok{    SE }\OperatorTok{=}\NormalTok{ sigma\_boys }\OperatorTok{/}\NormalTok{ np.sqrt(N)}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot(height\_list, norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).pdf(height\_list),}
\NormalTok{            color}\OperatorTok{=}\NormalTok{colors[i], label}\OperatorTok{=}\SpecialStringTok{f"N=}\SpecialCharTok{\{}\NormalTok{N}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    
\NormalTok{ax[}\DecValTok{1}\NormalTok{].hist(mean\_list\_10, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, color}\OperatorTok{=}\NormalTok{colors[}\DecValTok{0}\NormalTok{], label}\OperatorTok{=}\StringTok{"N=10"}\NormalTok{, align}\OperatorTok{=}\StringTok{\textquotesingle{}mid\textquotesingle{}}\NormalTok{, histtype}\OperatorTok{=}\StringTok{\textquotesingle{}step\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].hist(mean\_list\_30, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, color}\OperatorTok{=}\NormalTok{colors[}\DecValTok{1}\NormalTok{], label}\OperatorTok{=}\StringTok{"N=10"}\NormalTok{, align}\OperatorTok{=}\StringTok{\textquotesingle{}mid\textquotesingle{}}\NormalTok{, histtype}\OperatorTok{=}\StringTok{\textquotesingle{}step\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].hist(mean\_list\_100, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, color}\OperatorTok{=}\NormalTok{colors[}\DecValTok{2}\NormalTok{], label}\OperatorTok{=}\StringTok{"N=10"}\NormalTok{, align}\OperatorTok{=}\StringTok{\textquotesingle{}mid\textquotesingle{}}\NormalTok{, histtype}\OperatorTok{=}\StringTok{\textquotesingle{}step\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].text(}\FloatTok{0.99}\NormalTok{, }\FloatTok{0.98}\NormalTok{, }\StringTok{"number of samples}\CharTok{\textbackslash{}n}\StringTok{1000"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{1}\NormalTok{].transAxes, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{"pdf"}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"analytical"}
\NormalTok{       )}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\StringTok{"numerical"}
\NormalTok{          )}
\CommentTok{\# title that hovers over both subplots}
\NormalTok{fig.suptitle(}\SpecialStringTok{f"Distribution of the sample means for 3 different sample sizes"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{confidence_interval/analytical_confidence_interval_files/figure-pdf/cell-4-output-1.png}}

\section{confidence interval 1}\label{confidence-interval-1}

Let's use now the sample size \(N=30\). The confidence interval for a
significance level \(\alpha=0.05\) is the interval that leaves
\(\alpha/2\) of the pdf area in each tail of the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.subplots\_adjust(left}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, bottom}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, right}\OperatorTok{=}\FloatTok{0.9}\NormalTok{, top}\OperatorTok{=}\FloatTok{0.9}\NormalTok{, wspace}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, hspace}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{N }\OperatorTok{=} \DecValTok{30}
\NormalTok{SE }\OperatorTok{=}\NormalTok{ sigma\_boys }\OperatorTok{/}\NormalTok{ np.sqrt(N)}

\NormalTok{h\_min }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).ppf(}\FloatTok{0.001}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{h\_max }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).ppf(}\FloatTok{0.999}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{height\_list }\OperatorTok{=}\NormalTok{ np.arange(h\_min, h\_max, }\FloatTok{0.01}\NormalTok{)}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{z\_alpha\_over\_two\_hi }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).ppf(}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ alpha }\OperatorTok{/} \DecValTok{2}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{z\_alpha\_over\_two\_lo }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).ppf(alpha }\OperatorTok{/} \DecValTok{2}\NormalTok{), }\DecValTok{2}\NormalTok{)}


\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(height\_list, norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).pdf(height\_list))}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(height\_list, norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).cdf(height\_list))}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].fill\_between(height\_list, norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).pdf(height\_list),}
\NormalTok{                   where}\OperatorTok{=}\NormalTok{((height\_list }\OperatorTok{\textgreater{}}\NormalTok{ z\_alpha\_over\_two\_hi) }\OperatorTok{|}\NormalTok{ (height\_list }\OperatorTok{\textless{}}\NormalTok{ z\_alpha\_over\_two\_lo)),}
\NormalTok{                   color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{                   label}\OperatorTok{=}\StringTok{\textquotesingle{}rejection region\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].annotate(}\SpecialStringTok{f""}\NormalTok{,}
\NormalTok{               xy}\OperatorTok{=}\NormalTok{(z\_alpha\_over\_two\_hi, }\FloatTok{0.02}\NormalTok{),}
\NormalTok{               xytext}\OperatorTok{=}\NormalTok{(z\_alpha\_over\_two\_lo, }\FloatTok{0.02}\NormalTok{),}
\NormalTok{               arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"\textless{}{-}\textgreater{}"}\NormalTok{, lw}\OperatorTok{=}\FloatTok{1.5}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, shrinkA}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, shrinkB}\OperatorTok{=}\FloatTok{0.0}\NormalTok{),}
\NormalTok{               )}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].text(h\_max}\OperatorTok{+}\FloatTok{0.15}\NormalTok{, norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).cdf(z\_alpha\_over\_two\_lo), }\VerbatimStringTok{r"}\DecValTok{$}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha/2}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{           ha}\OperatorTok{=}\StringTok{"left"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].text(h\_max}\OperatorTok{+}\FloatTok{0.15}\NormalTok{, norm(loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{SE).cdf(z\_alpha\_over\_two\_hi), }\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{1{-}}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha/2}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{           ha}\OperatorTok{=}\StringTok{"left"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].axhline(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].axhline(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].text(mu\_boys, }\FloatTok{0.03}\NormalTok{, }\StringTok{"95}\SpecialCharTok{\% c}\StringTok{onfidence interval"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.42}\NormalTok{),}
\NormalTok{          ylabel}\OperatorTok{=}\StringTok{"pdf"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\VerbatimStringTok{r"significance level }\DecValTok{$}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha}\DecValTok{$}\VerbatimStringTok{ = 0}\DecValTok{.}\VerbatimStringTok{05"}\NormalTok{,}
\NormalTok{          )}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{),}
\NormalTok{          xlim}\OperatorTok{=}\NormalTok{(h\_min, h\_max),}
\NormalTok{          ylabel}\OperatorTok{=}\StringTok{"cdf"}\NormalTok{,}
\NormalTok{          xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{          )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{confidence_interval/analytical_confidence_interval_files/figure-pdf/cell-5-output-1.png}}

That's it. That's the whole story.

\section{confidence interval 2}\label{confidence-interval-2}

The rest is repackaging the above in a slightly different way. Instead
of finding the top and bottom of the confidence interval according to
the cdf of a normal distribution of mean \(\mu\) and variance
\(\sigma^2/N\), we first standardize this distribution to a standard
normal distribution \(Z \sim N(0,1)\), compute the confidence interval
for \(Z\), and then transform it back to the original distribution.

If the distribution of the sample mean \(\bar{X}\)

\[
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{N}\right),
\]

then the standardized variable \(Z\) is defined as:

\[
Z = \frac{\bar{x} - \mu}{\sigma/\sqrt{N}} \sim N(0,1).
\]

Why is this useful? Because we usually use the same significance level
\(\alpha\) for all confidence intervals, and we can compute the
confidence interval for \(Z\) once and use it for all confidence
intervals. For \(Z \sim N(0,1)\) and \(\alpha=0.05\), the top and bottom
of the confidence interval are \(Z_{\alpha/2}=\pm 1.96\). Now we only
have to invert the expression above to get the confidence interval for
\(\bar{X}\):

\[
X_{1,2} = \mu \pm Z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{N}}.
\]

The very last thing we have to account for is the fact that we don't
know the population statistics \(\mu\) and \(\sigma^2\). Instead, we
have to use the sample statistics \(\bar{x}\) and \(s^2\). Furthermore,
we have to use the t-distribution instead of the normal distribution,
because we are estimating the population variance from the sample
variance. The t-distribution has a shape similar to the normal
distribution, but it has heavier tails, which accounts for the
additional uncertainty introduced by estimating the population variance.
Thus, we replace \(\mu\) with \(\bar{x}\) and \(\sigma^2\) with \(s^2\),
and we use the t-distribution with \(N-1\) degrees of freedom. This
gives us the final expression for the confidence interval:

\[
X_{1,2} = \bar{x} \pm t^*_{N-1} \cdot \frac{s}{\sqrt{N}},
\]

where \(t^*_{N-1}\) is the critical value from the t-distribution with
\(N-1\) degrees of freedom.

\section{the solution}\label{the-solution}

Let's say I measured the heights of 30 7-year-old boys, and this is the
data I got:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OperatorTok{=} \DecValTok{30}
\NormalTok{np.random.seed(}\DecValTok{271}\NormalTok{)}
\NormalTok{sample }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Sample mean: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(sample)}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Sample mean: 122.60 cm
[114.15972134 128.21581493 122.9864136  117.94247325 132.11013925
 118.69131645 123.67695468 112.03152008 121.59853424 114.8629358
 121.90458112 115.68839748 127.18043069 118.33193499 125.28525617
 124.5287395  120.72706375 113.10575734 132.229147   129.16820684
 125.94682095 126.08299475 125.95056303 125.6858065  115.07854075
 124.93539918 125.12886271 126.91366971 120.88030405 127.04777082]
\end{verbatim}

Using the formula for the confidence interval we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{z\_crit }\OperatorTok{=}\NormalTok{ scipy.stats.t.isf(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, N}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{CI }\OperatorTok{=}\NormalTok{ z\_crit }\OperatorTok{*}\NormalTok{ sample.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ np.sqrt(N)}
\NormalTok{CI\_low }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(sample.mean() }\OperatorTok{{-}}\NormalTok{ CI, }\DecValTok{2}\NormalTok{)}
\NormalTok{CI\_high }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(sample.mean() }\OperatorTok{+}\NormalTok{ CI, }\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Sample mean: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(sample)}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 95}\SpecialCharTok{\% c}\StringTok{onfidence interval is [}\SpecialCharTok{\{\}}\StringTok{, }\SpecialCharTok{\{\}}\StringTok{] cm"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(CI\_low, CI\_high))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The true population mean is }\SpecialCharTok{\{}\NormalTok{mu\_boys}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Sample mean: 122.60 cm
The 95% confidence interval is [120.54, 124.67] cm
The true population mean is 121.74 cm
\end{verbatim}

\section{a few points to stress}\label{a-few-points-to-stress}

It is worth commenting on a few points:

\begin{itemize}
\item
  If we were to sample a great many number of samples of size \(N=30\),
  and compute the confidence interval for each sample, then
  approximately 95\% of these intervals would contain the true
  population mean \(\mu\).
\item
  It is not true that the probability that the true population mean
  \(\mu\) is in the confidence interval is 95\%. The true population
  mean is either in the interval or not, and it does not have a
  probability associated with it. The 95\% confidence level refers to
  the long-run frequency of intervals containing the true population
  mean if we were to repeat the sampling process many times. This is the
  common \emph{frequentist} interpretation of confidence intervals.
\item
  If you want to talk about confidence interval in the \emph{Bayesian}
  framework, then first we would have to assign a prior distribution to
  the population mean \(\mu\), and then we would compute the posterior
  distribution of \(\mu\) given the data. The credible interval is then
  the interval that contains 95\% of the posterior distribution of
  \(\mu\).
\item
  To sum up the difference between the frequentist and Bayesian
  interpretations of confidence intervals:

  \begin{itemize}
  \tightlist
  \item
    Frequentist CI: ``I am 95\% confident in the method'' (long-run
    frequency).
  \item
    Bayesian credible interval: ``There is a 95\% probability that 
    lies in this interval'' (degree of belief).
  \end{itemize}
\end{itemize}

\chapter{empirical confidence
interval}\label{empirical-confidence-interval}

Not always we want to compute the confidence interval of the mean.
Sometimes we are interested in a different statistic, such as the
median, the standard deviation, or the maximum. The equations we saw
before for the confidence interval of the mean do not apply to these
statistics. However, we can still compute a confidence interval for them
using the empirical bootstrap method.

\section{bootstrap confidence
interval}\label{bootstrap-confidence-interval}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw a sample of size \(N\) from the population. Let's assume you made
  an experiment and you could only afford to collect \(N\) samples. You
  will not have the opportunity to collect more samples, and that's all
  you have available.
\item
  Assume that the sample is representative of the population. This is a
  strong assumption, but we will use it to compute the confidence
  interval.
\item
  From this original sample, draw \(B\) bootstrap samples of size \(N\)
  with replacement. This means that you will randomly select \(N\)
  samples from the original sample, allowing for duplicates. This is
  like drawing pieces of paper from a hat, where you can put the paper
  back after drawing it.
\item
  For each bootstrap sample, compute the statistic of interest (e.g.,
  median, standard deviation, maximum).
\item
  Compute the cdf of the bootstrap statistics. This will give you the
  empirical distribution of the statistic.
\item
  Compute the confidence interval using the empirical distribution. For
  a 95\% confidence interval, you can take the 2.5th and 97.5th
  percentiles of the bootstrap statistics.
\end{enumerate}

That's it. Now let's do it in code.

\section{question}\label{question-3}

We have a sample of 30 7-year-old boys. What can we say about the
maximum height of 7-year-olds in the general population?

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm, ttest\_ind, t}
\ImportTok{import}\NormalTok{ scipy}
\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[}\FloatTok{7.0}\NormalTok{, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[}\FloatTok{7.0}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OperatorTok{=} \DecValTok{30}     \CommentTok{\# bootstrap sample size equal to original sample size}
\NormalTok{B }\OperatorTok{=} \DecValTok{10000}  \CommentTok{\# number of bootstrap samples}
\NormalTok{np.random.seed(}\DecValTok{1}\NormalTok{)}
\NormalTok{sample }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{median\_list }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(B):}
\NormalTok{    sample\_bootstrap }\OperatorTok{=}\NormalTok{ np.random.choice(sample, size}\OperatorTok{=}\NormalTok{N, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    median\_list.append(np.median(sample\_bootstrap))}
\NormalTok{median\_list }\OperatorTok{=}\NormalTok{ np.array(median\_list)}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{ci\_bottom }\OperatorTok{=}\NormalTok{ np.quantile(median\_list,alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{ci\_top }\OperatorTok{=}\NormalTok{ np.quantile(median\_list, }\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Bootstrap CI for median: }\SpecialCharTok{\{}\NormalTok{ci\_bottom}\SpecialCharTok{:.2f\}}\SpecialStringTok{ {-} }\SpecialCharTok{\{}\NormalTok{ci\_top}\SpecialCharTok{:.2f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Bootstrap CI for median: 118.65 - 124.53 cm
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].hist(median\_list, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, align}\OperatorTok{=}\StringTok{\textquotesingle{}mid\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].hist(median\_list, bins}\OperatorTok{=}\DecValTok{30}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, cumulative}\OperatorTok{=}\VariableTok{True}\NormalTok{, align}\OperatorTok{=}\StringTok{\textquotesingle{}mid\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].axhline(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].axhline(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}

\NormalTok{xlim }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{1}\NormalTok{].get\_xlim()}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].text(xlim[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\FloatTok{0.15}\NormalTok{, alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\VerbatimStringTok{r"}\DecValTok{$}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha/2}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{           ha}\OperatorTok{=}\StringTok{"left"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].text(xlim[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\FloatTok{0.15}\NormalTok{, }\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{1{-}}\CharTok{\textbackslash{}a}\VerbatimStringTok{lpha/2}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{           ha}\OperatorTok{=}\StringTok{"left"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].annotate(}
     \StringTok{\textquotesingle{}bottom CI\textquotesingle{}}\NormalTok{,}
\NormalTok{     xy}\OperatorTok{=}\NormalTok{(ci\_bottom, alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{), xycoords}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{,}
\NormalTok{     xytext}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{100}\NormalTok{, }\DecValTok{30}\NormalTok{), textcoords}\OperatorTok{=}\StringTok{\textquotesingle{}offset points\textquotesingle{}}\NormalTok{,}
\NormalTok{     color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{,}
\NormalTok{     arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{,}
\NormalTok{                     connectionstyle}\OperatorTok{=}\StringTok{"angle,angleA=0,angleB=90,rad=10"}\NormalTok{))}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].annotate(}
     \StringTok{\textquotesingle{}top CI\textquotesingle{}}\NormalTok{,}
\NormalTok{     xy}\OperatorTok{=}\NormalTok{(ci\_top, }\DecValTok{1}\OperatorTok{{-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{), xycoords}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{,}
\NormalTok{     xytext}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{100}\NormalTok{, }\DecValTok{15}\NormalTok{), textcoords}\OperatorTok{=}\StringTok{\textquotesingle{}offset points\textquotesingle{}}\NormalTok{,}
\NormalTok{     color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{,}
\NormalTok{     arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"{-}\textgreater{}"}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{,}
\NormalTok{                     connectionstyle}\OperatorTok{=}\StringTok{"angle,angleA=0,angleB=90,rad=10"}\NormalTok{))}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{"pdf"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\StringTok{"empirical distribution of median heights from bootstrap samples"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{"cdf"}\NormalTok{,}
\NormalTok{          xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{confidence_interval/empirical_confidence_interval_files/figure-pdf/cell-5-output-1.png}}

Clearly, the distribution of median height is not normal. The bootstrap
method gives us a way to compute the confidence interval of the median
height (or any other statistic of your choosing) without assuming
normality.

\part{regression}

\chapter{the geometry of regression}\label{the-geometry-of-regression}

\section{a very simple example}\label{a-very-simple-example}

It's almost always best to start with a simple and concrete example.

\textbf{Goal:} We wish to find the best straight line that describes the
following data points:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{import}\NormalTok{ scipy}

\CommentTok{\# \%matplotlib widget}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{])}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\CommentTok{\# linear regression}
\NormalTok{slope, intercept, r\_value, p\_value, std\_err }\OperatorTok{=}\NormalTok{ scipy.stats.linregress(x, y)}
\NormalTok{x\_domain }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{101}\NormalTok{)}
\NormalTok{ax.plot(x\_domain, intercept }\OperatorTok{+}\NormalTok{ slope }\OperatorTok{*}\NormalTok{ x\_domain, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}best line\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{),}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{),}
\NormalTok{       xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{       yticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{       xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}X{-}axis\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Y{-}axis\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/geometry-of-regression_files/figure-pdf/cell-3-output-1.png}}

\section{formalizing the problem}\label{formalizing-the-problem}

Let's translate this problem into the language of linear algebra.

The independent variable \(x\) is the column vector

\[
x=
\begin{pmatrix}
1 \\ 2 \\ 3
\end{pmatrix}
\]

and the dependent variable \(y\) is the column vector

\[
y=
\begin{pmatrix}
2 \\ 2 \\ 6
\end{pmatrix}.
\]

Because we are looking for a straight line, we can express the
relationship between \(x\) and \(y\) as

\[
\tilde{y} = \beta_0 + \beta_1 x.
\]

Here we introduced the notation \(\tilde{y}\) to denote the predicted
values of \(y\) based on the linear model. It is different from the
actual values of \(y\) because the straight line usually does not pass
exactly on top of \(y\).

The parameter \(\beta_0\) is the intercept and \(\beta_1\) is the slope
of the line.

\textbf{Which values of \(\beta_0,\beta_1\) will give us the very best
line?}

\section{higher dimensions}\label{higher-dimensions}

It is very informative to think about this problem not as a scatter plot
in the \(X-Y\) plane, but as taking place in a higher-dimensional space.
Because we have three data points, we can think of the problem in a
three-dimensional space. We want to explain the vector \(y\) as a linear
combination of the vector \(x\) and a constant vector (this is what our
linear model states).

In three dimensions, our building blocks are the vectors \(c\), the
intercept, and \(x\), the data points.

\[
c=
\begin{pmatrix}
1 \\ 1 \\ 1
\end{pmatrix}, \qquad
x=
\begin{pmatrix}
1 \\ 2 \\ 3
\end{pmatrix}.
\]

We can combine these \(c\) and \(x\) as column vectors in a matrix
called \textbf{design matrix}:

\[
X=
\begin{pmatrix}
1 & x_0 \\
| & | \\
1 & x_i \\
| & | \\
1 & x_n
\end{pmatrix}
=
\begin{pmatrix}
| & | \\
1 & x \\
| & |
\end{pmatrix}
\]

Why is this convenient? Because now the linear combination of
\(\vec{1}\) and \(x\) can be expressed as a matrix multiplication:

\[
\begin{pmatrix}
\hat{y}_0 \\
\hat{y}_1 \\
\hat{y}_2
\end{pmatrix} 
=
\begin{pmatrix}
1 & x_0 \\
1 & x_1 \\
1 & x_2 \\
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix}
=
\begin{pmatrix}
1\cdot\beta_0 + x_0\cdot\beta_1 \\
1\cdot\beta_0 + x_1\cdot\beta_1 \\
1\cdot\beta_0 + x_2\cdot\beta_1
\end{pmatrix} 
\]

In short, the linear combination of our two building blocks yields a
prediction vector \(\hat{y}\):

\[
\hat{y} = X \beta,
\]

where \(\beta\) is the column vector \((\beta_0, \beta_1)^T\).

This prediction vector \(\hat{y}\) lies on a plane in the 3d space, it
cannot be anywhere in this 3d space. Mathematically, we say that the
vector \(\hat{y}\) is in the subspace spanned by the columns of the
design matrix \(X\).

It will be extremely improbable that the vector \(y\) will also lie on
this plane, so we will have to find the best prediction \(\hat{y}\) that
lies on this plane. Geometrically, our goal is to find the point
\(\hat{y}\) on the plane that is \textbf{closest} to the point \(y\) in
the 3d space.

\begin{itemize}
\tightlist
\item
  When the distance \(r=y-\hat{y}\) is minimized, the vector \(r\) is
  orthogonal to the plane spanned by the columns of the design matrix
  \(X\).
\item
  We call this vector \(r\) the \textbf{residual vector}.
\item
  The residual is orthogonal to each of the columns of \(X\), that is,
  \(\vec{1}\cdot r=0\) and \(x\cdot r=0\).
\end{itemize}

I tried to summarize all the above in the 3d image below. This is, for
me, the \emph{geometry of regression}. If you have that in your head,
you'll never forget it.

\pandocbounded{\includegraphics[keepaspectratio]{regression/3dplot_A.png}}

Another angle of the image above. This time, because the view direction
is within the plane, we see that the residual vector \(r\) is orthogonal
to the plane spanned by the columns of the design matrix \(X\).
\pandocbounded{\includegraphics[keepaspectratio]{regression/3dplot_B.png}}

For a fully interactive version, see this
\href{https://www.geogebra.org/3d/yjsuhu96}{Geogebra applet}.

Taking advantage of the matrix notation, we can express the
orthogonality condition as follows:

\[
\begin{pmatrix}
- & 1 & - \\
- & x & -
\end{pmatrix} r =
X^T r =
0
\]

Let's substitute \(r = y - \hat{y} = y - X\beta\) into the equation
above.

\[
X^T(y - X\beta) = 0
\]

Distributing yields

\[
X^Ty - X^TX\beta = 0,
\]

and then

\[
X^TX\beta = X^Ty.
\]

We need to solve this equation for \(\beta\), so we left-multiply both
sides by the inverse of \(X^TX\),

\[
\beta = (X^TX)^{-1}X^Ty.
\]

That's it. We did it. Given the data points \(x\) and \(y\), we can
compute the parameters \(\beta_0\) and \(\beta_1\) that bring
\(\hat{y}\) as close as possible to \(y\). These parameters are the best
fit of the straight line to the data points.

\section{overdetermined system}\label{overdetermined-system}

The \emph{design matrix} \(X\) is a tall and skinny matrix, meaning that
it has more rows (\(n\)) than columns (\(m\)). This is called an
\textbf{overdetermined system}, because we have more equations (rows)
than unknowns (columns), so we have no hope in finding an exact solution
\(\beta\).

This is to say that, almost certainly, the vector \(y\) does not lie on
the plane spanned by the columns of the design matrix \(X\). No
combination of the parameters \(\beta\) will yield a vector \(\hat{y}\)
that is exactly equal to \(y\).

\section{least squares}\label{least-squares}

The method above for finding the best parameters \(\beta\) is called
\textbf{least squares}. The name comes from the fact that we are trying
to minimize the length of the residual vector

\[
r = y - \hat{y}.
\]

The length of the residual is given by the Euclidean norm (or \(L^2\)
norm), which is a direct generalization of the Pythagorean theorem for
many dimensions.

\begin{align}
\Vert r\Vert^2 &= \Vert y - \hat{y}\Vert^2  \\
 &= (y_0 - \hat{y}_0)^2 + (y_1 - \hat{y}_1)^2 + \cdots +  (y_{n-1} - \hat{y}_{n-1})^2 \\
 &= r_0^2 + r_1^2 + \cdots + r_{n-1}^2
\end{align}

The length (squared) of the residual vector is the sum of the squares of
all residuals. The best parameters \(\beta\) are those that yield the
\textbf{least squares}, thus the name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{x\_domain }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{101}\NormalTok{)}
\NormalTok{ax.plot(x\_domain, intercept }\OperatorTok{+}\NormalTok{ slope }\OperatorTok{*}\NormalTok{ x\_domain, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}best line\textquotesingle{}}\NormalTok{)}

\KeywordTok{def}\NormalTok{ linear(x, slope, intercept):}
    \ControlFlowTok{return}\NormalTok{ intercept }\OperatorTok{+}\NormalTok{ slope }\OperatorTok{*}\NormalTok{ x}

\ControlFlowTok{for}\NormalTok{ i, xi }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(x):}
\NormalTok{    ax.plot([xi, xi],}
\NormalTok{            [y[i], linear(xi, slope, intercept)],}
\NormalTok{            color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\FloatTok{0.5}\NormalTok{,}
\NormalTok{            label}\OperatorTok{=}\StringTok{\textquotesingle{}residuals\textquotesingle{}} \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0} \ControlFlowTok{else} \VariableTok{None}\NormalTok{)}
    
\NormalTok{ax.legend(loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{) }
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{),}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{),}
\NormalTok{       xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{       yticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{       xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}X{-}axis\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Y{-}axis\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/geometry-of-regression_files/figure-pdf/cell-4-output-1.png}}

\section{many more dimensions}\label{many-more-dimensions}

The concrete example here dealt with only three data points, therefore
we could visualize the problem in a three-dimensional space. However,
the same reasoning applies to any number of data points and any number
of independent variables.

\begin{itemize}
\tightlist
\item
  \textbf{any number of data points}: we call the number of data points
  \(n\), and that makes \(y\) be a vector in an \(n\)-dimensional space.
\item
  \textbf{any number of independent variables}: we calculated a
  regression for a straight line, and thus we had only two building
  blocks, the intercept \(\vec{1}\) and the independent variable \(x\).
  However, we can have any number of independent variables, say \(m\) of
  them. For example, we might want to predict the data using a
  polynomial of degree \(m\), or we might have any arbitrary \(m\)
  functions that we wish to use: \(\exp(x)\), \(\tanh(x^2)\), whatever
  we want. All this will work as long as the parameters \(\beta\)
  multiply these building blocks. That's the topic of the next chapter.
\end{itemize}

\chapter{least squares}\label{least-squares-1}

\section{ordinary least squares (OLS)
regression}\label{ordinary-least-squares-ols-regression}

Let's go over a few things that appear in this notebook,
\href{https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html}{statsmodels,
Ordinary Least Squares}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\NormalTok{np.random.seed(}\DecValTok{9876789}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{polynomial regression}\label{polynomial-regression}

Let's start with a simple polynomial regression example. We will start
by generating synthetic data for a quadratic equation plus some noise.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of points}
\NormalTok{nsample }\OperatorTok{=} \DecValTok{100}
\CommentTok{\# create independent variable x}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\CommentTok{\# create design matrix with linear and quadratic terms}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.column\_stack((x, x }\OperatorTok{**} \DecValTok{2}\NormalTok{))}
\CommentTok{\# create coefficients array}
\NormalTok{beta }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{5}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{])}
\CommentTok{\# create random error term}
\NormalTok{e }\OperatorTok{=}\NormalTok{ np.random.normal(size}\OperatorTok{=}\NormalTok{nsample)}
\end{Highlighting}
\end{Shaded}

\(x\) and \(e\) can be understood as column vectors of length \(n\),
while \(X\) and \(\beta\) are:

\[
X = 
\begin{pmatrix}
x_0 & x_0^2 \\
| & | \\
x_i & x_i^2 \\
| & | \\
x_n & x_n^2 \\
\end{pmatrix}, \qquad
\beta = 
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\beta_2
\end{pmatrix}.
\]

Oops, there is no intercept column \(\vec{1}\) in the design matrix
\(X\). Let's add it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ sm.add\_constant(X)}
\BuiltInTok{print}\NormalTok{(X[:}\DecValTok{5}\NormalTok{, :])  }\CommentTok{\# print first 5 rows of design matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1.         0.         0.        ]
 [1.         0.1010101  0.01020304]
 [1.         0.2020202  0.04081216]
 [1.         0.3030303  0.09182736]
 [1.         0.4040404  0.16324865]]
\end{verbatim}

This \texttt{add\_constant} function is smart, it has as default a
\texttt{prepend=True} argument, meaning that the intercept is added as
the first column, and a
\texttt{has\_constant=\textquotesingle{}skip\textquotesingle{}}
argument, meaning that it will not add a constant if one is already
present in the matrix.

The matrix \(X\) is now a \textbf{design matrix} for a polynomial
regression of degree 2. \[
X = 
\begin{pmatrix}
1 & x_0 & x_0^2 \\
| & | & | \\
1 & x_i & x_i^2 \\
| & | & | \\
1 & x_n & x_n^2 \\
\end{pmatrix}
\]

We now put everything together in the following equation:

\[
y = X \beta + e
\]

This creates the dependend variable \(y\) as a linear combination of the
independent variables in \(X\) and the coefficients in \(\beta\), plus
an error term \(e\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.dot(X, beta) }\OperatorTok{+}\NormalTok{ e}
\end{Highlighting}
\end{Shaded}

Let's visualize this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Simulated Data with Linear and Quadratic Terms\textquotesingle{}}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/least-squares_files/figure-pdf/cell-6-output-1.png}}

\section{solving the ``hard way''}\label{solving-the-hard-way}

I'm going to do something that nobody does. I will use the formula we
derived in the previous chapter to find the coefficients \(\beta\) of
the polynomial regression.

\[
\beta = (X^TX)^{-1}X^Ty.
\]

Translating this into code, and keeping in mind that matrix
multiplication in Python is done with the \texttt{@} operator, we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_opt }\OperatorTok{=}\NormalTok{ np.linalg.inv(X.T}\OperatorTok{@}\NormalTok{X)}\OperatorTok{@}\NormalTok{X.T}\OperatorTok{@}\NormalTok{y}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"beta = }\SpecialCharTok{\{}\NormalTok{beta\_opt}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
beta = [ 5.34233516 -2.14024948  0.51025357]
\end{verbatim}

That's it. We did it (again).

Let's take a look at the matrix \(X^TX\). Because \(X\) is a tall and
skinny matrix of shape \((n, 3)\), the matrix \(X^T\) is a wide and
short matrix of shape \((3, n)\). This is because we have many more data
points \(n\) than the number of predictors (\(\vec{1},x,x^2\)), which is
of course equal to the number of coefficients
\((\beta_0,\beta_1,\beta_2)\).

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(X.T}\OperatorTok{@}\NormalTok{X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1.00000000e+02 5.00000000e+02 3.35016835e+03]
 [5.00000000e+02 3.35016835e+03 2.52525253e+04]
 [3.35016835e+03 2.52525253e+04 2.03033670e+05]]
\end{verbatim}

When we multiply the matrices \(X^T_{3\times n}\) and \(X_{n\times 3}\),
we get a square matrix of shape \((3, 3)\), because the inner dimensions
match (the number of columns in \(X^T\) is equal to the number of rows
in \(X\)). The product \(X^TX\) is a square matrix of shape \((3, 3)\),
which is quite easy to invert. If it were the other way around
(\(X\,X^T\)), we would have a matrix of shape \((n, n)\), which is much
harder to invert, especially if \(n\) is large. Lucky us.

Now let's see if the parameters we found are any good.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"beta parameters used to generate data:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(beta)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"beta parameters estimated from data:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(beta\_opt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
beta parameters used to generate data:
[ 5.  -2.   0.5]
beta parameters estimated from data:
[ 5.34233516 -2.14024948  0.51025357]
\end{verbatim}

Pretty good, right? Now let's see the best fit polynomial on the graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax.plot(x, np.dot(X, beta\_opt), color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}fitted line\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Simulated Data with Linear and Quadratic Terms\textquotesingle{}}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/least-squares_files/figure-pdf/cell-10-output-1.png}}

Why did I call it the ``hard way''? Because these operations are so
common that of course there are libraries that do this for us. We don't
need to remember the equation, we can just use, for example,
\texttt{statsmodels} library's \texttt{OLS} function, which does exactly
this. Let's see how it works.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ sm.OLS(y, X)}
\NormalTok{results }\OperatorTok{=}\NormalTok{ model.fit()}
\end{Highlighting}
\end{Shaded}

Now we can compare the results of our manual calculation with the
results from \texttt{statsmodels}. We should get the same coefficients,
and we do.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"beta parameters used to generate data:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(beta)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"beta parameters from our calculation:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(beta\_opt)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"beta parameters from statsmodels:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(results.params)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
beta parameters used to generate data:
[ 5.  -2.   0.5]
beta parameters from our calculation:
[ 5.34233516 -2.14024948  0.51025357]
beta parameters from statsmodels:
[ 5.34233516 -2.14024948  0.51025357]
\end{verbatim}

\section{statmodels.OLS and the
summary}\label{statmodels.ols-and-the-summary}

Statmodels provides us a lot more information than just the
coefficients. Let's take a look at the summary of the OLS regression.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(results.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.988
Model:                            OLS   Adj. R-squared:                  0.988
Method:                 Least Squares   F-statistic:                     3965.
Date:                Mon, 23 Jun 2025   Prob (F-statistic):           9.77e-94
Time:                        12:50:31   Log-Likelihood:                -146.51
No. Observations:                 100   AIC:                             299.0
Df Residuals:                      97   BIC:                             306.8
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.3423      0.313     17.083      0.000       4.722       5.963
x1            -2.1402      0.145    -14.808      0.000      -2.427      -1.853
x2             0.5103      0.014     36.484      0.000       0.482       0.538
==============================================================================
Omnibus:                        2.042   Durbin-Watson:                   2.274
Prob(Omnibus):                  0.360   Jarque-Bera (JB):                1.875
Skew:                           0.234   Prob(JB):                        0.392
Kurtosis:                       2.519   Cond. No.                         144.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{verbatim}

I won't go into the details of the summary, but I encourage you to take
a look at it and see if you can make sense of it.

\section{R-squared}\label{r-squared}

R-squared is a measure of how well the model fits the data. It is
defined as the proportion of the variance in the dependent variable that
is predictable from the independent variables. It can be computed as
follows:

\[
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
\]

where \(SS_{res}\) and \(SS_{tot}\) are defined as follows:

\begin{align*}
SS_{res} &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
SS_{tot} &= \sum_{i=1}^n (y_i - \bar{y})^2
\end{align*}

The letters SS mean ``sum of squares'', and \(\bar{y}\) is the mean of
the dependent variable. Let's compute it manually, and then compare it
with the value from the \texttt{statsmodels} summary.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_hat }\OperatorTok{=}\NormalTok{ np.dot(X, beta\_opt)}
\NormalTok{SS\_res }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((y }\OperatorTok{{-}}\NormalTok{ y\_hat) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}
\NormalTok{SS\_tot }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((y }\OperatorTok{{-}}\NormalTok{ np.mean(y)) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}
\NormalTok{R2 }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ (SS\_res }\OperatorTok{/}\NormalTok{ SS\_tot)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"R{-}squared (manual calculation): "}\NormalTok{, R2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"R{-}squared (from statsmodels): "}\NormalTok{, results.rsquared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R-squared (manual calculation):  0.9879144521349076
R-squared (from statsmodels):  0.9879144521349076
\end{verbatim}

This high \(R^2\) value tells us that the model explains a very large
proportion of the variance in the dependent variable.

How can we know that the variance has anything to do with the \(R^2\)?
If we divide both the \(SS_{res}\) and \(SS_{tot}\) by \(n-1\), we get
the sample variances of the residuals and the dependent variable,
respectively.

\begin{align*}
s^2_{res} = \frac{SS_{res}}{n-1} &= \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n-1} \\
s^2_{tot} = \frac{SS_{tot}}{n-1} &= \frac{\sum_{i=1}^n (y_i - \bar{y})^2}{n-1}
\end{align*}

Then the \(R^2\) can then be expressed as: \[
R^2 = 1 - \frac{s^2_{res}}{s^2_{tot}}.
\]

I prefer this equation over the first, because it makes it clear that
\(R^2\) is the ratio of the variances, which is a more intuitive way to
think about it.

\section{any function will do}\label{any-function-will-do}

The formula we derived the the previous chapter works for predictors
(independent variables) of any kind, not only polynomials. The formula
will work as long as the parameters \(\beta\) are linear in the
predictors. For exammple, we could have a nonlinear function like this:

\[
y = \beta_0 + \beta_1 e^x + \beta_2 \sin(x^2)
\]

because each beta multiplies a predictor. On the other hand, the
following function would not work, because the parameters are not linear
in the predictors:

\[
y = \beta_0 + e^{\beta_1 x} + \sin(\beta_2 x^2)
\]

Let's this this in action, I'll use the same example provided by
\texttt{statsmodels} documentation, which is a nonlinear function of the
form:

\[
y = \beta_0 x + \beta_1 \sin(x) + \beta_2(x - 5)^2 + \beta_3
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nsample }\OperatorTok{=} \DecValTok{50}
\NormalTok{sig }\OperatorTok{=} \FloatTok{0.5}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, nsample)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.column\_stack((}
\NormalTok{    x,}
\NormalTok{    np.sin(x),}
\NormalTok{    (x }\OperatorTok{{-}} \DecValTok{5}\NormalTok{) }\OperatorTok{**} \DecValTok{2}\NormalTok{,}
\NormalTok{    np.ones(nsample)}
\NormalTok{    ))}
\NormalTok{beta }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.02}\NormalTok{, }\FloatTok{5.0}\NormalTok{]}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ np.dot(X, beta)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ y\_true }\OperatorTok{+}\NormalTok{ sig }\OperatorTok{*}\NormalTok{ np.random.normal(size}\OperatorTok{=}\NormalTok{nsample)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{,}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/least-squares_files/figure-pdf/cell-16-output-1.png}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OperatorTok{=}\NormalTok{ sm.OLS(y, X).fit()}
\BuiltInTok{print}\NormalTok{(result.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.933
Model:                            OLS   Adj. R-squared:                  0.928
Method:                 Least Squares   F-statistic:                     211.8
Date:                Mon, 23 Jun 2025   Prob (F-statistic):           6.30e-27
Time:                        12:51:08   Log-Likelihood:                -34.438
No. Observations:                  50   AIC:                             76.88
Df Residuals:                      46   BIC:                             84.52
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1             0.4687      0.026     17.751      0.000       0.416       0.522
x2             0.4836      0.104      4.659      0.000       0.275       0.693
x3            -0.0174      0.002     -7.507      0.000      -0.022      -0.013
const          5.2058      0.171     30.405      0.000       4.861       5.550
==============================================================================
Omnibus:                        0.655   Durbin-Watson:                   2.896
Prob(Omnibus):                  0.721   Jarque-Bera (JB):                0.360
Skew:                           0.207   Prob(JB):                        0.835
Kurtosis:                       3.026   Cond. No.                         221.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{verbatim}

Note something interesting: in our design matrix \(X\), we encoded the
intercept column as the last column, there is no reason why it should be
the first column (although first column is a common choice). The
function `statsmodels.OLS' sees this, and when we print the summary, it
will show the intercept as the last coefficient. Nice!

Let's see a graph of the data and the fitted model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax.plot(x, np.dot(X, result.params), color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}fitted line\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{,}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/least-squares_files/figure-pdf/cell-18-output-1.png}}

\chapter{equivalence}\label{equivalence}

\section{orthogonality}\label{orthogonality}

In the context of linear regression, the orthogonality condition states
that the residual vector \(r\) is orthogonal to the column space of the
design matrix \(X\):

\[
X^T r = 0
\]

Let's substitute \(r = y - \hat{y} = y - X\beta\) into the equation
above.

\[
X^T(y - X\beta) = 0
\]

Distributing yields

\[
X^Ty - X^TX\beta = 0,
\]

and then

\[
X^TX\beta = X^Ty.
\]

We need to solve this equation for \(\beta\), so we left-multiply both
sides by the inverse of \(X^TX\),

\[
\beta = (X^TX)^{-1}X^Ty.
\]

We already did that in a previous chapter. Now let's get exactly the
same result using another approach.

\section{optimization}\label{optimization}

We wish to minimize the sum of squared errors, which is given by

\[
L = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - X_{ij}\beta_j)^2.
\]

I'm not exactly sure, but I think we call this \(L\) because of the
lagrangian function. Later on when we talk about regularization, we will
see that the lagrangian function can be constrained by lagrange
multipliers. In any case, let's keep going with the optimization.

It is useful to express \(L\) in matrix notation. The sum of squared
errors can be thought as the dot product of the residual vector with
itself:

\begin{align*}
L &= (y - X\beta)^T(y - X\beta) \\
&= y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta,
\end{align*}

where we used the following properties of matrix algebra: 1. The dot
product of a vector with itself is the sum of the squares of its
components, i.e., \(a^Ta = \sum_{i=1}^n a_i^2\). We used this to express
the sum of squared errors in matrix notation. 2. The dot product is
bilinear, i.e., \((a - b)^T(c - d) = a^Tc - a^Td - b^Tc + b^Td\). We
used this to expand the expression for the sum of squared errors. 3. The
transpose of a product of matrices is the product of their transposes in
reverse order, i.e., \((AB)^T = B^TA^T\). We used this to compute
\((X\beta)^T\).

Let's use one more property to join the two middle terms,
\(- y^TX\beta - \beta^TX^Ty\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The dot product is symmetric, i.e., \(a^Tb = b^Ta\). This is evident
  we express the dot product as a summation:
\end{enumerate}

\[
  a^Tb = \sum_{i=1}^n a_i b_i = \sum_{i=1}^n b_i a_i = b^Ta.
  \]

Joining the two middle terms results in the following \(L\):

\[
L = y^Ty - 2y^TX\beta + \beta^TX^TX\beta,
\]

The set of parameters \(\beta\) that minimizes \(L\) is that which
satisfies the extreme condition of the function \(L\) (either maximum or
minimum). This means that the gradient of \(L\) with respect to
\(\beta\) must be zero:

\[
\frac{\partial L}{\partial \beta} = 0.
\]

Let's plug in the expression for \(L\):

\[
\frac{\partial}{\partial \beta} \left( y^Ty - 2\beta^TX^Ty + \beta^TX^TX\beta \right) = 0
\]

The quantity \(L\) is a scalar, and also each of the three terms that we
are differentiating is a scalar. Let's differentiate them one by one.

The first term, \(y^Ty\), is a constant with respect to \(\beta\), so
its derivative is zero.

The second term

\[
\frac{\partial}{\partial \beta} \left( - 2\beta^TX^Ty \right) = - 2\frac{\partial}{\partial \beta} \left( \beta^TX^Ty \right).
\]

The quantity being differentiated is a scalar, it's the product of the
row vector \(\beta^T\) and the column vector \(X^Ty\). Right now we
don't care much about \(X^Ty\), it could be any column vector, so let's
call it \(c\). The derivative of dot product \(\beta^T c\) with respect
to a specific element \(\beta_k\) can be written explicitly as

\[
\frac{\partial (\beta^T c)}{\partial \beta} = \frac{\partial}{\partial \beta_k} \left( \sum_{i=1}^p \beta_i c_i \right) = \frac{\partial}{\partial \beta_k} \left( \beta_1 c_1 + \beta_2 c_2 + \ldots + \beta_p c_p \right) =  c_k.
\]

Whatever value for the index \(k\) we choose, the derivative will be
zero for all indices except for \(k\), and that explain the result
above.

Since the gradient \(\nabla_{\beta}(\beta^T c)\) is a vector,

\[
\nabla_{\beta}(\beta^T c) = \frac{\partial (\beta^T c)}{\partial \beta} =
\begin{pmatrix}
\frac{\partial (\beta^T c)}{\partial \beta_1} \\
\frac{\partial (\beta^T c)}{\partial \beta_2} \\
\vdots \\
\frac{\partial (\beta^T c)}{\partial \beta_p}
\end{pmatrix}
\]

and we have just figured out what each component is, we can write the
solution as

\[
\frac{\partial (\beta^T c)}{\partial \beta} =
\begin{pmatrix}
c_1 \\ c_2 \\ \vdots \\ c_p
\end{pmatrix}
= c
= X^Ty.
\]

So the derivative of the second term is simply \(-2 X^Ty\).

To solve the derivatie of the third term, \(\beta^TX^TX\beta\), we use
the following property:

\[
\frac{\partial}{\partial \beta} \left( \beta^T A \beta \right) = 2A\beta,
\]

when \(A\) is a symmetric matrix. In our case, \(A = X^TX\), which is
symmetric because \((X^TX)^T = X^T(X^T)^T = X^TX\). Therefore we have:

\[
\frac{\partial}{\partial \beta} \left( \beta^T X^TX \beta \right) = 2X^TX\beta,
\]

and that is the derivative of the third term.

We still have to prove why the derivative of \(\beta^T A \beta\) is
\(2A\beta\). First, let's use the summation notation to express the term
\(\beta^T A \beta\):

\[
\beta^T A \beta = \sum_{i=1}^p \sum_{j=1}^p \beta_i A_{ij} \beta_j.
\]

Now, let's differentiate this expression with respect to \(\beta_k\),
using the chain rule:

\[
\frac{\partial}{\partial \beta_k} \left( \sum_{i=1}^p \sum_{j=1}^p \beta_i A_{ij} \beta_j \right) = \sum_{i=1}^p A_{ik} \beta_i + \sum_{j=1}^p \beta_j A_{kj}.
\]

Using the symmetry of \(A\) (\(A_{ij} = A_{ji}\)):

\[
\frac{\partial}{\partial \beta_k} \left( \sum_{i=1}^p \sum_{j=1}^p \beta_i A_{ij} \beta_j \right) = 2 \sum_{i=1}^p A_{ik} \beta_i = 2(A\beta)_k.
\]

This is the element \(k\) of the vector \(2A\beta\). Since this is true
for any index \(k\), we can write the gradient as

\[
\nabla_{\beta}(\beta^T A \beta) = 2A\beta.
\]

Now that we have the derivatives of all three terms, we can write the
gradient of \(L\):

\[
\frac{\partial L}{\partial \beta} = 0 - 2X^Ty + 2X^TX\beta = 0.
\]

Rearranging\ldots{}

\[
X^TX\beta = X^Ty
\]

\ldots and solving for \(\beta\):

\[
\beta = (X^TX)^{-1}X^Ty
\]

\section{discussion}\label{discussion}

Using two completely different approaches, we arrived at the same result
for the least squares solution:

\[
\beta = (X^TX)^{-1}X^Ty
\].

\begin{itemize}
\tightlist
\item
  \textbf{Approach 1}: We used the orthogonality condition, which states
  that the residual vector is orthogonal to the column space of the
  design matrix.
\item
  \textbf{Approach 2}: We applied the optimization method, minimizing
  the sum of squared errors---which corresponds to minimizing the
  squared length of the residual vector.
\end{itemize}

There is a deep connection here. The requirement that the residual
vector is orthogonal to the column space of the design matrix is
equivalent to minimizing the sum of squared errors. We can see this
visually: if the projection of the response vector \(y\) onto the column
space of \(X\) were anywhere else, the residual vector would be not only
not orthogonal, but also longer!

\[
\text{orthogonality} \iff \text{optimization}
\]

This result even tranfers to other contexts, as long as there is a
vector space with a well defined inner product (dot product) and an
orthogonal basis. In these cases, the least squares solution can be
interpreted as finding the projection of a vector onto a subspace
spanned by an orthogonal basis. Some examples include:

\begin{itemize}
\tightlist
\item
  Fourier series: the Fourier coefficients are the least squares
  solution to the problem of approximating a function by a sum of sines
  and cosines, where these functions are an orthogonal basis.
\item
  SVD (Singular Value Decomposition): A matrix can be decomposed into
  orthogonal matrices, and the singular values can be interpreted as the
  least squares solution to the problem of approximating a matrix by a
  sum of outer products of orthogonal vectors.
\end{itemize}

\section{other properties}\label{other-properties}

\subsection{the sum of residuals is
zero}\label{the-sum-of-residuals-is-zero}

As long as the model includes an intercept term, the sum of the
residuals is zero. The model could be anything, not necessarily linear
regression. Let's prove this property.

\[
\hat{y}_i = \beta_0 + f(x_i; \beta_1, \beta_2, \ldots, \beta_p)
\]

The Ordinary Least Squares (OLS) estimates of the parameters \(\beta\)
minimize the sum of squared residuals \(L\). The equation for
\(\beta_0\) reads:

\begin{align*}
\frac{\partial L}{\partial \beta_0} &=
\frac{\partial}{\partial \beta_0} \sum_{i=1}^n \left(y_i - \hat{y}_i \right)^2 \\
&= 
\frac{\partial}{\partial \beta_0} \sum_{i=1}^n \left[y_i - \beta_0 - f(x_i; \beta_1, \beta_2, \ldots, \beta_p) \right]^2 \\
&= -2 \sum_{i=1}^n \left[y_i - \beta_0 - f(x_i; \beta_1, \beta_2, \ldots, \beta_p) \right] \\
&= -2 \sum_{i=1}^n \left(y_i - \hat{y}_i \right) = 0
\end{align*}

From the last line it follows that the sum of the residuals is zero.

\chapter{partitioning of the sum of
squares}\label{partitioning-of-the-sum-of-squares}

One of the most important equations in regression analysis is the
partitioning of the sum of squares (SS).

\[
SS_\text{Total} = SS_\text{Model} + SS_\text{Error}
\]

This equation states that the total variability in the response variable
can be partitioned into two components: the variability explained by the
regression model and the variability that is not explained by the model
(the residuals).

In a more precise mathematical language, the equation states that:

\[
\sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

where:

\begin{itemize}
\tightlist
\item
  \(y_i\) are the observed values,
\item
  \(\hat{y}_i\) are the predicted values from the regression model,
\item
  \(\bar{y}\) is the mean of the observed values,
\item
  \(n\) is the number of observations.
\end{itemize}

\section{high-dimensional Pythagorean
theorem}\label{high-dimensional-pythagorean-theorem}

Why is this equation true? It's easiest to understand this equation by
thinking of it as a high-dimensional version of the Pythagorean theorem:

\[
\lVert T \rVert ^2 = \lVert M \rVert ^2 + \lVert E \rVert ^2,
\]

where:

\begin{itemize}
\tightlist
\item
  \(T = y - \bar{y}\) is the total vector, the vector of deviations of
  the observed values from their mean;
\item
  \(M = \hat{y} - \bar{y}\) is the model vector, the vector of
  deviations of the predicted values from the mean of the observed
  values;
\item
  \(E = y - \hat{y}\) is the error vector, the vector of residuals, or
  deviations of the observed values from the predicted values.
\end{itemize}

I omitted the subscript \(i\) to emphasize that these are vectors, not
scalars.

Make sure you understand the figure below, already presented in a
previous chapter.

\pandocbounded{\includegraphics[keepaspectratio]{regression/3dplot_A.png}}

The vector \(r=E\) in black is the residual or error vector. It is
orthogonal to the subspace spanned by the column vector of the design
matrix, represented in this image by the blue plane.

The vector \(M\) is not shown, but it necessarily lies in the blue
plane. How do we know that? Because the predicted values \(\hat{y}\) are
a linear combination of the columns of the design matrix, and therefore
\(\hat{y}\) lies in the column space of the design matrix. Since
\(\bar{y}\) is a constant vector (a multiple of the vector of ones), it
also lies in the column space of the design matrix. Therefore,
\(M = \hat{y} - \bar{y}\) lies in the column space of the design matrix.

From the above, we can already conclude that \(E\) is orthogonal to
\(M\). These are the two legs of a right triangle. We now need a
hypotenuse. The hypotenuse is the total vector \(T = y - \bar{y}\),
which is the sum of the other two vectors:

\[
T = M + E.
\]

This is easy to see by substituting the definitions of \(M\) and \(E\):

\[
T = y - \bar{y} = (\hat{y} - \bar{y}) + (y - \hat{y}) = M + E.
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{, adjustable}\OperatorTok{=}\StringTok{\textquotesingle{}box\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.annotate(}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, width}\OperatorTok{=}\DecValTok{2}\NormalTok{, headwidth}\OperatorTok{=}\DecValTok{10}\NormalTok{, headlength}\OperatorTok{=}\DecValTok{10}\NormalTok{))}
\NormalTok{ax.annotate(}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{), arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, width}\OperatorTok{=}\DecValTok{2}\NormalTok{, headwidth}\OperatorTok{=}\DecValTok{10}\NormalTok{, headlength}\OperatorTok{=}\DecValTok{10}\NormalTok{))}
\NormalTok{ax.annotate(}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, width}\OperatorTok{=}\DecValTok{2}\NormalTok{, headwidth}\OperatorTok{=}\DecValTok{10}\NormalTok{, headlength}\OperatorTok{=}\DecValTok{10}\NormalTok{))}

\NormalTok{ax.text(}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.0}\NormalTok{, }\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{20}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{2.03}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{20}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.55}\NormalTok{, }\StringTok{\textquotesingle{}T\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{20}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_xticks([])}
\NormalTok{ax.set\_yticks([])}
\NormalTok{ax.set\_frame\_on(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xlim(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{ax.set\_ylim(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/partitioning_files/figure-pdf/cell-2-output-1.png}}

\chapter{linear mixed effect model}\label{linear-mixed-effect-model}

A mixed effect model is an expansion of the ordinary linear regression
model that includes both fixed effects and random effects. The fixed
effects are the same as in a standard linear regression (could be with
or without interactions), while the random effects account for
variability across different groups or clusters in the data.

\section{practical example}\label{practical-example}

We are given a dataset of annual income (independent variable) and years
of education (independent variable) for individuals that studied
different majors in university (categorical variable). We want to
predict the annual income based on years of education and the major
studied, including an interaction term between years of education and
major. One more thing: each individual appears more than once in the
dataset, so we can assume that there is a random effect associated with
each individual.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{42}\NormalTok{)}
\CommentTok{\# define parameters}
\NormalTok{majors }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}Juggling\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Magic\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Dragon Taming\textquotesingle{}}\NormalTok{]}
\NormalTok{n\_individuals }\OperatorTok{=} \DecValTok{90}  \CommentTok{\# 30 per major}
\NormalTok{years\_per\_person }\OperatorTok{=}\NormalTok{ np.random.randint(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, size}\OperatorTok{=}\NormalTok{n\_individuals)  }\CommentTok{\# 1 to 4 time points}

\CommentTok{\# assign majors and person IDs}
\NormalTok{person\_ids }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f\textquotesingle{}P}\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{:03d\}}\SpecialStringTok{\textquotesingle{}} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n\_individuals)]}
\NormalTok{major\_assignment }\OperatorTok{=}\NormalTok{ np.repeat(majors, n\_individuals }\OperatorTok{//} \BuiltInTok{len}\NormalTok{(majors))}

\CommentTok{\# simulate data}
\NormalTok{records }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i, pid }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(person\_ids):}
\NormalTok{    major }\OperatorTok{=}\NormalTok{ major\_assignment[i]}
\NormalTok{    n\_years }\OperatorTok{=}\NormalTok{ years\_per\_person[i]}
\NormalTok{    years }\OperatorTok{=}\NormalTok{ np.sort(np.random.choice(np.arange(}\DecValTok{1}\NormalTok{, }\DecValTok{21}\NormalTok{), size}\OperatorTok{=}\NormalTok{n\_years, replace}\OperatorTok{=}\VariableTok{False}\NormalTok{))}
    
    \CommentTok{\# base intercept and slope by major}
    \ControlFlowTok{if}\NormalTok{ major }\OperatorTok{==} \StringTok{\textquotesingle{}Juggling\textquotesingle{}}\NormalTok{:}
\NormalTok{        base\_income }\OperatorTok{=} \DecValTok{25\_000}
\NormalTok{        growth }\OperatorTok{=} \DecValTok{800}
    \ControlFlowTok{elif}\NormalTok{ major }\OperatorTok{==} \StringTok{\textquotesingle{}Magic\textquotesingle{}}\NormalTok{:}
\NormalTok{        base\_income }\OperatorTok{=} \DecValTok{20\_000}
\NormalTok{        growth }\OperatorTok{=} \DecValTok{1500}
    \ControlFlowTok{elif}\NormalTok{ major }\OperatorTok{==} \StringTok{\textquotesingle{}Dragon Taming\textquotesingle{}}\NormalTok{:}
\NormalTok{        base\_income }\OperatorTok{=} \DecValTok{30\_000}
\NormalTok{        growth }\OperatorTok{=} \DecValTok{400}  \CommentTok{\# slower growth}
    
    \CommentTok{\# add person{-}specific deviation}
\NormalTok{    personal\_offset }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5000}\NormalTok{)}
\NormalTok{    slope\_offset }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{200}\NormalTok{)}
    
    \ControlFlowTok{for}\NormalTok{ y }\KeywordTok{in}\NormalTok{ years:}
\NormalTok{        income }\OperatorTok{=}\NormalTok{ base\_income }\OperatorTok{+}\NormalTok{ personal\_offset }\OperatorTok{+}\NormalTok{ (growth }\OperatorTok{+}\NormalTok{ slope\_offset) }\OperatorTok{*}\NormalTok{ y }\OperatorTok{+}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{3000}\NormalTok{)}
\NormalTok{        records.append(\{}
            \StringTok{\textquotesingle{}person\textquotesingle{}}\NormalTok{: pid,}
            \StringTok{\textquotesingle{}major\textquotesingle{}}\NormalTok{: major,}
            \StringTok{\textquotesingle{}years\_after\_grad\textquotesingle{}}\NormalTok{: y,}
            \StringTok{\textquotesingle{}income\textquotesingle{}}\NormalTok{: income}
\NormalTok{        \})}

\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(records)}
\end{Highlighting}
\end{Shaded}

Let's take a look at the dataset. There are many data points, so we will
only see 15 points in three different places.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(df[:}\DecValTok{5}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(df[}\DecValTok{90}\NormalTok{:}\DecValTok{95}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(df[}\DecValTok{190}\NormalTok{:}\DecValTok{195}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  person     major  years_after_grad        income
0   P001  Juggling                 3  37183.719609
1   P001  Juggling                 5  35238.112407
2   P001  Juggling                11  37905.435001
3   P002  Juggling                 2  27432.186391
4   P002  Juggling                 4  30617.926804
   person  major  years_after_grad        income
90   P034  Magic                 1  14151.072305
91   P034  Magic                 7  19716.656861
92   P035  Magic                12  41056.576643
93   P035  Magic                14  46339.987229
94   P036  Magic                16  41981.131518
    person          major  years_after_grad        income
190   P072  Dragon Taming                 7  36173.437735
191   P073  Dragon Taming                 8  33450.564557
192   P074  Dragon Taming                 9  35276.927416
193   P074  Dragon Taming                17  37271.203018
194   P075  Dragon Taming                 2  31819.051946
\end{verbatim}

Now let's see the data in a plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{gb }\OperatorTok{=}\NormalTok{ df.groupby(}\StringTok{\textquotesingle{}major\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ major, group }\KeywordTok{in}\NormalTok{ gb:}
\NormalTok{    ax.scatter(group[}\StringTok{\textquotesingle{}years\_after\_grad\textquotesingle{}}\NormalTok{], group[}\StringTok{\textquotesingle{}income\textquotesingle{}}\NormalTok{], label}\OperatorTok{=}\NormalTok{major, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{)}

\NormalTok{ax.legend(title}\OperatorTok{=}\StringTok{\textquotesingle{}Major\textquotesingle{}}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}years after graduation\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}income\textquotesingle{}}\NormalTok{,}
\NormalTok{       xticks}\OperatorTok{=}\NormalTok{np.arange(}\DecValTok{0}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/mixed-model_files/figure-pdf/cell-5-output-1.png}}

The model we will use is

\[
y = \underbrace{X \beta}_{\text{fixed effects}} + \underbrace{Z b}_{\text{random effects}} + \underbrace{\varepsilon}_{\text{residuals}}
\]

The only new term here is \(Zb\), the random effects, where \(Z\) is the
design matrix for the random effects and \(b\) is the vector of random
effects coefficients. We will discuss that a bit later. Let's start with
the fixed effects part:

\[
X \beta = \beta_0 + \beta_1 \cdot \text{years} + \beta_2 \cdot \text{major} + \beta_3 \cdot (\text{years} \cdot \text{major})
\]

The ``years'' variable is continuous, while the ``major'' variable is
categorical. How to include categorical variables in a linear regression
model? We can use dummy coding, where we create binary variables for
each category of the categorical variable (except one category, which
serves as the reference group). In our case, we have three majors:
Juggling, Magic, and Dragon Taming. Let's use ``Juggling'' as the
reference group. We can create two dummy variables that function as
toggles.

\begin{itemize}
\tightlist
\item
  \texttt{major\_Magic}: 1 if the major is Magic, 0 otherwise
\item
  \texttt{major\_DragonTaming}: 1 if the major is Dragon Taming, 0
  otherwise
\end{itemize}

\section{visualizing categories as
toggles}\label{visualizing-categories-as-toggles}

In the equation above, we have only one parameter for ``major''
(\(\beta_2\)), and only one parameter for the interaction terms
(\(\beta_3\)). In reality we have more, see:

\begin{align*}
\text{income} &= \beta_0 + \beta_1 \cdot \text{years} \\
&+ \beta_2 \cdot \text{major\_Magic} + \beta_3 \cdot \text{major\_DragonTaming} \\
&+ \beta_4 \cdot (\text{years} \cdot \text{major\_Magic}) + \beta_5 \cdot (\text{years} \cdot \text{major\_DragonTaming}) \\
&+ \epsilon
\end{align*}

The first line represents the linear relationship between income and
education of the reference group (Juggling). The second line adds the
effects on the intercept of having studied Magic or Dragon Taming
instead, and the third line adds the the effects on the slope of these
two majors.

Let's see for a few data points how this works. Below, dummy variables
represent the pair (major\_Magic, major\_DragonTaming).

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
years\_after\_grad & major & Dummy variables & income \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
3 & Juggling & (0, 0) & 37183.72 \\
5 & Magic & (1, 0) & 35101.07 \\
7 & Dragon Taming & (0, 1) & 27179.77 \\
10 & Juggling & (0, 0) & 26366.80 \\
12 & Magic & (1, 0) & 26101.53 \\
16 & Dragon Taming & (0, 1) & 39252.76 \\
\end{longtable}

The design matrix \(X\) would look like this:

\[
X = 
\begin{array}{c}
  \begin{array}{cccccc}
    \beta_0 & \beta_1 & \beta_2 & \beta_3 & \beta_4 & \beta_5
  \end{array} \\
  \begin{pmatrix}
    1 & 3 & 0 & 0 & 0 & 0 \\
    1 & 5 & 1 & 0 & 5 & 0 \\
    1 & 7 & 0 & 1 & 0 & 7 \\
    1 & 10 & 0 & 0 & 0 & 0 \\
    1 & 12 & 1 & 0 & 12 & 0 \\
    1 & 16 & 0 & 1 & 0 & 16
  \end{pmatrix}
\end{array}.
\]

The betas above the matrix are there just to label the columns, they are
not really part of the matrix. The 3rd and 4th columns are the dummy
variables for the majors, and the 5th and 6th columns are the
interaction terms between education and the majors.

If we were not interested in the random effects, we could stop here, and
just use the ordinary least squares (OLS) method already discussed to
estimate the coefficients \(\beta\).

\section{random effects}\label{random-effects}

The name ``mixed effects'' comes from the fact that we have both fixed
effects and random effects.

Conceptually, the random effects function in a very similar way to the
fixed effects. Instead of a small number of categories, now each person
in the dataset is a category. In our example we have 90 different people
represented in the dataset, so the quantity \(Z\) in \(Zb\) is the
design matrix for the random effects, which is a matrix with 90 columns,
one for each person, and as many rows as there are data points in the
dataset. Each row has a 1 in the column corresponding to the person, and
0s elsewhere. The vector \(b\) is a vector of random effects
coefficients, one for each person.

\section{implementation}\label{implementation}

We can use \texttt{statsmodels} function \texttt{smf.mixedlm} to do
everything for us. We just need to specify the formula, which includes
the interaction term, and the data.

If you don't mind which category is the reference group, you can skip
the cell below. If you want to make sure a give one is the reference
group (Juggling in our case), then you should run it.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pandas.api.types }\ImportTok{import}\NormalTok{ CategoricalDtype}
\CommentTok{\# define the desired order: Juggling as reference}
\NormalTok{major\_order }\OperatorTok{=}\NormalTok{ CategoricalDtype(categories}\OperatorTok{=}\NormalTok{[}\StringTok{"Juggling"}\NormalTok{, }\StringTok{"Magic"}\NormalTok{, }\StringTok{"Dragon Taming"}\NormalTok{], ordered}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df[}\StringTok{"major"}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[}\StringTok{"major"}\NormalTok{].astype(major\_order)}
\end{Highlighting}
\end{Shaded}

The syntax is fairly economic. The formula

\texttt{income\ \textasciitilde{}\ years\_after\_grad\ *\ major}

specifies a linear model where both the baseline income (intercept) and
the effect of time since graduation (slope) can vary by major. The *
operator includes both the main effects (years after graduation and
major) and their interaction, allowing the model to fit a different
intercept and slope for each major.

In the line

\texttt{model\ =\ smf.mixedlm(formula,\ data=df,\ groups=df{[}"person"{]})}

the \texttt{groups} argument specifies that the random effects are
associated with the ``person'' variable, meaning that each person can
have their own random intercept.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# formula with interaction}
\NormalTok{formula }\OperatorTok{=} \StringTok{"income \textasciitilde{} years\_after\_grad * major"}

\CommentTok{\# fit mixed model with random intercept for person}
\NormalTok{model }\OperatorTok{=}\NormalTok{ smf.mixedlm(formula, data}\OperatorTok{=}\NormalTok{df, groups}\OperatorTok{=}\NormalTok{df[}\StringTok{"person"}\NormalTok{])}
\NormalTok{result }\OperatorTok{=}\NormalTok{ model.fit()}
\end{Highlighting}
\end{Shaded}

Let's see the results

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(result.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                            Mixed Linear Model Regression Results
==============================================================================================
Model:                         MixedLM            Dependent Variable:            income       
No. Observations:              239                Method:                        REML         
No. Groups:                    90                 Scale:                         10690821.7105
Min. group size:               1                  Log-Likelihood:                -2327.5068   
Max. group size:               4                  Converged:                     Yes          
Mean group size:               2.7                                                            
----------------------------------------------------------------------------------------------
                                           Coef.     Std.Err.   z    P>|z|   [0.025    0.975] 
----------------------------------------------------------------------------------------------
Intercept                                  25206.095 1349.760 18.675 0.000 22560.615 27851.575
major[T.Magic]                             -2999.754 1995.748 -1.503 0.133 -6911.348   911.840
major[T.Dragon Taming]                      5579.198 1954.661  2.854 0.004  1748.133  9410.263
years_after_grad                             723.745   72.028 10.048 0.000   582.573   864.917
years_after_grad:major[T.Magic]              635.180  109.599  5.795 0.000   420.370   849.989
years_after_grad:major[T.Dragon Taming]     -295.862  106.315 -2.783 0.005  -504.235   -87.488
Group Var                               33814137.626 2268.953                                 
==============================================================================================
\end{verbatim}

\section{interpreting the results}\label{interpreting-the-results}

To interpret the coefficients, start with the reference group, which in
this model is someone who studied Juggling. Their predicted income is:

\[
\text{income} = 25206.10 + 723.75 \times \text{years}
\]

Now, for a person who studied Magic, the model \textbf{adjusts} both the
intercept and the slope:

Intercept shift: -2999.75 Slope shift: +635.18 So for Magic, the
predicted income becomes:

\begin{align*}
\text{income} &= (25206.10 - 2999.75) + (723.75 + 635.18) \times \text{years} \\
       &= 22206.35 + 1358.93 \times \text{years}
\end{align*}

This means that compared to Jugglers, Magicians start with a lower
baseline salary, but their income grows much faster with each year after
graduation.

The \texttt{Coef.} column shows the estimated value of each parameter
(e.g., intercepts, slopes, interactions). The \texttt{Std.Err.} column
reports the standard error of the estimate, reflecting its uncertainty.
The \texttt{z} column is the test statistic (estimate divided by
standard error), and \texttt{P\textgreater{}\textbar{}z\textbar{}} gives
the p-value, which helps assess whether the effect is statistically
significant. The final two columns, \texttt{{[}0.025\ and\ 0.975{]}},
show the 95\% confidence interval for the coefficient --- if this
interval does not include zero, the effect is likely meaningful.

The line labeled \texttt{Group\ Var} shows the estimated variance of the
random intercepts --- in this case, variation in baseline income between
individuals. The second number reported is the standard error associated
with this estimate, which indicates how much uncertainty there is in the
estimate of the variance.

If you like, you can print out all the variances for the random effects.
They are not explicity shown in the summary, but you can access them
through the model's \texttt{random\_effects} attribute:

\texttt{result.random\_effects}

Finally, the model as is does not include random slopes, meaning that
the effect of years after graduation is assumed to be the same for all
individuals. If you want to allow for different slopes for each
individual, you can modify the model to include random slopes as well.
This would require changing the formula and the \texttt{groups} argument
accordingly. Also, \texttt{result.random\_effects} will then contain not
only the random intercepts, but also the random slopes for each
individual.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ smf.mixedlm(}
    \StringTok{"income \textasciitilde{} years\_after\_grad * major"}\NormalTok{,}
\NormalTok{    data}\OperatorTok{=}\NormalTok{df,}
\NormalTok{    groups}\OperatorTok{=}\NormalTok{df[}\StringTok{"person"}\NormalTok{],}
\NormalTok{    re\_formula}\OperatorTok{=}\StringTok{"\textasciitilde{}years\_after\_grad"}
\NormalTok{)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ model.fit()}
\BuiltInTok{print}\NormalTok{(result.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  warnings.warn("Maximum Likelihood optimization failed to "
/Users/yairmau/miniforge3/envs/olympus/lib/python3.11/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs
  warnings.warn(
\end{verbatim}

\begin{verbatim}
                            Mixed Linear Model Regression Results
==============================================================================================
Model:                         MixedLM            Dependent Variable:            income       
No. Observations:              239                Method:                        REML         
No. Groups:                    90                 Scale:                         10125672.1682
Min. group size:               1                  Log-Likelihood:                -2323.7559   
Max. group size:               4                  Converged:                     Yes          
Mean group size:               2.7                                                            
----------------------------------------------------------------------------------------------
                                           Coef.     Std.Err.   z    P>|z|   [0.025    0.975] 
----------------------------------------------------------------------------------------------
Intercept                                  25133.841 1208.050 20.805 0.000 22766.106 27501.576
major[T.Magic]                             -2805.540 1811.051 -1.549 0.121 -6355.135   744.055
major[T.Dragon Taming]                      5980.367 1767.166  3.384 0.001  2516.786  9443.949
years_after_grad                             731.399   84.211  8.685 0.000   566.349   896.450
years_after_grad:major[T.Magic]              611.065  126.072  4.847 0.000   363.969   858.161
years_after_grad:major[T.Dragon Taming]     -329.530  122.977 -2.680 0.007  -570.561   -88.498
Group Var                               22392488.656 1835.422                                 
Group x years_after_grad Cov               90328.607   75.664                                 
years_after_grad Var                       39074.487    7.401                                 
==============================================================================================
\end{verbatim}

\section{back to OLS}\label{back-to-ols}

If you went this far, and now realized you don't care about random
effects, you can just use the \texttt{statsmodels} function
\texttt{smf.ols} to fit an ordinary least squares regression model. The
syntax is similar, but without the \texttt{groups} argument.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}

\CommentTok{\# formula with main effects and interaction}
\NormalTok{formula }\OperatorTok{=} \StringTok{"income \textasciitilde{} years\_after\_grad * major"}

\CommentTok{\# fit the model with OLS (no random effects)}
\NormalTok{ols\_model }\OperatorTok{=}\NormalTok{ smf.ols(formula, data}\OperatorTok{=}\NormalTok{df)}
\NormalTok{ols\_result }\OperatorTok{=}\NormalTok{ ols\_model.fit()}

\CommentTok{\# print summary}
\BuiltInTok{print}\NormalTok{(ols\_result.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 income   R-squared:                       0.455
Model:                            OLS   Adj. R-squared:                  0.443
Method:                 Least Squares   F-statistic:                     38.85
Date:                Tue, 24 Jun 2025   Prob (F-statistic):           6.27e-29
Time:                        16:16:38   Log-Likelihood:                -2437.0
No. Observations:                 239   AIC:                             4886.
Df Residuals:                     233   BIC:                             4907.
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
===========================================================================================================
                                              coef    std err          t      P>|t|      [0.025      0.975]
-----------------------------------------------------------------------------------------------------------
Intercept                                2.486e+04   1450.267     17.141      0.000     2.2e+04    2.77e+04
major[T.Magic]                          -4402.0846   2281.475     -1.929      0.055   -8897.041      92.872
major[T.Dragon Taming]                   7696.8705   2167.061      3.552      0.000    3427.332     1.2e+04
years_after_grad                          778.4674    123.280      6.315      0.000     535.582    1021.352
years_after_grad:major[T.Magic]           758.4393    185.397      4.091      0.000     393.170    1123.708
years_after_grad:major[T.Dragon Taming]  -510.1096    183.456     -2.781      0.006    -871.553    -148.666
==============================================================================
Omnibus:                        2.143   Durbin-Watson:                   1.088
Prob(Omnibus):                  0.343   Jarque-Bera (JB):                2.132
Skew:                           0.176   Prob(JB):                        0.344
Kurtosis:                       2.699   Cond. No.                         93.1
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{verbatim}

\chapter{logistic regression}\label{logistic-regression}

\section{question}\label{question-4}

We are given a list of heights for men and women. Given one more data
point (180 cm), could we assign a probability that it belongs to either
class?

\section{discriminative model}\label{discriminative-model}

The idea behind the logistic regression is to find a boundary between
our two classes (here men and women). The logistic regression models the
probability of a class given a data point, i.e.~\(P(y|x)\). We can use
the logistic function to model this probability:

\[P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}\]

where \(y\) is the class (0=women, 1=men), \(x\) is the data point
(height), and \(\beta_0\) and \(\beta_1\) are the parameters of the
model.

Our goal is to find the best s-shaped curve that describes the data.
This is done by finding the parameters \(\beta_0\) and \(\beta_1\) that
maximize the likelihood of the data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{20.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{mu\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}

\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{150}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{200}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)  }\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\CommentTok{\# pandas dataframe with the two samples in it}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{: np.concatenate([sample\_boys, sample\_girls]),}
    \StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{: [}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{] }\OperatorTok{*}\NormalTok{ N\_boys }\OperatorTok{+}\NormalTok{ [}\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{] }\OperatorTok{*}\NormalTok{ N\_girls}
\NormalTok{\})}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.sample(frac}\OperatorTok{=}\DecValTok{1}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{314}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& height (cm) & sex \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 178.558416 & M \\
1 & 173.334306 & M \\
2 & 183.084154 & M \\
3 & 178.236047 & F \\
4 & 175.868642 & M \\
... & ... & ... \\
345 & 177.387837 & M \\
346 & 157.122325 & F \\
347 & 166.891746 & F \\
348 & 181.090312 & M \\
349 & 171.479631 & M \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{].values.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(\{}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{: }\DecValTok{0}\NormalTok{\}).values}
\NormalTok{log\_reg\_2 }\OperatorTok{=}\NormalTok{ LogisticRegression(penalty}\OperatorTok{=}\VariableTok{None}\NormalTok{, solver }\OperatorTok{=} \StringTok{\textquotesingle{}newton{-}cg\textquotesingle{}}\NormalTok{, max\_iter}\OperatorTok{=} \DecValTok{150}\NormalTok{).fit(X,y)}
\NormalTok{beta1 }\OperatorTok{=}\NormalTok{ log\_reg\_2.coef\_[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}
\NormalTok{beta0 }\OperatorTok{=}\NormalTok{ log\_reg\_2.intercept\_[}\DecValTok{0}\NormalTok{]}
\KeywordTok{def}\NormalTok{ logistic\_function(x, beta0, beta1):}
    \ControlFlowTok{return} \DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{(beta0 }\OperatorTok{+}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ x)))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.plot(sample\_girls, np.zeros\_like(sample\_girls),}
\NormalTok{        linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{,}
\NormalTok{        markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(sample\_boys, np.ones\_like(sample\_boys),}
\NormalTok{        linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{,}
\NormalTok{        markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}

\NormalTok{x\_array }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{140}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y\_proba }\OperatorTok{=}\NormalTok{ log\_reg\_2.predict\_proba(x\_array)[:, }\DecValTok{1}\NormalTok{]}
\CommentTok{\# ax.plot(x\_array, y\_proba, color=\textquotesingle{}black\textquotesingle{})}
\NormalTok{ax.plot(x\_array, logistic\_function(x\_array, beta0, beta1), color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"best fit"}\NormalTok{)}
\NormalTok{ax.plot(x\_array, logistic\_function(x\_array, beta0}\OperatorTok{+}\DecValTok{2}\NormalTok{, beta1), color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"worse 1"}\NormalTok{)}
\NormalTok{ax.plot(x\_array, logistic\_function(x\_array, beta0, beta1}\OperatorTok{{-}}\FloatTok{0.02}\NormalTok{), color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"worse 2"}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/logistic-regression_files/figure-pdf/cell-5-output-1.png}}

\section{likelihood}\label{likelihood}

How do we know the parameters of the \textbf{best} s-shaped curve? Let's
pretend we have only three data points:

\begin{itemize}
\tightlist
\item
  Man, 180 cm. Data point \((180,1)\).
\item
  Man, 170 cm. Data point \((170,1)\).
\item
  Woman, 165 cm. Data point \((165,0)\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x3 }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{180.0}\NormalTok{, }\FloatTok{170.0}\NormalTok{, }\FloatTok{165.0}\NormalTok{])}
\NormalTok{y3 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{log\_reg\_3 }\OperatorTok{=}\NormalTok{ LogisticRegression(penalty}\OperatorTok{=}\VariableTok{None}\NormalTok{, solver }\OperatorTok{=} \StringTok{\textquotesingle{}newton{-}cg\textquotesingle{}}\NormalTok{, max\_iter}\OperatorTok{=} \DecValTok{150}\NormalTok{).fit(x3.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),y3)}
\NormalTok{beta1 }\OperatorTok{=}\NormalTok{ log\_reg\_3.coef\_[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}
\NormalTok{beta0 }\OperatorTok{=}\NormalTok{ log\_reg\_3.intercept\_[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{, sharey}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{x\_array }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{140}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{)}

\NormalTok{ya }\OperatorTok{=}\NormalTok{ logistic\_function(x\_array, beta0, beta1)}
\NormalTok{yb }\OperatorTok{=}\NormalTok{ logistic\_function(x\_array, beta0}\OperatorTok{+}\DecValTok{344}\NormalTok{, beta1}\OperatorTok{*}\FloatTok{0.28}\NormalTok{)}
\NormalTok{yc }\OperatorTok{=}\NormalTok{ logistic\_function(x\_array, beta0}\OperatorTok{+}\DecValTok{450}\NormalTok{, beta1}\OperatorTok{*}\FloatTok{0.06}\NormalTok{)}
\NormalTok{yhat3a }\OperatorTok{=}\NormalTok{ logistic\_function(x3, beta0, beta1)}
\NormalTok{yhat3b }\OperatorTok{=}\NormalTok{ logistic\_function(x3, beta0}\OperatorTok{+}\DecValTok{344}\NormalTok{, beta1}\OperatorTok{*}\FloatTok{0.28}\NormalTok{)}
\NormalTok{yhat3c }\OperatorTok{=}\NormalTok{ logistic\_function(x3, beta0}\OperatorTok{+}\DecValTok{450}\NormalTok{, beta1}\OperatorTok{*}\FloatTok{0.06}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ axi }\KeywordTok{in}\NormalTok{ ax:}
\NormalTok{    axi.plot(x3, y3,}
\NormalTok{            linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{,}
\NormalTok{            markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(x\_array, ya, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(x\_array, yb, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot(x\_array, yc, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x3)):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot([x3[i], x3[i]], [y3[i], yhat3a[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].plot([x3[i], x3[i]], [y3[i], yhat3b[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{2}\NormalTok{].plot([x3[i], x3[i]], [y3[i], yhat3c[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{          ylabel}\OperatorTok{=}\StringTok{"P"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\StringTok{"best fit"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\StringTok{"worse 1"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{            title}\OperatorTok{=}\StringTok{"worse 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/logistic-regression_files/figure-pdf/cell-7-output-1.png}}

As usual, our task in performing the regression is to find the
parameters \(\beta_0\) and \(\beta_1\) that minimize the distance
between the model and the data (the residual). See the figure above, we
plotted the same three data points, and in each panel we see a different
s-shaped curve (black) and the distance between the model and the data
(red lines).

{[}Note: this time, because we have only three data points, the best fit
gave us an extremely sharp logistic function, that neatly discriminates
between the data points. In the first example, the function was much
more ``shallow'', because of the overlap between the Men and Women
datasets.{]}

In the logistic regression, instead of \textbf{minimizing the residual},
we \textbf{maximize the likelihood} of the data given the model
parameters. The likelihood is the complement of the residual, see the
thick red bars in the figure below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{, sharey}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ axi }\KeywordTok{in}\NormalTok{ ax:}
\NormalTok{    axi.plot(x3, y3,}
\NormalTok{            linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{,}
\NormalTok{            markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(x\_array, ya, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(x\_array, yb, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot(x\_array, yc, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x3)):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot([x3[i], x3[i]], [y3[i], yhat3a[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot([x3[i], x3[i]], [}\KeywordTok{not} \BuiltInTok{bool}\NormalTok{(y3[i]), yhat3a[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].plot([x3[i], x3[i]], [y3[i], yhat3b[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].plot([x3[i], x3[i]], [}\KeywordTok{not} \BuiltInTok{bool}\NormalTok{(y3[i]), yhat3b[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{2}\NormalTok{].plot([x3[i], x3[i]], [y3[i], yhat3c[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{2}\NormalTok{].plot([x3[i], x3[i]], [}\KeywordTok{not} \BuiltInTok{bool}\NormalTok{(y3[i]), yhat3c[i]], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot([], [], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, label}\OperatorTok{=}\StringTok{"residual"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot([], [], color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, label}\OperatorTok{=}\StringTok{"likelihood"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{          ylabel}\OperatorTok{=}\StringTok{"P"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\StringTok{"best fit"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\StringTok{"worse 1"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{            title}\OperatorTok{=}\StringTok{"worse 2"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{, loc}\OperatorTok{=}\StringTok{\textquotesingle{}center left\textquotesingle{}}\NormalTok{, bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{), fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/logistic-regression_files/figure-pdf/cell-8-output-1.png}}

What is the probability that we would measure the observed data, given
the model parameters? For a single data point (height, class), the
length of the red bars can be described by the formula below:

\[
L(y_i|P_i) = P_i^{y_i} (1-P_i)^{1-y_i}.
\]

For instance, if the data point corresponds to a man (\(y_i=1\)), we
have \(L=P_i\). The likelihood (thick red bars) for men is just the
value of the logistic function for that value of \(x\). For women
(\(y_i=0\)), we have \(L=1-P_i\). The likelihood for women is just the
complement (one minus) of the logistic function.

Assuming that each data point is independent, the likelihood of the
entire dataset is the product of the likelihoods of each data point:

\[
L(\beta_0, \beta_1) = \prod_{i=1}^{N} P_i^{y_i} (1-P_i)^{1-y_i}.
\]

In the example below, the likelihood of the entire dataset for each
panel is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{La }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{Lb }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{Lc }\OperatorTok{=} \FloatTok{1.0}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(x3)):}
\NormalTok{    La }\OperatorTok{=}\NormalTok{ La }\OperatorTok{*}\NormalTok{ yhat3a[i]}\OperatorTok{**}\NormalTok{y3[i] }\OperatorTok{*}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{yhat3a[i])}\OperatorTok{**}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{y3[i])}
\NormalTok{    Lb }\OperatorTok{=}\NormalTok{ Lb }\OperatorTok{*}\NormalTok{ yhat3b[i]}\OperatorTok{**}\NormalTok{y3[i] }\OperatorTok{*}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{yhat3b[i])}\OperatorTok{**}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{y3[i])}
\NormalTok{    Lc }\OperatorTok{=}\NormalTok{ Lc }\OperatorTok{*}\NormalTok{ yhat3c[i]}\OperatorTok{**}\NormalTok{y3[i] }\OperatorTok{*}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{yhat3c[i])}\OperatorTok{**}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{y3[i])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Likelihood for best fit: "}\NormalTok{, La)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Likelihood for worse 1: "}\NormalTok{, Lb)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Likelihood for worse 2: "}\NormalTok{, Lc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Likelihood for best fit:  0.9984099891897481
Likelihood for worse 1:  0.7711068593851899
Likelihood for worse 2:  0.3173593316343797
\end{verbatim}

As we increase the number of data points, the likelihood becomes very
small (because we are multiplying many numbers between 0 and 1). To
avoid numerical issues, we usually work with the log-likelihood.

\section{log-likelihood}\label{log-likelihood}

The log-likelihood is the logarithm of the likelihood:

\[
\ell(\beta_0, \beta_1) = \log L(\beta_0, \beta_1) = \sum_{i=1}^{N} P_i^{y_i} (1-P_i)^{1-y_i}
\]

Using the properties of logarithms, we can rewrite the log-likelihood as
follows:

\[
\ell(\beta_0, \beta_1) = \sum_{i=1}^{N} \left( y_i \log P_i + (1-y_i) \log (1-P_i) \right).
\]

\section{binary cross-entropy, or log
loss}\label{binary-cross-entropy-or-log-loss}

We can use gradient descent to find the parameters that maximize the
log-likelihood. Most implementations of gradient descent are designed to
minimize a cost function. Therefore, instead of maximizing the
log-likelihood, we can minimize the negative log-likelihood:

\[
J(\beta_0, \beta_1) = -\ell(\beta_0, \beta_1) = -\sum_{i=1}^{N} \left( y_i \log P_i + (1-y_i) \log (1-P_i) \right).
\] This cost function is also known as binary cross-entropy or log loss.

It turns out that taking the log of the likelihood is very convenient.
What was before only a trick to avoid numerical issues, now has a nice
interpretation. The cross-entropy can be thought of as a measure of
``surprise''. The more the model is surprised by the data, the higher
the cross-entropy, and the poorer the fit. The less surprised the model
is by the data, the lower the cross-entropy, and the better the fit.

\section{wrapping up}\label{wrapping-up}

From the provided data:

\begin{itemize}
\tightlist
\item
  What is the probability that a person whose height is 180 cm is a man?
\item
  If we had to choose one height to discriminate between men and women,
  what would it be?
\end{itemize}

Let's run the code for the logistic regression again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{].values.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(\{}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{: }\DecValTok{0}\NormalTok{\}).values}
\NormalTok{log\_reg\_2 }\OperatorTok{=}\NormalTok{ LogisticRegression(penalty}\OperatorTok{=}\VariableTok{None}\NormalTok{, solver }\OperatorTok{=} \StringTok{\textquotesingle{}newton{-}cg\textquotesingle{}}\NormalTok{, max\_iter}\OperatorTok{=} \DecValTok{150}\NormalTok{).fit(X,y)}
\NormalTok{beta1 }\OperatorTok{=}\NormalTok{ log\_reg\_2.coef\_[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}
\NormalTok{beta0 }\OperatorTok{=}\NormalTok{ log\_reg\_2.intercept\_[}\DecValTok{0}\NormalTok{]}

\CommentTok{\#| code{-}summary: "plot " }
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.plot(sample\_girls, np.zeros\_like(sample\_girls),}
\NormalTok{        linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{,}
\NormalTok{        markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(sample\_boys, np.ones\_like(sample\_boys),}
\NormalTok{        linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{,}
\NormalTok{        markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}

\NormalTok{x\_array }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{140}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y\_proba }\OperatorTok{=}\NormalTok{ log\_reg\_2.predict\_proba(x\_array)[:, }\DecValTok{1}\NormalTok{]}
\CommentTok{\# ax.plot(x\_array, y\_proba, color=\textquotesingle{}black\textquotesingle{})}
\NormalTok{ax.plot(x\_array, logistic\_function(x\_array, beta0, beta1), color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"best fit"}\NormalTok{)}

\NormalTok{h }\OperatorTok{=} \FloatTok{180.0}
\NormalTok{p180 }\OperatorTok{=}\NormalTok{ log\_reg\_2.predict\_proba(np.array([[h]]))[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{ax.plot([h, h], [}\DecValTok{0}\NormalTok{, p180], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot([np.}\BuiltInTok{min}\NormalTok{(x\_array), h], [p180, p180], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(h}\OperatorTok{+}\DecValTok{1}\NormalTok{, p180}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, }\SpecialStringTok{f"P(}\SpecialCharTok{\{}\NormalTok{h}\SpecialCharTok{\}}\SpecialStringTok{ cm)=}\SpecialCharTok{\{}\NormalTok{p180}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\NormalTok{p\_50percent }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{beta0 }\OperatorTok{/}\NormalTok{ beta1}
\NormalTok{ax.plot([p\_50percent, p\_50percent], [}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot([np.}\BuiltInTok{min}\NormalTok{(x\_array), p\_50percent], [}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(p\_50percent}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\OperatorTok{+}\FloatTok{0.05}\NormalTok{, }\SpecialStringTok{f"P(}\SpecialCharTok{\{}\NormalTok{p\_50percent}\SpecialCharTok{:.0f\}}\SpecialStringTok{ cm)=0.5"}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{)}


\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{140}\NormalTok{, }\DecValTok{200}\NormalTok{),}
\NormalTok{       xlabel}\OperatorTok{=}\StringTok{"height (cm)"}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{"P"}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/logistic-regression_files/figure-pdf/cell-10-output-1.png}}

Answers:

\begin{itemize}
\tightlist
\item
  The probability that a person 180 cm tall is a man is 92\%.
\item
  The height that best discriminates between men (above) and women
  (below) is 171 cm.
\end{itemize}

This last result follows directly from:

\begin{align*}
P(x) = \frac{1}{1+\exp[-(\beta_0+\beta_1 x)]} &= \frac{1}{2} \\
& \text{therefore} \\
1+\exp[-(\beta_0+\beta_1 x)] &= 2 \\
\exp[-(\beta_0+\beta_1 x)] &= 1 \\
-(\beta_0+\beta_1 x) &= 0 \\
x &= -\frac{\beta_0}{\beta_1}
\end{align*}

Compare this result with the one we obtained with the
\href{./bayes/parametric-generative-classification.html}{parametric
generative model} discussed in the Bayes' theorem section.

If you want to see a nice tutorial, see
\href{https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae/}{Dr.~Roi
Yehoshua's ``Mastering Logistic Regression''}.

\section{connection to neural
networks}\label{connection-to-neural-networks}

The logistic regression can be understood as a single-layer perceptron
neural network model. This is to say, a neural network with no hidden
layers, and a single output neuron that uses the logistic (sigmoid)
activation function.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{archive/images/logistic-NN.jpeg}}

}

\caption{``source:
https://towardsdatascience.com/mastering-logistic-regression-3e502686f0ae/''}

\end{figure}%

\chapter{logistic 2d}\label{logistic-2d}

The figure below represents the distribution of height and weight for
boys aged 10 and 13 years old.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ scipy\_power\_normal\_draw\_random(N, age, sex):   }
    \CommentTok{\# Load LMS parameters from the CSV file}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/weight/wtage.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{    agemos }\OperatorTok{=}\NormalTok{ age }\OperatorTok{*} \DecValTok{12} \OperatorTok{+} \FloatTok{0.5}
\NormalTok{    df }\OperatorTok{=}\NormalTok{ df[(df[}\StringTok{\textquotesingle{}Agemos\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ agemos) }\OperatorTok{\&}\NormalTok{ (df[}\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{] }\OperatorTok{==}\NormalTok{ sex)]}
\NormalTok{    L }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}L\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    M }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}
\NormalTok{    S }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}S\textquotesingle{}}\NormalTok{].values[}\DecValTok{0}\NormalTok{]}

    \CommentTok{\# draw random z from standard normal distribution}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ np.random.normal(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, N)}
    \CommentTok{\# transform z to w}
\NormalTok{    w }\OperatorTok{=}\NormalTok{ M }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ L }\OperatorTok{*}\NormalTok{ S }\OperatorTok{*}\NormalTok{ z)}\OperatorTok{**}\NormalTok{(}\DecValTok{1} \OperatorTok{/}\NormalTok{ L)}
    
    \ControlFlowTok{return}\NormalTok{ w}

\CommentTok{\# 10{-}year{-}old boys}
\NormalTok{N\_10 }\OperatorTok{=} \DecValTok{120}
\NormalTok{w\_10 }\OperatorTok{=}\NormalTok{ scipy\_power\_normal\_draw\_random(N\_10, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{df\_height\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{mu\_boys\_10 }\OperatorTok{=}\NormalTok{ df\_height\_boys.loc[}\FloatTok{10.0}\NormalTok{, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys\_10 }\OperatorTok{=}\NormalTok{ df\_height\_boys.loc[}\FloatTok{10.0}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{h\_10 }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_10, loc}\OperatorTok{=}\NormalTok{mu\_boys\_10, scale}\OperatorTok{=}\NormalTok{sigma\_boys\_10)}

\CommentTok{\# 15{-}year{-}old boys}
\NormalTok{N\_13 }\OperatorTok{=} \DecValTok{80}
\NormalTok{w\_13 }\OperatorTok{=}\NormalTok{ scipy\_power\_normal\_draw\_random(N\_13, }\DecValTok{13}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mu\_boys\_13 }\OperatorTok{=}\NormalTok{ df\_height\_boys.loc[}\FloatTok{13.0}\NormalTok{, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys\_13 }\OperatorTok{=}\NormalTok{ df\_height\_boys.loc[}\FloatTok{13.0}\NormalTok{, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{h\_13 }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_13, loc}\OperatorTok{=}\NormalTok{mu\_boys\_13, scale}\OperatorTok{=}\NormalTok{sigma\_boys\_13)}

\CommentTok{\# Combine data into a single DataFrame}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{: np.concatenate([w\_10, w\_13]),}
    \StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{: np.concatenate([h\_10, h\_13]),}
    \StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{: [}\DecValTok{10}\NormalTok{] }\OperatorTok{*}\NormalTok{ N\_10 }\OperatorTok{+}\NormalTok{ [}\DecValTok{13}\NormalTok{] }\OperatorTok{*}\NormalTok{ N\_13}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{sns.scatterplot(data}\OperatorTok{=}\NormalTok{df, x}\OperatorTok{=}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, hue}\OperatorTok{=}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{ax.legend(title}\OperatorTok{=}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, loc}\OperatorTok{=}\StringTok{\textquotesingle{}lower right\textquotesingle{}}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}weight (kg)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{regression/logistic_2d_files/figure-pdf/cell-4-output-1.png}}

Our job is to find a decision boundary that separates the two classes.
Fundamentally, this is the same as the logistic regression we saw in the
previous chapter, but now we have two features instead of one. There are
two ways to equivalent ways to describe this, the statisics and the
machine learning way.

\section{statistics}\label{statistics}

We call our two features, height and weight, \(x_1\) and \(x_2\). We can
write the logistic regression model as

\[
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]

where \(p\) is the probability of being 13 years old. The left hand side
is called the log-odds or logit. The right hand side is a linear
combination of the features.

This is, of course, equivalent to the expression with the sigmoid
function:

\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}}
\]

Speaking a ``statistics language'', this linear relationship is
expressed by writing everything in matrix form: \[
z = X\beta,
\] where \(X\) is the design matrix, \(\beta\) is the vector of
coefficients, and \(z\) is the linear predictor.

\[
\begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{pmatrix} = \begin{pmatrix} 1 & x_{11} & x_{12} \\ 1 & x_{21} & x_{22} \\ \vdots & \vdots & \vdots \\ 1 & x_{n1} & x_{n2} \end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{pmatrix} = \begin{pmatrix} \beta_0 + \beta_1x_{11} + \beta_2x_{12} \\ \beta_0 + \beta_1x_{21} + \beta_2x_{22} \\ \vdots \\ \beta_0 + \beta_1x_{n1} + \beta_2x_{n2} \end{pmatrix}
\]

\section{machine learning}\label{machine-learning}

In machine learning, we often call the intercept term the bias, and we
call the coefficients weights. We can write the linear predictor as \[
z = w_1 x_1 + w_2 x_2 + b
\] where \(w_1\) and \(w_2\) are the weights, and \(b\) is the bias. In
matrix form: \[
z = w^T x + b,
\] which expands to \[
\begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_n \end{pmatrix} = \begin{pmatrix} w_1 & w_2 \end{pmatrix} \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \\ \vdots & \vdots \\ x_{n1} & x_{n2} \end{pmatrix} + \begin{pmatrix} b \\ b \\ \vdots \\ b \end{pmatrix} = \begin{pmatrix} w_1x_{11} + w_2x_{12} + b \\ w_1x_{21} + w_2x_{22} + b \\ \vdots \\ w_1x_{n1} + w_2x_{n2} + b \end{pmatrix}
\]

This is the same as the statistics formulation, just with different
names for the parameters.

\section{solving}\label{solving}

I boy from either 7th grade (13 years old) or 5th grade (10 years old)
is randomly selected. Given his height (150 cm) and weight (45 kg), we
want to predict his age group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{X }\OperatorTok{=}\NormalTok{ df[[}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{]]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{]}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegression()}
\NormalTok{model.fit(X, y)}
\NormalTok{xx, yy }\OperatorTok{=}\NormalTok{ np.meshgrid(np.linspace(}\DecValTok{10}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{100}\NormalTok{), np.linspace(}\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\NormalTok{grid\_points }\OperatorTok{=}\NormalTok{ np.c\_[xx.ravel(), yy.ravel()]}
\NormalTok{predict\_df }\OperatorTok{=}\NormalTok{ pd.DataFrame(grid\_points, columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{])}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ model.predict(predict\_df)}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ Z.reshape(xx.shape)}
\NormalTok{Z\_prob }\OperatorTok{=}\NormalTok{ model.predict\_proba(predict\_df)[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ Z\_prob.reshape(xx.shape)}
\CommentTok{\# contour\_fill = ax.contourf(xx, yy, Z, levels=np.arange(0, 1.1, 0.2), cmap=\textquotesingle{}RdBu\_r\textquotesingle{}, alpha=0.8)}
\NormalTok{cont }\OperatorTok{=}\NormalTok{ ax.contour(xx, yy, Z, levels}\OperatorTok{=}\NormalTok{[}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{], colors}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{], linestyles}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{], linewidths}\OperatorTok{=}\NormalTok{[}\FloatTok{0.8}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.8}\NormalTok{])}
\NormalTok{ax.clabel(cont, fmt}\OperatorTok{=}\StringTok{\textquotesingle{}p=}\SpecialCharTok{\%1.1f}\StringTok{\textquotesingle{}}\NormalTok{, inline}\OperatorTok{=}\VariableTok{True}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{sns.scatterplot(data}\OperatorTok{=}\NormalTok{df, x}\OperatorTok{=}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, hue}\OperatorTok{=}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}weight (kg)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{)}\OperatorTok{;}

\NormalTok{h1 }\OperatorTok{=} \DecValTok{150}
\NormalTok{w1 }\OperatorTok{=} \DecValTok{45}
\NormalTok{df1 }\OperatorTok{=}\NormalTok{ pd.DataFrame([[w1, h1]], columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{])}
\NormalTok{ax.plot([w1], [h1], ls}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{10}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{"new data point"}\NormalTok{)}
\NormalTok{ax.legend(title}\OperatorTok{=}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper right\textquotesingle{}}\NormalTok{, frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{p1 }\OperatorTok{=}\NormalTok{ model.predict\_proba(df1)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predicted probability of being 13 years old: }\SpecialCharTok{\{}\NormalTok{p1}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Predicted probability of being 13 years old: 0.848
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{regression/logistic_2d_files/figure-pdf/cell-5-output-2.png}}

The thick line in the figure above is the decision boundary, where the
probability of being 13 years old is 0.5. The equation of the line is

\[
0 =  w_1 x_1 + w_2 x_2 + b,
\]

were feature \(x_1\) is weight and feature \(x_2\) is height. The
weights \(w_1,w_2\) and bias \(b\) are

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.coef\_, model.intercept\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([[0.19047627, 0.37131303]]), array([-62.54777199]))
\end{verbatim}

\part{correlation}

\chapter{correlation}\label{correlation-1}

\section{variance}\label{variance}

To understand correlation, we need to start with variance. Let's say
\(X\) is a random variable with mean \(\mu\). The variance of \(X\),
denoted \(\text{var}(X)=\sigma^2\), is defined as the expected value of
the squared deviation from the mean:

\[
\sigma^2 = \text{var}(X) = E[(X - \mu)^2] = \frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2.
\]

I should point out that the formula above is for the \emph{population}
variance. If we are working with a \emph{sample}, we would use \(N-1\)
in the denominator instead of \(N\) to get an unbiased estimate of the
population variance. Also, the mean and variance for the sample are
denoted \(\bar{X}\) and \(s^2\) respectively. In any case, let's
continue with the population variance for simplicity.

In simple words, the variance measures how much the values of \(X\)
deviate from the mean \(\mu\). A high variance indicates that the data
points are spread out over a wider range of values, while a low variance
indicates that they are closer to the mean.

\section{covariance}\label{covariance}

Now, let's consider two random variables, \(X\) and \(Y\), with means
\(\mu_X\) and \(\mu_Y\). The covariance between \(X\) and \(Y\), denoted
\(\text{cov}(X, Y)\), is defined as:

\[
\text{cov}(X, Y) = E[(X - \mu_X)(Y - \mu_Y)] = \frac{1}{N} \sum_{i=1}^{N} (X_i - \mu_X)(Y_i - \mu_Y).
\]

A high covariance indicates that when \(X\) is above its mean, \(Y\)
tends to be above its mean as well (and vice versa). A low (or negative)
covariance indicates that when \(X\) is above its mean, \(Y\) tends to
be below its mean.

\section{correlation}\label{correlation-2}

The covariance can be any value, making it difficult to interpret. To
standardize the measure, we use the correlation coefficient, denoted
\(\rho\) (for population) or \(r\) (for sample). The correlation
coefficient is defined as:

\begin{align*}
\rho_{X,Y} &= \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y} \\
           &= E\left[\frac{(X - \mu_X)}{\sigma_X}\frac{(Y - \mu_Y)}{\sigma_Y}\right] \\
           &= \frac{1}{N} \sum_{i=1}^{N} \frac{X_i - \mu_X}{\sigma_x}\frac{Y_i - \mu_Y}{\sigma_Y}.
\end{align*}

where \(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\)
and \(Y\), respectively.

Something becomes clear now. If we calculate the correlation of \(X\)
with itself, we get:

\[
\rho_{X,X} = \frac{\text{cov}(X, X)}{\sigma_X \sigma_X} = \frac{\text{var}(X)}{\sigma_X^2} = 1.
\]

The highest possible correlation is 1, which indicates a perfect
positive linear relationship between the two variables. The lowest
possible correlation is -1, which indicates a perfect negative linear
relationship. A correlation of 0 indicates no linear relationship
between the variables.

\section{Pearson correlation
coefficient}\label{pearson-correlation-coefficient}

When we say ``correlation'', we usually mean the Pearson correlation
coefficient, which is the formula given above. Pearson invented this, so
it's named after him. There are other types of correlation coefficients,
such as Spearman's rank correlation coefficient and Kendall's tau
coefficient, which are used for non-parametric data or ordinal data.

\section{covariance of z-scored
variables}\label{covariance-of-z-scored-variables}

Notice that the correlation formula can be interpreted as the covariance
of the z-scored variables. The z-score of a variable \(X\) is defined
as:

\[
Z_X = \frac{X - \mu_X}{\sigma_X}
\]

Thus, the correlation can be rewritten as:

\[
\rho_{X,Y} = \text{cov}(Z_X, Z_Y) = \frac{1}{N} \sum_{i=1}^{N} Z_{X_i} Z_{Y_i}
\]

It is quite easy to compute the correlation on the computer. If \(X\)
and \(Y\) are two arrays, first we standardize (z-score) them, then we
compute their dot product (sum of piecewise multiplication), and finally
we divide by \(N\) (or \(N-1\) for sample correlation).

\section{linearity}\label{linearity}

The Pearson correlation coefficient measures the strength and direction
of a \textbf{linear} relationship between two variables. It does not
capture non-linear relationships. For example, if \(Y = X^2\), the
correlation between \(X\) and \(Y\) may be low or even zero, despite a
clear non-linear relationship. When we say ``correlation'', it is
usually implicit that we are referring to linear correlation.

\chapter{correlation and linear
regression}\label{correlation-and-linear-regression}

The correlation coefficient is closely related to linear regression. We
will see below a few instances of this relationship.

\section{prelude: finding the intercept and
slope}\label{prelude-finding-the-intercept-and-slope}

Let's derive the formulas for the intercept and slope of the regression
line. We want to minimize the sum of squared residuals \(L\):

\[
L = \sum_{i=1}^n (y_i - \hat{y}_i)^2,
\tag{1}
\]

where

\[
\hat{y}_i = \beta_0 + \beta_1 x_i.
\tag{2}
\]

To find the optimal values of \(\beta_0\) and \(\beta_1\), we take the
partial derivatives of \(L\) with respect to \(\beta_0\) and
\(\beta_1\), set them to zero, and solve the resulting equations.

\begin{align*}
\frac{\partial L}{\partial \beta_0} &= 0 \tag{3a}\\
\frac{\partial L}{\partial \beta_1} &= 0 \tag{3b}
\end{align*}

\href{./regression/equivalence.html\#optimization}{We already did that
for a general case}, but the calculation had the variables in
vector/matrix form. Here we will do it for the simple case of one
predictor variable, so that we can see the relationship with correlation
more clearly.

\subsection{intercept}\label{intercept}

Let's start with Eq. (3a), and substitute into it Eq. (2):

\begin{align*}
\frac{\partial L}{\partial \beta_0} &= \frac{\partial}{\partial \beta_0} (y_i - \hat{y}_i)^2 \tag{4a} \\
&= \frac{\partial}{\partial \beta_0} (y_i - \beta_0 - \beta_1 x_i)^2 \tag{4b} \\
&= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) = 0 \tag{4c}
\end{align*}

Eliminating the constant factor \(-2\) and expanding the summation, we
get:

\[
\sum_{i=1}^n y_i - n \beta_0 - \beta_1 \sum_{i=1}^n x_i = 0
\tag{5}
\]

We now divide by \(n\) and rearrange to isolate \(\beta_0\):

\[
\beta_0 = \bar{y} - \beta_1 \bar{x}
\tag{6}
\]

Note: we can rewrite equation (4c) as

\[
\sum_{i=1}^n (y_i - \hat{y}_i) = \sum_{i=1}^n \text{residuals} = 0,
\]

which is a nice thing to know.

\subsection{slope}\label{slope}

Now let's move on to Eq. (3b), and substitute the result of Eq. (6) into
it:

\begin{align*}
\frac{\partial L}{\partial \beta_1} &= \frac{\partial}{\partial \beta_1} (y_i - \hat{y}_i)^2 \tag{7a} \\
&= \frac{\partial}{\partial \beta_1} (y_i - \beta_0 - \beta_1 x_i)^2 \tag{7b} \\
&= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) x_i = 0 \tag{7c} \\
&= -2 \sum_{i=1}^n (y_i - \bar{y} + \beta_1 \bar{x} - \beta_1 x_i) x_i = 0 \tag{7d}
\end{align*}

Eliminating the constant factor \(2\) and expanding the summation, we
get:

\[
-\sum_{i=1}^n x_i y_i + \sum_{i=1}^n x_i \bar{y} - \beta_1 \sum_{i=1}^n x_i \bar{x} + \beta_1 \sum_{i=1}^n x_i^2 = 0
\tag{8}
\]

Let's group the terms involving \(\beta_1\) on one side and the rest on
the other side:

\[
\beta_1 \left( \sum_{i=1}^n x_i^2 - \sum_{i=1}^n x_i \bar{x} \right) = \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \bar{y}
\tag{9}
\]

Isolating \(\beta_1\), we have:

\[
\beta_1 = \frac{\sum x_i y_i - \sum x_i \bar{y}}{\sum x_i^2 - \sum x_i \bar{x}} = \frac{\text{numerator}}{\text{denominator}}
\tag{10}
\]

It's easier to interpret the numerator and denominator separately. To
each we will add and subtract a term that will allow us to express them
in simpler forms.

Numerator:

\[
\text{numerator} = \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \bar{y} + \sum_{i=1}^n \bar{x} y_i - \sum_{i=1}^n \bar{x} y_i
\tag{11}
\]

We express the third term thus:

\[
\text{third term} = \sum_{i=1}^n \bar{x} y_i = \bar{x} \sum_{i=1}^n y_i = n \bar{x} \bar{y} = \sum_{i=1}^n \bar{x} \bar{y}
\tag{12}
\]

The numerator now becomes:

\begin{align*}
\text{numerator} &= \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \bar{y} + \sum_{i=1}^n \bar{x} \bar{y} - \sum_{i=1}^n \bar{x} y_i \tag{13a} \\
&= \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \tag{13b} \\
\end{align*}

Now the denominator:

\[
\text{denominator} = \sum_{i=1}^n x_i^2 - \sum_{i=1}^n x_i \bar{x} + \sum_{i=1}^n x_i\bar{x} - \sum_{i=1}^n x_i\bar{x}
\tag{14}
\]

We group the second and fourth terms, and express the third term thus:

\[
\text{third term} = \sum_{i=1}^n x_i \bar{x} = \bar{x} \sum_{i=1}^n x_i = n \bar{x}^2 = \sum_{i=1}^n \bar{x}^2
\tag{15}
\]

The denominator now becomes:

\begin{align*}
\text{denominator} &= \sum_{i=1}^n x_i^2 - 2 \sum_{i=1}^n x_i \bar{x} + \sum_{i=1}^n \bar{x}^2 \tag{16a} \\
&= \sum_{i=1}^n (x_i - \bar{x})^2 \tag{16b}
\end{align*}

Putting it all together, we have:

\[
\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\tag{17}
\]

\section{slope and correlation}\label{slope-and-correlation}

Let's divide both the numerator and denominator of Eq. (17) by \(n-1\):

\[
\beta_1 = \frac{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2}
\tag{18}
\]

The numerator is the sample covariance \(\text{cov}(X, Y)\), and the
denominator is the sample variance \(\text{var}(X)\):

\[
\beta_1 = \frac{\text{cov}(X, Y)}{\text{var}(X)}
\tag{19}
\]

Now, we can express the covariance in terms of the correlation
coefficient \(\rho_{X,Y}\) and the standard deviations \(\sigma_X\) and
\(\sigma_Y\) (see
\href{./correlation/correlation.html\#correlation}{here}):

\[
\text{cov}(X, Y) = \rho_{X,Y} \sigma_X \sigma_Y
\tag{20}
\] Substituting Eq. (20) into Eq. (19), we get:

\[
\beta_1 = \frac{\rho_{X,Y} \sigma_X \sigma_Y}{\sigma_X^2}
\tag{21}
\]

And finally, we have:

\[
\beta_1 = \rho_{X,Y} \frac{\sigma_Y}{\sigma_X}
\tag{22}
\]

This shows that the slope of the regression line is directly
proportional to the correlation coefficient. A higher absolute value of
the correlation coefficient indicates a steeper slope, while a lower
absolute value indicates a flatter slope.

\section{\texorpdfstring{\(R^2=\) square of the correlation coefficient
\(r\)}{R\^{}2= square of the correlation coefficient r}}\label{r2-square-of-the-correlation-coefficient-r}

Let's start by saying that the correlation coefficient is called
\(\rho\) when referring to the population, and \(r\) when referring to a
sample. I'm playing loose with this convention here, but I hope it's
clear from the context.

Let's show now that the coefficient of determination \(R^2\) is equal to
the square of the correlation coefficient \(r\).

We start with the definition of \(R^2\):

\[
R^2 = 1 - \frac{\text{SS}_{\text{Error}}}{\text{SS}_{\text{Total}}} = \frac{\text{SS}_{\text{Total}}+\text{SS}_{\text{Error}}}{\text{SS}_{\text{Total}}} =  \frac{\text{SS}_{\text{Model}}}{\text{SS}_{\text{Total}}}
\tag{23}
\]

where we used the fact that
\(\text{SS}_{\text{Total}} = \text{SS}_{\text{Model}} + \text{SS}_{\text{Error}}\),
\href{./regression/partitioning.html}{already seen before}.

We now substitute into Eq. (23) the definitions of
\(\text{SS}_{\text{Model}}\) and \(\text{SS}_{\text{Total}}\):

\[
R^2 = \frac{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\tag{24}
\]

We now substitute into Eq. (24) the expression of \(\hat{y}_i\) from Eq.
(2):

\[
R^2 = \frac{\sum_{i=1}^n (\beta_0 + \beta_1 x_i - \bar{y})^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\tag{25}
\]

Now we substitute into Eq. (25) the expression of \(\beta_0\) from Eq.
(6):

\[
R^2 = \frac{\sum_{i=1}^n \left( \bar{y} - \beta_1 \bar{x} + \beta_1 x_i - \bar{y} \right)^2}{\sum_{i=1}^n (y_i - \bar{y})^2} = \frac{\sum_{i=1}^n \left( \beta_1 (x_i - \bar{x}) \right)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\tag{26}
\]

\(\beta_1\) is a number, so we can take it out of the summation in the
numerator:

\[
R^2 = \frac{\beta_1^2 \sum_{i=1}^n (x_i - \bar{x})^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\tag{26b}
\]

Now, let's substitute into Eq. (26) the expression of \(\beta_1\) from
Eq. (17):

\[
R^2 = \frac{\left( \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \right)^2 \sum_{i=1}^n (x_i - \bar{x})^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\tag{27}
\]

We can simplify Eq. (27) by canceling one instance of the denominator in
the squared term with the factor outside the squared term:

\[
R^2 = \frac{\left[ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \right]^2}{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n (y_i - \bar{y})^2}
\tag{28}
\]

Now, let's multiply both the numerator and denominator of Eq. (28) by
\(\frac{1}{(n-1)^2}\):

\[  
R^2 = \frac{\left[ \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \right]^2}{\left[ \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2 \right] \left[ \frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y})^2 \right]}
\tag{29}
\]

The numerator is the square of the sample covariance
\(\text{cov}(X, Y)\), and the denominator is the product of the sample
variances \(\text{var}(X)=\sigma_X^2\) and \(\text{var}(Y)=\sigma_Y^2\):

\[
R^2 = \frac{\text{cov}(X, Y)^2}{\sigma_X^2 \sigma_Y^2} = \left( \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y} \right)^2
\tag{30}
\]

This is exactly the square of the correlation coefficient \(r\) (see
\href{./correlation/correlation.html\#correlation}{here}):

\[
R^2 = r^2
\tag{31}
\]

\chapter{cosine}\label{cosine}

In the spirit of this website, beautiful things happen when we imagine
data in high dimensional spaces. Let's do that for the correlation
between two vectors.

\[
\rho(x,y) = \frac{1}{N} \sum_{i=1}^N \left(\frac{x_i - \bar{x}}{\sigma_x}\right)\left(\frac{y_i - \bar{y}}{\sigma_y}\right)
\tag{1}
\]

As we
\href{./correlation/correlation.html\#covariance-of-z-scored-variables}{saw
before}, it is particularly useful to rewrite this formula in terms of
z-scored variables:

\[
\rho(x,y) = \frac{1}{N} \sum_{i=1}^N z_{x_i} z_{y_i}
\tag{2}
\]

where \(z_{x_i} = \frac{x_i - \bar{x}}{\sigma_x}\) and
\(z_{y_i} = \frac{y_i - \bar{y}}{\sigma_y}\).

Now let's see the formula for the dot product of two vectors \(z_x\) and
\(z_y\):

\[
z_x \cdot z_y = \sum_{i=1}^N z_{x_i} z_{y_i} = \frac{1}{N} \sum_{i=1}^N z_{x_i} z_{y_i} \cdot N = \rho(x,y) \cdot N
\tag{3}
\]

There is another formula for the dot product that involves the angle
\(\theta\) between the two vectors:

\[
z_x \cdot z_y = \lVert z_x \rVert \, \lVert z_y \rVert \cos(\theta)
\tag{4}
\]

where \(||z_x||\) and \(||z_y||\) are the magnitudes (or lengths) of the
vectors \(z_x\) and \(z_y\).

The magnitude squared of a z-scored vector is:

\begin{align*}
\lVert z_x \rVert ^2 &= \sum_{i=1}^N z_{x_i}^2 \\
                     &= \sum_{i=1}^N \left(\frac{x_i - \bar{x}}{\sigma_x}\right)^2 \\
                     &= \frac{1}{\sigma_x^2} \sum_{i=1}^N (x_i - \bar{x})^2 \\
                     &= \frac{1}{\sigma_x^2} \left( \frac{1}{N} \sum_{i=1}^N (x_i - \bar{x})^2 \right) N \\
                     & = N\frac{\sigma_x^2}{\sigma_x^2} \\
                     &= N \tag{5}
\end{align*}

Of course, the same goes for \(z_y\), so we have
\(\lVert z_x \rVert = \lVert z_y \rVert = \sqrt{N}\). Substituting this
into Eq. (4) gives:

\[
z_x \cdot z_y = \sqrt{N} \cdot \sqrt{N} \cdot \cos(\theta) = N \cos(\theta)
\tag{6}
\]

Finally, we equate Eqs. (3) and (6):

\[
\rho(x,y) = \cos(\theta)
\tag{7}
\]

The correlation between two variables is equal to the cosine of the
angle between their corresponding z-scored vectors, in a
high-dimensional space.

\begin{itemize}
\tightlist
\item
  \(\theta=0\), the vectors point in the same direction,
  \(\cos(\theta) = 1\), indicating a perfect positive correlation.
\item
  \(\theta=\pi/2\), the vectors are orthogonal, \(\cos(\theta) = 0\),
  indicating no correlation.
\item
  \(\theta=\pi\), the vectors point in opposite directions,
  \(\cos(\theta) = -1\), indicating a perfect negative correlation.
\end{itemize}

\section{cosine similarity}\label{cosine-similarity}

The cosine similarity is a measure of similarity between two non-zero
vectors. It is defined as:

\[
\text{cosine\_similarity}(x,y) = \frac{x \cdot y}{\lVert x \rVert \, \lVert y \rVert}
\tag{8}
\]

This measure is common in text analysis, where a non-zero element of a
vector represents the presence of a word in a document, and the value of
the element represents the frequency of that word. The cosine similarity
measures the cosine of the angle between two vectors, which indicates
how similar the two vectors are in terms of their direction, regardless
of their magnitude. If we were to z-score the vectors, the absence of a
word would be represented by a negative value (below average), which is
not useful in this context.

When the vectors are z-scored, the cosine similarity is identical to the
Pearson correlation coefficient.

\begin{itemize}
\tightlist
\item
  \textbf{cosine similarity}: works on any non-zero vectors, does not
  require z-scoring. There is no need to alter the reference point.
\item
  \textbf{Pearson correlation}: works on z-scored vectors, requires
  centering and scaling. The reference point is the mean of each
  variable. Useful when comparing variables with different units or
  scales.
\end{itemize}

\chapter{significance (p-value)}\label{significance-p-value}

Given a correlation coefficient \(r\), we can assess its significance
using a p-value. Let's formulate the hypotheses:

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (\(H_0\))}: There is no correlation between
  the two variables (i.e., \(r = 0\)).
\item
  \textbf{Alternative Hypothesis (\(H_a\))}: There is a correlation
  between the two variables (i.e., \(r \neq 0\)).
\end{itemize}

To calculate the p-value, we can use the following formula for the test
statistic \(t\):

\[
t = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}
\tag{1}
\] where \(n\) is the number of data points.

This formula follows the fundamental structure of a t-statistic:

\[
t = \frac{\text{Signal}}{\text{Noise}} = \frac{\text{Observed Statistic} - \text{Null Value}}{\text{Standard Error of the Statistic}}
\tag{2}
\]

Let's rearrange Eq. (1) to match the structure of Eq. (2):

\[
t = \frac{r - 0}{\sqrt{\frac{1 - r^2}{n - 2}}}
\tag{3}
\]

The numerator is clear enough. Let's discuss the denominator, which
represents the standard error of the correlation coefficient.

The term \(1 - r^2\) is the proportion of \textbf{unexplained variance}
in the data. As the correlation \(r\) gets stronger (closer to 1 or -1),
the unexplained variance gets smaller. This makes intuitive sense: a
very strong correlation is less likely to be a result of random chance,
so the standard error (noise) should be smaller.

The term \(n2\) is the degrees of freedom. As your sample size n
increases, the denominator gets larger, which makes the overall standard
error smaller. This also makes sense: a correlation found in a large
sample is more reliable and less likely to be a fluke than the same
correlation found in a small sample.

Let's try a concrete example.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ stats}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed for reproducibility}
\NormalTok{np.random.seed(}\DecValTok{1}\NormalTok{)}
\NormalTok{N }\OperatorTok{=} \DecValTok{100}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, N)}
\NormalTok{y }\OperatorTok{=} \FloatTok{0.2} \OperatorTok{*}\NormalTok{ x }\OperatorTok{+} \DecValTok{7}\OperatorTok{*}\NormalTok{np.random.normal(size}\OperatorTok{=}\NormalTok{x.size)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute sample z{-}scores of x, y}
\NormalTok{zx }\OperatorTok{=}\NormalTok{ (x }\OperatorTok{{-}}\NormalTok{ np.mean(x)) }\OperatorTok{/}\NormalTok{ np.std(x, ddof}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{zy }\OperatorTok{=}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ np.mean(y)) }\OperatorTok{/}\NormalTok{ np.std(y, ddof}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\CommentTok{\# compute Pearson correlation coefficient}
\NormalTok{rho }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(zx }\OperatorTok{*}\NormalTok{ zy) }\OperatorTok{/}\NormalTok{ N}
\CommentTok{\# compute t{-}statistic}
\NormalTok{t }\OperatorTok{=}\NormalTok{ rho }\OperatorTok{*}\NormalTok{ np.sqrt((N}\OperatorTok{{-}}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{rho}\OperatorTok{**}\DecValTok{2}\NormalTok{))}
\CommentTok{\# compute two{-}sided p{-}value}
\NormalTok{p }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ stats.t.cdf(np.}\BuiltInTok{abs}\NormalTok{(t), df}\OperatorTok{=}\NormalTok{N}\OperatorTok{{-}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}

\CommentTok{\# linear fit and R2 with scipy}
\NormalTok{slope, intercept, r\_value, p\_value, std\_err }\OperatorTok{=}\NormalTok{ stats.linregress(x, y)}
\NormalTok{ax.plot(x, slope}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\NormalTok{ intercept, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}linear regression, R=}\SpecialCharTok{\{}\NormalTok{r\_value}\OperatorTok{**}\DecValTok{2}\SpecialCharTok{:.2f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# print p{-}value}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}our p{-}value:   }\SpecialCharTok{\{}\NormalTok{p}\SpecialCharTok{:.4f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}scipy p{-}value: }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.4f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}

\CommentTok{\# compute correlation coefficients and their p{-}values}
\NormalTok{r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, y)[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\SpecialStringTok{f"Pearson\textquotesingle{}s r = }\SpecialCharTok{\{}\NormalTok{rho}\SpecialCharTok{:.3f\}}\SpecialStringTok{, p{-}value = }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.4f\}}\SpecialStringTok{"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
our p-value:   0.0458
scipy p-value: 0.0458
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{correlation/significance_files/figure-pdf/cell-5-output-2.png}}

The linear regression accounts for 4\% of the variance in the data,
which corresponds to a correlation coefficient of \(r = 0.2\). This
correlation is statistically significant at \(p=0.0458<0.05\).

\part{bayes}

\chapter{Bayes' theorem from the ground
up}\label{bayes-theorem-from-the-ground-up}

\section{the scenario}\label{the-scenario}

Imagine we're in a room with a large group of people. We know the group
consists of men and women, and we have height measurements for everyone.
Someone walks in, we measure their height, but we don't know if they are
a man or a woman. Our goal is to figure out the probability that this
person is a man, given their height. For simplicity, let's say that
heights are categorized into three groups: short, medium, and tall. The
breakdown of the group is as follows:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& Short & Medium & Tall \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Man} & 15 & 30 & 20 \\
\textbf{Woman} & 25 & 35 & 10 \\
\end{longtable}

\section{joint and conditional
probabilities}\label{joint-and-conditional-probabilities}

\textbf{What is the probability that a person is both a man and tall?}

This is the same as asking: what fraction does the rectangle on the
bottom left have with respect to the whole area?

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}

\NormalTok{m1, m2, m3 }\OperatorTok{=} \DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{20}
\NormalTok{f1, f2, f3 }\OperatorTok{=} \DecValTok{25}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{10}

\NormalTok{m }\OperatorTok{=}\NormalTok{ m1 }\OperatorTok{+}\NormalTok{ m2 }\OperatorTok{+}\NormalTok{ m3}
\NormalTok{f }\OperatorTok{=}\NormalTok{ f1 }\OperatorTok{+}\NormalTok{ f2 }\OperatorTok{+}\NormalTok{ f3}
\NormalTok{total }\OperatorTok{=}\NormalTok{ m }\OperatorTok{+}\NormalTok{ f}
\NormalTok{h1 }\OperatorTok{=}\NormalTok{ m1 }\OperatorTok{+}\NormalTok{ f1}
\NormalTok{h2 }\OperatorTok{=}\NormalTok{ m2 }\OperatorTok{+}\NormalTok{ f2}
\NormalTok{h3 }\OperatorTok{=}\NormalTok{ m3 }\OperatorTok{+}\NormalTok{ f3}
\NormalTok{h }\OperatorTok{=}\NormalTok{ h1 }\OperatorTok{+}\NormalTok{ h2 }\OperatorTok{+}\NormalTok{ h3}

\NormalTok{p\_3 }\OperatorTok{=}\NormalTok{ h3 }\OperatorTok{/}\NormalTok{ total}
\NormalTok{p\_2 }\OperatorTok{=}\NormalTok{ h2 }\OperatorTok{/}\NormalTok{ total}
\NormalTok{p\_1 }\OperatorTok{=}\NormalTok{ h1 }\OperatorTok{/}\NormalTok{ total}
\NormalTok{p\_m }\OperatorTok{=}\NormalTok{ m }\OperatorTok{/}\NormalTok{ total}
\NormalTok{p\_f }\OperatorTok{=}\NormalTok{ f }\OperatorTok{/}\NormalTok{ total}

\NormalTok{p\_m\_given\_1 }\OperatorTok{=}\NormalTok{ m1 }\OperatorTok{/}\NormalTok{ h1}
\NormalTok{p\_f\_given\_1 }\OperatorTok{=}\NormalTok{ f1 }\OperatorTok{/}\NormalTok{ h1}
\NormalTok{p\_m\_given\_2 }\OperatorTok{=}\NormalTok{ m2 }\OperatorTok{/}\NormalTok{ h2}
\NormalTok{p\_f\_given\_2 }\OperatorTok{=}\NormalTok{ f2 }\OperatorTok{/}\NormalTok{ h2}
\NormalTok{p\_m\_given\_3 }\OperatorTok{=}\NormalTok{ m3 }\OperatorTok{/}\NormalTok{ h3}
\NormalTok{p\_f\_given\_3 }\OperatorTok{=}\NormalTok{ f3 }\OperatorTok{/}\NormalTok{ h3}

\NormalTok{p\_1\_given\_m }\OperatorTok{=}\NormalTok{ m1 }\OperatorTok{/}\NormalTok{ m}
\NormalTok{p\_2\_given\_m }\OperatorTok{=}\NormalTok{ m2 }\OperatorTok{/}\NormalTok{ m}
\NormalTok{p\_3\_given\_m }\OperatorTok{=}\NormalTok{ m3 }\OperatorTok{/}\NormalTok{ m}
\NormalTok{p\_1\_given\_f }\OperatorTok{=}\NormalTok{ f1 }\OperatorTok{/}\NormalTok{ f}
\NormalTok{p\_2\_given\_f }\OperatorTok{=}\NormalTok{ f2 }\OperatorTok{/}\NormalTok{ f}
\NormalTok{p\_3\_given\_f }\OperatorTok{=}\NormalTok{ f3 }\OperatorTok{/}\NormalTok{ f}

\CommentTok{\# tall shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], }\DecValTok{0}\NormalTok{, p\_3, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}
\CommentTok{\# medium shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], p\_3, p\_3 }\OperatorTok{+}\NormalTok{ p\_2, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}
\CommentTok{\# man given tall shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, p\_m\_given\_3], }\DecValTok{0}\NormalTok{, p\_3, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}
\CommentTok{\# man given medium shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, p\_m\_given\_2], p\_3, p\_3}\OperatorTok{+}\NormalTok{p\_2, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}
\CommentTok{\# man given short shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, p\_m\_given\_1], p\_3}\OperatorTok{+}\NormalTok{p\_2, }\FloatTok{1.0}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}


\NormalTok{sns.despine(ax}\OperatorTok{=}\NormalTok{ax, top}\OperatorTok{=}\VariableTok{True}\NormalTok{, right}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# tall arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.05}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.05}\NormalTok{, p\_3),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.05}\NormalTok{, p\_3 }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f" tall, }\SpecialCharTok{\{}\NormalTok{p\_3}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\CommentTok{\# medium arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.05}\NormalTok{, p\_3),}
\NormalTok{            (}\FloatTok{1.05}\NormalTok{, p\_2}\OperatorTok{+}\NormalTok{p\_3),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.05}\NormalTok{, p\_3 }\OperatorTok{+}\NormalTok{ (p\_2) }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f" medium, }\SpecialCharTok{\{}\NormalTok{p\_2}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\CommentTok{\# short arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.05}\NormalTok{, p\_2}\OperatorTok{+}\NormalTok{p\_3),}
\NormalTok{            (}\FloatTok{1.05}\NormalTok{, }\FloatTok{1.0}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.05}\NormalTok{, }\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_2 }\OperatorTok{{-}}\NormalTok{ p\_3) }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f" short, }\SpecialCharTok{\{}\FloatTok{1.0}\OperatorTok{{-}}\NormalTok{p\_2}\OperatorTok{{-}}\NormalTok{p\_3}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\CommentTok{\# man given short arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{),}
\NormalTok{            (p\_m\_given\_1, }\FloatTok{0.90}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m\_given\_1 }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{0.92}\NormalTok{, }\SpecialStringTok{f"man | short, }\SpecialCharTok{\{}\NormalTok{p\_m\_given\_1}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# woman given short arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_f\_given\_1, }\FloatTok{0.90}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.90}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ (p\_f\_given\_1) }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{0.92}\NormalTok{, }\SpecialStringTok{f"woman | short, }\SpecialCharTok{\{}\NormalTok{p\_f\_given\_1}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# man given medium arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\DecValTok{0}\NormalTok{, }\FloatTok{0.50}\NormalTok{),}
\NormalTok{            (p\_m\_given\_2, }\FloatTok{0.50}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m\_given\_2 }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{0.52}\NormalTok{, }\SpecialStringTok{f"man | medium, }\SpecialCharTok{\{}\NormalTok{p\_m\_given\_2}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# woman given medium arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_f\_given\_2, }\FloatTok{0.50}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.50}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ (p\_f\_given\_2) }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{0.52}\NormalTok{, }\SpecialStringTok{f"woman | medium, }\SpecialCharTok{\{}\NormalTok{p\_f\_given\_2}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# man given tall arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\DecValTok{0}\NormalTok{, }\FloatTok{0.15}\NormalTok{),}
\NormalTok{            (p\_m\_given\_3, }\FloatTok{0.15}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m\_given\_3 }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\SpecialStringTok{f"man | tall, }\SpecialCharTok{\{}\NormalTok{p\_m\_given\_3}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# woman given tall arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_f\_given\_3, }\FloatTok{0.15}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.0}\NormalTok{, }\FloatTok{0.15}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ (p\_f\_given\_3) }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\SpecialStringTok{f"woman | tall, }\SpecialCharTok{\{}\NormalTok{p\_f\_given\_3}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}


\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{       yticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{       xlabel}\OperatorTok{=}\StringTok{"sex"}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{"height"}\NormalTok{,)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/from-the-ground-up_files/figure-pdf/cell-3-output-1.png}}

\begin{itemize}
\tightlist
\item
  The numbers next to each category denote the proportions. That makes
  sense: according to the table, most of tall people are men, and most
  of the short people are women.
\item
  ``men \textbar{} tall'' is a short way to write ``men given tall''. In
  simple words, it is the fraction of men, given that we know the person
  is tall.
\end{itemize}

The answer to the question is obvious now. The probability that a person
is both \textbf{man} and \textbf{tall} the product of 0.22 with 0.67.

\begin{itemize}
\tightlist
\item
  22\% of people are tall.
\item
  67\% of those are men.
\end{itemize}

The answer is 0.22 * 0.67 = 0.1474, or about 15\%.

In mathematical notation, we write this as:

\[
P(\text{man } \cap \text{ tall}) = P(\text{tall}) \cdot P(\text{man|tall}),
\tag{1}
\]

where the symbol \(\cap\) means ``and''.

\begin{itemize}
\tightlist
\item
  P(\text{man } \cap \text{ tall}) is called the \textbf{joint
  probability}, because it describes the probability of two events
  happening together.
\item
  P(\text{man|tall}) is called the \textbf{conditional probability},
  because it describes the probability of one event happening, given
  that another event is already known to have occurred.
\end{itemize}

When Eq. (1) is rewritten in terms of \(P(\text{man|tall})\), it is
called the equation for conditional probability:

\[
P(\text{man|tall}) = \frac{P(\text{man } \cap \text{ tall})}{P(\text{tall})}.
\]

Of course, ``man'' and ``tall'' are only labels, the general formula we
should remember is:

\[
P(A|B) = \frac{P(A \cap B)}{P(B)}.
\]

\section{different perspective}\label{different-perspective}

We could have made sense of the data in a different way. Above, we first
categorized people by their height, and only then by sex. Let's try the
opposite.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.set\_aspect(}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}

\CommentTok{\# tall given man shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, p\_m], }\DecValTok{0}\NormalTok{, p\_3\_given\_m, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}
\CommentTok{\# medium given man shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, p\_m], p\_3\_given\_m, p\_3\_given\_m}\OperatorTok{+}\NormalTok{p\_2\_given\_m, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}

\CommentTok{\# man shaded area}
\NormalTok{ax.fill\_between([}\DecValTok{0}\NormalTok{, p\_m], }\DecValTok{0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}

\CommentTok{\# tall given woman shaded area}
\NormalTok{ax.fill\_between([p\_m, }\FloatTok{1.0}\NormalTok{], }\DecValTok{0}\NormalTok{, p\_3\_given\_f, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}
\CommentTok{\# medium given man shaded area}
\NormalTok{ax.fill\_between([p\_m, }\FloatTok{1.0}\NormalTok{], p\_3\_given\_f, p\_3\_given\_f}\OperatorTok{+}\NormalTok{p\_2\_given\_f, color}\OperatorTok{=}\StringTok{"blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"none"}\NormalTok{)}

\NormalTok{sns.despine(ax}\OperatorTok{=}\NormalTok{ax, top}\OperatorTok{=}\VariableTok{True}\NormalTok{, right}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# tall given man arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{            (p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_m),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_m }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f"tall | man, }\SpecialCharTok{\{}\NormalTok{p\_3\_given\_m}\SpecialCharTok{:.2f\}}\SpecialStringTok{  "}\NormalTok{, ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\CommentTok{\# medium given man arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_m),}
\NormalTok{            (p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_m}\OperatorTok{+}\NormalTok{p\_2\_given\_m),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_m }\OperatorTok{+}\NormalTok{ p\_2\_given\_m }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f"medium | man, }\SpecialCharTok{\{}\NormalTok{p\_2\_given\_m}\SpecialCharTok{:.2f\}}\SpecialStringTok{  "}\NormalTok{, ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\CommentTok{\# short given man arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\FloatTok{1.0}\NormalTok{),}
\NormalTok{            (p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\FloatTok{1.0}\OperatorTok{{-}}\NormalTok{p\_1\_given\_m),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_1\_given\_m }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f"short | man, }\SpecialCharTok{\{}\NormalTok{p\_1\_given\_m}\SpecialCharTok{:.2f\}}\SpecialStringTok{  "}\NormalTok{, ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\CommentTok{\# tall given woman arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_f),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_f }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f"tall | woman, }\SpecialCharTok{\{}\NormalTok{p\_3\_given\_f}\SpecialCharTok{:.2f\}}\SpecialStringTok{  "}\NormalTok{, ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\CommentTok{\# medium given woman arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_f),}
\NormalTok{            (}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_f}\OperatorTok{+}\NormalTok{p\_2\_given\_f),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, p\_3\_given\_f }\OperatorTok{+}\NormalTok{ p\_2\_given\_f }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f"medium | woman, }\SpecialCharTok{\{}\NormalTok{p\_2\_given\_f}\SpecialCharTok{:.2f\}}\SpecialStringTok{  "}\NormalTok{, ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\CommentTok{\# short given woman arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\FloatTok{1.0}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\FloatTok{1.0}\OperatorTok{{-}}\NormalTok{p\_1\_given\_f),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0}\OperatorTok{{-}}\FloatTok{0.03}\NormalTok{, }\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_1\_given\_f }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\SpecialStringTok{f"short | woman, }\SpecialCharTok{\{}\NormalTok{p\_1\_given\_f}\SpecialCharTok{:.2f\}}\SpecialStringTok{  "}\NormalTok{, ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}


\CommentTok{\# man arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\DecValTok{0}\NormalTok{, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{            (p\_m, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(p\_m }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{1.07}\NormalTok{, }\SpecialStringTok{f"man, }\SpecialCharTok{\{}\NormalTok{p\_m}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\CommentTok{\# woman arrow}
\NormalTok{ax.annotate(}\StringTok{""}\NormalTok{,}
\NormalTok{            (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ p\_f, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{            (}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.05}\NormalTok{),}
\NormalTok{            ha}\OperatorTok{=}\StringTok{"right"}\NormalTok{, va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{            size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{            arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}\textless{}{-}\textgreater{}\textquotesingle{}}\NormalTok{,}
\NormalTok{                            color}\OperatorTok{=}\StringTok{"black"}\NormalTok{,}
\NormalTok{                            ),}
\NormalTok{)}
\NormalTok{ax.text(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ (p\_f) }\OperatorTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{1.07}\NormalTok{, }\SpecialStringTok{f"woman, }\SpecialCharTok{\{}\NormalTok{p\_f}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{       yticks}\OperatorTok{=}\NormalTok{[],}
       \CommentTok{\# xlim=(0, 1),}
       \CommentTok{\# ylim=(0, 1),}
\NormalTok{       xlabel}\OperatorTok{=}\StringTok{"sex"}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{"height"}\NormalTok{,)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/from-the-ground-up_files/figure-pdf/cell-4-output-1.png}}

The probability that a person is both \textbf{man} and \textbf{tall} is
still the area of the purple rectangle on the bottom left. The rectangle
has a different shape, but it has to have the same area. The answer to
our question now can be understood thus:

\begin{itemize}
\tightlist
\item
  48\% of people are men.
\item
  31\% of those are tall.
\end{itemize}

The answer is 0.48 * 0.31 = 15\%, exactly the same result as before.

In mathematical notation, we write this as:

\[
P(\text{tall } \cap \text{ man}) = P(\text{man}) \cdot P(\text{tall|man}).
\tag{2}
\]

One thing should become clear from the images above. The probability
that a person is a man, given that they are tall, \textbf{is not the
same} as the probability that a person is tall, given that they are a
man. In mathematical notation:

\[
P(\text{man|tall}) \neq P(\text{tall|man}).
\]

\begin{itemize}
\tightlist
\item
  \(P(\text{man|tall})\): of all tall people, 67\% are men.
\item
  \(P(\text{tall|man})\): of all men, 31\% are tall.
\end{itemize}

\section{Bayes' theorem}\label{bayes-theorem}

When two things are true at the same time, it doesn't matter the order
we choose to write them. In mathematical notation:

\[
P(A \cap B) = P(B \cap A).
\]

Because of this, we equate the right-hand sides of Eqs. (1) and (2), and
we get Bayes' theorem:

\[
P(\text{man|tall}) \cdot P(\text{tall}) = P(\text{tall|man}) \cdot P(\text{man})
\]

Personally, I choose to remember this equation, because it is symmetric.
But the more common way to write Bayes' theorem is to solve for
\(P(\text{man|tall})\):

\[
P(\text{man|tall}) = \frac{P(\text{tall|man}) \cdot P(\text{man})}{P(\text{tall})},
\]

or in general terms:

\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}.
\]

Each term has a name:

\begin{itemize}
\tightlist
\item
  \(P(A|B)\) is the \textbf{posterior}. This is what we are trying to
  find out: the probability of \(A\) given that we know \(B\).
\item
  \(P(B|A)\) is the \textbf{likelihood}. This is the probability of
  \(B\) given that we know \(A\).
\item
  \(P(A)\) is the \textbf{prior}. This is what we know about \(A\)
  before we know anything about \(B\).
\item
  \(P(B)\) is the \textbf{evidence}. This is what we know about \(B\)
  before we know anything about \(A\).
\end{itemize}

\section{the law of total
probability}\label{the-law-of-total-probability}

In the example above, we had enough information to plug all the numbers
into Bayes' theorem. But this is not always the case. Sometimes, we
don't know \(P(B)\), the evidence. In this case, we can compute it using
the law of total probability. It is best to give a concrete example.

A famous use of Bayes' theorem is in desease testing.

\begin{itemize}
\tightlist
\item
  A given desease affects 1\% of the population.
\item
  A test for the desease is 95\% accurate. This means that:

  \begin{itemize}
  \tightlist
  \item
    If a person has the desease, the test will be positive 95\% of the
    time.
  \item
    If a person does not have the desease, the test will be negative
    95\% of the time.
  \end{itemize}
\item
  A person is randomly selected from the population, and they test
  positive. Should they be worried? What is the probability that they
  actually have the desease?
\end{itemize}

Let's translate this into the language of Bayes' theorem:

\begin{itemize}
\tightlist
\item
  \(A\) is the event ``the person has the desease''.
\item
  \(B\) is the event ``the test is positive''.
\item
  We need to find \(P(A|B)\), the probability that the person has the
  desease, given that they tested positive.
\item
  \(P(A) = 0.01\) is the prior, the probability that a random person has
  the desease.
\item
  \(P(B|A) = 0.95\) is the likelihood, the probability that the test is
  positive, given that the person has the desease.
\end{itemize}

We don't know \(P(B)\), the evidence, the probability that a random
person tests positive. But we can compute it using the law of total
probability:

\begin{align*}
P(\text{test is positive}) &= P(\text{test is positive } \textbf{and} \text{ person has the desease}) \\
                           &+ P(\text{test is positive } \textbf{and} \text{ person doesn't have the desease}).
\end{align*}

This has to be true, because a person can either have the desease or
not. In a more compact form:

\[
P(B) = P(B \cap A) + P(B \cap A^c),
\]

where \(A^c\) is the complement of \(A\), i.e., ``the person does not
have the desease''.

Using the definition of conditional probability, we can rewrite this as:
\[
P(B) = P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c).
\] We know all the terms on the right-hand side:

\begin{itemize}
\tightlist
\item
  \(P(B|A) = 0.95\), the probability that the test is positive, given
  that the person has the desease.
\item
  \(P(A) = 0.01\), the probability that a random person has the desease.
\item
  \(P(A^c) = 0.99\), the probability that a random person does not have
  the desease.
\item
  \(P(B|A^c) = 0.05\), the probability that the test is positive, given
  that the person does not have the desease. This is 1 minus the
  accuracy of the test for healthy people.
\end{itemize}

Plugging in the numbers, we get:

\[
P(B) = 0.95 \cdot 0.01 + 0.05 \cdot 0.99 = 0.059.
\]

Now we have everything we need to plug the numbers into Bayes' theorem:

\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} = \frac{0.95 \cdot 0.01}{0.059} = 0.161.
\]

This means that, even if the person tested positive, there is only a
16.1\% chance that they actually have the desease. This is
counter-intuitive, but it makes sense when we think about it. The
desease is very rare, so even if the test is accurate, most of the
positive results will be false positives.

In the case that there are several mutually exclusive ways for \(B\) to
happen, we can generalize the law of total probability:

\[
P(B) = \sum_i P(B \cap A_i) = \sum_i P(B|A_i) \cdot P(A_i),
\]

where the \(A_i\) are all the possible ways for \(B\) to happen.

\chapter{parametric generative
classification}\label{parametric-generative-classification}

\section{question}\label{question-5}

We are given a list of heights for men and women. Given one more data
point (180 cm), could we assign a probability that it belongs to either
class?

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_boys }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/boys\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{df\_girls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../archive/data/height/girls\_height\_stats.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{age }\OperatorTok{=} \FloatTok{20.0}
\NormalTok{mu\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{mu\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}mu\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_boys }\OperatorTok{=}\NormalTok{ df\_boys.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}
\NormalTok{sigma\_girls }\OperatorTok{=}\NormalTok{ df\_girls.loc[age, }\StringTok{\textquotesingle{}sigma\textquotesingle{}}\NormalTok{]}

\NormalTok{N\_boys }\OperatorTok{=} \DecValTok{150}
\NormalTok{N\_girls }\OperatorTok{=} \DecValTok{200}
\NormalTok{np.random.seed(}\DecValTok{314}\NormalTok{)  }\CommentTok{\# set scipy seed for reproducibility}
\NormalTok{sample\_boys }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_boys, loc}\OperatorTok{=}\NormalTok{mu\_boys, scale}\OperatorTok{=}\NormalTok{sigma\_boys)}
\NormalTok{sample\_girls }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\NormalTok{N\_girls, loc}\OperatorTok{=}\NormalTok{mu\_girls, scale}\OperatorTok{=}\NormalTok{sigma\_girls)}
\CommentTok{\# pandas dataframe with the two samples in it}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{: np.concatenate([sample\_boys, sample\_girls]),}
    \StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{: [}\StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{] }\OperatorTok{*}\NormalTok{ N\_boys }\OperatorTok{+}\NormalTok{ [}\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{] }\OperatorTok{*}\NormalTok{ N\_girls}
\NormalTok{\})}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df.sample(frac}\OperatorTok{=}\DecValTok{1}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{314}\NormalTok{).reset\_index(drop}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& height (cm) & sex \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 178.558416 & M \\
1 & 173.334306 & M \\
2 & 183.084154 & M \\
3 & 178.236047 & F \\
4 & 175.868642 & M \\
... & ... & ... \\
345 & 177.387837 & M \\
346 & 157.122325 & F \\
347 & 166.891746 & F \\
348 & 181.090312 & M \\
349 & 171.479631 & M \\
\end{longtable}

\section{explaining ``parametric generative
classification''}\label{explaining-parametric-generative-classification}

\begin{itemize}
\tightlist
\item
  \textbf{Parametric}: we assume a specific distribution for the data.
  In this case, we'll assume a Gaussian distribution. We call this
  parametric because the distribution can be fully described by a finite
  set of \textbf{parameters} (mean and variance for Gaussian).
\item
  \textbf{Generative}: we model the distribution of each class
  separately. This would allow us to \textbf{generate} new data points
  from the learned distributions. ``Learned'' means estimating the
  parameters of the distributions from the sample data.
\item
  \textbf{Classification}: we classify a new data point by comparing the
  likelihoods of it belonging to each class, given the learned
  distributions.
\end{itemize}

\section{visualizing the problem}\label{visualizing-the-problem}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{are\_male }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{]}\OperatorTok{==}\StringTok{\textquotesingle{}M\textquotesingle{}}
\NormalTok{boys\_sample }\OperatorTok{=}\NormalTok{ df[are\_male][}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{].to\_numpy()}
\NormalTok{girls\_sample }\OperatorTok{=}\NormalTok{ df[}\OperatorTok{\textasciitilde{}}\NormalTok{are\_male][}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{].to\_numpy()}

\NormalTok{xbar\_boys }\OperatorTok{=}\NormalTok{ boys\_sample.mean()}
\NormalTok{xbar\_girls }\OperatorTok{=}\NormalTok{ girls\_sample.mean()}
\NormalTok{s\_boys }\OperatorTok{=}\NormalTok{ boys\_sample.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{s\_girls }\OperatorTok{=}\NormalTok{ girls\_sample.std(ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\CommentTok{\# plot histogram}
\NormalTok{bins }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{135}\NormalTok{, }\DecValTok{210}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{ax.hist(boys\_sample, bins}\OperatorTok{=}\NormalTok{bins, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}men\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{, histtype}\OperatorTok{=}\StringTok{\textquotesingle{}stepfilled\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.hist(girls\_sample, bins}\OperatorTok{=}\NormalTok{bins, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}women\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{, histtype}\OperatorTok{=}\StringTok{\textquotesingle{}stepfilled\textquotesingle{}}\NormalTok{)}
\CommentTok{\# plot gaussian pdf based on sample mean and std}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{135}\NormalTok{, }\DecValTok{210}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{pdf\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(x, loc}\OperatorTok{=}\NormalTok{xbar\_boys, scale}\OperatorTok{=}\NormalTok{s\_boys)}
\NormalTok{pdf\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(x, loc}\OperatorTok{=}\NormalTok{xbar\_girls, scale}\OperatorTok{=}\NormalTok{s\_girls)}
\NormalTok{ax.plot(x, pdf\_boys, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(x, pdf\_girls, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{h0 }\OperatorTok{=} \DecValTok{180}
\CommentTok{\# plot vertical line at h0}
\NormalTok{ax.axvline(h0, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\CommentTok{\# plot circles where each pdf intersects h0}
\NormalTok{likelihood\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(h0, loc}\OperatorTok{=}\NormalTok{xbar\_boys, scale}\OperatorTok{=}\NormalTok{s\_boys)}
\NormalTok{likelihood\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(h0, loc}\OperatorTok{=}\NormalTok{xbar\_girls, scale}\OperatorTok{=}\NormalTok{s\_girls)}
\NormalTok{ax.plot(h0, likelihood\_boys, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:blue\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(h0, likelihood\_girls, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"men:   mean=}\SpecialCharTok{\{}\NormalTok{xbar\_boys}\SpecialCharTok{:.1f\}}\SpecialStringTok{ cm, std=}\SpecialCharTok{\{}\NormalTok{s\_boys}\SpecialCharTok{:.1f\}}\SpecialStringTok{ cm"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"women: mean=}\SpecialCharTok{\{}\NormalTok{xbar\_girls}\SpecialCharTok{:.1f\}}\SpecialStringTok{ cm, std=}\SpecialCharTok{\{}\NormalTok{s\_girls}\SpecialCharTok{:.1f\}}\SpecialStringTok{ cm"}\NormalTok{)}

\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}height (cm)\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
men:   mean=177.4 cm, std=6.7 cm
women: mean=163.5 cm, std=7.3 cm
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/parametric-generative-classification_files/figure-pdf/cell-5-output-2.png}}

From the sample data, we can compute the mean and standard deviation for
each sex (the generative part). We printed these values above. We can
then use these parameters to compute the likelihood of the new data
point (180 cm) belonging to each class using the Gaussian probability
density function (the parametric part). These are plotted as circles in
the graph.

\section{Bayes' theorem}\label{bayes-theorem-1}

We can then use Bayes' theorem to compute the posterior probabilities of
the new data point belonging to each class (the classification part).
Bayes' theorem states that:

\[
P(\text{man} | x) = \frac{P(x | \text{man})}{P(x)} P(\text{man})
\]

\begin{itemize}
\tightlist
\item
  \textbf{posterior}, \(P(\text{man} | x)\). This is what we are looking
  for, the probability of a new data point corresponding to a man, given
  that its height is \(x\).
\item
  \textbf{likelihood}, \(P(x | \text{man})\). This is the likelihood of
  observing a height \(x\) given that we know it is a man.
\item
  \textbf{evidence}, \(P(x)\). This is the total probability of
  observing a height \(x\) across all classes (regardless of sex). It
  acts as a normalization factor. We calculate the evidence using the
  law of total probability: \begin{align*}
  P(x) &= P(x \cap \text{man}) + P(x \cap \text{woman}) \\
       &=P(x|\text{man})\cdot P(\text{man}) + P(x|\text{woman})\cdot P(\text{woman})
  \end{align*}
\item
  \textbf{prior}, \(P(\text{man})\). This is the overall probability of
  a person being a man in my dataset (regardless of their height).
\end{itemize}

Think of this as a ``battle of likelihoods,'' adjusted for the group
sizes. You have two groups, men and women, and you've modeled their
typical heights. When you get a new height, \(x\), you ask two main
questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Likelihood Question:} How ``typical'' is height \(x\) for a
  man compared to how typical it is for a woman? If men in your data are
  generally tall and \(x\) is a tall height, it's more likely to be a
  man. We measure this ``typicalness'' using a probability distribution.
\item
  \textbf{Prior Belief Question:} In your dataset, are men or women more
  common? If your dataset contains 90 women and 10 men, any new person
  is, initially, more likely to be a woman, regardless of their height.
\end{enumerate}

\section{Step-by-Step Calculation}\label{step-by-step-calculation}

\textbf{Step 1: Model Your Data}

We assume a Gaussian distribution, and calculate the sample mean and
standard deviation for each sex.

\begin{verbatim}
men:   mean=177.4 cm, std=6.7 cm
women: mean=163.5 cm, std=7.3 cm
\end{verbatim}

\textbf{Step 2: Calculate the Priors}

The priors are simply the proportion of each group in the total dataset.

\[P(\text{Man}) = \frac{N_\text{men}}{N_\text{men} + N_\text{women}}\]

\[P(\text{Woman}) = \frac{N_\text{women}}{N_\text{men} + N_\text{women}}\]

\textbf{Step 3: Calculate the Likelihoods}

Using the normal distribution's probability density function (PDF), find
the likelihood of the new height \(h\) for each model. The PDF formula
is:

\[f(x | \bar{x}, s) = \frac{1}{s\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\bar{x}}{s}\right)^2}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Likelihood for Men:} Plug \(x\) into the PDF for the men's
  model.\\
  \(P(x | \text{Man}) = f(x | \bar{x}_\text{men}, s_\text{men})\)
\item
  \textbf{Likelihood for Women:} Plug \(x\) into the PDF for the women's
  model.\\
  \(P(x | \text{Woman}) = f(x | \bar{x}_\text{women}, s_\text{women})\)
\end{enumerate}

\textbf{Step 4: Put It All Together}

Now, apply Bayes' Theorem. The ``evidence'' term \(P(x)\) in the
denominator is the sum of all ways you could observe height \(x\):

\[
P(x) = P(x | \text{Man}) \cdot P(\text{Man}) + P(x | \text{Woman}) \cdot P(\text{Woman})
\]

So, the final calculation for the probability of being a man is:

\[
P(\text{Man} | x) = \frac{P(x | \text{Man}) \cdot P(\text{Man})}{P(x | \text{Man}) \cdot P(\text{Man}) + P(x | \text{Woman}) \cdot P(\text{Woman})}
\].

Crunching the number gives:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h0 }\OperatorTok{=} \FloatTok{180.0}
\NormalTok{likelihood\_boys }\OperatorTok{=}\NormalTok{ norm.pdf(h0, loc}\OperatorTok{=}\NormalTok{xbar\_boys, scale}\OperatorTok{=}\NormalTok{s\_boys)}
\NormalTok{likelihood\_girls }\OperatorTok{=}\NormalTok{ norm.pdf(h0, loc}\OperatorTok{=}\NormalTok{xbar\_girls, scale}\OperatorTok{=}\NormalTok{s\_girls)}
\NormalTok{prior\_boys }\OperatorTok{=}\NormalTok{ N\_boys }\OperatorTok{/}\NormalTok{ (N\_boys }\OperatorTok{+}\NormalTok{ N\_girls)}
\NormalTok{prior\_girls }\OperatorTok{=}\NormalTok{ N\_girls }\OperatorTok{/}\NormalTok{ (N\_boys }\OperatorTok{+}\NormalTok{ N\_girls)}
\NormalTok{evidence }\OperatorTok{=}\NormalTok{ likelihood\_boys }\OperatorTok{*}\NormalTok{ prior\_boys }\OperatorTok{+}\NormalTok{ likelihood\_girls }\OperatorTok{*}\NormalTok{ prior\_girls}
\NormalTok{p\_man\_given\_180 }\OperatorTok{=}\NormalTok{ likelihood\_boys }\OperatorTok{*}\NormalTok{ prior\_boys }\OperatorTok{/}\NormalTok{ evidence}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Answer: }\SpecialCharTok{\{}\NormalTok{p\_man\_given\_180 }\OperatorTok{*} \DecValTok{100}\SpecialCharTok{:.2f\}}\SpecialStringTok{\%."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Answer: 90.92%.
\end{verbatim}

The probability that the person is a man, given that their height is 180
cm, is 90.92\%

\chapter{odds and log likelihood}\label{odds-and-log-likelihood}

\section{the scenario}\label{the-scenario-1}

Imagine we are researchers studying a potential link between a specific
mutated gene and a certain disease. We have collected data from a sample
of 356 people.

Here's our data:

\begin{longtable}[]{@{}lcc@{}}
\toprule\noalign{}
& \textbf{Has Disease} & \textbf{No Disease} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Has Mutated Gene} & 23 & 117 \\
\textbf{No Mutated Gene} & 6 & 210 \\
\end{longtable}

Our goal is to figure out how finding this mutated gene in a person
should change our belief about whether they have the disease.

\section{prior}\label{prior}

The \textbf{prior probability} of someone having the disease is the
chance of having the disease before we know anything about their gene
status. We can calculate this from our data.

\[
P(\text{Disease}) = \frac{\text{Number of people with the disease}}{\text{Total number of people}}
\]

From our table: \[
P(\text{Disease}) = \frac{23 + 6}{23 + 117 + 6 + 210} = \frac{29}{356} \approx 0.081
\]

\section{odds}\label{odds}

Odds are a different way to express the same information. Odds compare
the chance of an event happening to the chance of it \emph{not}
happening.

\[
\text{Odds} = \frac{P(\text{event})}{P(\text{not event})}
\]

In our case, ``event'' is having the disease. Because we have only two
choices, the probability of not having the disease is simply
\(1 - P(\text{Disease})\), therefore:

\[
\text{Odds(Disease)} = \frac{P(\text{Disease})}{1 - P(\text{Disease})}
\]

Plugging in the numbers:

\[
\text{Odds(Disease)} = \frac{29/356}{1 - 29/356} \approx 0.0887 \approx \frac{1}{11}
\]

This means that for every person with the disease, about 11 do not have
it.

\subsection{log odds}\label{log-odds}

The \textbf{log odds} is simply the natural logarithm of the odds.

\[
\text{Log Odds} = \ln\left(\frac{P}{1-P}\right)
\]

We will see soon enough why this is useful. For now, let's point out
that:

\begin{itemize}
\tightlist
\item
  if the odds are 1 (meaning a 50/50 chance), the log odds is 0.
\item
  if the odds are greater than 1 (more likely than not), the log odds is
  positive.
\item
  if the odds are less than 1 (less likely than not), the log odds is
  negative.
\item
  the log odds have a symmetric shape around \(p=1/2\), see figure
  below.
\end{itemize}

For our example, the log odds of having the disease is:

\[
\text{Log Odds(Disease)} = \ln(\text{Odds(Disease)}) = \ln(0.0887) \approx -2.42
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}

\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{odds }\OperatorTok{=}\NormalTok{ p }\OperatorTok{/}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ p)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(p, odds, label}\OperatorTok{=}\StringTok{"odds"}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{)}\OperatorTok{;}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].axhline(}\DecValTok{1}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{"odds"}\NormalTok{,}
\NormalTok{          xlabel}\OperatorTok{=}\StringTok{"probability"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\VerbatimStringTok{r"f}\KeywordTok{(}\VerbatimStringTok{p}\KeywordTok{)}\VerbatimStringTok{ = }\DecValTok{$}\CharTok{\textbackslash{}f}\VerbatimStringTok{rac\{p\}\{1{-}p\}}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{          ylim}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{          xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{          xticks}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{;}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].annotate(}\StringTok{"more likely}\CharTok{\textbackslash{}n}\StringTok{than not"}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{1}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{3}\NormalTok{),}
\NormalTok{               ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"\textless{}{-}"}\NormalTok{, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].annotate(}\StringTok{"less likely"}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\DecValTok{1}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{),}
\NormalTok{               ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"\textless{}{-}"}\NormalTok{, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{))}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(p, np.log(odds), label}\OperatorTok{=}\StringTok{"odds"}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].axhline(}\DecValTok{0}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.set\_ticks\_position(}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.set\_label\_position(}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{"log odds"}\NormalTok{,}
\NormalTok{          xlabel}\OperatorTok{=}\StringTok{"probability"}\NormalTok{,}
\NormalTok{          title}\OperatorTok{=}\VerbatimStringTok{r"f}\KeywordTok{(}\VerbatimStringTok{p}\KeywordTok{)}\VerbatimStringTok{ = log}\DecValTok{$}\CharTok{\textbackslash{}f}\VerbatimStringTok{rac\{p\}\{1{-}p\}}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{          xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{          xticks}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{;}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].annotate(}\StringTok{"more likely}\CharTok{\textbackslash{}n}\StringTok{than not"}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{0}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{1}\NormalTok{),}
\NormalTok{               ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"\textless{}{-}"}\NormalTok{, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{))}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].annotate(}\StringTok{"less likely"}\NormalTok{, xy}\OperatorTok{=}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\DecValTok{0}\NormalTok{), xytext}\OperatorTok{=}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{),}
\NormalTok{               ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{, arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{"\textless{}{-}"}\NormalTok{, color}\OperatorTok{=}\StringTok{"gray"}\NormalTok{))}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10104/1581004145.py:5: RuntimeWarning: divide by zero encountered in divide
  odds = p / (1 - p)
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10104/1581004145.py:19: RuntimeWarning: divide by zero encountered in log
  ax[1].plot(p, np.log(odds), label="odds", color="tab:blue")
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/odds_files/figure-pdf/cell-3-output-2.png}}

\section{Bayes' theorem in odds form}\label{bayes-theorem-in-odds-form}

The standard form of Bayes' theorem is:

\[
P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}.
\tag{1}
\]

In our example, the hypothesis \(H\) is ``having the disease'', and the
evidence \(E\) is detecting the mutated gene.

Let's write Bayes' theorem for the alternative hypothesis \(\neg H\)
(``not having the disease''):

\[
P(\neg H|E) = \frac{P(E|\neg H) \cdot P(\neg H)}{P(E)}.
\tag{2}
\]

The \textbf{odds form} of Bayes' theorem is the ratio of these two
equations:

\[
\underbrace{\frac{P(H|E)}{P(\neg H|E)}}_ {\text{posterior odds}} = \underbrace{\frac{P(E|H)}{P(E|\neg H)}}_{\text{likelihood ratio}} \cdot \underbrace{\frac{P(H)}{P(\neg H)}}_{\text{prior odds}}
\tag{3}
\]

We already discussed the prior odds. The posterior odds represent the
odds of having the desease \emph{after} we have seen the evidence (the
mutated gene). The new piece we need is the \textbf{likelihood ratio}.

\section{likelihood ratio}\label{likelihood-ratio}

The likelihood ratio (LR) tells us how much more likely we are to see
the evidence if the hypothesis is true compared to if it is false.

\[
\text{LR} = \frac{P(E|H)}{P(E|\neg H)}
\]

We can compute this from our data:

\begin{itemize}
\tightlist
\item
  \(P(E|H)\): the probability of having the mutated gene given that the
  person has the disease. From the left column in our table, this is
  \(\frac{23}{23+6} \approx 0.793\).
\item
  \(P(E|\neg H)\): the probability of having the mutated gene given that
  the person does not have the disease. From the right column in our
  table, this is \(\frac{117}{117+210} \approx 0.358\).
\end{itemize}

Finally:

\[
\text{LR} = \frac{23/29}{117/327} \approx 2.22
\]

The interpretation is that seeing the mutated gene is about 2.22 times
more likely if the person has the disease than if they do not.

\section{log likelihood ratio}\label{log-likelihood-ratio}

Here too, taking the logarithm transforms a quantity between 0 and
infinity into a number between negative infinity and positive infinity.
The log likelihood ratio is: \[
\text{Log-LR} = \ln(\text{LR}) = \ln(2.22) \approx 0.797
\]

The fact that this is a positive number says that seeing the mutated
gene \textbf{increases} our belief that the person has the disease.

\section{Bayes' theorem in log odds
form}\label{bayes-theorem-in-log-odds-form}

Taking the logarith of Eq. (3) gives us the log odds form of Bayes'
theorem:

\[
\underbrace{\ln\left(\frac{P(H|E)}{P(\neg H|E)}\right)}_{\text{posterior log odds}} = \underbrace{\ln\left(\frac{P(E|H)}{P(E|\neg H)}\right)}_{\text{log-likelihood ratio}} + \underbrace{\ln\left(\frac{P(H)}{P(\neg H)}\right)}_{\text{prior log odds}}
\tag{4}
\]

Plugging in our numbers, we can see how our belief about the person
having the disease changes after seeing the evidence (the mutated gene).

\begin{align*}
\text{Posterior Log Odds} &= \text{Log-LR} + \text{Prior Log Odds} \\
&= 0.797 + (-2.42) \\
&\approx -1.623
\end{align*}

From the posterior log odds, we can get back to the posterior odds by
exponentiating:

\[
\text{Posterior Odds} = e^{\text{Posterior Log Odds}} = e^{-1.623} \approx 0.197
\]

Finally, we can convert the posterior odds back to a probability: \[
P(H|E) = \frac{\text{Posterior Odds}}{1 + \text{Posterior Odds}} = \frac{0.197}{1 + 0.197} \approx 0.164
\] This means that after seeing the mutated gene, our estimate of the
probability that the person has the disease has increased from about
8.1\% to about 16.4\%.

\chapter{logistic connection}\label{logistic-connection}

\section{from Bayes the logistic}\label{from-bayes-the-logistic}

The arguments below follow those in subsection 12.2 of
\href{https://doi.org/10.1017/9781107588493}{``Introduction to
Environmental Data Science'' by William W. Hsieh}.

We start with Bayes' theorem for two classes \(C_1\) and \(C_2\):

\[
P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x)}
\tag{1}
\]

Using the law of total probability in the denominator, we get:

\[
P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}
\tag{2}
\]

We now divide the numerator and denominator by \(P(x|C_1)P(C_1)\):

\[
P(C_1|x) = \frac{1}{1 + \frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}
\tag{3}
\]

We now note that the ratio \(P(C_2|x)/P(C_1|x)\) can be expressed as:

\[
\frac{P(C_2|x)}{P(C_1|x)} = \frac{\frac{P(x|C_2)P(C_2)}{P(x)}}{\frac{P(x|C_1)P(C_1)}{P(x)}} = \frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}
\tag{4}
\]

In the expression above, we used the Bayes' theorem in (1) to express
\(P(C_2|x)\) and \(P(C_1|x)\) in terms of \(P(x|C_2)\) and \(P(x|C_1)\).
We can now rewrite (3) as: \[
P(C_1|x) = \frac{1}{1 + \frac{P(C_2|x)}{P(C_1|x)}} = \frac{1}{1 + \left(\frac{P(C_1|x)}{P(C_2|x)}\right)^{-1}}
\tag{5}
\]

The posterior probability \(P(C_1|x)\) is a function of the ratio
\(P(C_1|x)/P(C_2|x)\). This ratio is called the \textbf{posterior odds},
or simply \textbf{odds}. We can make this function look like a sigmoid
function by taking the logarithm of the posterior odds. The logarithm of
the posterior odds is called the \textbf{log-odds} or \textbf{logit}: \[
\text{logit} = u = \ln\left(\frac{P(C_1|x)}{P(C_2|x)}\right)
\tag{6}
\]

We can now rewrite (5) in terms of the logit: \[
P(C_1|x) = \frac{1}{1 + e^{-u}}
\tag{7}
\]

Finally, we assume that there is a linear relationship between \(u\) and
the features \(x\):

\[
u = \sum_j w_j x_j + w_0 = \mathbf{w}^T \mathbf{x} + w_0
\tag{8}
\]

We now have the logistic function that connects the features \(x\) to
the posterior probability \(P(C_1|x)\): \[
P(C_1|x) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + w_0)}}
\tag{9}
\]

The one assumption that is needed to make the connection from Bayes'
theorem to the logistic function is that there is a linear relationship
between the log-odds and the features \(x\):

\[
\ln\left(\frac{P(C_1|x)}{P(C_2|x)}\right) = \mathbf{w}^T \mathbf{x} + w_0
\tag{10}
\]

This seems a rather arbitrary assumption. Why does this make sense?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A linear relationship between the log odds and the features is simple
  and easy to interpret.
\item
  Linear models are easy to implement and computationally efficient.
\item
  In a few specific cases (see below) the linearity doesn't have to be
  assumed, it emerges naturally from the model.
\end{enumerate}

\section{emergent linearity}\label{emergent-linearity}

Let's start from the log odds definition in (6):

\[
u = \ln\left(\frac{P(C_1|x)}{P(C_2|x)}\right) = \ln\left(\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}\right)
\tag{11}
\]

We rewrite this as:

\begin{align*}
u &= \ln \frac{P(x|C_1)}{P(x|C_2)} + \ln \frac{P(C_1)}{P(C_2)} \\
  &= \ln P(x|C_1) - \ln P(x|C_2) + \ln \frac{P(C_1)}{P(C_2)}. \tag{12}
\end{align*}

We now \textbf{make the assumption} that the likelihoods \(P(x|C_k)\)
are Gaussian distributions. For simplicity, let's assume that \(x\) is a
single feature (univariate case).

\[
P(x|C_k) = \frac{1}{\sqrt{2\pi}\sigma_k} \exp\left(-\frac{(x-\mu_k)^2}{2\sigma_k^2}\right),
\tag{13}
\]

where \(C_k\) are the two classes we have, \(C_1\) and \(C_2\).

We now calculate the log of the likelihoods:

\[
\ln P(x|C_k) = -\ln \sqrt{2\pi \sigma_k^2} - \frac{(x-\mu_k)^2}{2\sigma_k^2}.
\tag{14}
\]

We now substitute this into Eq. (12) for the log odds:

\begin{align*}
u &= -\ln \sqrt{2\pi \sigma_1^2} - \frac{(x-\mu_1)^2}{2\sigma_1^2} + \ln \sqrt{2\pi \sigma_2^2} + \frac{(x-\mu_2)^2}{2\sigma_2^2} + \ln \frac{P(C_1)}{P(C_2)} \\
  &= \ln \frac{\sigma_2}{\sigma_1} + \frac{(x-\mu_2)^2}{2\sigma_2^2} - \frac{(x-\mu_1)^2}{2\sigma_1^2} + \ln \frac{P(C_1)}{P(C_2)}.
  \tag{15}
\end{align*}

\textbf{KEY ASSUMPTION}: if we assume that the two classes have the same
variance, \(\sigma_1 = \sigma_2 = \sigma\), the expression simplifies
to:

\begin{align*}
u &= \frac{1}{2\sigma^2} \left( (x-\mu_2)^2 - (x-\mu_1)^2 \right) + \ln \frac{P(C_1)}{P(C_2)} \\
  &= \frac{1}{2\sigma^2} \left( x^2 - 2x\mu_2 + \mu_2^2 - x^2 + 2x\mu_1 - \mu_1^2 \right) + \ln \frac{P(C_1)}{P(C_2)} \\
  &= \frac{\mu_1 - \mu_2}{\sigma^2} x + \frac{\mu_2^2 - \mu_1^2}{2\sigma^2} + \ln \frac{P(C_1)}{P(C_2)}.
  \tag{16}
\end{align*}

The first term depends on \(x\) linearly, and the other two terms are
constants. We can thus rewrite the log odds \(u\) as:

\[
u = wx + w_0,
\tag{17}
\]

where \[
w = \frac{\mu_1 - \mu_2}{\sigma^2}, \quad w_0 = \frac{\mu_2^2 - \mu_1^2}{2\sigma^2} + \ln \frac{P(C_1)}{P(C_2)}.
\tag{18}
\]

Under the assumption that the distributions have equal variance, the
posterior probability can be expressed as a logistic function of a
linear combination of the input feature.

This is probably the simplest example of a connection between a
generative model (Gaussian distributions for each class) and a
discriminative model (logistic regression). It would work for other
distributions from the exponential family, e.g., Poisson, Bernoulli,
Exponential, etc. The one condition they all need to satisfy is that the
non-linear part of the log-likelihoods cancels out when we compute the
log-odds, leaving a linear function of \(x\).

When we have real data in our hands, we usually don't know the
underlying distributions. The calculation above showed us that a linear
relationship between the log odds and the features naturally emerges in
a few cases, and this is the motivation for the wider assumption in
(10).

\chapter{conjugate prior}\label{conjugate-prior}

We will learn about conjugate priors in Bayesian statistics from a
concrete example. Once we understand it, we will generalize the idea.

\section{question}\label{question-6}

Let's use here the same example from the chapter on
\href{./information_theory/cross-entropy.ipynb}{cross-entropy and KL
divergence}:

\begin{quote}
Assume I live in city A, where it rains 50\% of the days. A friend of
mine lives in city B, where it rains 10\% of the days. What happens when
my friend visits me in city A and, not knowing any better, assumes that
it rains 10\% of the days?
\end{quote}

The specific question we want to answer is: how can my friend update
their belief about the probability of rain when they arrive in city A?

\section{bayes' theorem}\label{bayes-theorem-2}

We will use Bayes' theorem to update our friend's belief. To makes
things easier to remember, let's call the hypothesis \(p\) (the
probability of rain), and the evidence \(R\) (a specific observation of
rain or no rain). Thus, Bayes' theorem can be rewritten as:

\begin{align}
P(p|R) &= \frac{P(R|p)\cdot P(p)}{P(R)} \\
\text{posterior}&= \frac{\text{likelihood}\cdot \text{prior}}{\text{evidence}}
\end{align}

where:

\begin{itemize}
\tightlist
\item
  \(p\) is the hypothesis, the probability of rain.
\item
  \(R\) is the evidence, the observation of rain or no rain.
\item
  \(P(p)\) is the prior probability, our friend's initial belief about
  the probability of rain.
\item
  \(P(R|p)\) is the likelihood, the likelihood of observing rain given
  the hypothesis that it rains with a certain probability.
\item
  \(P(R)\) is the evidence, the total probability of observing the
  evidence.
\item
  \(P(p|R)\) is the posterior probability, this is what we want to find:
  our friend's updated belief about the probability of rain \(p\) after
  observing the evidence.
\end{itemize}

\section{modeling the likelihood}\label{modeling-the-likelihood}

In this problem, every day that passes it either rains or it does not
rain. This can be understood as a Bernoulli process, where each day is
an independent trial with two possible outcomes. ``Success'' would be
ocurrence of rain (\(R=1\)), which happens with probability \(p\).
``Failure'' would be no rain (\(R=0\)), which happens with probability
\(1-p\). In mathematical terms, the likelihood can be modeled as:

\[
P(R=r|p) = 
\begin{cases}
p & \text{if } r=1 \\
1-p & \text{if } r=0
\end{cases}.
\]

This can be more compactly written as:

\[
P(R=r|p) = p^r (1-p)^{1-r}.
\]

This equation describes only one observation. However, we can extend it
to multiple observations. Suppose our friend observes, over a total of
\(n\) days, \(k\) days of rain and \(n-k\) days of no rain. The
likelihood of observing this specific sequence of rain and no rain,
given the probability \(p\), can be modeled using the binomial
distribution, which is the natural extension of the Bernoulli process
for multiple trials:

\[
P(R=k|p) = \binom{n}{k} p^k (1-p)^{n-k}.
\]

This is the time to be more precise. When we previously said that
``\(p\) is the hypothesis, the probability of rain'', we left behind the
modeling aspect. We assumed here a generative model, the Bernoulli
process. Rephrasing the statement more precisely: ``\(p\) is the
parameter of the generative model (Bernoulli process) that generates the
observations of rain and no rain''.

\section{modeling the prior}\label{modeling-the-prior}

The prior in the Bayesian framework is not a single value. From the
question above, we might think that our friend's prior belief about the
probability of rain is simply \(0.1=10\%\). However, in Bayesian
statistics, the prior is represented as a probability distribution over
all possible values of \(p\). In would make sense to choose a
probability distribution that is highest around \(0.1\) and lower
elsewhere. There are infinite possible distributions that could
represent this belief, so which should we choose? See below three
examples of possible prior distributions, all have their mean at
\(0.1\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib }\ImportTok{as}\NormalTok{ mpl}
\ImportTok{import}\NormalTok{ matplotlib.gridspec }\ImportTok{as}\NormalTok{ grid\_spec}
\ImportTok{from}\NormalTok{ matplotlib.lines }\ImportTok{import}\NormalTok{ Line2D}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ beta, norm, uniform, binom}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{location }\OperatorTok{=} \FloatTok{0.1}
\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{ax.plot(p, beta.pdf(p, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\OperatorTok{/}\NormalTok{location}\OperatorTok{{-}}\DecValTok{2}\NormalTok{), label}\OperatorTok{=}\StringTok{\textquotesingle{}Beta(2, 18)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{)}
\NormalTok{ax.plot(p, norm.pdf(p, loc}\OperatorTok{=}\NormalTok{location, scale}\OperatorTok{=}\FloatTok{0.02}\NormalTok{), label}\OperatorTok{=}\StringTok{\textquotesingle{}Normal(0.1, 0.05)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:orange"}\NormalTok{)}
\NormalTok{ax.plot(p, uniform.pdf(p, loc}\OperatorTok{=}\NormalTok{location}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, scale}\OperatorTok{=}\FloatTok{0.1}\NormalTok{), label}\OperatorTok{=}\StringTok{\textquotesingle{}Uniform(0.05, 0.15)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability of rain (p)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Possible prior distributions for the probability of rain\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/conjugate-prior_files/figure-pdf/cell-3-output-1.png}}

A particular good choice is the Beta distribution. The Beta distribution
is defined on the interval {[}0, 1{]}, which makes it suitable for
modeling probabilities. It is parameterized by two positive shape
parameters, \(\alpha\) and \(\beta\), which determine the shape of the
distribution. The probability density function (PDF) of the Beta
distribution is given by:

\[
\text{Beta}(p|\alpha, \beta) = \frac{p^{\alpha - 1} (1-p)^{\beta - 1}}{B(\alpha, \beta)},
\]

where \(B(\alpha, \beta)\) is the Beta function, which serves as a
normalization constant to ensure that the total probability integrates
to 1.

The Beta distribution in the graph above is
\(\text{Beta}(p|\alpha=2, \beta=18)\). How did I choose these
parameters? The mean of a Beta distribution is given by:

\[
\text{mean} = \frac{\alpha}{\alpha + \beta}.
\]

The derivation of this formula is not shown here, but it involves
calculating the expected value of the distribution using its pdf, and
using properties of the Beta function and of the Gamma function. Indeed,
by choosing \(\alpha=2\) and \(\beta=18\), we get a mean of \(0.1\).

Intuitively, a rain probability of \(0.1\) means that out of every \(n\)
days, we expect it to rain on average \(0.1n\) days. It terms of
``successes'' and ``failures'':

\begin{align*}
\text{mean} &= \frac{\alpha}{\alpha + \beta} \\
&= \frac{\text{expected successes}}{\text{expected successes} + \text{expected failures}} \\
&= \frac{\text{expected successes}}{\text{total number of trials}}
\end{align*}

Instead of choosing \(\alpha=2\) and \(\beta=18\), we could choose other
values that maintain the same mean but represent different levels of
confidence or prior knowledge. See the three Beta distributions plotted
below. All have their mean at \(0.1\), but the black one
(\(\alpha=2, \beta=18\)) is more spread out, indicating less certainty
about the probability of rain when only \(2+18=20\) days are considered.
Increasing the number of days to \(50\) (orange, \(\alpha=5, \beta=45\))
or \(200\) (blue, \(\alpha=20, \beta=180\)) makes the distribution more
peaked around the mean, indicating greater confidence in the estimate of
the probability of rain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{location }\OperatorTok{=} \FloatTok{0.1}
\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{ax.plot(p, beta.pdf(p, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\OperatorTok{/}\NormalTok{location}\OperatorTok{{-}}\DecValTok{2}\NormalTok{), label}\OperatorTok{=}\StringTok{\textquotesingle{}Beta(2, 18)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{)}
\NormalTok{ax.plot(p, beta.pdf(p, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\OperatorTok{/}\NormalTok{location}\OperatorTok{{-}}\DecValTok{5}\NormalTok{), label}\OperatorTok{=}\StringTok{\textquotesingle{}Beta(5, 45)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:orange"}\NormalTok{)}
\NormalTok{ax.plot(p, beta.pdf(p, }\DecValTok{20}\NormalTok{, }\DecValTok{20}\OperatorTok{/}\NormalTok{location}\OperatorTok{{-}}\DecValTok{20}\NormalTok{), label}\OperatorTok{=}\StringTok{\textquotesingle{}Beta(20, 180)\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{)}
\NormalTok{ax.legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability of rain (p)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Beta prior distributions with mean at 0.1\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/conjugate-prior_files/figure-pdf/cell-4-output-1.png}}

\section{posterior looks like an updated
prior}\label{posterior-looks-like-an-updated-prior}

Why is this distribution particulary convenient? Note that it has the
factors \(p^{\alpha - 1}\) and \((1-p)^{\beta - 1}\), which are similar
to the factors in the likelihood function \(p^k (1-p)^{n-k}\).

According to Bayes' theorem, the posterior distribution is proportional
to the product of the likelihood and the prior. Thus, if we choose a
Beta distribution as the prior, the posterior distribution will also be
a Beta distribution, but with updated parameters. Let's see how this
works mathematically:

\begin{align*}
P(p|R=k) &= \frac{1}{\underbrace{P(R=k)}_{\text{evidence}}} \cdot \underbrace{P(R=k|p)}_{\text{likelihood}} \cdot \underbrace{P(p)}_{\text{prior}} \\
&= \frac{1}{P(R=k)} \left( \binom{n}{k} p^k (1-p)^{n-k} \right) \cdot \left( \frac{p^{\alpha - 1} (1-p)^{\beta - 1}}{B(\alpha, \beta)} \right) \\
&= \underbrace{ \frac{1}{P(R=k)} \binom{n}{k} \frac{1}{B(\alpha, \beta)} }_{\text{normalization constant}} p^{k + \alpha - 1} (1-p)^{n - k + \beta - 1}\\
& = \text{Beta}(p | \alpha + k, \beta + n - k) \\
& = \text{Beta}(p | \alpha + \text{successes}, \beta + \text{failures}).
\end{align*}

This isn't magic. We simply chose a prior distribution (Beta) that, when
multiplied by the likelihood (Binomial), results in a posterior
distribution of the same family (Beta). This property is what defines
conjugate priors. Why is this useful?

\begin{itemize}
\tightlist
\item
  \textbf{Computational Simplicity:} The posterior distribution can be
  computed analytically without the need for complex numerical methods.
\item
  \textbf{Intuitive Interpretation:} The parameters of the posterior
  distribution can be interpreted as updated counts of successes and
  failures, making it easy to understand how new data influences our
  beliefs.
\end{itemize}

\section{iterative updating of
beliefs}\label{iterative-updating-of-beliefs}

Let's say that my friends Bob and Alice move to city A, where it rains
on 50\% of the days. Bob has a prior belief modeled as a Beta
distribution with parameters \(\alpha=2\) and \(\beta=18\), reflecting
his initial belief that it rains 10\% of the days, but with a low level
of confidence. Alice, however, has an initial belief closer to the
truth, 20\%, but with a much higher level of confidence, modeled as a
Beta distribution with parameters \(\alpha=100\) and \(\beta=400\).
Every week that passes, they observe the weather and write down the
number of rainy days, thus collecting the following data over 52 weeks
(one year):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{6}\NormalTok{)}
\NormalTok{N\_weeks }\OperatorTok{=} \DecValTok{52}
\NormalTok{success\_array\_daily }\OperatorTok{=}\NormalTok{ binom.rvs(n}\OperatorTok{=}\DecValTok{1}\NormalTok{, p}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, size}\OperatorTok{=}\NormalTok{N\_weeks}\OperatorTok{*}\DecValTok{7}\NormalTok{)  }\CommentTok{\# first generate daily data}
\NormalTok{success\_array }\OperatorTok{=}\NormalTok{ success\_array\_daily.reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# aggragate to weekly data}
\CommentTok{\# success\_array = binom.rvs(n=7, p=0.5, size=N\_weeks)  \# do this if you don\textquotesingle{}t need daily data}
\NormalTok{failure\_array }\OperatorTok{=} \DecValTok{7} \OperatorTok{{-}}\NormalTok{ success\_array}
\NormalTok{sf\_array }\OperatorTok{=}\NormalTok{ np.vstack([success\_array, failure\_array]).T}
\BuiltInTok{print}\NormalTok{(success\_array)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[4 4 5 6 4 4 2 3 5 4 2 3 5 3 3 5 4 3 1 3 4 4 2 2 3 5 1 2 6 4 3 2 3 5 6 3 4
 5 3 5 2 3 4 3 3 5 5 3 5 6 5 2]
\end{verbatim}

Now we just use the updating formula iteratively over the 52 weeks of
observations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bobs\_parameters }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{2}\NormalTok{, }\DecValTok{18}\NormalTok{]])}
\NormalTok{alices\_parameters }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{100}\NormalTok{, }\DecValTok{400}\NormalTok{]])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}t\textbackslash{}t}\StringTok{Bob}\CharTok{\textbackslash{}t\textbackslash{}t}\StringTok{Bob}\CharTok{\textbackslash{}t\textbackslash{}t}\StringTok{Alice}\CharTok{\textbackslash{}t}\StringTok{Alice"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"week}\CharTok{\textbackslash{}t}\StringTok{alpha}\CharTok{\textbackslash{}t}\StringTok{beta}\CharTok{\textbackslash{}t}\StringTok{alpha}\CharTok{\textbackslash{}t}\StringTok{beta"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\DecValTok{0}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{bobs\_parameters[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{bobs\_parameters[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{alices\_parameters[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{alices\_parameters[}\DecValTok{0}\NormalTok{][}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ week }\KeywordTok{in}\NormalTok{ np.arange(N\_weeks):}
\NormalTok{    bobs\_last\_weeks\_parameters }\OperatorTok{=}\NormalTok{ bobs\_parameters[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    bobs\_this\_weeks\_parameters }\OperatorTok{=}\NormalTok{ bobs\_last\_weeks\_parameters }\OperatorTok{+}\NormalTok{ sf\_array[week]}
\NormalTok{    bobs\_parameters }\OperatorTok{=}\NormalTok{ np.vstack([bobs\_parameters, bobs\_this\_weeks\_parameters])}

\NormalTok{    alices\_last\_weeks\_parameters }\OperatorTok{=}\NormalTok{ alices\_parameters[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    alices\_this\_weeks\_parameters }\OperatorTok{=}\NormalTok{ alices\_last\_weeks\_parameters }\OperatorTok{+}\NormalTok{ sf\_array[week]}
\NormalTok{    alices\_parameters }\OperatorTok{=}\NormalTok{ np.vstack([alices\_parameters, alices\_this\_weeks\_parameters])}

    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{week}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{bobs\_this\_weeks\_parameters[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{bobs\_this\_weeks\_parameters[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{alices\_this\_weeks\_parameters[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\CharTok{\textbackslash{}t\textbackslash{}t}\SpecialCharTok{\{}\NormalTok{alices\_this\_weeks\_parameters[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Bob     Bob     Alice   Alice
week    alpha   beta    alpha   beta
0       2       18      100     400
1       6       21      104     403
2       10      24      108     406
3       15      26      113     408
4       21      27      119     409
5       25      30      123     412
6       29      33      127     415
7       31      38      129     420
8       34      42      132     424
9       39      44      137     426
10      43      47      141     429
11      45      52      143     434
12      48      56      146     438
13      53      58      151     440
14      56      62      154     444
15      59      66      157     448
16      64      68      162     450
17      68      71      166     453
18      71      75      169     457
19      72      81      170     463
20      75      85      173     467
21      79      88      177     470
22      83      91      181     473
23      85      96      183     478
24      87      101     185     483
25      90      105     188     487
26      95      107     193     489
27      96      113     194     495
28      98      118     196     500
29      104     119     202     501
30      108     122     206     504
31      111     126     209     508
32      113     131     211     513
33      116     135     214     517
34      121     137     219     519
35      127     138     225     520
36      130     142     228     524
37      134     145     232     527
38      139     147     237     529
39      142     151     240     533
40      147     153     245     535
41      149     158     247     540
42      152     162     250     544
43      156     165     254     547
44      159     169     257     551
45      162     173     260     555
46      167     175     265     557
47      172     177     270     559
48      175     181     273     563
49      180     183     278     565
50      186     184     284     566
51      191     186     289     568
52      193     191     291     573
\end{verbatim}

Finally, we can plot how the probability densities of Bob and Alice are
updated over the 52 weeks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N\_weeks }\OperatorTok{=} \DecValTok{52}
\NormalTok{N\_panels }\OperatorTok{=}\NormalTok{ N\_weeks }\OperatorTok{+} \DecValTok{1}
\NormalTok{gs }\OperatorTok{=}\NormalTok{ grid\_spec.GridSpec(N\_panels,}\DecValTok{1}\NormalTok{)}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{12}\NormalTok{))}
\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{bob\_colors }\OperatorTok{=}\NormalTok{ mpl.cm.Blues(np.linspace(}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.8}\NormalTok{,N\_panels))}
\NormalTok{alice\_colors }\OperatorTok{=}\NormalTok{ mpl.cm.Reds(np.linspace(}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.8}\NormalTok{,N\_panels))}
\NormalTok{ax\_objs }\OperatorTok{=}\NormalTok{ []}

\ControlFlowTok{for}\NormalTok{ week }\KeywordTok{in}\NormalTok{ np.arange(N\_panels)[::}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]:}
    \CommentTok{\# creating new axes object, start from top = week 52}
\NormalTok{    ax\_objs.append(fig.add\_subplot(gs[N\_panels}\OperatorTok{{-}}\NormalTok{week}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:N\_panels}\OperatorTok{{-}}\NormalTok{week, }\DecValTok{0}\NormalTok{:]))}
    
\NormalTok{    bobs\_params }\OperatorTok{=}\NormalTok{ bobs\_parameters[week]}
\NormalTok{    alices\_params }\OperatorTok{=}\NormalTok{ alices\_parameters[week]}

    \CommentTok{\# don\textquotesingle{}t plot the whole distribution, only when greater than threshold}
\NormalTok{    range\_bob }\OperatorTok{=}\NormalTok{ beta.pdf(p, bobs\_params[}\DecValTok{0}\NormalTok{], bobs\_params[}\DecValTok{1}\NormalTok{])}
\NormalTok{    domain }\OperatorTok{=}\NormalTok{ np.where(range\_bob }\OperatorTok{\textgreater{}} \FloatTok{1e{-}3}\OperatorTok{*}\NormalTok{np.}\BuiltInTok{max}\NormalTok{(range\_bob))}
\NormalTok{    ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].fill\_between(p[domain], range\_bob[domain],}
\NormalTok{                             color}\OperatorTok{=}\NormalTok{bob\_colors[week], alpha}\OperatorTok{=}\FloatTok{1.0}\NormalTok{,}
\NormalTok{                             clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{, ec}\OperatorTok{=}\StringTok{"white"}\NormalTok{, zorder}\OperatorTok{=}\NormalTok{N\_panels}\OperatorTok{{-}}\NormalTok{week,}
\NormalTok{                             label}\OperatorTok{=}\StringTok{"Bob"}\NormalTok{)}
\NormalTok{    range\_alice }\OperatorTok{=}\NormalTok{ beta.pdf(p, alices\_params[}\DecValTok{0}\NormalTok{], alices\_params[}\DecValTok{1}\NormalTok{])}
\NormalTok{    domain }\OperatorTok{=}\NormalTok{ np.where(range\_alice }\OperatorTok{\textgreater{}} \FloatTok{1e{-}3}\OperatorTok{*}\NormalTok{np.}\BuiltInTok{max}\NormalTok{(range\_alice))}
\NormalTok{    ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].fill\_between(p[domain], range\_alice[domain],}
\NormalTok{                             color}\OperatorTok{=}\NormalTok{alice\_colors[week], alpha}\OperatorTok{=}\FloatTok{1.0}\NormalTok{,}
\NormalTok{                             clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{, ec}\OperatorTok{=}\StringTok{"white"}\NormalTok{, zorder}\OperatorTok{=}\NormalTok{N\_panels}\OperatorTok{{-}}\NormalTok{week,}
\NormalTok{                             label}\OperatorTok{=}\StringTok{"Alice"}\NormalTok{)}

\NormalTok{    ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.6}\NormalTok{),}
\NormalTok{                    ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{15}\NormalTok{),}
\NormalTok{                    yticks}\OperatorTok{=}\NormalTok{[])}
    \ControlFlowTok{if}\NormalTok{ week}\OperatorTok{\textgreater{}}\DecValTok{0}\NormalTok{:}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].set\_xticks([])}

    \CommentTok{\# make background transparent}
\NormalTok{    rect }\OperatorTok{=}\NormalTok{ ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].patch}
\NormalTok{    rect.set\_alpha(}\DecValTok{0}\NormalTok{)}
\NormalTok{    ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].set\_yticklabels([])}

    \ControlFlowTok{if}\NormalTok{ week }\OperatorTok{==}\NormalTok{ N\_panels}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{, loc}\OperatorTok{=}\StringTok{"upper left"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{, ncol}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ week}\OperatorTok{\%}\DecValTok{4}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].set\_yticks([}\DecValTok{0}\NormalTok{])}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].set\_yticklabels([}\SpecialStringTok{f"week }\SpecialCharTok{\{}\NormalTok{week}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{])}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].tick\_params(axis}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{, length}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].axhline(}\DecValTok{0}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{, zorder}\OperatorTok{=}\NormalTok{N\_panels}\OperatorTok{{-}}\NormalTok{week}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{    spines }\OperatorTok{=}\NormalTok{ [}\StringTok{"top"}\NormalTok{,}\StringTok{"right"}\NormalTok{,}\StringTok{"left"}\NormalTok{,}\StringTok{"bottom"}\NormalTok{]}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ spines:}
\NormalTok{        ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].spines[s].set\_visible(}\VariableTok{False}\NormalTok{)}

\NormalTok{ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability densities updated over 52 weeks\textquotesingle{}}\NormalTok{)}
\NormalTok{gs.update(hspace}\OperatorTok{={-}}\FloatTok{0.7}\NormalTok{)}

\NormalTok{ax\_top }\OperatorTok{=}\NormalTok{ ax\_objs[}\DecValTok{0}\NormalTok{]}
\NormalTok{ax\_bottom }\OperatorTok{=}\NormalTok{ ax\_objs[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{pos\_top }\OperatorTok{=}\NormalTok{ ax\_top.get\_position().extents }\CommentTok{\# left, bottom, right, top}
\NormalTok{pos\_bottom }\OperatorTok{=}\NormalTok{ ax\_bottom.get\_position().extents }\CommentTok{\# left, bottom, right, top}
\NormalTok{x\_dotted\_lines }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\FloatTok{0.6}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ x\_dotted\_lines:}
\NormalTok{    display\_coord }\OperatorTok{=}\NormalTok{ ax\_bottom.transData.transform([x, }\FloatTok{0.0}\NormalTok{])  }\CommentTok{\# convert data coordinates to display coordinates}
\NormalTok{    fig\_coord }\OperatorTok{=}\NormalTok{ fig.transFigure.inverted().transform(display\_coord)  }\CommentTok{\# convert display coordinates to figure coordinates}
\NormalTok{    line }\OperatorTok{=}\NormalTok{ Line2D([fig\_coord[}\DecValTok{0}\NormalTok{], fig\_coord[}\DecValTok{0}\NormalTok{]],     }\CommentTok{\# x coordinates}
\NormalTok{                [fig\_coord[}\DecValTok{1}\NormalTok{], pos\_top[}\DecValTok{1}\NormalTok{]],          }\CommentTok{\# y coordinates}
\NormalTok{                transform}\OperatorTok{=}\NormalTok{fig.transFigure, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    fig.add\_artist(line)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/conjugate-prior_files/figure-pdf/cell-7-output-1.png}}

Although Alice starts with a more accurate prior, Bob's belief converges
faster towards the true probability of rain (50\%) over time, because
his prior at week 0 was less confident (more spread out), allowing new
evidence to have a greater impact on his posterior belief. For each
person's pdf, we can plot its mean and standard deviation over time. For
a Beta distribution \(\text{Beta}(p|\alpha, \beta)\), the mean and
variance are given by:

\begin{align*}
\text{mean} &= \frac{\alpha}{\alpha + \beta}, \\
\text{variance} &= \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{t }\OperatorTok{=}\NormalTok{ np.arange(N\_weeks}\OperatorTok{+}\DecValTok{1}\NormalTok{)}

\KeywordTok{def}\NormalTok{ beta\_mean(alpha, beta):}
    \ControlFlowTok{return}\NormalTok{ alpha }\OperatorTok{/}\NormalTok{ (alpha }\OperatorTok{+}\NormalTok{ beta)}
\KeywordTok{def}\NormalTok{ beta\_variance(alpha, beta):}
    \ControlFlowTok{return}\NormalTok{ (alpha }\OperatorTok{*}\NormalTok{ beta) }\OperatorTok{/}\NormalTok{ ((alpha }\OperatorTok{+}\NormalTok{ beta)}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ (alpha }\OperatorTok{+}\NormalTok{ beta }\OperatorTok{+} \DecValTok{1}\NormalTok{))}

\NormalTok{bob\_means }\OperatorTok{=}\NormalTok{ beta\_mean(bobs\_parameters[:,}\DecValTok{0}\NormalTok{], bobs\_parameters[:,}\DecValTok{1}\NormalTok{])}
\NormalTok{bob\_sqrt\_variance }\OperatorTok{=}\NormalTok{ np.sqrt(beta\_variance(bobs\_parameters[:,}\DecValTok{0}\NormalTok{], bobs\_parameters[:,}\DecValTok{1}\NormalTok{]))}
\NormalTok{alice\_means }\OperatorTok{=}\NormalTok{ beta\_mean(alices\_parameters[:,}\DecValTok{0}\NormalTok{], alices\_parameters[:,}\DecValTok{1}\NormalTok{])}
\NormalTok{alice\_sqrt\_variance }\OperatorTok{=}\NormalTok{ np.sqrt(beta\_variance(alices\_parameters[:,}\DecValTok{0}\NormalTok{], alices\_parameters[:,}\DecValTok{1}\NormalTok{]))}

\NormalTok{line\_bob, }\OperatorTok{=}\NormalTok{ ax.plot(t, bob\_means, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Mean"}\NormalTok{)}
\NormalTok{fill\_bob }\OperatorTok{=}\NormalTok{ ax.fill\_between(t, bob\_means }\OperatorTok{{-}}\NormalTok{ bob\_sqrt\_variance, bob\_means }\OperatorTok{+}\NormalTok{ bob\_sqrt\_variance,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Std Dev"}\NormalTok{)}
\NormalTok{line\_alice, }\OperatorTok{=}\NormalTok{ ax.plot(t, alice\_means, color}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Alice\textquotesingle{}s Mean"}\NormalTok{)}
\NormalTok{fill\_alice }\OperatorTok{=}\NormalTok{ ax.fill\_between(t, alice\_means }\OperatorTok{{-}}\NormalTok{ alice\_sqrt\_variance, alice\_means }\OperatorTok{+}\NormalTok{ alice\_sqrt\_variance,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Alice\textquotesingle{}s Std Dev"}\NormalTok{)}

\NormalTok{ax.axhline(}\FloatTok{0.5}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{ax.legend([(line\_bob, fill\_bob), (line\_alice, fill\_alice)],}
\NormalTok{          [}\VerbatimStringTok{r"Bob\textquotesingle{}s mean}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{pm}\DecValTok{$}\VerbatimStringTok{std"}\NormalTok{, }\VerbatimStringTok{r"Alice\textquotesingle{}s mean}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{pm}\DecValTok{$}\VerbatimStringTok{std"}\NormalTok{],}
\NormalTok{          handler\_map}\OperatorTok{=}\NormalTok{\{}\BuiltInTok{tuple}\NormalTok{: mpl.legend\_handler.HandlerTuple(ndivide}\OperatorTok{=}\DecValTok{1}\NormalTok{)\},}
\NormalTok{          frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Weeks\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Estimated Expected Probability of Rain\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Convergence of Bob and Alice}\CharTok{\textbackslash{}\textquotesingle{}}\StringTok{s Estimates Over Time\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.6}\NormalTok{),}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,N\_weeks))}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/conjugate-prior_files/figure-pdf/cell-8-output-1.png}}

\section{comparison with frequentist
approach}\label{comparison-with-frequentist-approach}

Let's add to the graph above one more line, for Charlie, who follows a
frequentist approach to estimate the probability of rain over time.
Charlie doesn't have any prior belief; instead, he simply counts the
number of rainy days observed so far (\(k\)) and divides it by the total
number of days that passed (\(n\)). As shown in the chapter
\href{./foundations/MLE_and_summary_statistics.html\#binomial-distribution}{MLE
and summary statistics}, when assuming a generative model that is a
Bernoulli process (or Binomial distribution for multiple trials), the
maximum likelihood estimate (MLE) of the probability of success (rain)
is given by the ratio of the number of successes to the total number of
trials:

\[
\hat{p} = \frac{k}{n}.
\]

The conclusion here is that, even if Charlied doesn't know it, his
estimate is based on the Maximum Likelihood Estimation (MLE) principle.

This time, instead of plotting the weekly estimates, let's see what
happens if each of the residents of city A updates their beliefs daily.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{failure\_array\_daily }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ success\_array\_daily}
\NormalTok{sf\_array\_daily }\OperatorTok{=}\NormalTok{ np.vstack([success\_array\_daily, failure\_array\_daily]).T}

\NormalTok{bobs\_parameters\_daily }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{2}\NormalTok{, }\DecValTok{18}\NormalTok{]])}
\NormalTok{alices\_parameters\_daily }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{100}\NormalTok{, }\DecValTok{400}\NormalTok{]])}

\ControlFlowTok{for}\NormalTok{ day }\KeywordTok{in}\NormalTok{ np.arange(N\_weeks}\OperatorTok{*}\DecValTok{7}\NormalTok{):}
\NormalTok{    bobs\_yesterdays\_parameters }\OperatorTok{=}\NormalTok{ bobs\_parameters\_daily[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    bobs\_todays\_parameters }\OperatorTok{=}\NormalTok{ bobs\_yesterdays\_parameters }\OperatorTok{+}\NormalTok{ sf\_array\_daily[day]}
\NormalTok{    bobs\_parameters\_daily }\OperatorTok{=}\NormalTok{ np.vstack([bobs\_parameters\_daily, bobs\_todays\_parameters])}
\NormalTok{    alices\_yesterdays\_parameters }\OperatorTok{=}\NormalTok{ alices\_parameters\_daily[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    alices\_todays\_parameters }\OperatorTok{=}\NormalTok{ alices\_yesterdays\_parameters }\OperatorTok{+}\NormalTok{ sf\_array\_daily[day]}
\NormalTok{    alices\_parameters\_daily }\OperatorTok{=}\NormalTok{ np.vstack([alices\_parameters\_daily, alices\_todays\_parameters])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{4}\NormalTok{))}

\NormalTok{t }\OperatorTok{=} \DecValTok{7}\OperatorTok{*}\NormalTok{np.arange(N\_weeks}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{t\_day}\OperatorTok{=}\NormalTok{ np.arange(N\_weeks}\OperatorTok{*}\DecValTok{7}\OperatorTok{+}\DecValTok{1}\NormalTok{)}


\NormalTok{bob\_means\_daily }\OperatorTok{=}\NormalTok{ beta\_mean(bobs\_parameters\_daily[:,}\DecValTok{0}\NormalTok{], bobs\_parameters\_daily[:,}\DecValTok{1}\NormalTok{])}
\NormalTok{bob\_sqrt\_variance\_daily }\OperatorTok{=}\NormalTok{ np.sqrt(beta\_variance(bobs\_parameters\_daily[:,}\DecValTok{0}\NormalTok{], bobs\_parameters\_daily[:,}\DecValTok{1}\NormalTok{]))}
\NormalTok{alice\_means\_daily }\OperatorTok{=}\NormalTok{ beta\_mean(alices\_parameters\_daily[:,}\DecValTok{0}\NormalTok{], alices\_parameters\_daily[:,}\DecValTok{1}\NormalTok{])}
\NormalTok{alice\_sqrt\_variance\_daily }\OperatorTok{=}\NormalTok{ np.sqrt(beta\_variance(alices\_parameters\_daily[:,}\DecValTok{0}\NormalTok{], alices\_parameters\_daily[:,}\DecValTok{1}\NormalTok{]))}

\NormalTok{line\_bob\_daily, }\OperatorTok{=}\NormalTok{ ax.plot(t\_day, bob\_means\_daily, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Mean (Daily)"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fill\_bob\_daily }\OperatorTok{=}\NormalTok{ ax.fill\_between(t\_day, bob\_means\_daily }\OperatorTok{{-}}\NormalTok{ bob\_sqrt\_variance\_daily, bob\_means\_daily }\OperatorTok{+}\NormalTok{ bob\_sqrt\_variance\_daily,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Std Dev"}\NormalTok{)}
\NormalTok{line\_alice\_daily, }\OperatorTok{=}\NormalTok{ ax.plot(t\_day, alice\_means\_daily, color}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Alice\textquotesingle{}s Mean (Daily)"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fill\_alice\_daily }\OperatorTok{=}\NormalTok{ ax.fill\_between(t\_day, alice\_means\_daily }\OperatorTok{{-}}\NormalTok{ alice\_sqrt\_variance\_daily, alice\_means\_daily }\OperatorTok{+}\NormalTok{ alice\_sqrt\_variance\_daily,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Alice\textquotesingle{}s Std Dev"}\NormalTok{)}

\NormalTok{bob\_parameters\_frequentist }\OperatorTok{=}\NormalTok{ bobs\_parameters\_daily }\OperatorTok{{-}}\NormalTok{ bobs\_parameters\_daily[}\DecValTok{0}\NormalTok{]}
\NormalTok{bob\_means\_frequentist }\OperatorTok{=}\NormalTok{ bob\_parameters\_frequentist[}\DecValTok{1}\NormalTok{:,}\DecValTok{0}\NormalTok{] }\OperatorTok{/}\NormalTok{ (bob\_parameters\_frequentist[}\DecValTok{1}\NormalTok{:,}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ bob\_parameters\_frequentist[}\DecValTok{1}\NormalTok{:,}\DecValTok{1}\NormalTok{])}
\NormalTok{line\_bob\_frequentist, }\OperatorTok{=}\NormalTok{ ax.plot(t\_day[}\DecValTok{1}\NormalTok{:], bob\_means\_frequentist, color}\OperatorTok{=}\StringTok{"purple"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Frequentist Estimate"}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{3}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, clip\_on}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{ax.axhline(}\FloatTok{0.5}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{ax.legend([(line\_bob\_daily, fill\_bob\_daily), (line\_alice\_daily, fill\_alice\_daily), (line\_bob\_frequentist,)],}
\NormalTok{          [}\VerbatimStringTok{r"Bob\textquotesingle{}s mean}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{pm}\DecValTok{$}\VerbatimStringTok{std"}\NormalTok{, }\VerbatimStringTok{r"Alice\textquotesingle{}s mean}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{pm}\DecValTok{$}\VerbatimStringTok{std"}\NormalTok{, }\StringTok{"Charlie, the frequentist"}\NormalTok{],}
\NormalTok{          handler\_map}\OperatorTok{=}\NormalTok{\{}\BuiltInTok{tuple}\NormalTok{: mpl.legend\_handler.HandlerTuple(ndivide}\OperatorTok{=}\DecValTok{1}\NormalTok{)\},}
\NormalTok{          frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Days\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Estimated Expected Probability of Rain\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Frequentist vs Bayesian Daily Updates\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{150}\NormalTok{)}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/conjugate-prior_files/figure-pdf/cell-10-output-1.png}}

Let's digest what we see above.

\begin{itemize}
\tightlist
\item
  The frequentist approach does not use prior beliefs, it simply
  estimates the probability based on the observed data.
\item
  At day zero, Bob and Alice have beliefs about the world, but Charlie
  doesn't have any data to base his estimate on, so we see nothing for
  him at day zero.
\item
  The very first day was a rainy day, so Charlie's estimate begins at
  100\%.
\item
  In the following days, Charlie's estimate fluctuates more wildly than
  Bob's and Alice's, especially in the early weeks when the amount of
  data is still small. As more data is collected, Charlie's estimate
  stabilizes and converges towards the true probability of rain (50\%).
\item
  As the number of observations increases, all three estimates (Bob's,
  Alice's, and Charlie's) converge towards the true probability of rain
  (50\%).
\end{itemize}

Pierre-Simon Laplace came up with a solution to Charlie's ``small sample
size'' problem, where he estimates 100\% probability of rain on day one.
Laplace did that in a similar context, answering the question ``will the
sun rise tomorrow?''. Instead of not assuming anything like Charlie,
Laplace proposes the ``rule of succession'', which says that we assume
one success and one failure even before observing any data. Translating
that to a Beta prior, it means starting with \(\alpha=1\) and
\(\beta=1\), which is a uniform prior over \(p\), see the graph below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{laplaces\_parameters\_daily }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\ControlFlowTok{for}\NormalTok{ day }\KeywordTok{in}\NormalTok{ np.arange(N\_weeks}\OperatorTok{*}\DecValTok{7}\NormalTok{):}
\NormalTok{    laplaces\_yesterdays\_parameters }\OperatorTok{=}\NormalTok{ laplaces\_parameters\_daily[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{    laplaces\_todays\_parameters }\OperatorTok{=}\NormalTok{ laplaces\_yesterdays\_parameters }\OperatorTok{+}\NormalTok{ sf\_array\_daily[day]}
\NormalTok{    laplaces\_parameters\_daily }\OperatorTok{=}\NormalTok{ np.vstack([laplaces\_parameters\_daily, laplaces\_todays\_parameters])}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{6}\NormalTok{))}

\NormalTok{laplaces\_means\_daily }\OperatorTok{=}\NormalTok{ beta\_mean(laplaces\_parameters\_daily[:,}\DecValTok{0}\NormalTok{], laplaces\_parameters\_daily[:,}\DecValTok{1}\NormalTok{])}
\NormalTok{laplaces\_sqrt\_variance\_daily }\OperatorTok{=}\NormalTok{ np.sqrt(beta\_variance(laplaces\_parameters\_daily[:,}\DecValTok{0}\NormalTok{], laplaces\_parameters\_daily[:,}\DecValTok{1}\NormalTok{]))}

\NormalTok{line\_bob\_daily, }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].plot(t\_day, bob\_means\_daily, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Mean (Daily)"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fill\_bob\_daily }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].fill\_between(t\_day, bob\_means\_daily }\OperatorTok{{-}}\NormalTok{ bob\_sqrt\_variance\_daily, bob\_means\_daily }\OperatorTok{+}\NormalTok{ bob\_sqrt\_variance\_daily,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Std Dev"}\NormalTok{)}
\NormalTok{line\_alice\_daily, }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].plot(t\_day, alice\_means\_daily, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Alice\textquotesingle{}s Mean (Daily)"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fill\_alice\_daily }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].fill\_between(t\_day, alice\_means\_daily }\OperatorTok{{-}}\NormalTok{ alice\_sqrt\_variance\_daily, alice\_means\_daily }\OperatorTok{+}\NormalTok{ alice\_sqrt\_variance\_daily,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Alice\textquotesingle{}s Std Dev"}\NormalTok{)}

\NormalTok{bob\_parameters\_frequentist }\OperatorTok{=}\NormalTok{ bobs\_parameters\_daily }\OperatorTok{{-}}\NormalTok{ bobs\_parameters\_daily[}\DecValTok{0}\NormalTok{]}
\NormalTok{bob\_means\_frequentist }\OperatorTok{=}\NormalTok{ bob\_parameters\_frequentist[}\DecValTok{1}\NormalTok{:,}\DecValTok{0}\NormalTok{] }\OperatorTok{/}\NormalTok{ (bob\_parameters\_frequentist[}\DecValTok{1}\NormalTok{:,}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ bob\_parameters\_frequentist[}\DecValTok{1}\NormalTok{:,}\DecValTok{1}\NormalTok{])}
\NormalTok{line\_bob\_frequentist, }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].plot(t\_day[}\DecValTok{1}\NormalTok{:], bob\_means\_frequentist, color}\OperatorTok{=}\StringTok{"purple"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Bob\textquotesingle{}s Frequentist Estimate"}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{, marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{3}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, clip\_on}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{line\_laplaces\_daily, }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].plot(t\_day, laplaces\_means\_daily, color}\OperatorTok{=}\StringTok{"tab:orange"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Laplace\textquotesingle{}s Mean (Daily)"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}"}\NormalTok{)}
\NormalTok{fill\_laplaces\_daily }\OperatorTok{=}\NormalTok{ ax[}\DecValTok{0}\NormalTok{].fill\_between(t\_day, laplaces\_means\_daily }\OperatorTok{{-}}\NormalTok{ laplaces\_sqrt\_variance\_daily, laplaces\_means\_daily }\OperatorTok{+}\NormalTok{ laplaces\_sqrt\_variance\_daily,}
\NormalTok{                color}\OperatorTok{=}\StringTok{"tab:orange"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, label}\OperatorTok{=}\StringTok{"Laplace\textquotesingle{}s Std Dev"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].axhline(}\FloatTok{0.5}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{, ls}\OperatorTok{=}\StringTok{"{-}{-}"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].legend([(line\_bob\_daily, fill\_bob\_daily), (line\_alice\_daily, fill\_alice\_daily), (line\_bob\_frequentist,), (line\_laplaces\_daily, fill\_laplaces\_daily)],}
\NormalTok{          [}\VerbatimStringTok{r"Bob\textquotesingle{}s mean}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{pm}\DecValTok{$}\VerbatimStringTok{std"}\NormalTok{, }\VerbatimStringTok{r"Alice\textquotesingle{}s mean}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{pm}\DecValTok{$}\VerbatimStringTok{std"}\NormalTok{, }\StringTok{"Charlie, the frequentist"}\NormalTok{, }\StringTok{"Laplace"}\NormalTok{],}
\NormalTok{          handler\_map}\OperatorTok{=}\NormalTok{\{}\BuiltInTok{tuple}\NormalTok{: mpl.legend\_handler.HandlerTuple(ndivide}\OperatorTok{=}\DecValTok{1}\NormalTok{)\},}
\NormalTok{          frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Days\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Estimated Expected Probability of Rain\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Frequentist vs Bayesian Daily Updates\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{)}
\NormalTok{       )}\OperatorTok{;}

\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(p, beta.pdf(p, laplaces\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], laplaces\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]), label}\OperatorTok{=}\StringTok{\textquotesingle{}Beta(1, 1) {-} Laplace\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"black"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(p, beta.pdf(p, bobs\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], bobs\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]), label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Beta(}\SpecialCharTok{\{}\NormalTok{bobs\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{, }\SpecialCharTok{\{}\NormalTok{bobs\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{) {-} Bob\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:orange"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(p, beta.pdf(p, alices\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{], alices\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]), label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Beta(}\SpecialCharTok{\{}\NormalTok{alices\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{, }\SpecialCharTok{\{}\NormalTok{alices\_parameters\_daily[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{) {-} Alice\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{"tab:blue"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability of rain (p)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{\textquotesingle{}Beta prior distributions with mean at 0.1\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/conjugate-prior_files/figure-pdf/cell-11-output-1.png}}

Laplace's estimate doesn't suffer from Charlie's extreme initial
estimate. Just after a few days, however, it is almost indistinguishable
from Charlie's estimate.

\section{conjugate pairs}\label{conjugate-pairs}

In the example above, we saw how convenient it is to use a prior
function that is conjugate to the likelihood function. The generating
process behind the observations was a Bernoulli process (or Binomial
distribution for multiple trials), so we chose its conjugate, the Beta
distribution.

Some examples of processes that can be described by a Binomial
distribution (or Bernoulli process) are:

\begin{itemize}
\tightlist
\item
  \textbf{Conversion Rates:} A marketing team has a prior belief about
  how many people will click an email link (Success) vs.~ignore it
  (Failure).
\item
  \textbf{Quality Control:} A factory manager has a prior belief about
  the proportion of defective items (Success) vs.~non-defective items
  (Failure) in a production batch.
\item
  \textbf{Medical Trials:} A researcher has a prior belief about the
  effectiveness of a new drug (Success) vs.~ineffectiveness (Failure)
  based on preliminary studies.
\end{itemize}

What if the generating process was different?

\subsection{Poisson distribution}\label{poisson-distribution}

The Poisson distribution models the number of events occurring in a
fixed interval of time or space (the ``count''). The Gamma distribution
is its conjugate prior because it is defined for positive values (0 to
\(\infty\)), making it perfect for modeling the rate (\(\lambda\)) at
which these events happen. It can be used in scenarios such as:

\begin{itemize}
\tightlist
\item
  \textbf{Ecohydrology:} Modeling the number of rainfall pulses in a
  desert ecosystem during the growing season.
\item
  \textbf{Customer Service:} Estimating the rate of phone calls arriving
  at a help desk per hour.
\item
  \textbf{Radioactive Decay:} Predicting the number of particles emitted
  by a substance over a specific duration.
\end{itemize}

\subsection{normal distribution (known
variance)}\label{normal-distribution-known-variance}

When you assume your data follows a Normal distribution (like
measurement errors) and you know the variance, the conjugate prior for
the mean is also a Normal distribution. This creates a beautiful
``weighted average'' effect: the posterior mean will sit somewhere
between your prior guess and the data's mean, depending on which one is
more certain. It is useful in scenarios such as:

\begin{itemize}
\tightlist
\item
  \textbf{Measurement Errors:} Estimating the true value of a physical
  quantity when measurements are subject to random errors.
\item
  \textbf{Quality Control:} Determining the average weight of products
  in a manufacturing process where individual weights vary normally
  around a true mean.
\item
  \textbf{Psychometrics:} Estimating the average score of a
  psychological test when individual scores are normally distributed
  with known variance.
\end{itemize}

\subsection{categorical / multinomial
distribution}\label{categorical-multinomial-distribution}

This is the multi-dimensional version of the Beta distribution. Instead
of just ``Success/Failure,'' you have multiple categories (e.g.,
``Rain/Sun/Clouds''). The Dirichlet distribution allows you to track the
probabilities of all these categories simultaneously. It is useful in
scenarios such as:

\begin{itemize}
\tightlist
\item
  \textbf{Topic Modeling:} In Machine Learning, determining the
  ``theme'' of a document by looking at the frequency of different words
  (each word is a category).
\item
  \textbf{Political Polling:} Estimating the support for four different
  candidates in an upcoming election.
\item
  \textbf{Genetics:} Modeling the frequency of different alleles (gene
  variants) within a specific population.
\end{itemize}

\section{uniform distribution (known
bounds)}\label{uniform-distribution-known-bounds}

This is used when you are trying to find the maximum possible value
(\(\theta\)) of a process. If you assume the data is spread evenly
(Uniform) up to some unknown limit, the Pareto distribution is the
conjugate prior that helps you ``narrow in'' on where that upper limit
actually sits. It is useful in scenarios such as:

\begin{itemize}
\tightlist
\item
  \textbf{The German Tank Problem:} Estimating the total number of tanks
  produced by an enemy based on the highest serial number found on
  captured tanks.
\item
  \textbf{Ecological Limits:} Estimating the maximum possible size a
  specific fish species can reach based on the largest specimens caught.
\item
  \textbf{Quality Control:} Determining the maximum defect size in a
  batch of manufactured items based on the largest defect observed in a
  sample.
\item
  \textbf{Project Management:} Estimating the maximum time required to
  complete a project based on the longest time taken for similar past
  projects.
\end{itemize}

\chapter{the boy-girl paradox}\label{the-boy-girl-paradox}

Mary has two children.

\begin{itemize}
\tightlist
\item
  \textbf{Q0} What is the probability that both children are boys?
\item
  \textbf{Q1} What is the probability that both children are boys, given
  that at least one is a boy?
\item
  \textbf{Q2} What is the probability that both children are boys, given
  that at least one is a boy born during daytime?
\item
  \textbf{Q3} What is the probability that both children are boys, given
  that at least one is a boy born on a Sunday?
\item
  \textbf{Q4} What is the probability that both children are boys, given
  that at least one is a boy born on the 1st of the month?
\end{itemize}

Successively, we added more stringent condtions on the original problem.
We will first solve this problem analytically and then visually.

\section{analytical solution}\label{analytical-solution}

Let's denote by \(j \in {1,2}\) the child index, and by \(B_j\) and
\(G_j\) the events that child \(j\) is a boy or a girl. We call
\(Q_j=B_j \cap T_j\) the ``Qualified Boy'' event, meaning that child
\(j\) is a boy \textbf{and} has the trait \(T_j\) (being born during
daytime, or on a Sunday, or on the 1st of the month). Because the
probability of being a boy is \(P(B_j)=1/2\), and the probability of
having trait \(T_j\) is \(P(T_j)\), we have:

\[
P(Q_j) = P(B_j) \cdot P(T_j) = \frac{1}{2} P(T_j),
\] where we assumed independence between sex and trait.

All of the questions above ask, in mathematical notation,

\[
P(B_1 \cap B_2 \mid Q_1 \cup Q_2).
\]

Let's translate that to English:

\begin{itemize}
\tightlist
\item
  What is the probability, \(P()\)
\item
  that child 1 is a boy, \(B_1\)
\item
  and, \(\cap\)
\item
  that child 2 is a boy, \(B_2\),
\item
  given that, \(\mid\)
\item
  either child (at least one), \(\cup\)
\item
  is a qualified boy, \(Q_1\) or \(Q_2\).
\end{itemize}

The table below summarizes the qualifications and their probabilities
for each question:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Question & Qualification \(Q_j\) & \(P(Q_j)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Q0 & None & \(1\) \\
Q1 & Boy & \(1/2\) \\
Q2 & Boy born during daytime & \(1/2\times 1/2\) \\
Q3 & Boy born born on a Sunday & \(1/2\times 1/7\) \\
Q4 & Boy born on the 1st of month & \(1/2\times 1/30\) \\
\end{longtable}

Question Q0 is a special case where there is no qualification, so
\(Q_j\) is always true, with probability 1. We can solve that one
directly: the possible combinations of children are BB, BG, GB, GG, so
the probability that both are boys is \(1/4\). In mathematical notation:
\[
P(B_1 \cap B_2) = P(B_1)P(B_2) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4}.
\]

To solve the qualified cases (Q1 through Q4), we use Bayes' theorem:

\[
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)},
\]

where

\begin{itemize}
\tightlist
\item
  \(A\) is the event that both children are boys, \(B_1 \cap B_2\),
\item
  \(B\) is the event that at least one child is a qualified boy,
  \(Q_1 \cup Q_2\).
\end{itemize}

So we need to solve

\[
P(B_1 \cap B_2 \mid Q_1 \cup Q_2) = \frac{P\left( \left(Q_1 \cup Q_2\right) \mid \left(B_1 \cap B_2\right)\right) P\left(B_1 \cap B_2\right)}{P\left(Q_1 \cup Q_2\right)}.
\]

\subsection{numerator}\label{numerator}

\[
P\left( \left(Q_1 \cup Q_2\right) \mid \left(B_1 \cap B_2\right)\right) P\left(B_1 \cap B_2\right).
\]

The second term is easy: \[
P\left(B_1 \cap B_2\right) = P(B_1)P(B_2) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4},
\] since the children's sexes are independent.

The first term is: \[
P\left( \left(Q_1 \cup Q_2\right) \mid \left(B_1 \cap B_2\right)\right),
\] or in English: given that we know both children are boys, what is the
probability that at least one of them is a qualified boy? We formulated
the qualified boy as \(Q_j = B_j \cap T_j\), but now we \textbf{know}
that \(B_j\) is true, that is, \(B_j=1\), so we can simplify \(Q_j\) to
just \(T_j\). So we need to calculate \[
P\left( T_1 \cup T_2\right).
\]

The probability that at least one child has trait \(T\) is more easily
calculated via its complement, the probability that neither child has
trait \(T\):

\[
P\left( T_1 \cup T_2\right) = 1 - P\left(\text{neither } T_1 \text{ nor } T_2 \right) = 1 - (1-p)^2 = 2p - p^2,
\] where we used independence between the two children, and denoted
\(p=P(T_j)\).

Thus, the numerator is: \[
P\left( \left(Q_1 \cup Q_2\right) \mid \left(B_1 \cap B_2\right)\right) P\left(B_1 \cap B_2\right) = (2p - p^2) \cdot \frac{1}{4} = \frac{p}{4}(2 - p).
\]

\subsection{denominator}\label{denominator}

\[
P\left(Q_1 \cup Q_2\right).
\] Using the inclusion-exclusion principle, we have: \[
P\left(Q_1 \cup Q_2\right) = P(Q_1) + P(Q_2) - P(Q_1 \cap Q_2).
\] Because the two children are independent, we have: \[
P(Q_1 \cap Q_2) = P(Q_1)P(Q_2) = \left(\frac{p}{2}\right)^2 = \frac{p^2}{4}.
\] Thus, the denominator is: \[
P\left(Q_1 \cup Q_2\right) = \frac{p}{2} + \frac{p}{2} - \frac{p^2}{4} = p - \frac{p^2}{4} = \frac{p}{4}\left(4 - p\right).
\]

\subsection{final result}\label{final-result}

Finally, putting everything together, we have: \[
P(B_1 \cap B_2 \mid Q_1 \cup Q_2) = \frac{\frac{p}{4}(2 - p)}{\frac{p}{4}\left(4 - p\right)} = \frac{2 - p}{4 - p}.
\] We can now plug in the values of \(p\) for each question:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2128}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5745}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2128}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Question
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(P(T_j)=p\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Answer
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Q0 & None & \(1/4=0.25\) \\
Q1 & \(1\) & \(\frac{1}{3} \approx 0.333\) \\
Q2 & \(1/2\) & \(\frac{3}{7} \approx 0.429\) \\
Q3 & \(1/7\) & \(\frac{13}{27} \approx 0.481\) \\
Q4 & \(1/30\) & \(\frac{59}{119} \approx 0.496\) \\
\end{longtable}

When no qualifications are made, the probability is \(1/4\). When
qualifications are made, the probability increases, approaching \(1/2\)
as the qualification becomes more stringent.

The algebra is done, but I'm left with an uneasy feeling. WHY?! What
does it really matter what day of the week the boy was born on? How can
that possibly affect the result?!

\section{visual solution}\label{visual-solution}

\subsection{Q0, what is the probability that both children are
boys?}\label{q0-what-is-the-probability-that-both-children-are-boys}

In this non-qualified case, there are four equally likely possibilities
for the two children: BB, BG, GB, GG. It is obvious that only one of
these four possibilities corresponds to both children being boys (purple
square), so the probability is \(1/4\).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.patches }\ImportTok{as}\NormalTok{ patches}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{N }\OperatorTok{=} \DecValTok{2}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.ones((N,N))}
\NormalTok{a[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(a,}
\NormalTok{               interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, aspect}\OperatorTok{=}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Major ticks}
\NormalTok{ax.set\_xticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}
\NormalTok{ax.set\_yticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}

\CommentTok{\# Labels for major ticks}
\NormalTok{ax.set\_xticklabels([}\StringTok{"B"}\NormalTok{, }\StringTok{"G"}\NormalTok{])}
\NormalTok{ax.set\_yticklabels([}\StringTok{"B"}\NormalTok{, }\StringTok{"G"}\NormalTok{])}

\CommentTok{\# Minor ticks}
\NormalTok{ax.set\_xticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax.set\_yticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Gridlines based on minor ticks}
\NormalTok{ax.grid(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Remove minor ticks}
\NormalTok{ax.tick\_params(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, bottom}\OperatorTok{=}\VariableTok{False}\NormalTok{, left}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{ax.set\_title(}\StringTok{"Mary has two children. Probability that both are boys?"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/boy-girl-paradox_files/figure-pdf/cell-3-output-1.png}}

\subsection{Q1, what is the probability that both children are boys,
given that at least one is a
boy?}\label{q1-what-is-the-probability-that-both-children-are-boys-given-that-at-least-one-is-a-boy}

We divide the question into two parts: the qualification and the target
event.

The target event is the purple square, where both children are boys.

The qualification is represented by the red rectangles.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{N }\OperatorTok{=} \DecValTok{2}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.ones((N,N))}
\NormalTok{a[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(a,}
\NormalTok{               interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, aspect}\OperatorTok{=}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Major ticks}
\NormalTok{ax.set\_xticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}
\NormalTok{ax.set\_yticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}

\CommentTok{\# Labels for major ticks}
\NormalTok{ax.set\_xticklabels([}\StringTok{"B"}\NormalTok{, }\StringTok{"G"}\NormalTok{])}
\NormalTok{ax.set\_yticklabels([}\StringTok{"B"}\NormalTok{, }\StringTok{"G"}\NormalTok{])}

\CommentTok{\# Minor ticks}
\NormalTok{ax.set\_xticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax.set\_yticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Gridlines based on minor ticks}
\NormalTok{ax.grid(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Remove minor ticks}
\NormalTok{ax.tick\_params(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, bottom}\OperatorTok{=}\VariableTok{False}\NormalTok{, left}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), N, }\DecValTok{1}\NormalTok{, }
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}
\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), }\DecValTok{1}\NormalTok{, N,}
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}

\NormalTok{ax.set\_title(}\StringTok{"Mary has two children. Probability that both are boys}\CharTok{\textbackslash{}n}\StringTok{given that at least one is a boy?"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/boy-girl-paradox_files/figure-pdf/cell-4-output-1.png}}

The horizontal rectangle corresponds to the first child being a boy, and
the vertical rectangle corresponds to the second child being a boy. The
squares inside the red rectangles represent the remaining possibilities
after the qualification. Each rectangle is \(2\times 1\), the number
\(1\) representing the constraint that at least one child is a boy, and
the number \(2\) representing the two equally likely possibilities for
the other child. The total number of squares inside the red rectangles
is:

\(D = (2 \times 1)\cdot 2 - 1 = 3\).

\begin{itemize}
\tightlist
\item
  \((2 \times 1)\) is the size of each rectangle,
\item
  \(\cdot 2\) because there are two rectangles,
\item
  \(-1\) because the square on the top left corner is counted twice (it
  is inside both rectangles).
\end{itemize}

Of the remaining possibilities (\(D=3\)), only one (\(N=1\)) corresponds
to both children being boys (the purple square).

The probability is therefore: \[
P = \frac{N}{D} = \frac{1}{3} \approx 0.333.
\]

\subsection{Q1, what is the probability that both children are boys,
given that at least one is a boy born during
daytime?}\label{q1-what-is-the-probability-that-both-children-are-boys-given-that-at-least-one-is-a-boy-born-during-daytime}

The target event is still the purple square, where both children are
boys.

The qualification is now represented by larger red rectangles.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{N }\OperatorTok{=} \DecValTok{4}  \CommentTok{\# 2*2}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.ones((N,N))}
\NormalTok{a[}\DecValTok{0}\NormalTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{:}\DecValTok{2}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\CommentTok{\# a[0,:] = 0.2}

\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(a, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, aspect}\OperatorTok{=}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}


\CommentTok{\# Major ticks}
\NormalTok{ax.set\_xticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}
\NormalTok{ax.set\_yticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}

\CommentTok{\# Labels for major ticks}
\NormalTok{ax.set\_xticklabels([}\StringTok{"B{-}day"}\NormalTok{, }\StringTok{"B{-}night"}\NormalTok{, }\StringTok{"G{-}day"}\NormalTok{, }\StringTok{"G{-}night"}\NormalTok{])}
\NormalTok{ax.set\_yticklabels([}\StringTok{"B{-}day"}\NormalTok{, }\StringTok{"B{-}night"}\NormalTok{, }\StringTok{"G{-}day"}\NormalTok{, }\StringTok{"G{-}night"}\NormalTok{])}

\CommentTok{\# Minor ticks}
\NormalTok{ax.set\_xticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax.set\_yticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Gridlines based on minor ticks}
\NormalTok{ax.grid(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Remove minor ticks}
\NormalTok{ax.tick\_params(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, bottom}\OperatorTok{=}\VariableTok{False}\NormalTok{, left}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), N, }\DecValTok{1}\NormalTok{, }
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}
\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), }\DecValTok{1}\NormalTok{, N,}
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}

\NormalTok{ax.set\_title(}\StringTok{"Mary has two children. Probability that both are boys}\CharTok{\textbackslash{}n}\StringTok{given that at least one is a boy born during daytime?"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/boy-girl-paradox_files/figure-pdf/cell-5-output-1.png}}

Each rectangle is \(2\cdot 2\) over \(1\) squares, where

\begin{itemize}
\tightlist
\item
  the number \(1\) represents the constraint that at least one child is
  a boy born during daytime,
\item
  the first \(2\) represents the two equally likely possibilities for
  the sex of the other child,
\item
  the second \(2\) represents the two equally likely possibilities for
  the time of birth (daytime or nighttime).
\end{itemize}

The total number of squares inside the red rectangles is: \[
D = (2 \cdot 2)\cdot 2 - 1 = 7.
\]

The factor \((2 \cdot 2)\) is the length of the rectangles, we multiply
by \(2\) because there are two rectangles, and we subtract \(1\) because
the square on the top left corner is counted twice (it is inside both
rectangles).

How many of those squares correspond to both children being boys (the
intersection of the red rectangle and the purple square)? The reasoning
of discounting the overlap square still holds, but clearly half of each
original red rectangle should be discarded, since only half lay in the
purple region:

\[
N = \frac{(2 \cdot 2)}{2}\cdot 2 - 1 = 3.
\]

The probability is therefore: \[
P = \frac{N}{D} = \frac{3}{7} \approx 0.429.
\]

\subsection{Q2, what is the probability that both children are boys,
given that at least one is a boy born on a
Sunday?}\label{q2-what-is-the-probability-that-both-children-are-boys-given-that-at-least-one-is-a-boy-born-on-a-sunday}

It seems that a pattern is emerging.

The target event is still the purple square, as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{N }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\DecValTok{7}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.ones((N,N))}
\NormalTok{a[}\DecValTok{0}\NormalTok{:}\DecValTok{7}\NormalTok{,}\DecValTok{0}\NormalTok{:}\DecValTok{7}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\CommentTok{\# a[0,:] = 0.2}

\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(a, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, aspect}\OperatorTok{=}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}


\CommentTok{\# Major ticks}
\NormalTok{ax.set\_xticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}
\NormalTok{ax.set\_yticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}

\CommentTok{\# Labels for major ticks}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f"B}\SpecialCharTok{\{}\NormalTok{x}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{:d\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{7}\NormalTok{)] }\OperatorTok{+}\NormalTok{ [}\SpecialStringTok{f"G}\SpecialCharTok{\{}\NormalTok{x}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{:d\}}\SpecialStringTok{"} \ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{7}\NormalTok{)]}
\NormalTok{ax.set\_xticklabels(labels)}
\NormalTok{ax.set\_yticklabels(labels)}

\CommentTok{\# Minor ticks}
\NormalTok{ax.set\_xticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax.set\_yticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Gridlines based on minor ticks}
\NormalTok{ax.grid(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Remove minor ticks}
\NormalTok{ax.tick\_params(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, bottom}\OperatorTok{=}\VariableTok{False}\NormalTok{, left}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), N, }\DecValTok{1}\NormalTok{, }
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}
\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), }\DecValTok{1}\NormalTok{, N,}
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}

\NormalTok{ax.set\_title(}\StringTok{"Mary has two children. Probability that both are boys}\CharTok{\textbackslash{}n}\StringTok{given that at least one is a boy born on a Sunday?"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/boy-girl-paradox_files/figure-pdf/cell-6-output-1.png}}

The red rectangles are now \(2\cdot 7\) over \(1\) squares, where \(1\)
represents the constraint that at least one child is a boy born on a
Sunday, \(2\) represents the two equally likely possibilities for the
sex of the other child, and \(7\) represents the seven equally likely
possibilities for the day of the week of birth.

The total number of squares inside the red rectangles is: \[
D = (2 \cdot 7)\cdot 2 - 1 = 27.
\]

Of those squares, the number that lay in the purple region is: \[
N = \frac{(2 \cdot 7)}{2}\cdot 2 - 1 = 13.
\]

The probability is therefore: \[
P = \frac{N}{D} = \frac{13}{27} \approx 0.481.
\]

\subsection{generalization}\label{generalization}

We can generalize the calculation above for any trait with \(k\) equally
likely possibilities:

\begin{align*}
D & = (2 \cdot k)\cdot 2 - 1 = 4k - 1, \\
N & = \frac{(2 \cdot k)}{2}\cdot 2 - 1 = 2k - 1, \\
P & = \frac{N}{D} = \frac{2k - 1}{4k - 1}.
\end{align*}

This is very similar to the analytical result we obtained before.
Indeed, substituting \(k=1/p\) in the result above yields the same
expression: \[
P = \frac{2 - p}{4 - p}.
\]

\section{the moral explanation}\label{the-moral-explanation}

Mathematicians use the word ``moral'' to refer to explanations that
capture the underlying reason why something must be true, rather than
just providing a mechanical, step-by-step verification. See this last
image, for the case that at least one on the children is a boy born on
the 1st of the month.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{N }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\DecValTok{30}
\NormalTok{a }\OperatorTok{=}\NormalTok{ np.ones((N,N))}
\NormalTok{a[}\DecValTok{0}\NormalTok{:}\DecValTok{30}\NormalTok{,}\DecValTok{0}\NormalTok{:}\DecValTok{30}\NormalTok{] }\OperatorTok{=} \DecValTok{0}
\CommentTok{\# a[0,:] = 0.2}

\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(a, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{0}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{1}\NormalTok{, aspect}\OperatorTok{=}\StringTok{\textquotesingle{}equal\textquotesingle{}}\NormalTok{)}


\CommentTok{\# Major ticks}
\NormalTok{ax.set\_xticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}
\NormalTok{ax.set\_yticks(np.arange(}\DecValTok{0}\NormalTok{, N, }\DecValTok{1}\NormalTok{))}

\CommentTok{\# Labels for major ticks}
\CommentTok{\# labels = [f"B\{x+1:d\}" for x in range(7)] + [f"G\{x+1:d\}" for x in range(7)]}
\NormalTok{ax.set\_xticklabels([])}
\NormalTok{ax.set\_yticklabels([])}

\CommentTok{\# Minor ticks}
\NormalTok{ax.set\_xticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax.set\_yticks(np.arange(}\OperatorTok{{-}}\FloatTok{.5}\NormalTok{, N, }\DecValTok{1}\NormalTok{), minor}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# Gridlines based on minor ticks}
\NormalTok{ax.grid(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# Remove minor ticks}
\NormalTok{ax.tick\_params(which}\OperatorTok{=}\StringTok{\textquotesingle{}minor\textquotesingle{}}\NormalTok{, bottom}\OperatorTok{=}\VariableTok{False}\NormalTok{, left}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), N, }\DecValTok{1}\NormalTok{, }
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}
\NormalTok{rect }\OperatorTok{=}\NormalTok{ patches.Rectangle((}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{), }\DecValTok{1}\NormalTok{, N,}
\NormalTok{                         linewidth}\OperatorTok{=}\DecValTok{4}\NormalTok{, }
\NormalTok{                         edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }
\NormalTok{                         facecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{,}
\NormalTok{                         clip\_on}\OperatorTok{=}\VariableTok{False}\NormalTok{,}
\NormalTok{                         zorder}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax.add\_patch(rect)}

\NormalTok{ax.set\_title(}\StringTok{"Mary has two children. Probability that both are boys}\CharTok{\textbackslash{}n}\StringTok{given that at least one is a boy born on the 1st of the month?"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/boy-girl-paradox_files/figure-pdf/cell-7-output-1.png}}

We can see that the answer we are seeking is the ratio between the
number of purple squares inside the red rectangles and the total number
of squares inside the red rectangles (discounting the overlap square for
each of them). As the qualification becomes more stringent (the trait
has more equally likely possibilities), the red rectangles become taller
and taller, and this ratio approaches \(1/2\). The act of discounting
the overlap square becomes negligible for large \(k\), since it is only
one square out of many.

In question Q1, saying that one child is a boy is a vague statement.
Which of them? Could be either. This great vagueness comes from the huge
overlap between the two red rectangles. In question Q4, saying that one
child is a boy born on the 1st of the month is a very specific
statement. Almost certainly it could only be one of the children, since
it is very unlikely that both children were born on the 1st of the
month. This lack of vagueness comes from the tiny overlap between the
two red rectangles. A super specific qualification effectively points at
a \textbf{specific} child, and thus the problem reduces to finding the
probability that the other child is a boy, which is clearly \(1/2\).
That is the moral argument.

I have two children. One is a boy, born on 29 February, he plays the
flute, goes to karate classes, and got stiches in his chin last summer
after a minor bike accident. What is the probability that my other child
is also a boy? We can all agree that there is very little doubt that I'm
talking about a specific child, since it is so unlikely that both
children share all those traits. Therefore, the probability that my
other child is a boy is \(1/2\).

\section{information}\label{information}

When I \href{https://youtu.be/JSE4oy0KQ2Q?si=JtV3X-UAzMIOkcMF}{first
heard this ``paradox''}, I could not fathom how adding irrelevant
information (the day of the week of birth) could possibly affect the
answer. As we have seen, it does. I want to quantify the amount of
information added by the qualification.

As we saw in the
\href{./information_theory/cross-entropy.html\#kullback-leibler-divergence}{cross-entropy
chapter}, we can use the Kullback-Leibler divergence to quantify the
information gained when updating our beliefs from a prior distribution
to a posterior distribution:

\[
D_{KL}(P \| Q) = \sum_i P(i) \log\frac{P(i)}{Q(i)},
\] where \(P\) is the posterior distribution and \(Q\) is the prior
distribution. The greater the difference between the two distributions,
the greater the information gain. Another way to say this is that the
qualification that most changes our beliefs is the one that provides the
most information.

Before we plug in the numbers, we need to define the prior and posterior
distributions.

\begin{itemize}
\tightlist
\item
  \textbf{\(\mathbf{q}\), Prior distribution:} our belief about the sex
  of the second child before knowing anything about the first child.
  This is simply \[
  \mathbf{q} = \begin{bmatrix} P(B_2) \\ P(G_2) \end{bmatrix} = \begin{bmatrix} 1/2 \\ 1/2 \end{bmatrix}.
  \]
\item
  \textbf{\(\mathbf{p}\), Posterior distribution:} our belief about the
  sex of the second child after knowing that at least one child is a
  qualified boy. This is \[
  \mathbf{p} = \begin{bmatrix} P(B_2) \\ P(G_2) \end{bmatrix} = \begin{bmatrix} \frac{2 - p}{4 - p} \\ 1-\frac{2 - p}{4 - p} \end{bmatrix}.
  \]
\end{itemize}

Plugging in these values, we can calculate the Kullback-Leibler
divergence for a qualification with probability \(p\):

\[
D_{KL}(\mathbf{p} \| \mathbf{q}) = \frac{2 - p}{4 - p} \log\left(\frac{2 - p}{4 - p} \cdot \frac{1}{1/2}\right) + \left(1-\frac{2 - p}{4 - p}\right) \log\left(\left(1-\frac{2 - p}{4 - p}\right) \cdot \frac{1}{1/2}\right).
\]

There are other possibilities to consider regarding the prior
distribution. For example, what if we considered the joint distribution
of both children, instead of just the second child? In this case, the
prior distribution would be: \[
\mathbf{q} = \begin{bmatrix} P(B_1 \cap B_2) \\ P(B_1 \cap G_2) \\ P(G_1 \cap B_2) \\ P(G_1 \cap G_2) \end{bmatrix} = \begin{bmatrix} 1/4 \\ 1/4 \\ 1/4 \\ 1/4 \end{bmatrix}.
\]

and the posterior distribution would be: \[
\mathbf{p} = \begin{bmatrix} P(B_1 \cap B_2) \\ P(B_1 \cap G_2) \\ P(G_1 \cap B_2) \\ P(G_1 \cap G_2) \end{bmatrix} = \begin{bmatrix} \frac{2 - p}{4 - p} \\ \frac{1}{4 - p} \\ \frac{1}{4 - p} \\ 0 \end{bmatrix}.
\]

\begin{itemize}
\tightlist
\item
  The first entry corresponds to both children being boys. This
  expression is what we found before.
\item
  The last entry corresponds to both children being girls, which is
  impossible given the qualification, so its probability is zero.
\item
  The two middle entries correspond to one boy and one girl, which are
  equally likely, so they have the same probability. Since the total
  probability must sum to one, each of them has probability
  \(\frac{1}{4 - p}\).
\end{itemize}

Let's plot the Kullback-Leibler divergence for both definitions of prior
and posterior distributions, as a function of \(p\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{,figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{), sharex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{fig.subplots\_adjust(hspace}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}

\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\FloatTok{0.001}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{q1A, q2A }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{]}
\NormalTok{q1B, q2B }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{]}
\NormalTok{p1 }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ p: (}\DecValTok{2} \OperatorTok{{-}}\NormalTok{ p) }\OperatorTok{/}\NormalTok{ (}\DecValTok{4} \OperatorTok{{-}}\NormalTok{ p)}
\NormalTok{p2 }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ p: }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ p1(p)}
\NormalTok{DKL\_A }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ p: p1(p) }\OperatorTok{*}\NormalTok{ np.log2(p1(p) }\OperatorTok{/}\NormalTok{ q1A) }\OperatorTok{+}\NormalTok{ p2(p) }\OperatorTok{*}\NormalTok{ np.log2(p2(p) }\OperatorTok{/}\NormalTok{ q2A)}
\NormalTok{DKL\_B }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ p: p1(p) }\OperatorTok{*}\NormalTok{ np.log2(p1(p) }\OperatorTok{/}\NormalTok{ q1B) }\OperatorTok{+} \DecValTok{2}\OperatorTok{*}\NormalTok{(p2(p)}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.log2((p2(p)}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\NormalTok{ q2B)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(p, p1(p), lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, label}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{P}\KeywordTok{(}\DecValTok{\textbackslash{}m}\VerbatimStringTok{athrm\{BB\} }\DecValTok{\textbackslash{}m}\VerbatimStringTok{id }\DecValTok{\textbackslash{}m}\VerbatimStringTok{athrm\{B\}}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(p, }\DecValTok{0}\OperatorTok{*}\NormalTok{p}\OperatorTok{+}\FloatTok{0.5}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(p, DKL\_A(p), lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\CommentTok{\# ax[1].plot(p, 0*p, lw=1, ls=\textquotesingle{}{-}{-}\textquotesingle{}, color=\textquotesingle{}gray\textquotesingle{})}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\StringTok{"probability that the}\CharTok{\textbackslash{}n}\StringTok{other child is a boy"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylabel}\OperatorTok{=}\VerbatimStringTok{r"Information gain }\KeywordTok{(}\DecValTok{$}\VerbatimStringTok{D\_\{KL\}}\DecValTok{$}\VerbatimStringTok{, bits}\KeywordTok{)}\VerbatimStringTok{"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot(p, DKL\_B(p), lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"p: probability of the specific trait"}\NormalTok{,}
\NormalTok{          ylabel}\OperatorTok{=}\VerbatimStringTok{r"Information gain }\KeywordTok{(}\DecValTok{$}\VerbatimStringTok{D\_\{KL\}}\DecValTok{$}\VerbatimStringTok{, bits}\KeywordTok{)}\VerbatimStringTok{"}\NormalTok{)}

\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}\StringTok{"Q1"}\NormalTok{, }\StringTok{"Q2"}\NormalTok{, }\StringTok{"Q3"}\NormalTok{, }\StringTok{"Q4"}\NormalTok{]}
\NormalTok{ps }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{7}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{30}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ i, p\_val }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ps):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot([p\_val], [p1(p\_val)], ls}\OperatorTok{=}\VariableTok{None}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{8}\NormalTok{, markeredgewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].text(p\_val}\OperatorTok{+}\FloatTok{0.02}\NormalTok{, p1(p\_val), labels[i], ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{)}

\NormalTok{    ax[}\DecValTok{1}\NormalTok{].plot([p\_val], [DKL\_A(p\_val)], ls}\OperatorTok{=}\VariableTok{None}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{8}\NormalTok{, markeredgewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].text(p\_val}\OperatorTok{{-}}\FloatTok{0.06}\NormalTok{, DKL\_A(p\_val), labels[i], ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{)}

\NormalTok{    ax[}\DecValTok{2}\NormalTok{].plot([p\_val], [DKL\_B(p\_val)], ls}\OperatorTok{=}\VariableTok{None}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{8}\NormalTok{, markeredgewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{2}\NormalTok{].text(p\_val}\OperatorTok{{-}}\FloatTok{0.06}\NormalTok{, DKL\_B(p\_val), labels[i], ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].text(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.98}\NormalTok{, }\StringTok{"Prior: other child is equally likely to be a boy or a girl"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{1}\NormalTok{].transAxes, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].text(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.98}\NormalTok{, }\StringTok{"Prior: joint distribution of both children"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{2}\NormalTok{].transAxes, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/boy-girl-paradox_files/figure-pdf/cell-8-output-1.png}}

Some considerations:

\begin{itemize}
\tightlist
\item
  Question Q0 does not appear on the graph because there is no
  qualification, there is no \(p\) to plot.
\item
  In the top graph we see that as the qualification becomes more
  stringent (smaller \(p\)), the probability that the other child is a
  boy increases from \(1/3\) to \(1/2\).
\item
  In the middle and bottom graphs we see divergent behavior of the
  information gain as \(p\) approaches zero, depending on the choice of
  prior distribution.
\item
  When considering only the second child (middle graph), the information
  decreases to zero with increasing qualification (read graph from right
  to left). This makes sense, because we initally assumed that the other
  child was equally likely to be a boy or a girl, and that's exactly
  what we end up believing when the qualification becomes very
  stringent. The qualification that gives maximum information is ``at
  least one of the children is a boy''.
\item
  When considering the joint distribution of both children (bottom
  graph), the information increases with increasing qualification (read
  graph from right to left): we initially believed that all four
  combinations of children were equally likely (1/4), but the more
  stringent the qualification, the more certain we become that we're
  talking about a specific child, and thus the problem reduces to
  finding the probability that the other child. In this case, the
  qualification that gives maximum information is the most specific one.
\end{itemize}

\section{obsevation vs information}\label{obsevation-vs-information}

This is a beautiful example of the difference between observation and
information. Data alone does not contain information. In the information
theory sense, information is a measure of how much our beliefs (priors)
change when we observe data.

\begin{itemize}
\tightlist
\item
  The simple observation ``at least one child is a boy'' provides the
  maximum information for one choice of prior distribution (middle
  graph), and the least information for the other choice of prior
  distribution (bottom graph).
\item
  The very specific and long observation ``at least one child is a boy
  born on 29 February, he likes sushi but doesn't like pasta, and he
  dreams of becoming an animal ophthalmologist'' provides almost zero
  information for one choice of prior distribution, and (almost) maximum
  information for the other choice of prior distribution.
\end{itemize}

The arguments above are very much related to the idea of
``theory-ladenness of observation'' in the philosophy of science. Laden
means ``heavily loaded'', and the idea is that every observation we make
only has meaning within a theoretical framework: our prior beliefs,
theories, and expectations. This philosophical idea goes against
empiricism, which states that knowledge comes only or primarily from
sensory experience. The accumulation of sensory experiences alone does
not lead to knowledge without our expectations of how the world works.

\chapter{monty hall}\label{monty-hall}

The famous Monty Hall problem is a great opportunity to apply Bayes'
theorem. The problem is named after Monty Hall, the original host of the
television game show \emph{Let's Make a Deal}.

Imagine you're a contestant on the show, and you're presented with three
doors. Behind one door is a car (the prize you want), and behind the
other two doors are goats (which you don't want). You pick a door, say
door 1. Monty, who knows what's behind each door, then opens another
door, say door 2, which has a goat behind it. He then gives you the
option to switch your choice to the remaining unopened door (door 3) or
stick with your original choice (door 1). Would it be smarter to switch
or stay?

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.patches }\ImportTok{as}\NormalTok{ patches}
\ImportTok{from}\NormalTok{ matplotlib.offsetbox }\ImportTok{import}\NormalTok{ OffsetImage, AnnotationBbox}
\ImportTok{import}\NormalTok{ urllib.request}
\ImportTok{import}\NormalTok{ io}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ add\_emoji(x, y, emoji\_code, ax, style}\OperatorTok{=}\StringTok{\textquotesingle{}apple\textquotesingle{}}\NormalTok{, zoom}\OperatorTok{=}\FloatTok{0.4}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Styles available: }
\CommentTok{    \textquotesingle{}apple\textquotesingle{}  {-}\textgreater{} Glossy, high{-}detail (via EmojiGraph)}
\CommentTok{    \textquotesingle{}google\textquotesingle{} {-}\textgreater{} Flat, rounded (via Google Noto)}
\CommentTok{    \textquotesingle{}open\textquotesingle{}   {-}\textgreater{} Artistic, outlined (via OpenMoji)}
\CommentTok{    """}
\NormalTok{    code }\OperatorTok{=}\NormalTok{ emoji\_code.lower()}
    
    \CommentTok{\# URL Options}
    \ControlFlowTok{if}\NormalTok{ style }\OperatorTok{==} \StringTok{\textquotesingle{}apple\textquotesingle{}}\NormalTok{:}
        \CommentTok{\# High{-}res Apple style}
\NormalTok{        url }\OperatorTok{=} \SpecialStringTok{f"https://emojigraph.org/media/apple/}\SpecialCharTok{\{}\NormalTok{code}\SpecialCharTok{\}}\SpecialStringTok{\_apple.png"}
    \ControlFlowTok{elif}\NormalTok{ style }\OperatorTok{==} \StringTok{\textquotesingle{}google\textquotesingle{}}\NormalTok{:}
        \CommentTok{\# Google Noto Color Emoji}
\NormalTok{        url }\OperatorTok{=} \SpecialStringTok{f"https://raw.githubusercontent.com/googlefonts/noto{-}emoji/master/png/128/emoji\_u}\SpecialCharTok{\{}\NormalTok{code}\SpecialCharTok{\}}\SpecialStringTok{.png"}
    \ControlFlowTok{elif}\NormalTok{ style }\OperatorTok{==} \StringTok{\textquotesingle{}open\textquotesingle{}}\NormalTok{:}
        \CommentTok{\# OpenMoji (Professional vector{-}style)}
\NormalTok{        url }\OperatorTok{=} \SpecialStringTok{f"https://raw.githubusercontent.com/hfg{-}gmuend/openmoji/master/color/64/}\SpecialCharTok{\{}\NormalTok{code}\SpecialCharTok{.}\NormalTok{upper()}\SpecialCharTok{\}}\SpecialStringTok{.png"}
    \ControlFlowTok{else}\NormalTok{:}
        \CommentTok{\# Fallback to Twemoji}
\NormalTok{        url }\OperatorTok{=} \SpecialStringTok{f"https://abs.twimg.com/emoji/v2/72x72/}\SpecialCharTok{\{}\NormalTok{code}\SpecialCharTok{\}}\SpecialStringTok{.png"}

    \ControlFlowTok{try}\NormalTok{:}
        \CommentTok{\# We add a User{-}Agent header because some CDNs block Python\textquotesingle{}s default scraper}
\NormalTok{        req }\OperatorTok{=}\NormalTok{ urllib.request.Request(url, headers}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}User{-}Agent\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}Mozilla/5.0\textquotesingle{}}\NormalTok{\})}
        \ControlFlowTok{with}\NormalTok{ urllib.request.urlopen(req) }\ImportTok{as}\NormalTok{ res:}
\NormalTok{            img\_data }\OperatorTok{=}\NormalTok{ io.BytesIO(res.read())}
\NormalTok{            img }\OperatorTok{=}\NormalTok{ plt.imread(img\_data, }\BuiltInTok{format}\OperatorTok{=}\StringTok{\textquotesingle{}png\textquotesingle{}}\NormalTok{)}
            
\NormalTok{        imagebox }\OperatorTok{=}\NormalTok{ OffsetImage(img, zoom}\OperatorTok{=}\NormalTok{zoom)}
\NormalTok{        ab }\OperatorTok{=}\NormalTok{ AnnotationBbox(imagebox, (x, y), frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{        ax.add\_artist(ab)}
    \ControlFlowTok{except} \PreprocessorTok{Exception} \ImportTok{as}\NormalTok{ e:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Error loading }\SpecialCharTok{\{}\NormalTok{style}\SpecialCharTok{\}}\SpecialStringTok{ emoji }\SpecialCharTok{\{}\NormalTok{emoji\_code}\SpecialCharTok{\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{e}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\CommentTok{\# door}
\NormalTok{add\_emoji(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\StringTok{"1f6aa"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{add\_emoji(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\StringTok{"1f6aa"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{add\_emoji(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\StringTok{"1f6aa"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\DecValTok{1}\NormalTok{) }
\CommentTok{\# goats}
\NormalTok{add\_emoji(}\DecValTok{1}\NormalTok{, }\FloatTok{1.65}\NormalTok{, }\StringTok{"1f410"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\NormalTok{add\_emoji(}\DecValTok{2}\NormalTok{, }\FloatTok{1.65}\NormalTok{, }\StringTok{"1f410"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\CommentTok{\# car}
\NormalTok{add\_emoji(}\DecValTok{3}\NormalTok{, }\FloatTok{1.65}\NormalTok{, }\StringTok{"1f697"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\CommentTok{\# person shrugging}
\NormalTok{add\_emoji(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\StringTok{"1f937"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\FloatTok{0.7}\NormalTok{)}
\CommentTok{\# man in tuxedo}
\NormalTok{add\_emoji(}\FloatTok{3.2}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\StringTok{"1f935"}\NormalTok{, ax, style}\OperatorTok{=}\StringTok{"google"}\NormalTok{, zoom}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\NormalTok{ax.text(}\DecValTok{1}\NormalTok{, }\FloatTok{1.1}\NormalTok{, }\StringTok{"1"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{24}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(}\DecValTok{2}\NormalTok{, }\FloatTok{1.1}\NormalTok{, }\StringTok{"2"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{24}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(}\DecValTok{3}\NormalTok{, }\FloatTok{1.1}\NormalTok{, }\StringTok{"3"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{24}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{), ylim}\OperatorTok{=}\NormalTok{(}\FloatTok{0.25}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{ax.axis(}\StringTok{\textquotesingle{}off\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/monty-hall_files/figure-pdf/cell-4-output-1.png}}

\section{Bayes' Theorem}\label{bayes-theorem-3}

We can formulate this problem as the conditional probability:

\[
P(\text{car behind 1} \mid \text{Monty opens 2})
\]

We can use Bayes' theorem to compute this:

\[
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)},
\]

where

\begin{itemize}
\tightlist
\item
  \(A\) is the event ``car behind door 1''
\item
  \(B\) is the event ``Monty opens door 2''
\end{itemize}

Let's rewrite Bayes' theorem for our specific events:

\[
P(\text{car behind 1} \mid \text{Monty opens 2}) = \frac{P(\text{Monty opens 2} \mid \text{car behind 1}) P(\text{car behind 1})}{P(\text{Monty opens 2})}
\]

Let's compute each term.

\subsection{likelihood}\label{likelihood-1}

\(P(\text{Monty opens 2} \mid \text{car behind 1})\)

In English: assuming we know the car is behind door 1, what is the
probability that Monty opens door 2? In that case, Monty could have
opened either door 2 or door 3 with equal probability, so the answer is
1/2.

\subsection{prior}\label{prior-1}

\(P(\text{car behind 1})\)

Before Monty opened any door, the probability that the car is behind
door 1 is 1/3.

\subsection{evidence}\label{evidence}

\(P(\text{Monty opens 2})\)

We can compute this using the law of total probability. The evidence is
a marginal probability, and it is the sum of the joint probabilities
over all possible locations of the car:

\begin{align*}
P(\text{Monty opens 2}) &= P(\text{Monty opens 2} \cap \text{car behind 1}) \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind 2}) \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind 3})
\end{align*}

By the rules of the game, the second term is zero, because Monty will
never ruin the fun by opening the door with the car behind it. For the
other two terms, we can use the definition of conditional probability:

\begin{itemize}
\tightlist
\item
  First term: \[
    P(\text{Monty opens 2} \cap \text{car behind 1}) = P(\text{Monty opens 2} \mid \text{car behind 1}) \cdot P(\text{car behind 1})
    \] The first part is the likelihood we computed above (1/2), and the
  second part is the prior (1/3). So the first term is 1/6.
\item
  Third term: \[
    P(\text{Monty opens 2} \cap \text{car behind 3}) = P(\text{Monty opens 2} \mid \text{car behind 3}) \cdot P(\text{car behind 3})
    \] In this case, if the car is behind door 3, Monty has no choice
  but to open door 2, so the first part is 1. The second part is again
  the prior (1/3). So the third term is 1/3.
\end{itemize}

Putting it all together, the evidence is: \[
P(\text{Monty opens 2}) = \frac{1}{6} + 0 + \frac{1}{3} = \frac{1}{2}
\]

Finally, we can plug everything back into Bayes' theorem: \[
P(\text{car behind 1} \mid \text{Monty opens 2}) = \frac{\frac{1}{2} \cdot \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}
\]

The probability that the car is behind door 1 is still 1/3. The logical
conclusion is that the probability that the car is behind door 3 is 2/3,
so you should switch!

\section{reflections}\label{reflections}

I posed this question to my children, and they said the same thing:
``What does it matter if Monty reveals that another door has a goat?
That doesn't change anything! There are now two doors, each equally
likely to have the car behind it.'' (those weren't exactly their words,
I'm paraphrasing).

There is both truth and confusion in what they said!

\textbf{``It doesn't change anything!''} This is ``sorta'' true. Our
prior probability that the car is behind door 1 was 1/3, and our
posterior probability is still 1/3. The information that Monty revealed
a goat behind door 2 did not change our belief about door 1. Nothing
changed here!

\textbf{``There are now two doors, each equally likely to have the car
behind it.''} Since the probability that the car is behind door 1 is
1/3, the probability that it is behind door 3 must be 2/3. The two doors
are not equally likely to have the car behind them!

How could all the probability from door 2 have been fully transferred to
door 3?

\section{Monty selects at random}\label{monty-selects-at-random}

Let's imagine a different game, where Monty opens a door at random (one
you haven't chosen), and it just so happens that he reveals a goat. How
would the calculation change?

The \textbf{likelihood}
\(P(\text{Monty opens 2} \mid \text{car behind 1})\) remains the same
(1/2). If the car is behind door 1, Monty could have opened either door
2 or door 3 with equal probability.

The \textbf{prior} \(P(\text{car behind 1})\) is also the same (1/3).

Now let's see the \textbf{evidence}:

\begin{align*}
P(\text{Monty opens 2}) &= P(\text{Monty opens 2} \cap \text{car behind 1}) \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind 2}) \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind 3}),
\end{align*}

becomes

\begin{align*}
P(\text{Monty opens 2}) &= P(\text{Monty opens 2} \mid \text{car behind 1})P(\text{car behind 1}) \\
                        &+ P(\text{Monty opens 2} \mid \text{car behind 2})P(\text{car behind 2}) \\
                        &+ P(\text{Monty opens 2} \mid \text{car behind 3})P(\text{car behind 3}),
\end{align*}

All three terms are the same now! Monty will randomly open one of the
remaining two doors, regardless of where the car is. Sure, that could
ruin the game, but bear with me. The three terms above are all the
product of 1/2 (the probability that Monty randomly opens door 2) and
1/3 (the prior probability that the car is behind each door). So we
have:

\[
P(\text{Monty opens 2}) = \frac{1}{2}\cdot \frac{1}{3} + \frac{1}{2}\cdot \frac{1}{3} + \frac{1}{2}\cdot \frac{1}{3} = \frac{1}{2}
\]

Hmmm\ldots{} I just assumed that the evidence would be different from
the original problem, but it turns out to be the same (1/2). All results
being the same, the posterior probability that the car is behind door 1
is still 1/3. Where's the catch?!

I think that now we know that with a 1/3 probability Monty would have
revealed the car behind door 2, ruining the game. But since he revealed
a goat, we are in the 2/3 probability case where the car is behind
either door 1 or door 3. Since we calculated that the probability that
the car is behind door 1 is still 1/3, it follows that the probability
that it is behind door 3 is 1/3. When Monty opens a door at random there
is no ``transfer of probability'', and it doesn't matter if we switch or
not! The transfer of probability happens only when Monty judiciously
chooses which door to open. Let's try another variation.

\section{many doors to select from}\label{many-doors-to-select-from}

What if the game had \(N\) doors (think 100, something big), and you
picked door 1? Let's call ``door 2'' the door that Monty opens,
revealing a goat. How do the probabilities change now?

\textbf{Prior:} \(P(\text{car behind 1}) = \frac{1}{N}\)

\textbf{Likelihood:}
\(P(\text{Monty opens 2} \mid \text{car behind 1}) = \frac{1}{N-1}\). If
the car is behind door 1, Monty can open any of the other \(N-1\) doors
with equal probability.

\textbf{Evidence:}

\begin{align*}
P(\text{Monty opens 2}) &= P(\text{Monty opens 2} \cap \text{car behind 1}) \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind 2}) \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind 3}) \\
                        &+ \ldots \\
                        &+ P(\text{Monty opens 2} \cap \text{car behind N})
\end{align*}

This becomes

\begin{align*}
P(\text{Monty opens 2}) &= P(\text{Monty opens 2} \mid \text{car behind 1})\cdot P(\text{car behind 1}) \\
                        &+ 0 \\
                        &+ P(\text{Monty opens 2} \mid \text{car behind 3})\cdot P(\text{car behind 3}) \\
                        &+ \ldots \\
                        &+ P(\text{Monty opens 2} \mid \text{car behind N})\cdot P(\text{car behind N})
\end{align*}

Again, Monty will never open the door with the car behind it, so the
second term is zero. The first term is the product of the likelihood and
the prior, while all the rest of the \(N-2\) terms are the same: if the
car is behind any of those doors (3, 4, \ldots, N), Monty can open door
2 with probability \(1/(N-2)\), because door 1 is taken by you, and the
door with the car behind it cannot be opened. The second part of those
terms is the prior, which is \(1/N\). So we have:

\begin{align*}
P(\text{Monty opens 2}) &= \frac{1}{N-1}\cdot \frac{1}{N} \\
                        &+ 0 \\
                        &+ \frac{1}{N-2} \cdot \frac{1}{N} \\
                        &+ \ldots \\
                        &+ \frac{1}{N-2} \cdot \frac{1}{N}
\end{align*}

Clearly, there are \(N-2\) identical terms in the sum, so we can write:

\begin{align*}
P(\text{Monty opens 2}) &= \frac{1}{N-1}\cdot \frac{1}{N} + \frac{1}{N} \\
                        &= \frac{1}{N}\left( \frac{1}{N-1} + 1 \right) \\
                        &= \frac{1}{N}\cdot \frac{N}{N-1} \\
                        &= \frac{1}{N-1}
\end{align*}

This ``miraculous'' simplification is surely the key to understanding
the transfer of probability! Let's keep going.

\textbf{Posterior:} Plugging everything back into Bayes' theorem: \[
P(\text{car behind 1} \mid \text{Monty opens 2}) = \frac{\frac{1}{N-1} \cdot \frac{1}{N}}{\frac{1}{N-1}} = \frac{1}{N}.
\]

The probability that the car is behind door 1 is still 1/N. All
remaining doors (3, 4, \ldots, N) are equally likely to have the car
behind them, so the probability that the car is behind any of those
doors is the complement of \(1/N\) divided by the number of those doors
(\(N-2\)).

\section{transfer of probability}\label{transfer-of-probability}

The fact that Monty knows where the car is and will never open a door
with the car behind it creates a dependency between the doors. The
``miraculous'' simplification we saw before means that the evidence term
cancels out the likelihood term when computing the posterior for door 1,
leaving its probability unchanged.

Bayes' theorem can be written as:

\[
\text{Posterior} = \left( \frac{\text{Likelihood}}{\text{Evidence}} \right) \times \text{Prior}
\]

The term in the parentheses, what we found to be 1 in this case, is
called the Bayes factor. In some contexts, this term is also called
updating factor, because it tells us how much to update our prior belief
based on the evidence we observed. Here, the updating factor for door 1
is 1, meaning that our belief about door 1 does not change.

When Monty opens door 2, all the probability mass that was assigned to
that door is equally redistributed among the remaining doors (except
door 1, which we already established remains at \(1/N\)).

\section{information}\label{information-1}

How much information did we get from Monty opening door 2? We can
measure this using the
\href{./information_theory/cross-entropy.html\#kullback-leibler-divergence}{Kullback-Leibler
divergence} between the prior and posterior distributions:

\[
D_{KL}(\mathbf{p} \| \mathbf{q}) = \sum_i \mathbf{p}(i) \log\frac{\mathbf{p}(i)}{\mathbf{q}(i)},
\] where \(\mathbf{p}\) is the posterior distribution and \(\mathbf{q}\)
is the prior distribution.

In the case of three doors, the prior and posterior distributions are:

\[
\mathbf{q} = \left[ \frac{1}{3}, \frac{1}{3}, \frac{1}{3} \right], \quad \mathbf{p} = \left[ \frac{1}{3}, 0, \frac{2}{3} \right].
\]

When summing over all doors, the first term contributes 0, since the
prior and posterior probabilities are the same (1/3). The second term
also contributes 0, since the posterior probability is 0. Only the third
term contributes to the information gain:

\[
D_{KL}(\mathbf{p} \| \mathbf{q}) = \frac{2}{3} \log\frac{\frac{2}{3}}{\frac{1}{3}} = \frac{2}{3} \log 2 \approx 0.462.
\]

For N doors, the prior and posterior distributions are:

\[
\mathbf{q} = \left[ \frac{1}{N}, \frac{1}{N}, \ldots, \frac{1}{N} \right], \quad \mathbf{p} = \left[ \frac{1}{N}, 0, \ldots, \frac{1}{N-2} \cdot \frac{N-1}{N}, \ldots, \frac{1}{N-2} \cdot \frac{N-1}{N} \right].
\]

Once more, the first two terms contribute 0 to the information gain, and
only the remaining \(N-2\) terms contribute: \[
D_{KL}(\mathbf{p} \| \mathbf{q}) = (N-2) \cdot \frac{1}{N-2} \cdot \frac{N-1}{N} \log\frac{\frac{1}{N-2} \cdot \frac{N-1}{N}}{\frac{1}{N}} = \frac{N-1}{N} \log\frac{N-1}{N-2}.
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{N }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{3}\NormalTok{,}\DecValTok{21}\NormalTok{)}
\NormalTok{DKL }\OperatorTok{=}\NormalTok{ ((N}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{N) }\OperatorTok{*}\NormalTok{ np.log2((N}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{(N}\OperatorTok{{-}}\DecValTok{2}\NormalTok{))}
\NormalTok{ax.plot(N, DKL, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"Number of doors (N)"}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{"Information Gain (bits)"}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"Information Gain from Monty Opening a Door"}\NormalTok{,}
\NormalTok{       xticks}\OperatorTok{=}\NormalTok{N,}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{       )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{bayes/monty-hall_files/figure-pdf/cell-5-output-1.png}}

We learn from computing the posterior that it is equal to the prior
after Monty opens door 2. That, of course, is not the whole story. The
calculation of the information gain makes it explicit that the posterior
is not a number, but a distribution. This is important to remember when
we work with Bayesian statistics. Let's state the first sentence in a
more precise way: The probability that the car is behind door 1 remains
unchanged, but the probabilities of the other doors change, and we learn
something from that change. The more doors there are, the less
informative is the revelation that door 2 has a goat behind it, as shown
in the plot above.

\section{the best explanations}\label{the-best-explanations}

A few days after I've written this chapter on the Monty Hall problem, I
saw the following post by David Deutsch on X:

\pandocbounded{\includegraphics[keepaspectratio]{bayes/david-deutsch-x-post-monty-hall.jpeg}}

The first link is to the paper
\href{http://dx.doi.org/10.1007/s11229-025-05389-6}{``Right for the
wrong reasons: common bad arguments for the correct answer to the Monty
Hall Problem''} by Don Fallis and Peter J. Lewis, published on 15
January 2026 (exactly one week ago from when I'm writing this), in the
philosophy journal \emph{Synthese}. This is a great paper, I highly
recommend it. There, we find this beautiful argument:

\begin{quote}
\textbf{The Wi-Phi Probability Swap argument}\\
There is a 1/3 chance that the car is behind the door that you initially
chose. So 1/3 of the times that you play the game you will win by
sticking with that door. And 2/3 of the times that you play you will
lose by sticking. However if you switch to the door that Monty did not
open you will lose all of the times that you would have won by sticking.
And you will win all of the times that you would have lost by sticking.
(In those cases, there is a car behind one of the two remaining doors
and a goat behind the other. So, when Monty opens one of those doors and
reveals a goat, you will win by switching to the other door.) So, you
should switch to the last remaining door when Monty opens a door and
reveals a goat
\end{quote}

The second link is really the mind blowing one. It leads to a very short
post in David Deutsch's website, titled
\href{https://www.daviddeutsch.org.uk/2013/10/monty-hall-problem/}{''
Monty Hall Problem''}, published on 26 October 2013. There, Deutsch
writes:

\begin{quote}
Consider a different problem first: you're faced with the same three
boxes but now you can choose any one box OR any two boxes, and in the
latter case receive the better of the two contents. It's always better
to choose two boxes, right? But the rules of the original game allow you
to choose two! Here's how. First point to the remaining box i.e.~the one
you're not going to choose. Then Monty will open the worse of the two
boxes you chose, and you take the better one.
\end{quote}

Beautiful! Thanks, David 

\part{svd and pca}

\chapter{SVD for image compression}\label{svd-for-image-compression}

This chapter is partially based on these sources:

\begin{itemize}
\tightlist
\item
  ``Data-Driven Science and Engineering: Machine Learning, Dynamical
  Systems, and Control'' by Steven L. Brunton, J. Nathan Kutz. SVD is
  covered in Chapter 1.
\item
  \href{https://towardsdatascience.com/singular-value-decomposition-svd-demystified-57fc44b802a0/}{Dr.~Roi
  Yehoshua's ``Singular Value Decomposition (SVD), Demystified''}.
\end{itemize}

Check out also this
\href{https://timbaumann.info/svd-image-compression-demo/}{cool demo} by
Tim Baumann.

\section{the image}\label{the-image}

I will use a black-and-white version of the photo below as the matrix to
decompose. There are two reasons to use this image:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  it is a tall and skinny matrix (width 1600 px, height 2600 px). Tall
  and skinny matrices are usually used in overdetermined systems, which
  are common in data science.
\item
  this is the image of the juice of a tomato I ate, as it fell on the
  concrete floor. I found this splat pattern so beautiful that I took a
  picture, and I wanted to immortalize it in this tutorial. You're
  welcome.
\end{enumerate}

\pandocbounded{\includegraphics[keepaspectratio]{archive/images/splat.jpg}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ PIL }\ImportTok{import}\NormalTok{ Image}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ time}
\ImportTok{from}\NormalTok{ sklearn.decomposition }\ImportTok{import}\NormalTok{ TruncatedSVD}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image }\OperatorTok{=}\NormalTok{ Image.}\BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}../archive/images/splat.jpg\textquotesingle{}}\NormalTok{)}
\NormalTok{gray\_image }\OperatorTok{=}\NormalTok{ image.convert(}\StringTok{\textquotesingle{}L\textquotesingle{}}\NormalTok{)  }\CommentTok{\# convert to grayscale}
\NormalTok{image\_array }\OperatorTok{=}\NormalTok{ np.array(gray\_image)  }\CommentTok{\# make it a numpy array}
\CommentTok{\# display the image}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.imshow(image\_array, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.axis(}\StringTok{\textquotesingle{}off\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Hide axis}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{svd_and_pca/svd_image_compression_files/figure-pdf/cell-3-output-1.png}}

\section{decomposition}\label{decomposition}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U, S, Vt }\OperatorTok{=}\NormalTok{ np.linalg.svd(image\_array)}
\end{Highlighting}
\end{Shaded}

Let's see how the magnitude of the singular values decreases as we go
from the first to the last (k goes from zero to 900-1). Also, let's see
how much of the total energy accumulated up to the k-th singular value
squared.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{lenS }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(S)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(np.arange(lenS), S, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{"black"}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{5}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_yscale(}\StringTok{\textquotesingle{}log\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Set y{-}axis to log scale}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{,}
\NormalTok{          ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}singular value\textquotesingle{}}
\NormalTok{         )}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].grid(}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Enable grid on the first panel}

\NormalTok{cumS2 }\OperatorTok{=}\NormalTok{ np.cumsum(S}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(S}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(np.arange(lenS), cumS2, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{"black"}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{5}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{klist }\OperatorTok{=}\NormalTok{ [}\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{400}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in}\NormalTok{ klist:}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].plot([k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], [cumS2[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, markeredgecolor}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, markerfacecolor}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, markersize}\OperatorTok{=}\DecValTok{5}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].text(k}\OperatorTok{+}\DecValTok{20}\NormalTok{, cumS2[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\FloatTok{0.001}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}k=}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{,}
\NormalTok{          ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}cumulative energy\textquotesingle{}}
\NormalTok{         )}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].grid(}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Enable grid on the second panel}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.set\_label\_position(}\StringTok{"right"}\NormalTok{)  }\CommentTok{\# Move ylabel to the right}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.tick\_right()  }\CommentTok{\# Move yticks to the right}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{svd_and_pca/svd_image_compression_files/figure-pdf/cell-5-output-1.png}}

The square of the Frobenius norm of the matrix \(X\) is equal to the sum
of the squares of all its singular values.

\[
\lVert X \rVert_F^2 = \sum_{i=1}^{r} \sigma_i^2
\]

The Frobenius norm is a measure of the ``magnitude'' or ``size'' of the
matrix, which can be interpreted as the total amount of ``information''
in the data. By taking the cumulative sum of the squared singular
values, we are effectively measuring how much of this total information
is retained with each successive truncation. The term ``energy'' is an
analogy from physics and signal processing. In these fields, the total
energy of a signal is often defined as the integral of its squared
magnitude over time. This concept carries over to data analysis where
the squared singular values are a direct measure of the variance in the
data along each singular vector, and the sum of these squares represents
the total variance.

\section{truncation and
reconstruction}\label{truncation-and-reconstruction}

Our original matrix \(X\) has dimensions (1600, 2600) and rank 1600.
Therefore, it has 1600 non-zero singular values. We can truncate the SVD
to a lower rank \(k < 1600\) and reconstruct an approximation of the
original matrix using only the first \(k\) singular values and their
corresponding singular vectors:

\[
X_k = U_k \Sigma_k V_k^T = \sum_{i=1}^{k} \sigma_i \cdot \text{outer}(u_i, v_i^T)
\]

where \(U_k\) is the matrix of the first \(k\) left singular vectors,
\(\Sigma_k\) is the diagonal matrix of the first \(k\) singular values,
and \(V_k^T\) is the transpose of the matrix of the first \(k\) right
singular vectors. The outer product \(\text{outer}(u_i, v_i^T)\) creates
a rank-1 matrix from the \(i\)-th left and right singular vectors.

Using the same truncation values \(k\) shown in red in the plot above,
we can reconstruct approximations of the original image. As \(k\)
increases, the reconstructed image becomes more detailed and closer to
the original image.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ reconstruct\_image(U, S, Vt, k):}
\NormalTok{    X\_reconstructed }\OperatorTok{=}\NormalTok{ np.zeros\_like(image\_array, dtype}\OperatorTok{=}\NormalTok{np.float64)}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k): }
\NormalTok{        X\_reconstructed }\OperatorTok{+=}\NormalTok{ S[i] }\OperatorTok{*}\NormalTok{ np.outer(U[:, i], Vt[i, :])   }
\NormalTok{    X\_reconstructed }\OperatorTok{=}\NormalTok{ np.clip(X\_reconstructed, }\DecValTok{0}\NormalTok{, }\DecValTok{255}\NormalTok{)  }\CommentTok{\# ensure values are in byte range}
\NormalTok{    X\_reconstructed }\OperatorTok{=}\NormalTok{ X\_reconstructed.astype(np.uint8)  }\CommentTok{\# convert to uint8}
    \ControlFlowTok{return}\NormalTok{ X\_reconstructed}

\NormalTok{reconstructed\_images }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in}\NormalTok{ klist:}
\NormalTok{    X\_reconstructed }\OperatorTok{=}\NormalTok{ reconstruct\_image(U, S, Vt, k)}
\NormalTok{    reconstructed\_images.append(X\_reconstructed)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{ i, k }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(klist):}
\NormalTok{    ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{    X\_k }\OperatorTok{=}\NormalTok{ reconstructed\_images[i]}
\NormalTok{    ax.imshow(X\_k, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_title(}\SpecialStringTok{f\textquotesingle{}k=}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.axis(}\StringTok{\textquotesingle{}off\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Hide axis}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{svd_and_pca/svd_image_compression_files/figure-pdf/cell-7-output-1.png}}

The truncation for \(k=5\) gives a blurry image, but for \(k=20\) it is
recognizably a tomato splat. The reconstructions for \(k=100\) and
\(k=400\) seem indistinguishable at this resolution. Let's zoom in on a
small section of the image to see the differences more clearly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{ i, k }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(klist):}
\NormalTok{    ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{    X\_k }\OperatorTok{=}\NormalTok{ reconstructed\_images[i][}\DecValTok{700}\NormalTok{:}\DecValTok{1000}\NormalTok{, }\DecValTok{700}\NormalTok{:}\DecValTok{1000}\NormalTok{]  }\CommentTok{\# zoom in}
\NormalTok{    ax.imshow(X\_k, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_title(}\SpecialStringTok{f\textquotesingle{}k=}\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.axis(}\StringTok{\textquotesingle{}off\textquotesingle{}}\NormalTok{)  }\CommentTok{\# Hide axis}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{svd_and_pca/svd_image_compression_files/figure-pdf/cell-8-output-1.png}}

To capture all the details in the concrete floor we need more than 100
singular values. If we're interested in the overall shape of the splat,
100 singular values are more than enough. This justifies the name of
this chapter: \textbf{SVD for image compression}. We can compress images
by storing only the first \(k\) singular values and their corresponding
singular vectors, instead of the entire image matrix.

\section{computational efficiency}\label{computational-efficiency}

We calculated the reconstruction ``the hard way'', by explicitly forming
the outer products and summing them. However, we can also use matrix
multiplication to achieve the same result more efficiently.

\[
X_k = U_k \Sigma_k V_k^T
\] where:

\begin{itemize}
\tightlist
\item
  \(U_k\) is the matrix formed by the first \(k\) columns of \(U\).
\item
  \(\Sigma_k\) is the \(k \times k\) diagonal matrix formed by the first
  \(k\) singular values, \(\Sigma_{11}=\sigma_1\),
  \(\Sigma_{22}=\sigma_2\), etc.
\item
  \(V_k^T\) is the matrix formed by the first \(k\) rows of \(V^T\).
\end{itemize}

Let's leverage matrix multiplication to compute the reconstruction:
\(X_k = U_k (\Sigma_k V_k^T)\).

First, let's look at the product of the diagonal singular value matrix
\(\Sigma_k\) and the truncated \(V^T\) matrix, \(V_k^T\). \(\Sigma_k\)
is a \(k \times k\) diagonal matrix with singular values
\(\sigma_1, \sigma_2, ..., \sigma_k\) on the diagonal. \(V_k^T\) is a
\(k \times n\) matrix where each row is a singular vector \(v_i^T\).

\[
\Sigma_k V_k^T = \begin{bmatrix}
\sigma_1 & 0 & \dots & 0 \\
0 & \sigma_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma_k
\end{bmatrix}
\begin{bmatrix}
 & v_1^T &  \\
 & v_2^T &  \\
& \vdots & \\
 & v_k^T & 
\end{bmatrix}
\]

Multiplying a diagonal matrix by another matrix from the left scales
each row of the second matrix by the corresponding diagonal element of
the first matrix. \[
\Sigma_k V_k^T = \begin{bmatrix}
 & \sigma_1 v_1^T &  \\
 & \sigma_2 v_2^T &  \\
& \vdots & \\
 & \sigma_k v_k^T & 
\end{bmatrix}
\]

This matrix contains the scaled singular vectors as its rows.

Now, we multiply the truncated \(U\) matrix, \(U_k\), with the result
from the first step. \(U_k\) is an \(m \times k\) matrix whose columns
are the singular vectors \(u_1, u_2, ..., u_k\). Let's call the result
from the first step, the matrix \(A\). The product is \(U_k A\).

\[
X_k = U_k A = \begin{bmatrix}
| & | & & | \\
u_1 & u_2 & \dots & u_k \\
| & | & & |
\end{bmatrix}
\begin{bmatrix}
 & \sigma_1 v_1^T &  \\
 & \sigma_2 v_2^T &  \\
& \vdots & \\
 & \sigma_k v_k^T & 
\end{bmatrix}
\]

Matrix multiplication can be seen as a sum of outer products of the
columns of the first matrix and the rows of the second matrix.

\[
X_k = \sum_{i=1}^{k} (\text{column } i \text{ of } U_k) \cdot (\text{row } i \text{ of } A)
\]

\[
X_k = \sum_{i=1}^{k} u_i (\sigma_i v_i^T)
\]

\[
X_k = \sum_{i=1}^{k} \sigma_i (u_i v_i^T)
\]

Let's time the two methods of reconstruction to see the efficiency gain
from using matrix multiplication.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OperatorTok{=} \DecValTok{100}

\NormalTok{start\_time1 }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ reconstruct\_image(U, S, Vt, k)}
\NormalTok{end\_time1 }\OperatorTok{=}\NormalTok{ time.time()}

\NormalTok{start\_time2 }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ np.dot(U[:, :k], np.dot(np.diag(S[:k]), Vt[:k, :])) }
\NormalTok{end\_time2 }\OperatorTok{=}\NormalTok{ time.time()}

\NormalTok{T1 }\OperatorTok{=}\NormalTok{ end\_time1 }\OperatorTok{{-}}\NormalTok{ start\_time1}
\NormalTok{T2 }\OperatorTok{=}\NormalTok{ end\_time2 }\OperatorTok{{-}}\NormalTok{ start\_time2}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"explicitly computing the outer products: }\SpecialCharTok{\{}\NormalTok{T1}\SpecialCharTok{:.6f\}}\SpecialStringTok{ seconds"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"leveraging matrix multiplication: }\SpecialCharTok{\{}\NormalTok{T2}\SpecialCharTok{:.6f\}}\SpecialStringTok{ seconds"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"speedup: }\SpecialCharTok{\{}\NormalTok{T1}\OperatorTok{/}\NormalTok{T2}\SpecialCharTok{:.2f\}}\SpecialStringTok{x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
explicitly computing the outer products: 1.508656 seconds
leveraging matrix multiplication: 0.011750 seconds
speedup: 128.40x
\end{verbatim}

There are two equivalent ways of leveraging matrix multiplication.
Because of the associativity of matrix multiplication, we can compute
the product in two different orders:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First compute \(B = \Sigma_k V_k^T\), then compute \(X_k = U_k B\).
\item
  First compute \(C = U_k \Sigma_k\), then compute \(X_k = C V_k^T\).
\end{enumerate}

For a tall-and-skinny matrix like our image (m\textgreater n), the first
method is more efficient because it involves multiplying a smaller
intermediate matrix \(B\) (of size \(k \times n\)) with \(U_k\) (of size
\(m \times k\)). The second method would involve multiplying a larger
intermediate matrix \(C\) (of size \(m \times k\)) with \(V_k^T\) (of
size \(k \times n\)), which is less efficient. For our 2600x1600 image,
the difference is tiny, but for larger datasets it can be significant.

SVD is such a common operation that most numerical computing libraries
have highly optimized implementations. See below \texttt{sklearn}'s
\texttt{TruncatedSVD}, which uses \texttt{scipy.sparse.linalg.svds}
under the hood. It is designed to compute only the first \texttt{k}
singular values and vectors, making it more efficient for large datasets
where only a few singular values are needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_time2b }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{X2b }\OperatorTok{=}\NormalTok{ np.dot(np.dot(U[:, :k], np.diag(S[:k])), Vt[:k, :])}
\NormalTok{end\_time2b }\OperatorTok{=}\NormalTok{ time.time()}

\NormalTok{svd }\OperatorTok{=}\NormalTok{ TruncatedSVD(n\_components}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
\NormalTok{truncated\_image }\OperatorTok{=}\NormalTok{ svd.fit\_transform(image\_array)}
\NormalTok{start\_time3 }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{X3 }\OperatorTok{=}\NormalTok{ svd.inverse\_transform(truncated\_image)}
\NormalTok{end\_time3 }\OperatorTok{=}\NormalTok{ time.time()}

\NormalTok{T2b }\OperatorTok{=}\NormalTok{ end\_time2b }\OperatorTok{{-}}\NormalTok{ start\_time2b}
\NormalTok{T3 }\OperatorTok{=}\NormalTok{ end\_time3 }\OperatorTok{{-}}\NormalTok{ start\_time3}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"matrix multiplication, option 2: }\SpecialCharTok{\{}\NormalTok{T2b}\SpecialCharTok{:.6f\}}\SpecialStringTok{ seconds"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"using sklearn\textquotesingle{}s TruncatedSVD: }\SpecialCharTok{\{}\NormalTok{T3}\SpecialCharTok{:.6f\}}\SpecialStringTok{ seconds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
matrix multiplication, option 2: 0.068557 seconds
using sklearn's TruncatedSVD: 0.011218 seconds
\end{verbatim}

\chapter{SVD for regression}\label{svd-for-regression}

This tutorial is partly based on the following sources:

\begin{itemize}
\tightlist
\item
  \href{https://sthalles.github.io/svd-for-regression}{Understanding
  Linear Regression using the Singular Value Decomposition} by Thalles
  Silva.
\item
  Brunton and Kutz's book,
  \href{./\#data-driven-science-and-engineering-machine-learning-dynamical-systems-and-control}{Data-Driven
  Science and Engineering}, subsection 1.4 called ``Pseudo-inverse,
  least-squares, and regression''.
\end{itemize}

\section{the problem with Ordinary Least
Squares}\label{the-problem-with-ordinary-least-squares}

Let's say I want to predict the weight of a person based on their
height. I have the following data:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Height (cm) & Weight (kg) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
150 & 53 \\
160 & 65 \\
170 & 68 \\
180 & 82 \\
190 & 88 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{150}\NormalTok{ ,}\DecValTok{160}\NormalTok{, }\DecValTok{170}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{190}\NormalTok{])}
\NormalTok{w }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{53}\NormalTok{,  }\DecValTok{65}\NormalTok{,  }\DecValTok{68}\NormalTok{,  }\DecValTok{82}\NormalTok{,  }\DecValTok{88}\NormalTok{])}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.scatter(h, w)}
\NormalTok{ax.set\_xlabel(}\StringTok{"Height (cm)"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"Weight (kg)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0, 0.5, 'Weight (kg)')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{svd_and_pca/svd_regression_files/figure-pdf/cell-3-output-2.png}}

I can try to predict the weight using a linear model:

\[
\text{weight} = \beta_0 + \beta_1 \cdot \text{height}.
\tag{1}
\]

In a general form, we can write this as:

\[
X\beta = y,
\tag{2}
\]

where \(X\) is the design matrix, \(\beta\) is the vector of
coefficients, and \(y\) is the vector of outputs (weights). This problem
probably has no exact solution for \(\beta\), because the design matrix
\(X\) is not square (there are more data points than parameters). So we
want to find the best approximation \(\hat{\beta}\) that minimizes the
error:

\[
\hat{\beta} = \arg\min_\beta \|y - X\beta\|^2.
\tag{3}
\]

We know how to solve this, we use the equation we derived in the chapter
\href{./regression/geometry-of-regression.html}{``the geometry of
regression''}:

\[
\hat{\beta} = (X^TX)^{-1}X^Ty,
\tag{4}
\]

For a linear model, the design matrix is:

\[
X = 
\begin{bmatrix}
| & | \\
\mathbf{1} & h\\
| & |
\end{bmatrix}
=
\begin{bmatrix}
1 & 150 \\
1 & 160 \\
1 & 170 \\
1 & 180 \\
1 & 190
\end{bmatrix}
.
\tag{5}
\]

What does the matrix \(X^T X\) look like?

\begin{align*}
X^TX &=
\begin{bmatrix}
- & \mathbf{1}^T & - \\
- & h^T & -
\end{bmatrix}
\begin{bmatrix}
| & | \\
\mathbf{1} & h\\
| & |
\end{bmatrix}\\
&=
\begin{bmatrix}
\mathbf{1}^T\mathbf{1} & \mathbf{1}^Th \\
h^T\mathbf{1} & h^Th
\end{bmatrix}\\
&=
\begin{bmatrix}
5 & 850 \\
850 & 153000
\end{bmatrix}
\tag{6}
\end{align*}

There's no problem inverting this matrix, so we can find the coefficient
estimates \(\hat{\beta}\) using the formula above.

Suppose now that we have a new predictor, the height of the person in
inches. The design matrix now looks like this:

\[
X =
\begin{bmatrix}
| & | & | \\
\mathbf{1} & h_{cm} & h_{inch}\\
| & | & |
\end{bmatrix}
\tag{7}
\]

Obviously, the columns \(h_{cm}\) and \(h_{inch}\) are linearly
dependent (\(h_{cm}=ah_{inch}\)). This means that the matrix \(X^TX\)
also has linearly dependent columns:

\begin{align*}
X^TX &=
\begin{bmatrix}
- & \mathbf{1}^T & - \\
- & h_{cm}^T & - \\
- & h_{inch}^T & -
\end{bmatrix}
\begin{bmatrix}
| & | & | \\
\mathbf{1} & h_{cm} & h_{inch}\\
| & | & |
\end{bmatrix}
\\
&=
\begin{bmatrix}
\mathbf{1}^T\mathbf{1} & \mathbf{1}^Th_{cm} & \mathbf{1}^Th_{inch} \\
h_{cm}^T\mathbf{1} & h_{cm}^Th_{cm} & h_{cm}^Th_{inch} \\
h_{inch}^T\mathbf{1} & h_{inch}^Th_{cm} & h_{inch}^Th_{inch}
\end{bmatrix}
\tag{8}
\end{align*}

Using the fact that \(h_{cm}=ah_{inch}\), we can see that the second and
third columns are linearly dependent. This means that the matrix
\(X^TX\) is not invertible, and we cannot use the formula above to find
the coefficient estimates \(\hat{\beta}\). What now?

This is an extreme case, but problems similar to this can happen in real
life. For example, if we have two predictors that are highly correlated,
the matrix \(X^TX\) will be close to singular (not invertible). In this
case, the coefficients \(\beta\) will be very sensitive to small changes
in the data.

\section{SVD to the rescue}\label{svd-to-the-rescue}

Let's use Singular Value Decomposition (SVD) to find the coefficients
\(\beta\). SVD is a powerful technique that can handle multicollinearity
and other issues in the data.

The SVD of a matrix \(X\) is given by:

\[
X = U\Sigma V^T,
\tag{9}
\]

where \(U\) and \(V\) are orthogonal matrices and \(\Sigma\) is a
diagonal matrix with singular values on the diagonal.

We can plug the SVD of \(X\) into the least squares problem, which is to
find the \(\hat{\beta}\) that best satisfies \(X\hat{\beta} = \hat{y}\):

\[
U\Sigma V^T\hat{\beta} = \hat{y}.
\tag{10}
\]

We define now the Moore-Penrose pseudo-inverse of \(X\), which is given
by:

\[
X^+ = V\Sigma^+U^T,
\tag{11}
\]

where \(\Sigma^+\) is obtained by taking the reciprocal of the non-zero
singular values in \(\Sigma\) and transposing the resulting matrix.

This pseudo-inverse has the following properties:

\begin{itemize}
\item
  \(X^+X = I\). This means that \(X^+\) is a left-inverse of \(X\).
\item
  \(XX^+\) is a projection matrix onto the column space of \(X\). In
  other words, left-multiplying \(XX^+\) to any vector gives the
  projection of that vector onto the column space of \(X\). In
  particular, \(XX^+y = \hat{y}\).

  This is very similar to the property we used in the chapter
  \href{./regression/geometry-of-regression.html}{``the geometry of
  regression''}, where we had \(P_Xy = \hat{y}\), with \(P_X\) being the
  projection matrix onto the column space of \(X\). There, we found that

  \[
  \hat{\beta} = (X^TX)^{-1}X^Ty,
  \]

  Left-multiplying both sides by \(X\) gives: \[
  X\hat{\beta} = X(X^TX)^{-1}X^Ty = P_Xy = \hat{y}.
  \] So we found that in the OLS case, \(X(X^TX)^{-1}X^T\) is the
  projection matrix onto the column space of \(X\). Here, we have a more
  general result that works even when \(X^TX\) is not invertible:
  \(XX^+\) is the projection matrix onto the column space of \(X\).
\end{itemize}

We left-multiply Eq. (10) by \(X^+\), and also substitute
\(\hat{y}=XX^+y\) as we just saw:

\begin{align*}
X^+ (U\Sigma V^T\hat{\beta}) &= X^+XX^+y\\
(V\Sigma^+U^T)(U\Sigma V^T)\hat{\beta} &= X^+y
\tag{12}
\end{align*}

The term \((V\Sigma^+U^T)(U\Sigma V^T)\) simplifies to the identity
matrix, so we have:

\[
\hat{\beta} = X^+y = V\Sigma^+U^Ty.
\tag{13}
\]

This is the formula we will use to find the coefficients \(\hat{\beta}\)
using SVD. This formula works even when \(X^TX\) is not invertible, and
it is more stable than the OLS formula.

\section{in practice}\label{in-practice}

Let's go back to the problematic example with height in cm and inches.
We can use the SVD to find the coefficients \(\hat{\beta}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# design matrix with intercept, height in cm and height in inches}
\NormalTok{conversion\_factor }\OperatorTok{=} \FloatTok{2.54}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.vstack([np.ones(}\BuiltInTok{len}\NormalTok{(h)), h, h}\OperatorTok{/}\NormalTok{conversion\_factor]).T}
\CommentTok{\# compute SVD of X}
\NormalTok{U, S, VT }\OperatorTok{=}\NormalTok{ np.linalg.svd(X, full\_matrices}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{Sigma }\OperatorTok{=}\NormalTok{ np.diag(S)}
\NormalTok{V }\OperatorTok{=}\NormalTok{ VT.T}
\CommentTok{\# compute coefficients using SVD}
\NormalTok{y }\OperatorTok{=}\NormalTok{ w}
\NormalTok{Sigma\_plus }\OperatorTok{=}\NormalTok{ np.zeros(Sigma.T.shape)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(S)):}
    \ControlFlowTok{if}\NormalTok{ S[i] }\OperatorTok{\textgreater{}} \FloatTok{1e{-}10}\NormalTok{:  }\CommentTok{\# avoid division by zero}
\NormalTok{        Sigma\_plus[i, i] }\OperatorTok{=} \DecValTok{1} \OperatorTok{/}\NormalTok{ S[i]}
\NormalTok{X\_plus }\OperatorTok{=}\NormalTok{ V }\OperatorTok{@}\NormalTok{ Sigma\_plus }\OperatorTok{@}\NormalTok{ U.T}
\NormalTok{beta\_hat }\OperatorTok{=}\NormalTok{ X\_plus }\OperatorTok{@}\NormalTok{ y}
\CommentTok{\# make predictions}
\NormalTok{y\_hat }\OperatorTok{=}\NormalTok{ X }\OperatorTok{@}\NormalTok{ beta\_hat}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{ax.scatter(h, w, label}\OperatorTok{=}\StringTok{"data"}\NormalTok{)}
\CommentTok{\# plot predictions}
\NormalTok{ax.plot(h, y\_hat, color}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{, label}\OperatorTok{=}\StringTok{"SVD fit"}\NormalTok{)}
\NormalTok{ax.legend()}
\NormalTok{ax.set\_xlabel(}\StringTok{"height (cm)"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"weight (kg)"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{svd_and_pca/svd_regression_files/figure-pdf/cell-5-output-1.png}}

\part{decision trees}

\chapter{CART: classification}\label{cart-classification}

\section{a jungle party}\label{a-jungle-party}

Imagine you're throwing a party in the jungle. You know you have three
types of guests---koalas, foxes, and bonobos---but you don't know who is
who. To make sure you serve the right food, you need to automatically
figure out which animal is which. Koalas only eat eucalyptus leaves,
foxes prefer meat, and bonobos love fruit, so it's important to get it
right!

To solve this problem, you'll use a decision tree. You've gathered some
data on your past animal guests, and for each one, you have their height
and weight, as well as their species (their label). Your goal is to
build a system that can learn from this historical data to correctly
classify a new, unlabeled animal based on its height and weight alone.

The data is structured as:

\begin{itemize}
\tightlist
\item
  \textbf{Features:} height and weight, continuous, numerical features.
\item
  \textbf{Categories (Classes):} Koala, Fox, Bonobo
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.gridspec }\ImportTok{as}\NormalTok{ gridspec}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_iris}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeClassifier, plot\_tree}
\end{Highlighting}
\end{Shaded}

We are using the famous Iris dataset structure, but pretending they are
animals for this example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OperatorTok{=}\NormalTok{ load\_iris()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ iris.data[:, [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ iris.target}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{markers }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{]}
\NormalTok{colors }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}tab:red\textquotesingle{}}\NormalTok{]}
\NormalTok{iris.target\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}koala\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}fox\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bonobo\textquotesingle{}}\NormalTok{]}
\NormalTok{iris.feature\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{]}
\ControlFlowTok{for}\NormalTok{ i, marker }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(markers):}
\NormalTok{    ax.scatter(X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{0}\NormalTok{], X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{1}\NormalTok{], }
\NormalTok{               c}\OperatorTok{=}\NormalTok{colors[i], }
\NormalTok{               edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{30}\NormalTok{, marker}\OperatorTok{=}\NormalTok{marker, label}\OperatorTok{=}\NormalTok{iris.target\_names[i])}
\NormalTok{ax.legend()}
\NormalTok{ax.set\_xlabel(iris.feature\_names[}\DecValTok{0}\NormalTok{])}
\NormalTok{ax.set\_ylabel(iris.feature\_names[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/3638754974.py:8: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax.scatter(X[y == i, 0], X[y == i, 1],
\end{verbatim}

\begin{verbatim}
Text(0, 0.5, 'weight')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-4-output-3.png}}

We will use the CART (Classification and Regression Trees) algorithm to
build a decision tree. There are many other methods, but CART is one of
the most popular, it's important to know it well.

\section{the split}\label{the-split}

We will cut the data into two parts along one of the features. We will
try \textbf{all possible cut thresholds} for each of the features and
see which one gives us the best split. Ideally, we want to end up with
two groups, ``left'' and ``right'', that are as ``pure'' as possible,
meaning that each group contains animals of only one species. It might
not be possible to get perfect homogeneity, so we will need to quantify
how good a split is. We will use two different metrics to evaluate the
quality of a split:

\begin{itemize}
\item
  \textbf{Information Gain (based on Entropy):} This measures how much
  uncertainty in the data is reduced after the split. A higher
  information gain means a better split. \[
    IG(T, X) = \underbrace{H(T)}_{\text{original entropy}} - \underbrace{\frac{N_\text{left}}{N} H(T_\text{left})}_{\text{left group}} - \underbrace{\frac{N_\text{right}}{N} H(T_\text{right})}_{\text{right group}}
  \] where \(T_\text{left}\) and \(T_\text{right}\) are the subsets
  after the split, and \(N\) denotes the total number of samples in a
  (sub) dataset.
\item
  \textbf{Gini Impurity:} This measure is often explained as the
  impurity of a split. A better point of view is to interpret is as the
  probability of misclassification. If we picked an element at random
  from the dataset, what is the probability that we would misclassify it
  if we labeled it according to the distribution of classes in the
  dataset? The probability of picking an element of class \(i\) is
  \(P_i\), and we would misclassify it with probability \(1 - P_i\). So
  the total probability of misclassification is \(P_i(1 - P_i)\). If we
  sum this over all classes, we get the Gini Impurity: \[
    G(T) = \sum_{i=1}^{C} P_i (1 - P_i) = 1 - \sum_{i=1}^{C} P_i^2
  \]
\end{itemize}

Now let's take our dataset and try to find the best split using both
metrics.

\subsection{entropy}\label{entropy}

We will start with the \textbf{entropy} metric.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ calculate\_gini(y\_subset):}
    \CommentTok{"""Calculates the Gini Impurity for a given subset of class labels."""}
    \CommentTok{\# If the subset is empty, there\textquotesingle{}s no impurity.}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(y\_subset) }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return} \FloatTok{0.0}
    
    \CommentTok{\# Get the counts of each unique class in the subset.}
\NormalTok{    unique\_classes, counts }\OperatorTok{=}\NormalTok{ np.unique(y\_subset, return\_counts}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    
    \CommentTok{\# Calculate the probability of each class.}
\NormalTok{    probabilities }\OperatorTok{=}\NormalTok{ counts }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(y\_subset)}
    
    \CommentTok{\# Gini Impurity formula: 1 {-} sum(p\^{}2) for each class}
    \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(probabilities}\OperatorTok{**}\DecValTok{2}\NormalTok{)}

\KeywordTok{def}\NormalTok{ calculate\_entropy(y\_subset):}
    \CommentTok{"""Calculates the Entropy for a given subset of class labels."""}
    \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(y\_subset) }\OperatorTok{==} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return} \FloatTok{0.0}
\NormalTok{    unique\_classes, counts }\OperatorTok{=}\NormalTok{ np.unique(y\_subset, return\_counts}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    probabilities }\OperatorTok{=}\NormalTok{ counts }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(y\_subset)}
\NormalTok{    epsilon }\OperatorTok{=} \FloatTok{1e{-}9}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(probabilities }\OperatorTok{*}\NormalTok{ np.log2(probabilities }\OperatorTok{+}\NormalTok{ epsilon))}

\KeywordTok{def}\NormalTok{ find\_best\_split(X\_feature, y, criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Finds the best split point for a single feature based on a specified criterion.}
\CommentTok{    Returns the best threshold and the score (Information Gain or Gini Impurity) after the split.}
\CommentTok{    """}
    \CommentTok{\# Get the unique values of the feature to consider as split points.}
\NormalTok{    unique\_values }\OperatorTok{=}\NormalTok{ np.sort(np.unique(X\_feature))}
\NormalTok{    differences }\OperatorTok{=}\NormalTok{ np.diff(unique\_values)}
\NormalTok{    threshold\_candidates }\OperatorTok{=}\NormalTok{ unique\_values[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ differences }\OperatorTok{/} \DecValTok{2}
    \ControlFlowTok{if}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{:}
\NormalTok{        initial\_score }\OperatorTok{=}\NormalTok{ calculate\_entropy(y)}
\NormalTok{        best\_score }\OperatorTok{=} \FloatTok{0.0}  \CommentTok{\# We want to maximize Information Gain}
\NormalTok{        criterion\_function }\OperatorTok{=}\NormalTok{ calculate\_entropy}
    \ControlFlowTok{elif}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{:}
\NormalTok{        best\_score }\OperatorTok{=} \FloatTok{1.0}  \CommentTok{\# We want to minimize Gini Impurity}
\NormalTok{        criterion\_function }\OperatorTok{=}\NormalTok{ calculate\_gini}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Criterion must be \textquotesingle{}entropy\textquotesingle{} or \textquotesingle{}gini\textquotesingle{}"}\NormalTok{)}

\NormalTok{    best\_threshold }\OperatorTok{=} \VariableTok{None}

    \CommentTok{\# Iterate through each unique value as a potential split point}
    \ControlFlowTok{for}\NormalTok{ threshold }\KeywordTok{in}\NormalTok{ threshold\_candidates:}
        \CommentTok{\# Split the data into two groups based on the threshold}
\NormalTok{        condition }\OperatorTok{=}\NormalTok{ X\_feature }\OperatorTok{\textless{}=}\NormalTok{ threshold}
\NormalTok{        y\_left }\OperatorTok{=}\NormalTok{ y[condition]    }\CommentTok{\# condition is True}
\NormalTok{        y\_right }\OperatorTok{=}\NormalTok{ y[}\OperatorTok{\textasciitilde{}}\NormalTok{condition]  }\CommentTok{\# condition is False}

\NormalTok{        score\_left }\OperatorTok{=}\NormalTok{ criterion\_function(y\_left)}
\NormalTok{        score\_right }\OperatorTok{=}\NormalTok{ criterion\_function(y\_right)}
\NormalTok{        fraction\_left }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(y\_left) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(y)}
\NormalTok{        fraction\_right }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(y\_right) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(y)}
\NormalTok{        weighted\_score }\OperatorTok{=}\NormalTok{ fraction\_left }\OperatorTok{*}\NormalTok{ score\_left }\OperatorTok{+}\NormalTok{ fraction\_right }\OperatorTok{*}\NormalTok{ score\_right}

        \ControlFlowTok{if}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{:}
\NormalTok{            information\_gain }\OperatorTok{=}\NormalTok{ initial\_score }\OperatorTok{{-}}\NormalTok{ weighted\_score}
            \CommentTok{\# If this split is the best so far, save it!}
            \ControlFlowTok{if}\NormalTok{ information\_gain }\OperatorTok{\textgreater{}}\NormalTok{ best\_score:  }\CommentTok{\# Max Information Gain}
\NormalTok{                best\_score }\OperatorTok{=}\NormalTok{ information\_gain}
\NormalTok{                best\_threshold }\OperatorTok{=}\NormalTok{ threshold}
        
        \ControlFlowTok{elif}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{:}
            \CommentTok{\# If this split is the best so far, save it!}
            \ControlFlowTok{if}\NormalTok{ weighted\_score }\OperatorTok{\textless{}}\NormalTok{ best\_score:  }\CommentTok{\# Min Gini Impurity}
\NormalTok{                best\_score }\OperatorTok{=}\NormalTok{ weighted\_score}
\NormalTok{                best\_threshold }\OperatorTok{=}\NormalTok{ threshold}

    \ControlFlowTok{return}\NormalTok{ best\_threshold, best\_score}

\KeywordTok{def}\NormalTok{ quantify\_all\_splits(X\_feature, y, criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Finds the best split point for a single feature based on a specified criterion.}
\CommentTok{    Returns the best threshold and the score (Information Gain or Gini Impurity) after the split.}
\CommentTok{    """}
    \CommentTok{\# Get the unique values of the feature to consider as split points.}
\NormalTok{    unique\_values }\OperatorTok{=}\NormalTok{ np.sort(np.unique(X\_feature))}
\NormalTok{    differences }\OperatorTok{=}\NormalTok{ np.diff(unique\_values)}
\NormalTok{    threshold\_candidates }\OperatorTok{=}\NormalTok{ unique\_values[:}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ differences }\OperatorTok{/} \DecValTok{2}
\NormalTok{    score\_list }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{if}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{:}
\NormalTok{        initial\_score }\OperatorTok{=}\NormalTok{ calculate\_entropy(y)}
\NormalTok{        best\_score }\OperatorTok{=} \FloatTok{0.0}  \CommentTok{\# We want to maximize Information Gain}
\NormalTok{        criterion\_function }\OperatorTok{=}\NormalTok{ calculate\_entropy}
    \ControlFlowTok{elif}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{:}
\NormalTok{        best\_score }\OperatorTok{=} \FloatTok{1.0}  \CommentTok{\# We want to minimize Gini Impurity}
\NormalTok{        criterion\_function }\OperatorTok{=}\NormalTok{ calculate\_gini}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"Criterion must be \textquotesingle{}entropy\textquotesingle{} or \textquotesingle{}gini\textquotesingle{}"}\NormalTok{)}

\NormalTok{    best\_threshold }\OperatorTok{=} \VariableTok{None}

    \CommentTok{\# Iterate through each unique value as a potential split point}
    \ControlFlowTok{for}\NormalTok{ threshold }\KeywordTok{in}\NormalTok{ threshold\_candidates:}
        \CommentTok{\# Split the data into two groups based on the threshold}
\NormalTok{        condition }\OperatorTok{=}\NormalTok{ X\_feature }\OperatorTok{\textless{}=}\NormalTok{ threshold}
\NormalTok{        y\_left }\OperatorTok{=}\NormalTok{ y[condition]    }\CommentTok{\# condition is True}
\NormalTok{        y\_right }\OperatorTok{=}\NormalTok{ y[}\OperatorTok{\textasciitilde{}}\NormalTok{condition]  }\CommentTok{\# condition is False}

\NormalTok{        score\_left }\OperatorTok{=}\NormalTok{ criterion\_function(y\_left)}
\NormalTok{        score\_right }\OperatorTok{=}\NormalTok{ criterion\_function(y\_right)}
\NormalTok{        fraction\_left }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(y\_left) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(y)}
\NormalTok{        fraction\_right }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(y\_right) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(y)}
\NormalTok{        weighted\_score }\OperatorTok{=}\NormalTok{ fraction\_left }\OperatorTok{*}\NormalTok{ score\_left }\OperatorTok{+}\NormalTok{ fraction\_right }\OperatorTok{*}\NormalTok{ score\_right}

        \ControlFlowTok{if}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{:}
\NormalTok{            information\_gain }\OperatorTok{=}\NormalTok{ initial\_score }\OperatorTok{{-}}\NormalTok{ weighted\_score}
\NormalTok{            score\_list.append((threshold, information\_gain))}
            \CommentTok{\# If this split is the best so far, save it!}

        \ControlFlowTok{elif}\NormalTok{ criterion }\OperatorTok{==} \StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{:}
\NormalTok{            score\_list.append((threshold, weighted\_score))}

    \ControlFlowTok{return}\NormalTok{ np.array(score\_list)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(}\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{gs }\OperatorTok{=}\NormalTok{ gridspec.GridSpec(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, width\_ratios}\OperatorTok{=}\NormalTok{[}\FloatTok{0.2}\NormalTok{,}\DecValTok{1}\NormalTok{], height\_ratios}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{,}\FloatTok{0.2}\NormalTok{])}
\NormalTok{gs.update(left}\OperatorTok{=}\FloatTok{0.16}\NormalTok{, right}\OperatorTok{=}\FloatTok{0.86}\NormalTok{,top}\OperatorTok{=}\FloatTok{0.88}\NormalTok{, bottom}\OperatorTok{=}\FloatTok{0.13}\NormalTok{, hspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{, wspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}

\NormalTok{ax\_main }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{ax\_height }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{ax\_weight }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}

\ControlFlowTok{for}\NormalTok{ i, marker }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(markers):}
\NormalTok{    ax\_main.scatter(X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{0}\NormalTok{], X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{1}\NormalTok{], }
\NormalTok{                    c}\OperatorTok{=}\NormalTok{colors[i], }
\NormalTok{                    edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{30}\NormalTok{, marker}\OperatorTok{=}\NormalTok{marker, label}\OperatorTok{=}\NormalTok{iris.target\_names[i])}

\NormalTok{split\_feature\_height }\OperatorTok{=}\NormalTok{ quantify\_all\_splits(X[:, }\DecValTok{0}\NormalTok{], y, criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{)}
\NormalTok{split\_feature\_weight }\OperatorTok{=}\NormalTok{ quantify\_all\_splits(X[:, }\DecValTok{1}\NormalTok{], y, criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{)}

\NormalTok{ax\_height.plot(split\_feature\_height[:, }\DecValTok{0}\NormalTok{], split\_feature\_height[:, }\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_weight.plot(split\_feature\_weight[:, }\DecValTok{1}\NormalTok{], split\_feature\_weight[:, }\DecValTok{0}\NormalTok{], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)  }

\NormalTok{best\_height\_split }\OperatorTok{=}\NormalTok{ np.argmax(split\_feature\_height[:, }\DecValTok{1}\NormalTok{])}
\NormalTok{best\_weight\_split }\OperatorTok{=}\NormalTok{ np.argmax(split\_feature\_weight[:, }\DecValTok{1}\NormalTok{])}

\NormalTok{ax\_main.axvline(split\_feature\_height[best\_height\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_height.axvline(split\_feature\_height[best\_height\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_main.axhline(split\_feature\_weight[best\_weight\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_weight.axhline(split\_feature\_weight[best\_weight\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}

\NormalTok{ax\_main.}\BuiltInTok{set}\NormalTok{(xticklabels}\OperatorTok{=}\NormalTok{[],}
\NormalTok{            yticklabels}\OperatorTok{=}\NormalTok{[],}
\NormalTok{            title}\OperatorTok{=}\StringTok{"splits using Entropy"}\NormalTok{,}
\NormalTok{            xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{),}
\NormalTok{            ylim}\OperatorTok{=}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{5.0}\NormalTok{)}
\NormalTok{            )}
\NormalTok{ax\_main.legend()}

\NormalTok{ax\_height.}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{),}
\NormalTok{              xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{),}
\NormalTok{              xlabel}\OperatorTok{=}\StringTok{"height"}
\NormalTok{              )}
\NormalTok{ax\_weight.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.6}\NormalTok{),}
\NormalTok{              ylim}\OperatorTok{=}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{5.0}\NormalTok{),}
\NormalTok{                ylabel}\OperatorTok{=}\StringTok{"weight"}\NormalTok{,}
\NormalTok{              )}

\NormalTok{ax\_weight.spines[}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax\_weight.spines[}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax\_height.spines[}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax\_height.spines[}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}

\NormalTok{ax\_height.text(}\OperatorTok{{-}}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\StringTok{"Information}\CharTok{\textbackslash{}n}\StringTok{Gain"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax\_height.transAxes)}
\NormalTok{plt.tight\_layout()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Height: highest Information Gain: }\SpecialCharTok{\{}\NormalTok{split\_feature\_height[best\_height\_split, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{ at height=}\SpecialCharTok{\{}\NormalTok{split\_feature\_height[best\_height\_split, }\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Weight: highest Information Gain: }\SpecialCharTok{\{}\NormalTok{split\_feature\_weight[best\_weight\_split, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{ at weight=}\SpecialCharTok{\{}\NormalTok{split\_feature\_weight[best\_weight\_split, }\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/2101311549.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax_main.scatter(X[y == i, 0], X[y == i, 1],
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/2101311549.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
\end{verbatim}

\begin{verbatim}
Height: highest Information Gain: 0.56 at height=5.55
Weight: highest Information Gain: 0.28 at weight=3.35
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-6-output-3.png}}

\begin{itemize}
\tightlist
\item
  The best split for the `height' feature is at a height 5.55, with
  IG=0.56.
\item
  The best split for the `weight' feature is at a weight 3.35, with
  IG=0.28.
\end{itemize}

Using the Entropy metric, the first split will be on the `height'
feature at a height of 5.55, since it has the highest information gain.

\subsection{Gini impurity}\label{gini-impurity}

Now let's try the \textbf{Gini impurity} metric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(}\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{gs }\OperatorTok{=}\NormalTok{ gridspec.GridSpec(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, width\_ratios}\OperatorTok{=}\NormalTok{[}\FloatTok{0.2}\NormalTok{,}\DecValTok{1}\NormalTok{], height\_ratios}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{,}\FloatTok{0.2}\NormalTok{])}
\NormalTok{gs.update(left}\OperatorTok{=}\FloatTok{0.16}\NormalTok{, right}\OperatorTok{=}\FloatTok{0.86}\NormalTok{,top}\OperatorTok{=}\FloatTok{0.88}\NormalTok{, bottom}\OperatorTok{=}\FloatTok{0.13}\NormalTok{, hspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{, wspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}

\NormalTok{ax\_main }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{ax\_height }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{ax\_weight }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}

\ControlFlowTok{for}\NormalTok{ i, marker }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(markers):}
\NormalTok{    ax\_main.scatter(X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{0}\NormalTok{], X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{1}\NormalTok{], }
\NormalTok{                    c}\OperatorTok{=}\NormalTok{colors[i], }
\NormalTok{                    edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{30}\NormalTok{, marker}\OperatorTok{=}\NormalTok{marker, label}\OperatorTok{=}\NormalTok{iris.target\_names[i])}

\NormalTok{split\_feature\_height }\OperatorTok{=}\NormalTok{ quantify\_all\_splits(X[:, }\DecValTok{0}\NormalTok{], y, criterion}\OperatorTok{=}\StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{)}
\NormalTok{split\_feature\_weight }\OperatorTok{=}\NormalTok{ quantify\_all\_splits(X[:, }\DecValTok{1}\NormalTok{], y, criterion}\OperatorTok{=}\StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{)}

\NormalTok{ax\_height.plot(split\_feature\_height[:, }\DecValTok{0}\NormalTok{], split\_feature\_height[:, }\DecValTok{1}\NormalTok{], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_weight.plot(split\_feature\_weight[:, }\DecValTok{1}\NormalTok{], split\_feature\_weight[:, }\DecValTok{0}\NormalTok{], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{)  }

\NormalTok{best\_height\_split }\OperatorTok{=}\NormalTok{ np.argmin(split\_feature\_height[:, }\DecValTok{1}\NormalTok{])}
\NormalTok{best\_weight\_split }\OperatorTok{=}\NormalTok{ np.argmin(split\_feature\_weight[:, }\DecValTok{1}\NormalTok{])}

\NormalTok{ax\_main.axvline(split\_feature\_height[best\_height\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_height.axvline(split\_feature\_height[best\_height\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_main.axhline(split\_feature\_weight[best\_weight\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{ax\_weight.axhline(split\_feature\_weight[best\_weight\_split, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}

\NormalTok{ax\_main.}\BuiltInTok{set}\NormalTok{(xticklabels}\OperatorTok{=}\NormalTok{[],}
\NormalTok{            yticklabels}\OperatorTok{=}\NormalTok{[],}
\NormalTok{            title}\OperatorTok{=}\StringTok{"splits using Gini Impurity"}\NormalTok{,}
\NormalTok{            xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{),}
\NormalTok{            ylim}\OperatorTok{=}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{5.0}\NormalTok{)}
\NormalTok{            )}
\NormalTok{ax\_main.legend()}

\NormalTok{ax\_height.}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.70}\NormalTok{),}
\NormalTok{              xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{),}
\NormalTok{              xlabel}\OperatorTok{=}\StringTok{"height"}
\NormalTok{              )}
\NormalTok{ax\_weight.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.70}\NormalTok{),}
\NormalTok{              ylim}\OperatorTok{=}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{5.0}\NormalTok{),}
\NormalTok{                ylabel}\OperatorTok{=}\StringTok{"weight"}\NormalTok{,}
\NormalTok{              )}

\NormalTok{ax\_weight.spines[}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax\_weight.spines[}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax\_height.spines[}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax\_height.spines[}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{].set\_visible(}\VariableTok{False}\NormalTok{)}

\NormalTok{ax\_height.text(}\OperatorTok{{-}}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\StringTok{"Gini}\CharTok{\textbackslash{}n}\StringTok{Impurity"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax\_height.transAxes)}
\NormalTok{plt.tight\_layout()}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Height: lowest Gini Impurity: }\SpecialCharTok{\{}\NormalTok{split\_feature\_height[best\_height\_split, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{ at height=}\SpecialCharTok{\{}\NormalTok{split\_feature\_height[best\_height\_split, }\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Weight: lowest Gini Impurity: }\SpecialCharTok{\{}\NormalTok{split\_feature\_weight[best\_weight\_split, }\DecValTok{1}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{ at weight=}\SpecialCharTok{\{}\NormalTok{split\_feature\_weight[best\_weight\_split, }\DecValTok{0}\NormalTok{]}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/3659548085.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax_main.scatter(X[y == i, 0], X[y == i, 1],
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/3659548085.py:52: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  plt.tight_layout()
\end{verbatim}

\begin{verbatim}
Height: lowest Gini Impurity: 0.44 at height=5.45
Weight: lowest Gini Impurity: 0.54 at weight=3.35
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-7-output-3.png}}

\begin{itemize}
\tightlist
\item
  The best split for the `height' feature is at a height 5.45, with
  Gini=0.44.
\item
  The best split for the `weight' feature is at a weight 3.35, with
  Gini=0.54.
\end{itemize}

Using the Gini Impurity metric, the first split will be on the `height'
feature at a height of 5.45, since it has the lowest Gini impurity.

\section{successive splits}\label{successive-splits}

The same idea used for the first split is then applied to each of the
subsets, recursively. The process continues until one of the stopping
criteria is met, such as:

\begin{itemize}
\tightlist
\item
  All samples in a node belong to the same class.
\item
  The maximum depth of the tree is reached.
\item
  The number of samples in a node is less than a predefined minimum.
\end{itemize}

I find it useful to visualize the decision tree as a series of splits in
the feature space:
\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/decision-tree-2d.png}}

Source: \href{https://doi.org/10.1007/s11162-019-09546-y}{``Predicting
University Students' Academic Success and Major Using Random Forests'',
by Cdric Beaulac and Jeffrey S. Rosenthal}

Splitting the feature space with vertical and horizontal lines reminds
me of a classic 1990's computer game, JazzBall.
\href{https://www.youtube.com/watch?v=lKPa9ITMt-g}{Check out a video of
the game here}, and see it reminds you of the basic algorithm discussed
so far.

\section{sklearn tree
DecisionTreeClassifier}\label{sklearn-tree-decisiontreeclassifier}

We will now use the \texttt{DecisionTreeClassifier} from
\texttt{sklearn.tree} to build a decision tree classifier. Let's
restrict the maximum depth of the tree to 3, so we can visualize it
easily. All the hard work was already coded for use, it'll take us only
two lines of code to create and fit the model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# using gini impurity}
\NormalTok{classifier\_gini }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{, max\_depth}\OperatorTok{=}\DecValTok{3}\NormalTok{)}\CommentTok{\#, random\_state=42)}
\NormalTok{classifier\_gini.fit(X, y)}

\CommentTok{\# using entropy}
\NormalTok{classifier\_entropy }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{, max\_depth}\OperatorTok{=}\DecValTok{3}\NormalTok{)}\CommentTok{\#, random\_state=42)}
\NormalTok{classifier\_entropy.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{} & criterion~ & \textquotesingle entropy\textquotesingle{} \\
\emph{} & splitter~ & \textquotesingle best\textquotesingle{} \\
\emph{} & max\_depth~ & 3 \\
\emph{} & min\_samples\_split~ & 2 \\
\emph{} & min\_samples\_leaf~ & 1 \\
\emph{} & min\_weight\_fraction\_leaf~ & 0.0 \\
\emph{} & max\_features~ & None \\
\emph{} & random\_state~ & None \\
\emph{} & max\_leaf\_nodes~ & None \\
\emph{} & min\_impurity\_decrease~ & 0.0 \\
\emph{} & class\_weight~ & None \\
\emph{} & ccp\_alpha~ & 0.0 \\
\emph{} & monotonic\_cst~ & None \\
\end{longtable}

Now let's visualize the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Helper function to plot decision boundaries}
\KeywordTok{def}\NormalTok{ plot\_decision\_boundaries(ax, model, X, y, title):}
    \CommentTok{"""}
\CommentTok{    Plots the decision boundaries for a given classifier.}
\CommentTok{    """}
    \CommentTok{\# Define a mesh grid to color the background based on predictions}
\NormalTok{    x\_min, x\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{    y\_min, y\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{    xx, yy }\OperatorTok{=}\NormalTok{ np.meshgrid(np.arange(x\_min, x\_max, }\FloatTok{0.02}\NormalTok{),}
\NormalTok{                         np.arange(y\_min, y\_max, }\FloatTok{0.02}\NormalTok{))}

    \CommentTok{\# Predict the class for each point in the mesh grid}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ model.predict(np.c\_[xx.ravel(), yy.ravel()])}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ Z.reshape(xx.shape)}

    \CommentTok{\# Plot the colored regions}
\NormalTok{    ax.contourf(xx, yy, Z, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, cmap}\OperatorTok{=}\NormalTok{plt.cm.RdYlBu)}

    \CommentTok{\# Set labels and title}
\NormalTok{    ax.set\_title(title)}
\NormalTok{    ax.set\_xlabel(iris.feature\_names[}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(iris.feature\_names[}\DecValTok{1}\NormalTok{])}
\NormalTok{    ax.set\_xticks(())}
\NormalTok{    ax.set\_yticks(())}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{plot\_decision\_boundaries(ax[}\DecValTok{0}\NormalTok{], classifier\_entropy, X, y, }\StringTok{"Entropy Criterion"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i, marker }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(markers):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].scatter(X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{0}\NormalTok{], X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{1}\NormalTok{], }
\NormalTok{                  c}\OperatorTok{=}\NormalTok{colors[i], }
\NormalTok{                  edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{30}\NormalTok{, marker}\OperatorTok{=}\NormalTok{marker, label}\OperatorTok{=}\NormalTok{iris.target\_names[i])}

\NormalTok{plot\_decision\_boundaries(ax[}\DecValTok{1}\NormalTok{], classifier\_gini, X, y, }\StringTok{"Gini Criterion"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i, marker }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(markers):}
\NormalTok{    ax[}\DecValTok{1}\NormalTok{].scatter(X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{0}\NormalTok{], X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{1}\NormalTok{], }
\NormalTok{                  c}\OperatorTok{=}\NormalTok{colors[i], }
\NormalTok{                  edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{30}\NormalTok{, marker}\OperatorTok{=}\NormalTok{marker, label}\OperatorTok{=}\NormalTok{iris.target\_names[i])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/759299291.py:5: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax[0].scatter(X[y == i, 0], X[y == i, 1],
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/759299291.py:11: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax[1].scatter(X[y == i, 0], X[y == i, 1],
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-10-output-2.png}}

Just by using different criteria, we get different boundaries! We can
now predict the species of a new animal based on its height and weight.

For example, an animal with height 6.3 and weight 4.0 would be
classified as:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predict the species of a new animal with height 5.7 and weight 4.0 using the entropy{-}based classifier}
\NormalTok{sample }\OperatorTok{=}\NormalTok{ np.array([[}\FloatTok{6.3}\NormalTok{, }\FloatTok{4.0}\NormalTok{]])}
\NormalTok{predicted\_class\_entropy }\OperatorTok{=}\NormalTok{ classifier\_entropy.predict(sample)}
\NormalTok{predicted\_class\_gini }\OperatorTok{=}\NormalTok{ classifier\_gini.predict(sample)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predicted species (Entropy): }\SpecialCharTok{\{}\NormalTok{iris}\SpecialCharTok{.}\NormalTok{target\_names[predicted\_class\_entropy[}\DecValTok{0}\NormalTok{]]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Predicted species (Gini): }\SpecialCharTok{\{}\NormalTok{iris}\SpecialCharTok{.}\NormalTok{target\_names[predicted\_class\_gini   [}\DecValTok{0}\NormalTok{]]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Predicted species (Entropy): koala
Predicted species (Gini): bonobo
\end{verbatim}

See also the decision trees for each criterion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plot\_tree(classifier\_entropy,}
\NormalTok{          filled}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          rounded}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          class\_names}\OperatorTok{=}\NormalTok{iris.target\_names,}
\NormalTok{          feature\_names}\OperatorTok{=}\NormalTok{[iris.feature\_names[}\DecValTok{0}\NormalTok{], iris.feature\_names[}\DecValTok{1}\NormalTok{]],}
\NormalTok{          ax}\OperatorTok{=}\NormalTok{ax)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-12-output-1.png}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plot\_tree(classifier\_gini,}
\NormalTok{          filled}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          rounded}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          class\_names}\OperatorTok{=}\NormalTok{iris.target\_names,}
\NormalTok{          feature\_names}\OperatorTok{=}\NormalTok{[iris.feature\_names[}\DecValTok{0}\NormalTok{], iris.feature\_names[}\DecValTok{1}\NormalTok{]],}
\NormalTok{          ax}\OperatorTok{=}\NormalTok{ax)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-13-output-1.png}}

\section{overfitting}\label{overfitting}

What would happen if we chose to grow our decision tree until all leaves
are pure? This would lead to a very complex tree that perfectly
classifies the training data, but might not generalize well to new,
unseen data. This is known as overfitting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{classifier\_gini }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{)}
\NormalTok{classifier\_gini.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{} & criterion~ & \textquotesingle gini\textquotesingle{} \\
\emph{} & splitter~ & \textquotesingle best\textquotesingle{} \\
\emph{} & max\_depth~ & None \\
\emph{} & min\_samples\_split~ & 2 \\
\emph{} & min\_samples\_leaf~ & 1 \\
\emph{} & min\_weight\_fraction\_leaf~ & 0.0 \\
\emph{} & max\_features~ & None \\
\emph{} & random\_state~ & None \\
\emph{} & max\_leaf\_nodes~ & None \\
\emph{} & min\_impurity\_decrease~ & 0.0 \\
\emph{} & class\_weight~ & None \\
\emph{} & ccp\_alpha~ & 0.0 \\
\emph{} & monotonic\_cst~ & None \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{plot\_decision\_boundaries(ax[}\DecValTok{0}\NormalTok{], classifier\_gini, X, y, }\StringTok{"Decision Boundaries in feature space (Gini Criterion)"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i, marker }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(markers):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].scatter(X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{0}\NormalTok{], X[y }\OperatorTok{==}\NormalTok{ i, }\DecValTok{1}\NormalTok{], }
\NormalTok{                  c}\OperatorTok{=}\NormalTok{colors[i], }
\NormalTok{                  edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{30}\NormalTok{, marker}\OperatorTok{=}\NormalTok{marker, label}\OperatorTok{=}\NormalTok{iris.target\_names[i])}

\NormalTok{plot\_tree(classifier\_gini,}
\NormalTok{          filled}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          rounded}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          class\_names}\OperatorTok{=}\NormalTok{iris.target\_names,}
\NormalTok{          feature\_names}\OperatorTok{=}\NormalTok{[iris.feature\_names[}\DecValTok{0}\NormalTok{], iris.feature\_names[}\DecValTok{1}\NormalTok{]],}
\NormalTok{          ax}\OperatorTok{=}\NormalTok{ax[}\DecValTok{1}\NormalTok{])}\OperatorTok{;}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_title(}\StringTok{"Decision Tree (Gini Criterion)"}\NormalTok{)}\OperatorTok{;}
\NormalTok{fig.savefig(}\StringTok{"decision\_boundaries\_no\_max\_depth.png"}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_10048/359429702.py:5: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  ax[0].scatter(X[y == i, 0], X[y == i, 1],
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_classification_files/figure-pdf/cell-15-output-2.png}}

We can see that some of the regions are very small and specific to the
training data points. This means that the model has learned not only the
underlying patterns in the data but also the noise and outliers, which
can lead to poor performance on new data.

To avoid overfitting, we can use techniques such as:

\begin{itemize}
\tightlist
\item
  Setting a maximum depth. We limit how deep the tree can grow. Read
  about \texttt{max\_depth}.
\item
  Setting a minimum number of samples required to split a node. Read
  about \texttt{min\_samples\_split}.
\item
  Post-pruning: Cutting back the tree after it has been grown to remove
  branches that do not provide significant predictive power. Read about
  \texttt{cost\_complexity\_pruning\_path}.
\end{itemize}

Other machine learning algorithms, such as Random Forests and Gradient
Boosted Trees, use ensemble methods that combine multiple decision trees
to improve performance and reduce overfitting.

\chapter{CART: regression}\label{cart-regression}

In the previous classification example, we saw how to put each animal
into a category (koala, fox, bonobo) based on its features (height and
weight). If you haven't read it yet,
\href{./decision_trees/CART_classification.ipynb}{go back to the
classification example}.

In regression, instead of putting things into categories, we predict a
continuous value. Building on the previous example, let's say we want to
predict the age of an animal based on its height and weight.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.gridspec }\ImportTok{as}\NormalTok{ gridspec}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_iris}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor, plot\_tree}
\end{Highlighting}
\end{Shaded}

Again, we are using the famous Iris dataset structure:

\begin{itemize}
\tightlist
\item
  sepal length (cm) \(\rightarrow\) height
\item
  sepal width (cm) \(\rightarrow\) weight
\item
  petal length (cm) \(\rightarrow\) age
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OperatorTok{=}\NormalTok{ load\_iris()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ iris.data[:, [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ iris.data[:,[}\DecValTok{2}\NormalTok{]].flatten()}
\NormalTok{iris.feature\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{scatter }\OperatorTok{=}\NormalTok{ ax.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\NormalTok{y, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}plasma\textquotesingle{}}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{100}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{7}\NormalTok{)}
\NormalTok{ax.set\_xlabel(iris.feature\_names[}\DecValTok{0}\NormalTok{])}
\NormalTok{ax.set\_ylabel(iris.feature\_names[}\DecValTok{1}\NormalTok{])}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}height\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{,}
\NormalTok{       title}\OperatorTok{=}\StringTok{"Animal data: height vs weight colored by age"}\NormalTok{)}
\NormalTok{fig.colorbar(scatter, label}\OperatorTok{=}\StringTok{\textquotesingle{}Age (years)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_regression_files/figure-pdf/cell-4-output-1.png}}

\section{the split}\label{the-split-1}

We will follow a similar procedure to split the data along the features,
but this time our target variable is continuous (age) instead of
categorical (animal type). In classification we wanted to have leaves as
pure as possible, and we quantified that either with Gini impurity or
entropy. In regression, we want to minimize the variance of the target
variable within each leaf. In our example, this means that we want to
split the data in a way that the ages of the animals in each leaf are as
similar as possible.

The cost function we will use to evaluate the quality of a split is the
Weighted Mean Squared Error (MSE):

\[
J(j,s) = \frac{N_\text{left}}{N_\text{total}} \cdot \text{MSE}_\text{left} + \frac{N_\text{right}}{N_\text{total}} \cdot \text{MSE}_\text{right},
\] where \(N_\text{left}\) and \(N_\text{right}\) are the number of
samples in the left and right child nodes, respectively, and
\(N_\text{total}\) is the total number of samples in the parent node.
\(\text{MSE}_\text{left}\) and \(\text{MSE}_\text{right}\) are the mean
squared errors of the target variable in the left and right child nodes:

\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \bar{y})^2,
\] where \(y_i\) are the target values in the node, \(\bar{y}\) is the
mean target value in the node, and \(N\) is the number of samples in the
node.

The cost function is weighted by the number of samples in each child
node to account for the fact that larger nodes have a greater impact on
the overall variance.

So how do we know which split is the best? We will evaluate all possible
split thresholds \(s\) along all features \(j\) and choose the one that
minimizes the Weighted MSE. In a mathematical language:

\[
(j^*, s^*) = \arg\min_{j,s} J(j,s).
\]

\section{sklearn tree
DecisionTreeRegressor}\label{sklearn-tree-decisiontreeregressor}

We will use the \texttt{DecisionTreeRegressor} class from
\texttt{sklearn.tree} to build our regression tree.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regressor }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(max\_depth}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{regressor.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{} & criterion~ &
\textquotesingle squared\_error\textquotesingle{} \\
\emph{} & splitter~ & \textquotesingle best\textquotesingle{} \\
\emph{} & max\_depth~ & 3 \\
\emph{} & min\_samples\_split~ & 2 \\
\emph{} & min\_samples\_leaf~ & 1 \\
\emph{} & min\_weight\_fraction\_leaf~ & 0.0 \\
\emph{} & max\_features~ & None \\
\emph{} & random\_state~ & None \\
\emph{} & max\_leaf\_nodes~ & None \\
\emph{} & min\_impurity\_decrease~ & 0.0 \\
\emph{} & ccp\_alpha~ & 0.0 \\
\emph{} & monotonic\_cst~ & None \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Helper function to plot decision boundaries}
\KeywordTok{def}\NormalTok{ plot\_decision\_boundaries(ax, model, X, y, title):}
    \CommentTok{"""}
\CommentTok{    Plots the decision boundaries for a given classifier.}
\CommentTok{    """}
    \CommentTok{\# Define a mesh grid to color the background based on predictions}
\NormalTok{    x\_min, x\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{    y\_min, y\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{    xx, yy }\OperatorTok{=}\NormalTok{ np.meshgrid(np.arange(x\_min, x\_max, }\FloatTok{0.02}\NormalTok{),}
\NormalTok{                         np.arange(y\_min, y\_max, }\FloatTok{0.02}\NormalTok{))}

    \CommentTok{\# Predict the class for each point in the mesh grid}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ model.predict(np.c\_[xx.ravel(), yy.ravel()])}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ Z.reshape(xx.shape)}

    \CommentTok{\# Plot the colored regions}
\NormalTok{    ax.contourf(xx, yy, Z, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}plasma\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{7}\NormalTok{)}

    \CommentTok{\# Set labels and title}
\NormalTok{    ax.set\_title(title)}
\NormalTok{    ax.set\_xlabel(iris.feature\_names[}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(iris.feature\_names[}\DecValTok{1}\NormalTok{])}
\NormalTok{    ax.set\_xticks(())}
\NormalTok{    ax.set\_yticks(())}

\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}

\CommentTok{\# Helper function to plot decision boundaries}
\KeywordTok{def}\NormalTok{ plot\_decision\_boundaries\_3d(ax, model, X, y, title):}
    \CommentTok{\# Define a mesh grid}
\NormalTok{    x\_min, x\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{0}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{    y\_min, y\_max }\OperatorTok{=}\NormalTok{ X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{{-}} \FloatTok{0.5}\NormalTok{, X[:, }\DecValTok{1}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{+} \FloatTok{0.5}
\NormalTok{    xx, yy }\OperatorTok{=}\NormalTok{ np.meshgrid(np.arange(x\_min, x\_max, }\FloatTok{0.1}\NormalTok{),}
\NormalTok{                         np.arange(y\_min, y\_max, }\FloatTok{0.1}\NormalTok{))}
    
    \CommentTok{\# Predict the values for each point in the mesh grid}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ model.predict(np.c\_[xx.ravel(), yy.ravel()])}
\NormalTok{    Z }\OperatorTok{=}\NormalTok{ Z.reshape(xx.shape)}
    
    \CommentTok{\# Plot the surface}
\NormalTok{    surf }\OperatorTok{=}\NormalTok{ ax.plot\_surface(xx, yy, Z, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}plasma\textquotesingle{}}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{7}\NormalTok{)}
    
    \CommentTok{\# Plot the actual data points}
\NormalTok{    scatter }\OperatorTok{=}\NormalTok{ ax.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], y, c}\OperatorTok{=}\NormalTok{y, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}plasma\textquotesingle{}}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{50}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{7}\NormalTok{)}
    
    \CommentTok{\# Set labels and title}
\NormalTok{    ax.set\_xlabel(iris.feature\_names[}\DecValTok{0}\NormalTok{])}
\NormalTok{    ax.set\_ylabel(iris.feature\_names[}\DecValTok{1}\NormalTok{])}
\NormalTok{    ax.set\_zlabel(iris.feature\_names[}\DecValTok{2}\NormalTok{])}
\NormalTok{    ax.set\_title(title)}
    
    \CommentTok{\# fig.colorbar(scatter, ax=ax, label=\textquotesingle{}Age (years)\textquotesingle{}, shrink=0.5)}
    
    \ControlFlowTok{return}\NormalTok{ fig, ax}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{plot\_decision\_boundaries(ax, regressor, X, y, }\StringTok{"Regression Tree Predictions"}\NormalTok{)}
\NormalTok{scatter }\OperatorTok{=}\NormalTok{ ax.scatter(X[:, }\DecValTok{0}\NormalTok{], X[:, }\DecValTok{1}\NormalTok{], c}\OperatorTok{=}\NormalTok{y, cmap}\OperatorTok{=}\StringTok{\textquotesingle{}plasma\textquotesingle{}}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, s}\OperatorTok{=}\DecValTok{50}\NormalTok{, vmin}\OperatorTok{=}\DecValTok{1}\NormalTok{, vmax}\OperatorTok{=}\DecValTok{7}\NormalTok{)}
\NormalTok{fig.colorbar(scatter, label}\OperatorTok{=}\StringTok{\textquotesingle{}Age (years)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_regression_files/figure-pdf/cell-7-output-1.png}}

Each region (leaf) of the tree will predict the mean age of the training
samples that fall into that region. Visualizing this in 3d shows us
steps, because the regression tree creates a \textbf{piecewise constant}
approximation of the target variable (age) over the feature space
(height and weight).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{121}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{plot\_decision\_boundaries\_3d(ax, regressor, X, y, }\StringTok{"3d plot"}\NormalTok{)}
\NormalTok{ax.view\_init(elev}\OperatorTok{=}\DecValTok{20}\NormalTok{, azim}\OperatorTok{=}\DecValTok{110}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/CART_regression_files/figure-pdf/cell-8-output-1.png}}

The same consideration regarding overfitting applies here as in
classification.

\chapter{random forest}\label{random-forest}

The motivation behind random forests is to avoid the weird regions in
the feature space that a single decision tree might create. In the
\href{./decision_trees/CART_classification.html\#overfitting}{tutorial
for classification with CART}, we saw this:
\pandocbounded{\includegraphics[keepaspectratio]{decision_trees/decision_boundaries_no_max_depth.png}}

The small blue regions inside the yellow region are highly undesirable.
The decision tree learned very well the data it was given, but it will
probably not generalize well to new data points. Random forests solve
this problem in three steps.

\section{step 1: bootstrap sampling}\label{step-1-bootstrap-sampling}

Instead of running the decision tree algorithm once on the entire
training set, we run it multiple times on different bootstrap samples of
the training set. We already learned about bootstrap sampling in the
\href{confidence_interval/empirical_confidence_interval.html}{empirical
confidence interval} tutorial. In a nutshell, if we have a data set with
\(N\) data points, we create a bootstrap sample by sampling \(N\) data
points \textbf{with replacement} from the original data set. This means
that some data points will appear multiple times in the bootstrap
sample, while others will not appear at all. We can choose how many
bootstrap samples we want to create. The default used by
\texttt{sklearn}'s \texttt{RandomForestRegressor} is 100 (this argument
is called \texttt{n\_estimators}).

What fraction of the dataset will a given bootstrap sample contain, on
average? The probability of choosing a specific data point in one draw
is \(1/N\). Therefore, the probability of \textbf{not} choosing that
data point in one draw is \(1 - 1/N\). If we draw \(N\) times with
replacement, the probability of never choosing that data point is

\[
\left(1 - \frac{1}{N}\right)^N
\]

As \(N\) becomes large\ldots{}

\[
\lim_{N \to \infty} \left(1 - \frac{1}{N}\right)^N = e^{-1} \approx 0.37.
\]

\marginnote{\begin{footnotesize}

This follows from the definition of the exponential function: \[
e^x = \lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n,
\] just set \(x = -1\).

\end{footnotesize}}

This means that, on average, a bootstrap sample will contain about 63\%
of the original data points (because about 37\% of them will not be
chosen at all).

\section{step 2: random feature
selection}\label{step-2-random-feature-selection}

When training each decision tree on a bootstrap sample, we also randomly
select a subset of the features to consider \textbf{for each split in
the tree}. For example, if we have 10 features in total, we might
randomly select 3 of them to consider for each split. This further
increases the diversity among the trees in the forest, which helps to
reduce overfitting.

Why is this important? Imagine a dataset where feature number 1 is very
strongly correlated with the target variable. In that case, most
decision trees will likely use that feature for the top split, leading
to similar trees and less diversity in the forest.

As a rule of thumb, we typically use the square root or the logarithm
(base 2) of the total number of features as the number of features to
consider for each split. The argument in \texttt{sklearn}'s
\texttt{RandomForestRegressor} that controls this is called
\texttt{max\_features}, and its default value is 1.0. This means that if
we don't specify anything, it will use 100\% of the features in each
split, and we will not have ``feature decorrelation''. In the
documentation, search for
\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{max\_features}
to find more details.

\section{step 3: bagging}\label{step-3-bagging}

Finally, to make a prediction for a new data point, we pass it through
each of the decision trees in the forest and average their predictions
(for regression) or take a majority vote (for classification). This
process is called ``bagging'' (short for \textbf{b}ootstrap
\textbf{agg}regat\textbf{ing}). By averaging the predictions of multiple
trees, we can reduce the variance of the model and improve its
generalization performance.

\section{example: iris dataset}\label{example-iris-dataset}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.gridspec }\ImportTok{as}\NormalTok{ gridspec}
\ImportTok{from}\NormalTok{ sklearn.datasets }\ImportTok{import}\NormalTok{ load\_iris}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor, plot\_tree}
\ImportTok{from}\NormalTok{ sklearn.ensemble }\ImportTok{import}\NormalTok{ RandomForestRegressor}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OperatorTok{=}\NormalTok{ load\_iris()}

\CommentTok{\# 1. Prepare Data (3 Inputs, 1 Output)}
\CommentTok{\# columns: 0=SepalLen, 1=SepalWid, 2=PetalLen, 3=PetalWid}
\NormalTok{X\_full }\OperatorTok{=}\NormalTok{ iris.data[:, [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]]  }\CommentTok{\# 3 features}
\NormalTok{y\_full }\OperatorTok{=}\NormalTok{ iris.data[:, }\DecValTok{3}\NormalTok{]          }\CommentTok{\# Target: Petal Width}

\CommentTok{\# 2. Train Models}
\NormalTok{tree\_model }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{rf\_model }\OperatorTok{=}\NormalTok{ RandomForestRegressor(n\_estimators}\OperatorTok{=}\DecValTok{100}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\NormalTok{tree\_model.fit(X\_full, y\_full)}
\NormalTok{rf\_model.fit(X\_full, y\_full)}

\CommentTok{\# 3. Compare Errors (The numerical proof)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Single Tree Score: }\SpecialCharTok{\{}\NormalTok{tree\_model}\SpecialCharTok{.}\NormalTok{score(X\_full, y\_full)}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Random Forest Score: }\SpecialCharTok{\{}\NormalTok{rf\_model}\SpecialCharTok{.}\NormalTok{score(X\_full, y\_full)}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}

\CommentTok{\# X = iris.data[:, [0, 1]]}
\CommentTok{\# y = iris.data[:,[2]].flatten()}
\CommentTok{\# iris.feature\_names = [\textquotesingle{}height\textquotesingle{}, \textquotesingle{}weight\textquotesingle{}, \textquotesingle{}age\textquotesingle{}]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Single Tree Score: 0.999
Random Forest Score: 0.991
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}

\CommentTok{\# 1. Split the data (80\% for training, 20\% for testing)}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X\_full, y\_full, test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\CommentTok{\# 2. Train on ONLY the training set}
\NormalTok{tree\_model.fit(X\_train, y\_train)}
\NormalTok{rf\_model.fit(X\_train, y\_train)}

\CommentTok{\# 3. Test on the unseen data}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Single Tree Test Score:   }\SpecialCharTok{\{}\NormalTok{tree\_model}\SpecialCharTok{.}\NormalTok{score(X\_test, y\_test)}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Random Forest Test Score: }\SpecialCharTok{\{}\NormalTok{rf\_model}\SpecialCharTok{.}\NormalTok{score(X\_test, y\_test)}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Single Tree Test Score:   0.845
Random Forest Test Score: 0.931
\end{verbatim}

\part{information theory}

\chapter{entropy}\label{entropy-1}

Let's derive the formula for entropy from first principles.

\section{image-generating machines}\label{image-generating-machines}

We're given machines that generate images of cats () and dogs ().
See the following outputs from the machines:

machine 1:\\
\texttt{\ \ \ \ \ }, 5 cats and 1 dog

machine 2:\\
\texttt{\ \ \ \ \ \ \ \ \ \ \ \ \ }, 14 cats
and 0 dogs

machine 3:\\
\texttt{\ \ \ \ \ \ \ \ \ }, 4 cats and 6 dogs

machine 4:\\
\texttt{\ \ \ \ \ \ \ \ \ \ \ }, 6 cats and 6
dogs

\section{surprise}\label{surprise}

How surprised are we by each output?

For machine 2, we wouldn't be surprised at all if the next image it
generates is a cat. It's been generating only cats so far. However, if
it generates a dog, that would be quite surprising.

Machine 4 produced equal numbers of cats and dogs. So we would be
equally surprised if the next image is a cat or a dog.

Machines 1 and 3 are somewhere in between.

\section{information}\label{information-2}

We could say similar things about information. If machine 2 generates
another cat, we don't learn anything new, it's the same machine as ever.
But if it generates a dog, we learn a lot about this machine's behavior.
In contrast, machine 4 generates equal amounts of information whether it
produces a cat or a dog, since both have had the same number so far.

To say that we are surprised by an event is the same as saying that we
learn some information from it. If an event is not surprising at all,
then it doesn't provide us with any new information.

\begin{quote}
The sun will rise tomorrow.
\end{quote}

This is neither surprising nor informative.

\begin{quote}
I have a dragon living in my garage.
\end{quote}

This is both surprising and informative.

\section{quantifying surprise and
information}\label{quantifying-surprise-and-information}

An event that is very likely to happen is not surprising, and doesn't
provide us with much information. An event that is unlikely to happen is
surprising, and provides us with a lot of information. It seems
reasonable to say that the amount of information \(I\) we gain from an
event \(x\), whose probability is \(P(x)\), is inversely proportional to
the probability:

\[
I(P(x)) = \frac{1}{P(x)}
\tag{1}
\]

\section{problems with the inverse
formula}\label{problems-with-the-inverse-formula}

There are at least two problems with this inverse formula.

\textbf{Problem one}

An event with zero probability would provide us with infinite
information. Maybe that's okay, since I could be infinitely surprised if
the sun didn't rise tomorrow. However, a certain event (probability 1)
would provide us with only 1 unit of information. That doesn't seem
right. A certain event should provide us with no information at all,
zero.

\textbf{Problem two}

Let's say that we learn about two completely independent events that
happened yesterday, \(x\) and \(y\):

\begin{itemize}
\tightlist
\item
  \(x\): My sister flipped a coin and got ``heads''.
\item
  \(y\): My cousin has won the lottery.
\end{itemize}

Since the events are independent, the probability that both have
happened is the product of their probabilities:

\[
P(x \text{ and } y) = P(x)P(y)
\tag{2}
\]

Now let's calculate the information we gained from learning that both
events happened:

\begin{align*}
I(P(x \text{ and } y)) &= I(P(x)P(y)) \\
                       &= \frac{1}{P(x)P(y)} \\
                       &= \frac{1}{P(x)} \cdot \frac{1}{P(y)} \\
                       &= I(P(x)) \cdot I(P(y))
                       \tag{3}
\end{align*}

The total information I gained is the product of the information I
gained from each event. There's something wrong with that. Assume that
my sister flipped a fair coin, so the probability of heads is 1/2. From
the inverse formula, this means that the ``heads'' outcome
\textbf{doubled} the information I got from yesterday's events. My
surprise from learning that my cousin won the lottery was multiplied by
2 by a mere coin toss. That's obviously not ok.

What would make sense is if the total information I gained from both
events was the \textbf{sum} of the information I gained from each
independent event. Then, if my sister flipped a fair coin, the
information I gained from learning that my cousin won the lottery would
be increased by a fixed amount, regardless of how surprising the lottery
win was.

\section{a new and better formula}\label{a-new-and-better-formula}

We are looking for a function \(I(P(x))\) such that:

\begin{itemize}
\tightlist
\item
  \(I(1) = 0\)\\
  A sure event provides no information.
\item
  \(I(P(x)P(y)) = I(P(x)) + I(P(y))\)\\
  The information from two independent events is the sum of the
  information from each event.
\end{itemize}

We can guess another formula that satisfies these two properties:

\[
I(P(x)) = \log\left(\frac{1}{P(x)}\right)
\tag{4}
\]

\begin{itemize}
\tightlist
\item
  This formula satisfies the first property, since \(\log(1) = 0\).
\item
  It also satisfies the second property, since
  \(\log(ab) = \log(a) + \log(b)\).
\end{itemize}

Claude Shannon, the father of information theory, proposed this formula
in his seminal 1948 paper
\href{https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf}{``A
Mathematical Theory of Communication''}. There, he proved (see Appendix
2) that this is the only function that satisfies three properties,
related to the two stated above, but more restrictive. See Shannon's
three properties, stated in Section 6 of his paper, titled ``Choice,
Uncertainty, and Entropy''.

\section{entropy}\label{entropy-2}

Shannon writes:

\begin{quote}
Suppose we have a set of possible events whose probabilities of
occurrence are \(p_1, p_2, \ldots, p_n\). These probabilities are known
but that is all we know concerning which event will occur. Can we find a
measure of how much ``choice'' is involved in the selection of the event
or of how uncertain we are of the outcome?
\end{quote}

After introducing the three properties that a measure \(H\) must satisfy
to answer the question above, Shannon presents the formula for the
entropy:

\begin{quote}
We shall call \[
H = - \sum p_i \log(p_i)
\tag{5}
\] the entropy of the set of probabilities \(p_1, \ldots, p_n\).
\end{quote}

This is the standard formula for entropy, but a better rendition is:

\[
H = \sum P_i \log\left(\frac{1}{P_i}\right)
\tag{6}
\]

(Shannon uses \(p_i\), but I'll go back to using capital \(P\) for
probabilities.)

This formula is better because it clearly answers the following
question:

\begin{quote}
What would be the expected amount of information (or surprise) we would
gain from a probabilistic event?
\end{quote}

We don't know what image our machines will produce next. But we do know
the probabilities of each outcome. So we can calculate the
\textbf{expected} information from the next image:

\[
H = \sum_i P(x_i) I(P(x_i)) = \sum_i P(x_i) \log\left(\frac{1}{P(x_i)}\right)
\tag{7}
\]

The information from each possible outcome \(x_i\) is weighted by the
probability of that outcome, and we sum over all possible outcomes.

For instance, machine 1 produces dogs with a probability of 1/6, and
cats with a probability of 5/6. The expected information from the next
image is the information from a dog times the probability of getting a
dog, plus the information from a cat times the probability of getting a
cat. If our machine produced \(N\) possible images, we would do the
same, summing over all \(N\) possible images with their respective
probabilities as weights.

\section{binary machines}\label{binary-machines}

In the example of the machines that produce only two outcomes, we have
that one probability is \(P\) (say, for getting cats) and the other is
\(Q=1-P\). So we can write the entropy as a function of a single
probability:

\[
H(P) = - P \log(P) - (1-P) \log(1-P)
\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.gridspec }\ImportTok{as}\NormalTok{ gridspec}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{p }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{eps }\OperatorTok{=} \FloatTok{1e{-}10}
\KeywordTok{def}\NormalTok{ entropy\_H(p):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{p}\OperatorTok{*}\NormalTok{np.log2(p}\OperatorTok{+}\NormalTok{eps)}\OperatorTok{{-}}\NormalTok{(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p)}\OperatorTok{*}\NormalTok{np.log2(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p}\OperatorTok{+}\NormalTok{eps)}
\NormalTok{ax.plot(p, entropy\_H(p), label}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{  H}\KeywordTok{(}\VerbatimStringTok{p}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{)}

\NormalTok{p\_machine1 }\OperatorTok{=} \DecValTok{5}\OperatorTok{/}\DecValTok{6}
\NormalTok{ax.plot([p\_machine1], [entropy\_H(p\_machine1)], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(p\_machine1}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, entropy\_H(p\_machine1), }\StringTok{"M1"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}

\NormalTok{p\_machine2 }\OperatorTok{=} \FloatTok{1.0}
\NormalTok{ax.plot([p\_machine2], [entropy\_H(p\_machine2)], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(p\_machine2}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, entropy\_H(p\_machine2), }\StringTok{"M2"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}

\NormalTok{p\_machine3 }\OperatorTok{=} \DecValTok{4}\OperatorTok{/}\DecValTok{10}
\NormalTok{ax.plot([p\_machine3], [entropy\_H(p\_machine3)], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(p\_machine3}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, entropy\_H(p\_machine3), }\StringTok{"M3"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}

\NormalTok{p\_machine4 }\OperatorTok{=} \DecValTok{6}\OperatorTok{/}\DecValTok{12}
\NormalTok{ax.plot([p\_machine4], [entropy\_H(p\_machine4)], marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.text(p\_machine4, entropy\_H(p\_machine4)}\OperatorTok{{-}}\FloatTok{0.05}\NormalTok{, }\StringTok{"M4"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}tab:orange\textquotesingle{}}\NormalTok{)}

\NormalTok{ax.set\_xlabel(}\StringTok{"P"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"bits"}\NormalTok{)}
\NormalTok{ax.set\_title(}\StringTok{"Binary Entropy Function"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Binary Entropy Function')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{information_theory/entropy_files/figure-pdf/cell-3-output-2.png}}

The expected information is zero for machine 2, which certainly produces
only cats (\(P=1\)). The expected information is maximal for machine 4,
which produces cats and dogs with equal probabilities (\(P=1/2\)).

In Equations (5) and (6), we didn't explicitly say what the base of the
logarithm is. It doesn't matter, since changing the base only changes
the units of information. In the example above, we use base 2 because we
have a binary choice, and the units for entropy are called bits,
suggested by JW Tukey, meaning \emph{binary digits}.

\section{one last example}\label{one-last-example}

Consider the frequency of the letters in the Hebrew alphabet (there are
22 letters). The most common letter is ``'', which appears with a
frequency of about 11.06\%. The least common letter is ``'', which
appears with a frequency of about 1.24\%. If we opened a book written in
Hebrew to a random page, and picked a random letter on that page, what
would be the expected information from that letter?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"../archive/data/letter\_frequency\_hebrew.csv"}\NormalTok{, sep}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}

\NormalTok{fig}\OperatorTok{=}\NormalTok{ plt.figure(}\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{))}

\NormalTok{gs }\OperatorTok{=}\NormalTok{ gridspec.GridSpec(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, width\_ratios}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{,}\FloatTok{0.2}\NormalTok{], height\_ratios}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\NormalTok{gs.update(left}\OperatorTok{=}\FloatTok{0.16}\NormalTok{, right}\OperatorTok{=}\FloatTok{0.86}\NormalTok{,top}\OperatorTok{=}\FloatTok{0.88}\NormalTok{, bottom}\OperatorTok{=}\FloatTok{0.13}\NormalTok{, hspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{, wspace}\OperatorTok{=}\FloatTok{0.05}\NormalTok{)}

\NormalTok{ax0 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{ax1 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], sharex}\OperatorTok{=}\NormalTok{ax0)}
\NormalTok{ax2 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{], sharex}\OperatorTok{=}\NormalTok{ax0)}
\NormalTok{ax3 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[:, }\DecValTok{1}\NormalTok{])}

\NormalTok{bar\_width }\OperatorTok{=} \FloatTok{0.8}

\NormalTok{ax0.bar(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{], df[}\StringTok{\textquotesingle{}Frequency(\%)\textquotesingle{}}\NormalTok{]}\OperatorTok{/}\DecValTok{100}\NormalTok{, width}\OperatorTok{=}\NormalTok{bar\_width)}
\ControlFlowTok{for}\NormalTok{ i, letter }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{]):}
\NormalTok{    ax0.text(i, df[}\StringTok{\textquotesingle{}Frequency(\%)\textquotesingle{}}\NormalTok{][i]}\OperatorTok{/}\DecValTok{100} \OperatorTok{+} \FloatTok{0.002}\NormalTok{, letter, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{ax0.set\_xticklabels(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{], rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}\OperatorTok{;}
\NormalTok{ax0.set\_ylabel(}\StringTok{\textquotesingle{}prob. mass function\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{ax0.}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\BuiltInTok{len}\NormalTok{(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{])}\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{))}
\NormalTok{ax0.text(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\StringTok{\textquotesingle{}letter frequency\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax0.transAxes, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{ax0.text(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.85}\NormalTok{, }\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\VerbatimStringTok{P\_i}\DecValTok{$}\VerbatimStringTok{\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax0.transAxes, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{16}\NormalTok{)}

\NormalTok{ax1.bar(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{], np.log2(}\DecValTok{1}\OperatorTok{/}\NormalTok{(df[}\StringTok{\textquotesingle{}Frequency(\%)\textquotesingle{}}\NormalTok{]}\OperatorTok{/}\DecValTok{100}\NormalTok{)), width}\OperatorTok{=}\NormalTok{bar\_width)}
\NormalTok{ax1.set\_ylabel(}\StringTok{\textquotesingle{}self{-}information}\CharTok{\textbackslash{}n}\StringTok{(bits)\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{ax1.text(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\StringTok{\textquotesingle{}letter self{-}information\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax1.transAxes, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{ax1.text(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.85}\NormalTok{, }\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{log\_2}\KeywordTok{(}\VerbatimStringTok{1/P\_i}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax1.transAxes, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{16}\NormalTok{)}

\NormalTok{Hi }\OperatorTok{=}\NormalTok{ (df[}\StringTok{\textquotesingle{}Frequency(\%)\textquotesingle{}}\NormalTok{]}\OperatorTok{/}\DecValTok{100}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.log2(}\DecValTok{1}\OperatorTok{/}\NormalTok{(df[}\StringTok{\textquotesingle{}Frequency(\%)\textquotesingle{}}\NormalTok{]}\OperatorTok{/}\DecValTok{100}\NormalTok{))}
\NormalTok{colors }\OperatorTok{=}\NormalTok{ sns.color\_palette(}\StringTok{"deep"}\NormalTok{, n\_colors}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(df))}
\ControlFlowTok{for}\NormalTok{ i, (letter, h) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{], Hi)):}
\NormalTok{    ax2.bar(letter, h, color}\OperatorTok{=}\NormalTok{colors[i }\OperatorTok{\%} \BuiltInTok{len}\NormalTok{(colors)], width}\OperatorTok{=}\NormalTok{bar\_width)}
\NormalTok{ax2.set\_ylabel(}\StringTok{\textquotesingle{}entropy (bits)\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{ax2.}\BuiltInTok{set}\NormalTok{(yticks}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\NormalTok{ax2.set\_ylim(}\DecValTok{0}\NormalTok{,}\FloatTok{4.5}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\NormalTok{ax2.text(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\StringTok{\textquotesingle{}letter entropy contribution\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax2.transAxes, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{ax2.text(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.85}\NormalTok{, }\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\VerbatimStringTok{P\_i}\ErrorTok{\textbackslash{}}\VerbatimStringTok{cdot}\ErrorTok{\textbackslash{}}\VerbatimStringTok{log\_2}\KeywordTok{(}\VerbatimStringTok{1/P\_i}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{\textquotesingle{}}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax2.transAxes, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{16}\NormalTok{)}

\CommentTok{\# Annotate arrow from x=10 to x=20 on ax2}
\NormalTok{ax2.annotate(}
    \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }
\NormalTok{    xy}\OperatorTok{=}\NormalTok{(}\BuiltInTok{len}\NormalTok{(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.3}\NormalTok{), }
\NormalTok{    xytext}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FloatTok{0.3}\NormalTok{), }
\NormalTok{    arrowprops}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(arrowstyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textgreater{}\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{)}
\NormalTok{)}
\NormalTok{ax2.text(}\FloatTok{15.5}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\VerbatimStringTok{r\textquotesingle{}}\DecValTok{$}\VerbatimStringTok{H=}\DecValTok{\textbackslash{}s}\VerbatimStringTok{um P\_i }\ErrorTok{\textbackslash{}}\VerbatimStringTok{log\_2}\KeywordTok{(}\VerbatimStringTok{1/P\_i}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{\textquotesingle{}}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{16}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ spine }\KeywordTok{in}\NormalTok{ [}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{]:}
\NormalTok{    ax0.spines[spine].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{    ax1.spines[spine].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{    ax2.spines[spine].set\_visible(}\VariableTok{False}\NormalTok{)}

\CommentTok{\# Plot stacked bars of the entropy contributions in ax3}
\NormalTok{bottom }\OperatorTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ i, H }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(Hi):}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ ax3.bar(}\DecValTok{0}\NormalTok{, H, width}\OperatorTok{=}\NormalTok{bar\_width, label}\OperatorTok{=}\NormalTok{df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{][i], bottom}\OperatorTok{=}\NormalTok{bottom)}
\NormalTok{    ax3.text(}\DecValTok{0}\NormalTok{, bottom }\OperatorTok{+}\NormalTok{ H}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{][i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{, ha}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, va}\OperatorTok{=}\StringTok{\textquotesingle{}center\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{    bottom }\OperatorTok{+=}\NormalTok{ H}
\NormalTok{ax3.set\_xlim(}\OperatorTok{{-}}\BuiltInTok{len}\NormalTok{(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{])}\OperatorTok{/}\DecValTok{2}\OperatorTok{*}\FloatTok{0.2}\NormalTok{, }\BuiltInTok{len}\NormalTok{(df[}\StringTok{\textquotesingle{}Letter\textquotesingle{}}\NormalTok{])}\OperatorTok{/}\DecValTok{2}\OperatorTok{*}\FloatTok{0.2}\NormalTok{)}
\NormalTok{ax3.set\_ylim(}\DecValTok{0}\NormalTok{,}\FloatTok{4.5}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ spine }\KeywordTok{in}\NormalTok{ [}\StringTok{\textquotesingle{}left\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{]:}
\NormalTok{    ax3.spines[spine].set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax3.yaxis.tick\_right()}

\NormalTok{total\_H }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(Hi)}
\NormalTok{ax3.axhline(total\_H, color}\OperatorTok{=}\StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{ax3.}\BuiltInTok{set}\NormalTok{(}
\NormalTok{    xticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{    yticks}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,total\_H],}
\NormalTok{    yticklabels}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{, }\SpecialStringTok{f\textquotesingle{}H = }\SpecialCharTok{\{}\NormalTok{total\_H}\SpecialCharTok{:.2f\}}\SpecialStringTok{ bits\textquotesingle{}}\NormalTok{]}
\NormalTok{)}
\NormalTok{ax3.tick\_params(axis}\OperatorTok{=}\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{, which}\OperatorTok{=}\StringTok{\textquotesingle{}major\textquotesingle{}}\NormalTok{, length}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\NormalTok{fig.tight\_layout()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_99634/300921586.py:19: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax0.set_xticklabels(df['Letter'], rotation=0);
/var/folders/cn/m58l7p_j6j10_4c43j4pd8gw0000gq/T/ipykernel_99634/300921586.py:74: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  fig.tight_layout();
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{information_theory/entropy_files/figure-pdf/cell-4-output-2.png}}

The expected information would be 4.14 bits. Because Hebrew has
structure (like any other language), this is less than the maximum
possible information. If all 22 letters were equally likely, the
expected information would be:

\begin{align*}
H_\text{max} &= \sum_{i=1}^{22} \frac{1}{22} \log_2\left(\frac{1}{\frac{1}{22}}\right) \\
             &= \log_2(22) \\
             &\approx 4.46 \text{ bits}
\end{align*}

From this exercise, we learned that in the case that all outcomes are
equally likely, the entropy is simply the logarithm of the number of
possible outcomes. This is a useful fact to remember.

\chapter{cross-entropy and KL
divergence}\label{cross-entropy-and-kl-divergence}

Assume I live in city A, where it rains 50\% of the days. A friend of
mine lives in city B, where it rains 10\% of the days.

\section{wrong model}\label{wrong-model}

What happens when my friend visits me in city A and, not knowing any
better, assumes that it rains 10\% of the days?

We have two probability distributions, the true weather distribution
\(P\), and the assumed weather distribution \(Q\):

\begin{itemize}
\tightlist
\item
  \textbf{True distribution:} P(rain) = 0.5, P(no rain) = 0.5
\item
  \textbf{Assumed distribution:} Q(rain) = 0.1, Q(no rain) = 0.9
\end{itemize}

What will be the expected surprise of my friend, when he visits me in
city A? Now that we have discussed surprise (information) and entropy,
we can calculate the following quantity, called \emph{cross-entropy}:

\[H(P, Q) = - \sum_x P(x) \log Q(x)\]

My friend will evaluate his surprise using the mental model that he has,
i.e., the assumed distribution \(Q\). For example, because he comes from
a dry city, every time it rains he is surprised a lot more than when it
does not rain.

However, since my friend is visiting me in city A, he will actually
experience the weather according to the true distribution \(P\). He will
not weigh the big surprise of rain with the probability of rain in city
B (10\%), but with the probability of rain in city A (50\%).

This reasoning explains the asymetry of the cross-entropy: the first
argument is the true distribution, which determines how often each event
happens, while the second argument is the assumed distribution, which
determines how surprised my friend will be when each event happens.

Let's compute my friend's expected surprise when he visits me in city A:

\begin{align*}
H(P, Q) &= - \sum_x P(x) \log Q(x) \\
        &= - (P(\text{rain}) \log Q(\text{rain}) + P(\text{no rain}) \log Q(\text{no rain})) \\
        &= - (0.5 \log 0.1 + 0.5 \log 0.9) \\
        &= - (0.5 \cdot -1 + 0.5 \cdot -0.045757) \\
        &= 1.74 \text{ bits},
\end{align*}

where we used the base-2 logarithm, so the result is in bits.

To see the asymetry of the cross-entropy, let's compute my expected
surprise when I visit my friend in city B:

\begin{align*}
H(Q, P) &= - \sum_x Q(x) \log P(x) \\
        &= - (Q(\text{rain}) \log P(\text{rain}) + Q(\text{no rain}) \log P(\text{no rain})) \\
        &= - (0.1 \log 0.5 + 0.9 \log 0.5) \\
        &= - (0.1 \cdot -1 + 0.9 \cdot -1) \\
        &= 1 \text{ bits},
\end{align*}

My friend's expected surprise will be higher when he visits me in city A
(1.74 bits) than my expected surprise when I visit him in city B (1
bit).

\section{Kullback-Leibler divergence}\label{kullback-leibler-divergence}

Not all of my friend's surprise is due to the fact that he has an
inaccurate mental model of the weather. Some of his surprise is simply
due to the inherent randomness of the weather. This would be the same
surprise that I myself experience when I live in city A, and I have the
correct mental model of the weather.

It would make sense to separate the surprise that is due to the inherent
randomness of the weather from the surprise that is due to my friend's
wrong mental model. We can do this by subtracting the entropy of the
true distribution \(P\) from the cross-entropy \(H(P, Q)\):

\[D_{KL}(P \| Q) = H(P, Q) - H(P)\]

This quantity is called the Kullback-Leibler divergence, and it measures
the amount of surprise that is only due to my friend's wrong mental
model.

Let's use the properties of logarithms to rewrite the KL divergence in a
more convenient form:

\begin{align*}
D_{KL}(P \| Q) &= H(P, Q) - H(P)\\
                &= - \sum_x P(x) \log Q(x) + \sum_x P(x) \log P(x) \\
                &= \sum_x P(x) (\log P(x) - \log Q(x)) \\
                &= \sum_x P(x) \log \frac{P(x)}{Q(x)}.
\end{align*}

This is the most common form of the KL divergence.

When our model is perfect, i.e., when \(P = Q\), the KL divergence is
zero \((\log(P/P) = 0)\), because there is no extra surprise due to a
wrong mental model. When our model is not perfect, the KL divergence is
always positive, because we are always more surprised when our mental
model is wrong.

\section{model training and objective
functions}\label{model-training-and-objective-functions}

We might want to train a model that classifies photos. We have a dataset
of photos, and for each photo we know the correct label (cat, dog,
elephant, etc.). The goal of the model is to predict the correct label
for each photo.

At every step of the training process, we need to evaluate how well the
model is doing. The true data distribution \(P\) is given by the labels
in the training dataset, while the model's predicted distribution \(Q\)
is given by the model's output. Ideally, our model's predicted
distribution \(Q\) should be as close as possible to the true data
distribution \(P\). That's sounds like a job for the KL divergence!

We will adjust the model's parameters to minimize the KL divergence
between the true data distribution \(P\) and the model's predicted
distribution \(Q\). In practice, we will minimize the cross-entropy
\(H(P, Q)\) instead of the KL divergence \(D_{KL}(P \| Q)\), because the
entropy \(H(P)\) does not depend on the model's parameters, and
therefore does not affect the optimization process. Think about it: no
matter what the model's parameters are, the entropy of the true data
distribution \(P\) will always be the same. So minimizing the KL
divergence is equivalent to minimizing the cross-entropy. We don't care
if they differ by a constant.

\part{foundations}

\chapter{probability and likelihood}\label{probability-and-likelihood}

In the bus stop next to where I live, buses arrive on average every 20
minutes. The timing is not exact, due to traffic, weather, or the number
of passengers getting on and off.

\begin{itemize}
\tightlist
\item
  Q1: What is the probability that I will have to wait more than 30
  minutes for the next bus?
\item
  Q2: Assume now that we don't know the average interval between buses.
  Given that I waited 15 minutes today, what is the most likely average
  interval between buses?
\item
  Q3: This week I waited 10, 25, 30, 2, and 20 minutes for the bus on
  different days. Given this data, what is the most likely average
  interval between buses?
\end{itemize}

Before we go about solving these questions, we note that they require of
us two different things.

\begin{itemize}
\tightlist
\item
  \textbf{Deduction:} In first question we start with a ``State of the
  World'' (the model and its parameters), and use that to predict what
  the data will look like. This is the realm of \textbf{probability}.\\
\item
  \textbf{Induction:} In second and third questions we start with the
  ``Evidence'' (the Data), and use it to work backward to the ``State of
  the World.'' This is the realm of \textbf{likelihood}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{6}\NormalTok{), tight\_layout}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# deduction}
\NormalTok{ax.text(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.7}\NormalTok{,}
        \StringTok{"Hypothesis,}\CharTok{\textbackslash{}n}\StringTok{Model / Parameters"}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        transform}\OperatorTok{=}\NormalTok{ax.transData,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{, pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, fc}\OperatorTok{=}\DecValTok{3}\OperatorTok{*}\NormalTok{[}\FloatTok{0.85}\NormalTok{], ec}\OperatorTok{=}\StringTok{"k"}\NormalTok{))}

\NormalTok{ax.text(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.7}\NormalTok{,}
        \StringTok{"Observations, }\CharTok{\textbackslash{}n}\StringTok{Data"}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        transform}\OperatorTok{=}\NormalTok{ax.transData,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{, pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, fc}\OperatorTok{=}\DecValTok{3}\OperatorTok{*}\NormalTok{[}\FloatTok{0.85}\NormalTok{], ec}\OperatorTok{=}\StringTok{"k"}\NormalTok{))}

\NormalTok{ax.text(}\FloatTok{0.52}\NormalTok{, }\FloatTok{0.7}\NormalTok{,}
        \StringTok{"Deduction"}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        transform}\OperatorTok{=}\NormalTok{ax.transData,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(boxstyle}\OperatorTok{=}\StringTok{"rarrow"}\NormalTok{, fc}\OperatorTok{=}\StringTok{"w"}\NormalTok{, ec}\OperatorTok{=}\StringTok{"k"}\NormalTok{))}

\NormalTok{ax.text(}
    \FloatTok{0.1}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }
    \VerbatimStringTok{r"Given that I have a fair coin }\KeywordTok{(}\DecValTok{$}\VerbatimStringTok{p\_}\CharTok{\textbackslash{}t}\VerbatimStringTok{ext\{heads\}=0}\DecValTok{.}\VerbatimStringTok{5}\DecValTok{$}\KeywordTok{)}\VerbatimStringTok{,"}\OperatorTok{+}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}
    \VerbatimStringTok{r"what is the }\DecValTok{$\textbackslash{}m}\VerbatimStringTok{athbf\{PROBABILITY\}}\DecValTok{$}\VerbatimStringTok{ of observing 8 heads in 10 tosses}\OperatorTok{?}\VerbatimStringTok{"}\NormalTok{, }
\NormalTok{    fontsize}\OperatorTok{=}\DecValTok{12}
\NormalTok{)}

\CommentTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# induction}
\NormalTok{ax.text(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{,}
        \StringTok{"Hypothesis,}\CharTok{\textbackslash{}n}\StringTok{Model / Parameters"}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        transform}\OperatorTok{=}\NormalTok{ax.transData,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{, pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, fc}\OperatorTok{=}\DecValTok{3}\OperatorTok{*}\NormalTok{[}\FloatTok{0.85}\NormalTok{], ec}\OperatorTok{=}\StringTok{"k"}\NormalTok{))}

\NormalTok{ax.text(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.3}\NormalTok{,}
        \StringTok{"Observations, }\CharTok{\textbackslash{}n}\StringTok{Data"}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        transform}\OperatorTok{=}\NormalTok{ax.transData,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(boxstyle}\OperatorTok{=}\StringTok{"round"}\NormalTok{, pad}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, fc}\OperatorTok{=}\DecValTok{3}\OperatorTok{*}\NormalTok{[}\FloatTok{0.85}\NormalTok{], ec}\OperatorTok{=}\StringTok{"k"}\NormalTok{))}

\NormalTok{ax.text(}\FloatTok{0.53}\NormalTok{, }\FloatTok{0.3}\NormalTok{,}
        \StringTok{"Induction"}\NormalTok{,}
\NormalTok{        ha}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        va}\OperatorTok{=}\StringTok{"center"}\NormalTok{,}
\NormalTok{        size}\OperatorTok{=}\DecValTok{14}\NormalTok{,}
\NormalTok{        transform}\OperatorTok{=}\NormalTok{ax.transData,}
\NormalTok{        bbox}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(boxstyle}\OperatorTok{=}\StringTok{"larrow"}\NormalTok{, fc}\OperatorTok{=}\StringTok{"w"}\NormalTok{, ec}\OperatorTok{=}\StringTok{"k"}\NormalTok{))}

\NormalTok{ax.text(}
    \FloatTok{0.1}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }
    \VerbatimStringTok{r"Given that I measured 8 heads in 10 tosses,"}\OperatorTok{+}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}
    \VerbatimStringTok{r"what is the }\DecValTok{$\textbackslash{}m}\VerbatimStringTok{athbf\{LIKELIHOOD\}}\DecValTok{$}\VerbatimStringTok{ that the coin is fair}\OperatorTok{?}\VerbatimStringTok{"}\NormalTok{, }
\NormalTok{    fontsize}\OperatorTok{=}\DecValTok{12}
\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ spine }\KeywordTok{in}\NormalTok{ ax.spines.values():}
\NormalTok{        spine.set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_xticks([])}
\NormalTok{ax.set\_yticks([])}
\NormalTok{ax.patch.set\_visible(}\VariableTok{False}\NormalTok{)}
\NormalTok{ax.set\_frame\_on(}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/probability_and_likelihood_files/figure-pdf/cell-3-output-1.png}}

\section{Q1: deduction and
probability}\label{q1-deduction-and-probability}

\subsection{A slightly different
situation}\label{a-slightly-different-situation}

If we knew that buses arrive exactly every 20 minutes, the question
would be quite easy to answer. The probability of waiting more than 30
minutes would be zero. The only uncertainty would be when we arrive at
the bus stop relative to the bus schedule. We can assume that we arrive
uniformly at random between two bus arrivals, so the probability density
of us waiting \(x\) minutes would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{0}\NormalTok{,}\DecValTok{41}\NormalTok{)}
\NormalTok{pdf }\OperatorTok{=}\NormalTok{ np.where(x }\OperatorTok{\textless{}} \DecValTok{20}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{20}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{ax.plot(x, pdf, drawstyle}\OperatorTok{=}\StringTok{\textquotesingle{}steps{-}post\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax.plot(x, }\DecValTok{0}\OperatorTok{*}\NormalTok{pdf, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{1}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.fill\_between(x, }\DecValTok{0}\NormalTok{, pdf, where}\OperatorTok{=}\NormalTok{((x}\OperatorTok{\textgreater{}=}\DecValTok{10}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (x}\OperatorTok{\textless{}=}\DecValTok{15}\NormalTok{)), color}\OperatorTok{=}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, step}\OperatorTok{=}\StringTok{\textquotesingle{}post\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Area = 0.25\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{\textquotesingle{}Probability Density Function for Bus Wait Time\textquotesingle{}}\NormalTok{,}
\NormalTok{       xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Wait Time (minutes)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{40}\NormalTok{))}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/probability_and_likelihood_files/figure-pdf/cell-4-output-1.png}}

This is a probability density function (pdf), because the probability
that we wait exactly \(x\) minutes is zero. Instead, we can calculate
the probability that we wait between two times, say between 10 and 15
minutes, by calculating the area under the curve between those two
times. The shaded region in the plot above has area
\(5\times0.05=0.25\), meaning that there is a 25\% chance that we will
wait between 10 and 15 minutes. It would be impossible to wait more than
20 minutes because the area under the curve after 20 minutes is zero. Of
course, if we ask what is the probability that we wait for more than
zero minutes, the answer is 1=100\%, because logically there is no other
option. This tells us that the total area under the curve must be equal
to 1, not only for this specific example, but for any valid probability
density function.

\subsection{now the real situation}\label{now-the-real-situation}

Let's contrast the real situation with the simplified one from before:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
1st case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
2nd case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Buses arrive \textbf{exactly} every 20 minutes. & Buses arrive
\textbf{on average} every 20 minutes. \\
Uncertainty only in our arrival time. & Uncertainty in both our arrival
time and bus arrival times. \\
Every minute that passes I feel more certain that the bus is coming
soon. & Every minute that passes, my expected wait time doesn't change!
The fact that I already waited 20 minutes doesn't decrease my expected
time, the universe doesn't ``owe'' me a bus just because I've been
standing there. \\
I can wait at most 20 minutes. & There is not upper bound, I could wait
for a very long time. \\
My expected wait time is 10 minutes. & My expected wait time is 20
minutes. \\
\end{longtable}

I will make the follwing assumption: every minute has the same chance of
seeing a bus arrive. No matter how long I already waited, the chance of
seeing a bus in the next minute is the same. This is called the
\textbf{memoryless property}. It means that the bus arrival process has
no memory of what happened before. This can be a reasonable assumption
for buses arriving randomly, but it would not be reasonable for
something like a train schedule, where if I just missed a train, I know
I will have to wait a long time for the next one.

What is the probability that I will have to wait more than t minutes?

\subsection{full derivation}\label{full-derivation}

This is not strictly necessary to understand the rest of the notebook,
but I'm a nerd and these derivations are fun. If you don't agree just
skip to the final result below.

Let's start by imagining the 20-minute average wait. Instead of looking
at the full 20 minutes, let's slice time into tiny, microscopic
intervals of size \(\Delta t\) (e.g., one millisecond). In each tiny
slice, only two things can happen, either the bus arrives, or it does
not. Since the average wait is 20 minutes, the rate \(\lambda\) is 1/20
buses per minute. In a tiny slice of time \(\Delta t\), the probability
p of a bus arriving is \(p\approx \lambda \Delta t\).

If I am waiting for the bus at time \(t\), it means the bus failed to
show up in every single tiny slice of time from \(0\) up to \(t\). If
each slice is \(\Delta t\), the number of slices in time t is
\(n=t/\Delta t\). The probability of \textbf{not} seeing a bus in one
slice is \((1p)\). Therefore, the probability of not seeing a bus in
\(n\) consecutive slices is:

\[
Pr(T > t) = (1 - p)^n = (1 - \lambda \Delta t)^{t/\Delta t}.
\]

To make this accurate, we want the slices to be ``infinitely small''
\((\Delta t\rightarrow 0)\). As \(\Delta t\) gets smaller, \(n\) gets
larger. Let's rewrite the exponent to use the definition of e: \[
Pr(T > t) = \left[(1 - \lambda \Delta t)^{\frac{1}{\lambda \Delta t}}\right]^{\lambda t}.
\]

Recall the fundamental definition of the exponential constant:

\[
\lim_{x \to 0} (1 - x)^{1/x} = e^{-1}
\]

As \(\Delta t \to 0\), the term inside the brackets becomes \(e^{-1}\).

\[
Pr(T>t)=e^{-\lambda t}
\]

What we just found is the Survival Function (the chance you are still
waiting at time t). To get the Cumulative Distribution Function
(CDF)---the chance the bus has arrived by time t:

\[
F(t) = Pr(T \leq t) = 1 - Pr(T > t) = 1 - e^{-\lambda t}
\]

Finally, the Probability Density Function (PDF) is the rate of change
(derivative) of the CDF:

\begin{align*}
f(t) &= \frac{d}{dt}F(t) \\
&= \frac{d}{dt}\left(1 - e^{-\lambda t}\right) \\
&= \lambda e^{-\lambda t}
\end{align*}

\subsection{the result}\label{the-result}

Let's remember the question:

Q1: What is the probability that I will have to wait more than 30
minutes for the next bus?

Now that we have the probability density function,

\[
f(t) = \lambda e^{-\lambda t}
\]

we can calculate the probability of waiting more than t minutes as the
integral of the PDF from T to infinity:

\begin{align*}
Pr(T > t) &= \int_{t}^{\infty} \lambda e^{-\lambda t} dt\\
&= \left[-e^{-\lambda t}\right]_{t}^{\infty} \\
&= 0 - \left(-e^{-\lambda \cdot t}\right) \\
&= e^{-\lambda \cdot t}
\end{align*}

For our specific case, with an average wait of 20 minutes
(\(\lambda=1/20\)) and \(t=30\) minutes:

\[
Pr(T > 30) = e^{-\frac{1}{20} \cdot 30} = e^{-1.5} \approx 0.22.
\]

There is a 22\% chance that I will have to wait more than 30 minutes for
the next bus. \(\square\)

\section{Q2: induction and
likelihood}\label{q2-induction-and-likelihood}

Assume now that we don't know the average interval between buses. Given
that I waited 15 minutes today, what is the most likely average interval
between buses?

In other words, given a specific observed wait time, what is the most
likely value of \(\lambda\)?

\begin{itemize}
\tightlist
\item
  If \(\lambda\) is \textbf{very} small, this means that the average
  interval between buses is very long, say, one a day. In that case,
  waiting 15 minutes would be quite unlikely, it would be an astonishing
  stroke of luck.
\item
  If \(\lambda\) is \textbf{very} large, this means that the average
  interval between buses is very short, say, one every 10 seconds. In
  that case, waiting 15 minutes would also be quite unlikely, it would
  be an astonishing stroke of bad luck.
\end{itemize}

It is clear from the reasoning above that there is a sweet spot, an
intermediate value of \(\lambda\) that makes waiting 15 minutes most
likely.

The likelihood is written as:

\[
\mathcal{L}(\lambda \mid t).
\]

It should \textbf{not} be read as ``the probability of \(\lambda\) given
t'', because \(\lambda\) is not a random variable, it's a parameter.
Instead, it should be read as ``the likelihood of \(\lambda\) given t'',
meaning how plausible is the value of \(\lambda\) in light of the
observed data t.

For given observed wait time t=15, the likelihood function is equal to
the probability density function evaluated at t=15:

\[
\mathcal{L}(\lambda \mid t=15) = P(t=15 \mid \lambda)
\]

Let's vizualize this. On the left plot below we see the PDF for a few
different values of \(\lambda\). For each curve, I marked the observed
wait time of 15 minutes with a circle. It is hard to see which value of
\(\lambda\) makes waiting 15 minutes most likely just by looking at the
left plot.

On the right plot we see the likelihood function for the observed wait
time of 15 minutes. Clearly there is a peak for the likelihood.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{t }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{,}\DecValTok{40}\NormalTok{, }\DecValTok{101}\NormalTok{)}
\NormalTok{f }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ t, lam: lam }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{lam }\OperatorTok{*}\NormalTok{ t)}
\NormalTok{time }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{60}\NormalTok{])}
\NormalTok{lam }\OperatorTok{=} \DecValTok{1} \OperatorTok{/}\NormalTok{ time}

\NormalTok{color\_min }\OperatorTok{=} \FloatTok{0.4}
\NormalTok{color\_max }\OperatorTok{=} \FloatTok{0.99}
\NormalTok{colors }\OperatorTok{=}\NormalTok{ plt.cm.Blues(np.linspace(color\_min, color\_max, }\BuiltInTok{len}\NormalTok{(lam)))}
\ControlFlowTok{for}\NormalTok{ i,l }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(lam):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot(t, f(t, l), lw}\OperatorTok{=}\DecValTok{1}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}=1/}\SpecialCharTok{\{}\NormalTok{time[i]}\SpecialCharTok{:.0f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\NormalTok{colors[i])}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{].plot([}\DecValTok{15}\NormalTok{], [f(}\DecValTok{15}\NormalTok{, l)], ls}\OperatorTok{=}\VariableTok{None}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\NormalTok{colors[i], markersize}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].legend()}
\CommentTok{\# ax.plot(t, f(t, lam), color=\textquotesingle{}blue\textquotesingle{}, lw=2)}
\CommentTok{\# ax.plot(t, 0*t, color=\textquotesingle{}black\textquotesingle{}, lw=1, ls=\textquotesingle{}{-}{-}\textquotesingle{})}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}Wait Time (minutes)\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Probability Density\textquotesingle{}}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{40}\NormalTok{),}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.12}\NormalTok{))}\OperatorTok{;}

\NormalTok{l\_array }\OperatorTok{=}\NormalTok{ np.linspace(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{t\_obs }\OperatorTok{=} \DecValTok{15}
\NormalTok{L }\OperatorTok{=}\NormalTok{ l\_array }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{l\_array }\OperatorTok{*}\NormalTok{ t\_obs)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(l\_array, L, color}\OperatorTok{=}\NormalTok{colors[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\VerbatimStringTok{r\textquotesingle{}Rate }\KeywordTok{(}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\DecValTok{$}\VerbatimStringTok{, min}\DecValTok{$\^{}}\VerbatimStringTok{\{{-}1\}}\DecValTok{$}\KeywordTok{)}\VerbatimStringTok{\textquotesingle{}}\NormalTok{,}
\NormalTok{       ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}Likelihood\textquotesingle{}}\NormalTok{,}
\NormalTok{       xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.3}\NormalTok{),}
\NormalTok{       ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.03}\NormalTok{)}
\NormalTok{       )}\OperatorTok{;}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.tick\_right()}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.set\_label\_position(}\StringTok{"right"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/probability_and_likelihood_files/figure-pdf/cell-5-output-1.png}}

\subsection{maximum likelihood}\label{maximum-likelihood}

The final step here is obvious. We need to find which lambda maximizes
the likelihood function. This value is called the Maximum Likelihood
Estimate (MLE) of lambda, denoted \(\hat{\lambda}\):

\[
\hat{\lambda} = \underset{\lambda}{\mathrm{argmax}} \ \mathcal{L}(\lambda \mid t=15),
\]

or in English, ``the value of lambda that maximizes the argument of the
likelihood function, given the observation t=15.''

Let's take the derivative of the likelihood function with respect to
lambda, set it to zero, and solve for lambda:

\begin{align*}
\mathcal{L}(\lambda \mid t) &= \lambda e^{-\lambda t} \\
\frac{d}{d\lambda} \mathcal{L}(\lambda \mid t) &= e^{-\lambda t} - \lambda t e^{-\lambda t} \\
0 &= e^{-\lambda t} - \lambda t e^{-\lambda t} \\
\lambda t e^{-\lambda t} &= e^{-\lambda t} \\
\lambda t &= 1 \\
\hat{\lambda} &= \frac{1}{t}
\end{align*}

We found that the MLE of lambda given a single observation t is simply
the reciprocal of t. In our case, with t=15 minutes,
\(\hat{\lambda} = 1/15\) buses per minute, meaning that the most likely
average interval between buses is 15 minutes. \(\square\)

\subsection{likelihood surface}\label{likelihood-surface}

I find it useful to visualize both the probability and likelihood
functions together in a 3D surface plot. When we derived \(f(t)\), we
wrote it as a function of time, and the parameter \(\lambda\) was
assumed to be known. The \textbf{joint probability distribution} is the
same equation, but we now recognize that it is a function of both \(t\)
and \(\lambda\):

\[
f(t, \lambda) = \lambda e^{-\lambda t}.
\]

The joint pdf can be visualized as a surface, representing the
exponential distribution's PDF across a range of wait times (\(t\)) and
rate parameters (\(\lambda\)). The common expression we see when we
learn about probability and likelihood is: \[
\mathcal{L}(\theta \mid x) = P(x \mid \theta),
\] where \(x\) is the data and \(\theta\) is the parameter (wait time
\(t\) and rate \(\lambda\) in our example). At first this seemed
baffling to me to define the likelihood function using the same formula
as the probability density function. What's the point of having two
names for the same formula? I now picture the probability and likelihood
functions as slices (projections) of the same surface. Keeping the
parameters fixed and varying the data gives us the probability function.
Keeping the data fixed and varying the parameters gives us the
likelihood function.

This also helps me understand why the likelihood function is not a valid
probability distribution over the parameters. For a fixed data point,
the area under the likelihood curve over all possible parameter values
does not equal 1. Instead, it can take any value, depending on the
observed data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define ranges}
\NormalTok{t }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{lmbda }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{1}\OperatorTok{/}\DecValTok{120}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{T, L }\OperatorTok{=}\NormalTok{ np.meshgrid(t, lmbda)}

\CommentTok{\# Exponential PDF: f(t; lambda) = lambda * exp({-}lambda * t)}
\NormalTok{joint\_pdf }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ T, L: L }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{L }\OperatorTok{*}\NormalTok{ T)}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ joint\_pdf(T, L)}

\CommentTok{\# 1. 3D Surface Plot}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{), constrained\_layout}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\CommentTok{\# fig.subplots\_adjust(right=0.80)}
\NormalTok{surf }\OperatorTok{=}\NormalTok{ ax.plot\_surface(T, L, Z, color}\OperatorTok{=}\StringTok{\textquotesingle{}lightgray\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{, rcount}\OperatorTok{=}\DecValTok{20}\NormalTok{, ccount}\OperatorTok{=}\DecValTok{20}\NormalTok{)}

\CommentTok{\# Highlight slices}
\NormalTok{fixed\_lambda }\OperatorTok{=} \DecValTok{1}\OperatorTok{/}\DecValTok{15}
\NormalTok{fixed\_t }\OperatorTok{=} \DecValTok{15}

\CommentTok{\# Probability slice (fixed lambda, varying t)}
\NormalTok{t\_slice }\OperatorTok{=}\NormalTok{ t}
\NormalTok{z\_prob }\OperatorTok{=}\NormalTok{ joint\_pdf(t\_slice, fixed\_lambda)}
\NormalTok{ax.plot(t\_slice, [fixed\_lambda]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(t\_slice), z\_prob, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{3}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Probability Slice $f(t|}\ErrorTok{\textbackslash{}}\SpecialStringTok{lambda=1/15)$\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ lbd\_i }\KeywordTok{in}\NormalTok{ [}\DecValTok{1}\OperatorTok{/}\DecValTok{5}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{7}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{10}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{20}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{30}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{45}\NormalTok{]:}
\NormalTok{    zi }\OperatorTok{=}\NormalTok{ joint\_pdf(t\_slice, lbd\_i)}
\NormalTok{    ax.plot(t\_slice, [lbd\_i]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(t\_slice), [zi], color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Likelihood slice (fixed t, varying lambda)}
\NormalTok{l\_slice }\OperatorTok{=}\NormalTok{ lmbda}
\NormalTok{z\_lik }\OperatorTok{=}\NormalTok{ joint\_pdf([fixed\_t]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(l\_slice), l\_slice)}
\NormalTok{ax.plot([fixed\_t]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(l\_slice), l\_slice, z\_lik, color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{3}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}Likelihood Slice $L(}\ErrorTok{\textbackslash{}}\SpecialStringTok{lambda|t=}\SpecialCharTok{\{}\NormalTok{fixed\_t}\SpecialCharTok{\}}\SpecialStringTok{)$\textquotesingle{}}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ ti }\KeywordTok{in}\NormalTok{ [}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{45}\NormalTok{]:}
\NormalTok{    zi }\OperatorTok{=}\NormalTok{ joint\_pdf([ti]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(l\_slice), l\_slice)}
\NormalTok{    ax.plot([ti]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(l\_slice), l\_slice, [zi], color}\OperatorTok{=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}


\NormalTok{ax.set\_xlabel(}\StringTok{\textquotesingle{}Wait Time ($t$)\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}Rate ($}\ErrorTok{\textbackslash{}}\StringTok{lambda$)\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{\textquotesingle{}Joint PDF\textquotesingle{}}\NormalTok{, labelpad}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{ax.legend()}
\NormalTok{ax.view\_init(elev}\OperatorTok{=}\DecValTok{15}\NormalTok{, azim}\OperatorTok{={-}}\DecValTok{20}\NormalTok{)}
\CommentTok{\# fig.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/probability_and_likelihood_files/figure-pdf/cell-6-output-1.png}}

\section{Q3: likelihood with multiple
observations}\label{q3-likelihood-with-multiple-observations}

This week I waited 10, 25, 30, 2, and 20 minutes for the bus on
different days. Given this data, what is the most likely average
interval between buses?

We will now make an important assumption: the wait times on different
days are independent of each other. This means that the likelihood of
observing all the wait times is the product of the individual
likelihoods:

\[
\mathcal{L}(\lambda \mid t_1, t_2, \ldots, t_n) = \prod_{i=1}^{n} \mathcal{L}(\lambda \mid t_i) = \prod_{i=1}^{n} P( t_i \mid \lambda).
\]

Given

\[
P(t) = \lambda e^{-\lambda t}
\]

we have

\begin{align*}
\mathcal{L}(\lambda \mid t_1, t_2, \ldots, t_n) &= \prod_{i=1}^{n} \lambda e^{-\lambda t_i} \\
&= \lambda^n e^{-\lambda \sum_{i=1}^{n} t_i}
\end{align*}

Let's take the derivative of the likelihood function with respect to
lambda, set it to zero, and solve for lambda:

\begin{align*}
\mathcal{L}(\lambda \mid t_1, t_2, \ldots, t_n) &= \lambda^n e^{-\lambda \sum_{i=1}^{n} t_i} \\
\frac{d}{d\lambda} \mathcal{L}(\lambda \mid t_1, t_2, \ldots, t_n) &= n \lambda^{n-1} e^{-\lambda \sum t_i} - \lambda^n \left(\sum t_i\right) e^{-\lambda \sum t_i} \\
0 &= n \lambda^{n-1} e^{-\lambda \sum t_i} - \lambda^n \left(\sum t_i\right) e^{-\lambda \sum t_i} \\
\lambda^n \left(\sum t_i\right) e^{-\lambda \sum t_i} &= n \lambda^{n-1} e^{-\lambda \sum t_i} \\
\lambda \left(\sum t_i\right) &= n \\
\hat{\lambda} &= \frac{n}{\sum_{i=1}^{n} t_i} \\
\hat{\lambda} &= \frac{1}{\frac{\sum_{i=1}^{n} t_i}{n}} \\
\hat{\lambda} &= \frac{1}{\bar{t}},
\end{align*}

where \(\bar{t}\) is the sample mean of the observed wait times. Because
the average of 10, 25, 30, 2, and 20 minutes is 17.4 minutes, the MLE of
lambda is approximately 1/17.4 buses per minute, quite close to the
value 1/20 in the first problem. \(\square\)

The result is (perhaps) surprisingly simple, certainly compared with the
full derivation above. There is something deep to be said about the fact
that the formula for the mean naturally emerges from the principle of
maximum likelihood. This, and much more, is the subject of the next
chapter.

\chapter{maximum likelihood
estimation}\label{maximum-likelihood-estimation}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/always-has-been.png}}

The one idea that permeates all of statistics and machine learning is
that of maximum likelihood estimation. I cannot overstate its
centrality.

In the chapter on
\href{./foundations/probability_and_likelihood.ipynb}{probability and
likelihood}, we solved a practical example of estimating the rate
parameter of an exponential distribution using maximum likelihood
estimation. For the case where we observed many samples, we found that
the formula for the mean naturally emerged as the maximum likelihood
estimator for the rate parameter. We will dig deeper into this idea, and
show deep connections between maximum likelihood estimation and many
other statistical concepts.

\section{log likelihood}\label{log-likelihood-1}

We already saw the formula for the likelihood for multiple independent
observations:

\[
L(\lambda \mid t_1, t_2, \ldots, t_n) = \prod_{i=1}^{n} L(\lambda \mid t_i),
\]

We justified the product form by assuming that the observations are
``iid'', meaning independent and identically distributed. Instead of
working with the product form, it is often more convenient to work with
the log likelihood, which converts the product into a sum:

\[
\ell(\theta \mid x_{1:n}) = \log L(\theta \mid x_{1:n}) = \sum_{i=1}^{n} \log L(\theta \mid x_i) = \sum_{i=1}^{n} \log P( x_i \mid \theta).
\]

Here we changed the notation a bit to use \(\theta\) for the parameters
(which could be a vector) and \(x\) for the data vector, which is more
common in statistics. There are a few reasons why we prefer to work with
the log likelihood instead of the likelihood itself:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Numerical Stability (Underflow):} In
  \href{./foundations/probability_and_likelihood.html\#q3-likelihood-with-multiple-observations}{likelihood
  with multiple observations}, we saw that likelihoods are products:
  \(\prod P(x_i)\). If you have 100 observations, you are multiplying
  100 small numbers, which can lead to numerical underflow where a
  computer rounds the result to zero. Summing logs, \(\sum \ln P(x_i)\),
  keeps the numbers in a range computers can handle.
\item
  \textbf{Mathematical Elegance:} Many common distributions (like the
  Exponential or Normal) use \(e\), Euler's number. The natural log
  cancels the exponent, turning products of complex terms into simple
  sums that are much easier to differentiate.
\item
  \textbf{Preservation of the Maximum:} Because ln(x) is a monotonically
  increasing function, the value of \(\theta\) that maximizes the
  log-likelihood is identical to the value that maximizes the
  likelihood.
\item
  \textbf{Optimization Surface:} Taking the log reshapes the likelihood
  surface. It stretches out the extremely steep slopes near the peak,
  creating a well-behaved ``hill'' that optimization algorithms can
  navigate reliably without the fluctuations caused by nearly-vertical
  gradients.
\end{enumerate}

In the following chapters, we will see the deep connections between
maximum likelihood estimation and many fundamental concepts in
statistics and machine learning.

\chapter{MLE and summary statistics}\label{mle-and-summary-statistics}

Let's do the following exercise. Suppose we have \(n\) iid observations
\(x\) from a model with parameters \(\theta\). Using the definition of
the log likelihood, derive the maximum likelihood estimator for
\(\theta\).

\section{exponential distribution
revisited}\label{exponential-distribution-revisited}

We can now revisit our original example of estimating the rate parameter
\(\lambda\) of an exponential distribution using maximum likelihood
estimation. Recall that the joint probability density function of the
exponential distribution is given by: \[
f(x, \lambda) = \lambda e^{-\lambda x}.
\] The log likelihood reads:

\begin{align*}
\ell(\lambda \mid x_{1:n}) &= \sum_{i=1}^{n} \log f(x_i, \lambda) \\
&= \sum_{i=1}^{n} \left( \log \lambda - \lambda x_i \right) \\
&= n \log \lambda - \lambda \sum_{i=1}^{n} x_i.
\end{align*} To find the maximum likelihood estimator for \(\lambda\),
we take the derivative of the log likelihood with respect to \(\lambda\)
and set it to zero:

\begin{align*}
\frac{d\ell}{d\lambda} &= \frac{n}{\lambda} - \sum_{i=1}^{n} x_i = 0.
\end{align*}

Solving for \(\lambda\) gives: \[
\hat{\lambda} = \frac{n}{\sum_{i=1}^{n} x_i} = \frac{1}{\bar{x}},
\] where \(\bar{x}\) is the sample mean of the observed data. This
result aligns with our previous derivation using the likelihood function
directly.

Let's plot the log likelihood surface (the joint probability
distribution), and we'll see how the four reasons for using the log
likelihood come into play.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ expon, uniform, norm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pdf\_expon }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ x, lambd: lambd }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{lambd }\OperatorTok{*}\NormalTok{ x) }\OperatorTok{*}\NormalTok{ (x }\OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{)}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{6}\NormalTok{), sharex}\OperatorTok{=}\StringTok{\textquotesingle{}col\textquotesingle{}}\NormalTok{)}
\NormalTok{fig.subplots\_adjust(wspace}\OperatorTok{=}\FloatTok{0.4}\NormalTok{, hspace}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{].set\_axis\_off()}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{lmbda }\OperatorTok{=}\NormalTok{ np.linspace(}\FloatTok{1e{-}5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1000}\NormalTok{)}

\NormalTok{ideal\_lambda }\OperatorTok{=} \FloatTok{0.7}
\NormalTok{np.random.seed(seed}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ expon.rvs(size}\OperatorTok{=}\DecValTok{100}\NormalTok{, scale}\OperatorTok{=}\DecValTok{1}\OperatorTok{/}\NormalTok{ideal\_lambda)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{].plot(x, pdf\_expon(x, ideal\_lambda))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{].plot(data, np.zeros\_like(data), ls}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}|\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{, ylabel}\OperatorTok{=}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{)}

\NormalTok{LAM, X }\OperatorTok{=}\NormalTok{ np.meshgrid(lmbda, x)}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ pdf\_expon(X, LAM)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{].contourf(LAM, X, Z, levels}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{].contourf(LAM, X, np.log(Z), levels}\OperatorTok{=}\DecValTok{10}\NormalTok{)}

\NormalTok{like }\OperatorTok{=}\NormalTok{ [np.prod(pdf\_expon(data, l)) }\ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in}\NormalTok{ lmbda]}
\NormalTok{log\_like }\OperatorTok{=}\NormalTok{ [np.}\BuiltInTok{sum}\NormalTok{(np.log(pdf\_expon(data, l))) }\ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in}\NormalTok{ lmbda]}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{].plot(lmbda, like)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{].plot(lmbda, log\_like)}

\ControlFlowTok{for}\NormalTok{ i, xx }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(data):}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{].axhline(xx, color}\OperatorTok{=}\StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{    ax[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{].axhline(xx, color}\OperatorTok{=}\StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{ideal\_lambda}\SpecialCharTok{\}}\SpecialStringTok{ was the ideal lambda used to generate the data."}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\DecValTok{1}\OperatorTok{/}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(data)}\SpecialCharTok{:.3f\}}\SpecialStringTok{ is the MLE estimate of lambda from the data."}\NormalTok{)}


\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{),}
\NormalTok{            xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{),}
\NormalTok{            title}\OperatorTok{=}\StringTok{"joint pdf"}\NormalTok{,}
\NormalTok{            xlabel}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{            )}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{x}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{),}
\NormalTok{            xlim}\OperatorTok{=}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{),}
\NormalTok{            title}\OperatorTok{=}\StringTok{"joint pdf, log scale"}\NormalTok{,}
\NormalTok{            xlabel}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{            )}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{x}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{"likelihood"}\NormalTok{,}
\NormalTok{             xlabel}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{             ylabel}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{prod P}\KeywordTok{(}\VerbatimStringTok{x}\DecValTok{\textbackslash{}m}\VerbatimStringTok{id}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{].yaxis.tick\_right()}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{].yaxis.set\_label\_position(}\StringTok{"right"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(title}\OperatorTok{=}\StringTok{"log likelihood"}\NormalTok{,}
\NormalTok{             xlabel}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{,}
\NormalTok{             ylabel}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$\textbackslash{}s}\VerbatimStringTok{um }\ErrorTok{\textbackslash{}}\VerbatimStringTok{ln P}\KeywordTok{(}\VerbatimStringTok{x}\DecValTok{\textbackslash{}m}\VerbatimStringTok{id}\ErrorTok{\textbackslash{}}\VerbatimStringTok{lambda}\KeywordTok{)}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{].text(}\FloatTok{0.97}\NormalTok{, }\FloatTok{0.97}\NormalTok{, }\VerbatimStringTok{r"a"}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{].transAxes,}
\NormalTok{         horizontalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{,}
\NormalTok{         fontweight}\OperatorTok{=}\StringTok{"bold"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{].text(}\FloatTok{0.97}\NormalTok{, }\FloatTok{0.97}\NormalTok{, }\VerbatimStringTok{r"b"}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{].transAxes,}
\NormalTok{         horizontalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{,}
\NormalTok{         fontweight}\OperatorTok{=}\StringTok{"bold"}\NormalTok{, color}\OperatorTok{=}\StringTok{"white"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{].text(}\FloatTok{0.97}\NormalTok{, }\FloatTok{0.97}\NormalTok{, }\VerbatimStringTok{r"c"}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{].transAxes,}
\NormalTok{         horizontalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{,}
\NormalTok{         fontweight}\OperatorTok{=}\StringTok{"bold"}\NormalTok{, color}\OperatorTok{=}\StringTok{"white"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{].text(}\FloatTok{0.97}\NormalTok{, }\FloatTok{0.97}\NormalTok{, }\VerbatimStringTok{r"d"}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{].transAxes,}
\NormalTok{         horizontalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{,}
\NormalTok{         fontweight}\OperatorTok{=}\StringTok{"bold"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{].text(}\FloatTok{0.97}\NormalTok{, }\FloatTok{0.97}\NormalTok{, }\VerbatimStringTok{r"e"}\NormalTok{, transform}\OperatorTok{=}\NormalTok{ax[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{].transAxes,}
\NormalTok{         horizontalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}right\textquotesingle{}}\NormalTok{, verticalalignment}\OperatorTok{=}\StringTok{\textquotesingle{}top\textquotesingle{}}\NormalTok{,}
\NormalTok{         fontweight}\OperatorTok{=}\StringTok{"bold"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7 was the ideal lambda used to generate the data.
0.738 is the MLE estimate of lambda from the data.
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/MLE_and_summary_statistics_files/figure-pdf/cell-3-output-2.png}}

\begin{itemize}
\tightlist
\item
  \textbf{Panel a:} graph of the exponential PDF for \(\lambda=0.7\). I
  drew 100 random samples from this distribution, shown as small
  vertical red lines.''
\item
  \textbf{Panel b:} joint probability distribution in the space of \(x\)
  and \(\lambda\). The horizontal white lines correspond to the observed
  data points. The Brighter colors represent higher values.
\item
  \textbf{Panel c:} log joint probability distribution in the space of
  \(x\) and \(\lambda\). Note how it is much easier to see the structure
  of the surface.
\item
  \textbf{Panel d:} likelihood as a function of \(\lambda\) for the
  observed data. How to read this: for each value of \(\lambda\) on the
  x-axis, look up to see where it intersects each of the horizontal
  white lines in panel b, read off the corresponding probability density
  values, and multiply them together to get the likelihood value at that
  \(\lambda\). Note the following:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The curve is extremely steep near the peak.
  \item
    The likelihood values are very small.
  \end{enumerate}
\item
  \textbf{Panel e:} log likelihood as a function of \(\lambda\) for the
  observed data. Note the following:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The curve is much smoother.
  \item
    The log likelihood values are in a ``manageable'' range.
  \item
    The maximum occurs at the same \(\lambda\) value as in panel d.
  \end{enumerate}
\end{itemize}

Because this problem has only one parameter, all we have to do to find
the MLE is to find the peak of the curve in panel e. We can do this
analytically by taking the derivative of the log likelihood function we
derived above, or numerically using optimization methods. For 100 data
points drawn from an exponential distribution with \(\lambda=0.7\), the
MLE estimate of \(\lambda\) is approximately 0.0.738, which is close to
the true value.

The same ideas shown here apply to all the examples below. For models
with more than one parameter, the log likelihood surface becomes a
multi-dimensional surface, and we cannot visualize it easily. However,
the principles remain the same: we seek the set of parameters
\(\{\theta_1, \theta_2, \ldots, \theta_k\}\) that maximize the log
likelihood.

\section{normal distribution}\label{normal-distribution}

The normal distribution is defined by two parameters: the mean \(\mu\)
and the standard deviation \(\sigma\). As an example, consider measuring
the heights of a group of people. We can assume that the heights are
normally distributed with some unknown mean and standard deviation.

Its joint probability density function is given by: \[
f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right).
\]

The log likelihood reads:

\begin{align*}
\ell(\mu, \sigma \mid x_{1:n}) &= \sum_{i=1}^{n} \log f(x_i; \mu, \sigma) \\
&= \sum_{i=1}^{n} \log \left( \frac{1}{\sigma \sqrt{2\pi}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) \right) \\
&= \sum_{i=1}^{n} \left( -\log(\sigma \sqrt{2\pi}) - \frac{(x_i - \mu)^2}{2\sigma^2} \right) \\
&= -n \log(\sigma) -n \log(\sqrt{2\pi}) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2.
\end{align*}

Maximizing the log likelihood with respect to \(\mu\) and \(\sigma\)
involves taking partial derivatives, setting them to zero, and solving
for the parameters.

For \(\mu\), we have:

\begin{align*}
\frac{\partial \ell}{\partial \mu} &= \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i - \mu) \\
&= \frac{1}{\sigma^2} \left( \sum_{i=1}^{n} x_i - n\mu \right) = 0.
\end{align*}

Solving for \(\mu\) gives: \[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i,
\] which is the sample mean.

Now, for \(\sigma\), we have:

\begin{align*}
\frac{\partial \ell}{\partial \sigma} &= -\frac{n}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{n} (x_i - \mu)^2 \\
&= \frac{1}{\sigma^3} \left( -n\sigma^2 + \sum_{i=1}^{n} (x_i - \mu)^2 \right) = 0.
\end{align*}

Solving for \(\sigma\) gives: \[
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{\mu})^2,
\] which is the sample variance (using \(n\) in the denominator for
MLE).

Going back to the height example, our MLEs for the mean and standard
deviation of heights are simply the sample mean and sample standard
deviation of the observed heights.

\section{binomial distribution}\label{binomial-distribution}

The binomial distribution models the number of successes in \(n\)
independent Bernoulli trials, each with success probability \(p\). A
very common example is flipping a biased coin, whose probability of
yielding heads is \(p\), flipping it \(n\) times and counting the number
of heads \(k\). Its joint probability mass function is given by: \[
P(X = k; n, p) = \binom{n}{k} p^k (1 - p)^{n - k}.
\] The log likelihood reads:

\begin{align*}
\ell(p \mid k, n) &= \log P(X = k; n, p) \\
&= \log \left( \binom{n}{k} p^k (1 - p)^{n - k} \right) \\
&= \log \binom{n}{k} + k \log p + (n - k) \log (1 - p).
\end{align*}

To find the maximum likelihood estimator for \(p\), we take the
derivative of the log likelihood with respect to \(p\) and set it to
zero:

\begin{align*}
\frac{d\ell}{dp} &= \frac{k}{p} - \frac{n - k}{1 - p} = 0.
\end{align*}

Solving for \(p\) gives: \[
\hat{p} = \frac{k}{n},
\] which is the sample proportion of successes. Going back to the coin
example, if we flip the coin 100 times and get 60 heads, our MLE for
\(p\) would be \(\hat{p} = 60/100 = 0.6\).

\section{Laplace distribution}\label{laplace-distribution}

The Laplace distribution, also known as the double exponential
distribution, is characterized by its location parameter \(\mu\) and
scale parameter \(b\). We use the normal distribution for `well-behaved'
data like human heights. We use the Laplace distribution for `wild' data
like stock market data, where outliers aren't just mistakes---they are
part of the system. For example, most days the price of a stock barely
moves, but rare events (market crashes or surges) happen much more often
than a Normal distribution would allow.

Its joint probability density function is given by: \[
f(x; \mu, b) = \frac{1}{2b} \exp\left(-\frac{|x - \mu|}{b}\right).
\]

The log likelihood reads:

\begin{align*}
\ell(\mu, b \mid x_{1:n}) &= \sum_{i=1}^{n} \log f(x_i; \mu, b) \\
&= \sum_{i=1}^{n} \log \left( \frac{1}{2b} \exp\left(-\frac{|x_i - \mu|}{b}\right) \right) \\
&= \sum_{i=1}^{n} \left( -\log(2b) - \frac{|x_i - \mu|}{b} \right) \\
&= -n \log(2b) - \frac{1}{b} \sum_{i=1}^{n} |x_i - \mu|.
\end{align*}

Maximizing the log likelihood with respect to \(\mu\) and \(b\) involves
taking partial derivatives, setting them to zero, and solving for the
parameters.

For \(\mu\), we have:

\begin{align*}
\frac{\partial \ell}{\partial \mu} &= \frac{1}{b} \sum_{i=1}^{n} \text{sgn}(x_i - \mu) = 0,
\end{align*} where \(\text{sgn}(x)\) is the sign function, which returns
-1 for negative values, 1 for positive values, and 0 for zero.

Solving for \(\mu\) is essentially a balancing act. It requires us
finding a value for \(\mu\) such that the number of positive ones
(points above \(\mu\)) exactly cancels out the number of negative ones
(points below \(\mu\)). This happens when \(\mu\) is the median of the
data: \[
\hat{\mu} = \text{median}(x_{1:n}).
\]

Now, for \(b\), we have:

\begin{align*}
\frac{\partial \ell}{\partial b} &= -\frac{n}{b} + \frac{1}{b^2} \sum_{i=1}^{n} |x_i - \mu| = 0.
\end{align*}

Solving for \(b\) gives: \[
\hat{b} = \frac{1}{n} \sum_{i=1}^{n} |x_i - \hat{\mu}|,
\] which is the mean absolute deviation (MAD) from the median. This
measure of spread is more robust to outliers than the standard
deviation.

Going back to the stock market example, our MLEs for the location and
scale parameters are simply the sample median and mean absolute
deviation from the median of the observed returns.

\section{Poisson distribution}\label{poisson-distribution-1}

The Poisson distribution models the number of events occurring in a
fixed interval of time or space, given a known average rate of
occurrence \(\lambda\). A common example is counting the number of
emails received in an hour, or the number of radioactive decays detected
in a piece of radioactive material over a certain period. Its joint
probability mass function is given by: \[
P(X = k, \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}.
\] The log likelihood reads:

\begin{align*}
\ell(\lambda \mid k_{1:n}) &= \sum_{i=1}^{n} \log P(X = k_i, \lambda) \\
&= \sum_{i=1}^{n} \left( k_i \log \lambda - \lambda - \log(k_i!) \right) \\
&= \left(\sum_{i=1}^{n} k_i \right) \log \lambda - n \lambda - \sum_{i=1}^{n} \log(k_i!).
\end{align*}

To find the maximum likelihood estimator for \(\lambda\), we take the
derivative of the log likelihood with respect to \(\lambda\) and set it
to zero:

\begin{align*}
\frac{d\ell}{d\lambda} &= \frac{\sum_{i=1}^{n} k_i}{\lambda} - n = 0.
\end{align*}

Solving for \(\lambda\) gives: \[
\hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} k_i,
\] which is the sample mean of the observed counts.

\section{uniform distribution}\label{uniform-distribution}

The uniform distribution models a situation where all outcomes in a
given range are equally likely. A common example is rolling a fair die,
where each face (1 through 6) has an equal probability of landing face
up. Its joint probability density function is given by: \[
f(x; a, b) = \frac{1}{b - a} \text{ for } a \leq x \leq b.
\]

The log likelihood reads:

\begin{align*}
\ell(a, b \mid x_{1:n}) &= \sum_{i=1}^{n} \log f(x_i; a, b) \\
&= \sum_{i=1}^{n} \log \left( \frac{1}{b - a} \right) \\
&= -n \log(b - a).
\end{align*}

In this instance, it is much easier to find the maximum likelihood
estimators for \(a\) and \(b\) using the probability density function
directly, rather than taking derivatives of the log likelihood. The
likelihood reads: \[
L(a, b \mid x_{1:n}) = \left( \frac{1}{b - a} \right)^n.
\]

The likelihood doesn't have a peak in the usual sense; it increases as
the interval \([a, b]\) narrows. In order to maximize the likelihood,
the interval must be as small as possible while still containing all
observed data points. If at least one data point falls outside the
interval, the likelihood becomes zero, because the uniform distribution
assigns zero probability to values outside \([a, b]\), and the product
of probabilities will include a zero term. The smallest interval that
contains all observed data points is defined by the minimum and maximum
of the data. Therefore, \[
\hat{a} = \min(x_{1:n}), \quad \hat{b} = \max(x_{1:n}).
\]

\section{summary}\label{summary}

By appliying the reasoning of maximum likelihood estimation to various
probability distributions, we have derived the maximum likelihood
estimators for their parameters. Notably, we found that:

\begin{itemize}
\tightlist
\item
  For the \textbf{exponential} distribution, the MLE is the reciprocal
  of the sample mean.
\item
  For the \textbf{normal} distribution, the MLEs are the sample mean and
  sample standard deviation.
\item
  For the \textbf{binomial} distribution, the MLE is the sample
  proportion of successes.
\item
  For the \textbf{Laplace} distribution, the MLEs are the sample median
  and mean absolute deviation from the median.
\item
  For the \textbf{Poisson} distribution, the MLE is the sample mean of
  the observed counts.
\item
  For the \textbf{uniform} distribution, the MLEs are the sample minimum
  and sample maximum.
\end{itemize}

All these estimators are intuitive and align with common summary
statistics used in data analysis, highlighting the deep connection
between maximum likelihood estimation and descriptive statistics.

\chapter{MLE and linear regression}\label{mle-and-linear-regression}

The equation for linear regression is given by:

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + \epsilon,
\] where \(\beta_0\) is the intercept,
\(\beta_1, \beta_2, \ldots, \beta_k\) are the coefficients for the
predictor variables \(x_1, x_2, \ldots, x_k\), and \(\epsilon\) is the
error term.

Reminder: the regression is called linear because it is linear in the
parameters \(\beta_i\), not necessarily in the predictor variables
\(x_i\). For example, a model like
\(y = \beta_0 + \beta_1 x + \beta_2 x^2 + \epsilon\) is still considered
linear regression because it is linear in \(\beta_0, \beta_1,\) and
\(\beta_2\). If you are still not convinced, consider that you can
always create new predictor variables that are transformations of the
original ones (e.g., \(x_2 = x^2\)) and include them in a linear
regression model.

The \(\epsilon\) term is typically assumed to be normally distributed
with mean 0 and constant variance \(\sigma^2\). \textbf{This assumption
is key!} It allows us to use maximum likelihood estimation to estimate
the parameters \(\beta_0, \beta_1, \ldots, \beta_k\). Let's see how.

First, we solve the equation above for \(\epsilon\):

\[
\epsilon = y - (\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k).
\]

See the figure below for a practical example.

\begin{itemize}
\tightlist
\item
  \textbf{left panel:} scatter plot of data points (blue dots) and the
  underlying model (parabola) used as the basis (red line).
\item
  \textbf{middle panel:} scatter plot for epsilon, which is, of course,
  just the vertical distance from each data point to the red line in the
  left panel. This distance is the error term, also called the residual.
\item
  \textbf{right panel:} the probability density function from which the
  epsilon values were drawn (normal distribution with mean 0 and
  standard deviation 5).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ expon, uniform, norm}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{3}\NormalTok{), gridspec\_kw}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(width\_ratios}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{]))}
\NormalTok{fig.subplots\_adjust(wspace}\OperatorTok{=}\FloatTok{0.4}\NormalTok{)}
\NormalTok{beta0 }\OperatorTok{=} \DecValTok{2}
\NormalTok{beta1 }\OperatorTok{=} \DecValTok{13}
\NormalTok{beta2 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{1}

\NormalTok{np.random.seed(seed}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ uniform.rvs(size}\OperatorTok{=}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \DecValTok{15}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.sort(x)}
\NormalTok{epsilon }\OperatorTok{=}\NormalTok{ norm.rvs(size}\OperatorTok{=}\DecValTok{100}\NormalTok{, scale}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ beta0 }\OperatorTok{+}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ x }\OperatorTok{+}\NormalTok{ beta2 }\OperatorTok{*}\NormalTok{ x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+}\NormalTok{ epsilon}

\NormalTok{ax[}\DecValTok{0}\NormalTok{].scatter(x, y, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, label}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$}\VerbatimStringTok{y=}\DecValTok{\textbackslash{}b}\VerbatimStringTok{eta\_0 }\OperatorTok{+}\VerbatimStringTok{ }\DecValTok{\textbackslash{}b}\VerbatimStringTok{eta\_1 x }\OperatorTok{+}\VerbatimStringTok{ }\DecValTok{\textbackslash{}b}\VerbatimStringTok{eta\_2 x}\DecValTok{\^{}}\VerbatimStringTok{2x }\OperatorTok{+}\VerbatimStringTok{ }\ErrorTok{\textbackslash{}}\VerbatimStringTok{epsilon}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].plot(x, beta0 }\OperatorTok{+}\NormalTok{ beta1 }\OperatorTok{*}\NormalTok{ x }\OperatorTok{+}\NormalTok{ beta2 }\OperatorTok{*}\NormalTok{ x}\OperatorTok{**}\DecValTok{2}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{, label}\OperatorTok{=}\VerbatimStringTok{r"}\DecValTok{$\textbackslash{}b}\VerbatimStringTok{eta\_0 }\OperatorTok{+}\VerbatimStringTok{ }\DecValTok{\textbackslash{}b}\VerbatimStringTok{eta\_1 x}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"x"}\NormalTok{,}
\NormalTok{          ylim}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{30}\NormalTok{, }\DecValTok{90}\NormalTok{),}
\NormalTok{          )}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].legend(frameon}\OperatorTok{=}\VariableTok{False}\NormalTok{, loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].scatter(x, epsilon, alpha}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].axhline(}\DecValTok{0}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, ls}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{"x"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"}\DecValTok{$}\ErrorTok{\textbackslash{}}\VerbatimStringTok{epsilon}\DecValTok{$}\VerbatimStringTok{"}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.set\_label\_position(}\StringTok{"right"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].yaxis.tick\_right()}

\NormalTok{eps\_vec }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{12}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].plot(norm.pdf(eps\_vec, loc}\OperatorTok{=}\DecValTok{0}\NormalTok{, scale}\OperatorTok{=}\DecValTok{5}\NormalTok{), eps\_vec)}
\NormalTok{ax[}\DecValTok{2}\NormalTok{].}\BuiltInTok{set}\NormalTok{(ylim}\OperatorTok{=}\NormalTok{(}\OperatorTok{{-}}\DecValTok{12}\NormalTok{, }\DecValTok{12}\NormalTok{),}
\NormalTok{          xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}pdf\textquotesingle{}}\NormalTok{,}
\NormalTok{          xticks}\OperatorTok{=}\NormalTok{[],}
\NormalTok{          yticks}\OperatorTok{=}\NormalTok{[]}
\NormalTok{          )}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{foundations/MLE_and_linear_regression_files/figure-pdf/cell-3-output-1.png}}

We can now ``forget'' about the \(y\) and \(x\) variables for a moment,
and model the error term \(\epsilon\) directly. Since we assumed that
\(\epsilon\) is normally distributed with mean 0 and variance
\(\sigma^2\), we can write the following joint probability density
function for \(\epsilon\):

\[
f(\epsilon \mid \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{\epsilon^2}{2\sigma^2}\right).
\]

Using this expression for the joint pdf in the log likelihood function
yields: \[
\ell(\sigma^2 \mid \epsilon_{1:n}) = \sum_{i=1}^{n} \log f(\epsilon_i \mid \sigma^2).
\]

This is the time to go back to the original variables. Substituting the
expression for \(\epsilon\) into the log likelihood function gives:

\begin{align*}
\ell(\beta_0, \beta_1, \ldots, \beta_k, \sigma^2 \mid y_{1:n}, x_{1:n}) &= \sum_{i=1}^{n} \log f\left(y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik}) \mid \sigma^2\right)\\
&= \sum_{i=1}^{n} \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik}))^2}{2\sigma^2}\right) \right)\\
&= \sum_{i=1}^{n} \left( -\frac{1}{2} \log(2\pi \sigma^2) - \frac{(y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik}))^2}{2\sigma^2} \right)\\
&= -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik}))^2.
\end{align*}

In the expression above, instead of solving for the specific case of two
predictor variables (as shown in the graph), I solved for the general
case of \(k\) predictor variables, and therefore we would like to find
the best values for the \(k+1\) coefficients
\(\beta_0, \beta_1, \ldots, \beta_k\). Notice that the first term in the
log likelihood does not depend on the \(\beta\) parameters, so we can
ignore it for the purpose of maximization.

Now, here's the kicker: because of the negative sign in front of the
second term, maximizing the log likelihood is equivalent to minimizing
the sum of squared errors (SSE):

\[
\text{SSE} = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik}))^2.
\]

Mind blown.

The general case that the best set of parameters \(\hat{\theta}\) are
those that maximize the log likelihood, \[
\hat{\theta} = \arg\max_{\theta_i} \ell(\theta_i \mid x_{1:n}),
\] translates in the specific case of linear regression to ``the best
set of parameters \(\hat{\beta}\) are those that minimize the sum of
squared errors'': \[
\hat{\beta} = \arg\min_{\beta_i} \text{SSE}.
\]

Final thought: whenever we use the Least Squares method to fit a linear
regression model, we are assuming (implicitly or explicitly) that the
error terms are normally distributed.

\chapter{MLE and information theory}\label{mle-and-information-theory}

We will show here how the maximizing the log likelihood naturally
connects to minimizing the Kullback-Leibler (KL) divergence.

We will start from a true data generating distribution \(P(x)\), and we
will assume we have \(n\) independent and identically distributed
(i.i.d.) observations \(x_1, x_2, \ldots, x_n\) drawn from it. We model
the data using a parametric family of distributions \(Q(x | \theta)\),
where \(\theta\) is the parameter we want to estimate. Our goal is to
find the parameters \(\theta\) that make the model distribution
\(Q(x | \theta)\) as close as possible to the true distribution
\(P(x)\). This set of parameters is typically found using Maximum
Likelihood Estimation (MLE):

\[
\hat{\theta} = \arg\max_{\theta} \ell(\theta \mid x_{1:n})
\]

In English: our best estimation of the parameters \(\theta\) is the one
that maximizes the parameters (also called arguments) of the log
likelihood of the observed data \(x_{1:n} = (x_1, x_2, \ldots, x_n)\).

The log likelihood function for the observed data given the parameters
\(\theta\) is defined as:

\[
\ell(\theta \mid x_{1:n}) = \sum_{i=1}^{n} \log Q(x_i | \theta)
\]

Here we used \(Q\) to denote the model distribution, to distinguish it
from the true data generating distribution \(P\). We will now divide
both sides by \(n\) to express the average log likelihood per
observation:

\[
\frac{1}{n} \ell(\theta \mid x_{1:n}) = \frac{1}{n} \sum_{i=1}^{n} \log Q(x_i | \theta)
\]

This step can be justified in a few ways:

\begin{itemize}
\tightlist
\item
  Dividing by a constant does not change the location of the maximum.
\item
  It is clear that \(\ell\) scales linearly with \(n\), that is, the
  more data we have, the larger the log likelihood will be. Dividing by
  \(n\) normalizes this effect.
\item
  I already know where I want to go with this, and this steps helps me
  :)
\end{itemize}

Note that now the right-hand side is the sample mean of the log
probabilities \(\log Q(x | \theta)\) evaluated at the observed data
points \(x_i\). This sample mean can also be understood as an empirical
expectation over the observed data, where each data point is given equal
weight \(1/n\):

\begin{align*}
\frac{1}{n} \ell(\theta \mid x_{1:n}) &= \frac{1}{n} \sum_{i=1}^{n} \log Q(x_i | \theta) \\
&= \mathbb{E}_{x \sim \hat{P}_n}[\log Q(x | \theta)],
\end{align*}

where the subscript \(x \sim \hat{P}_n\) indicates that the expectation
\((\mathbb{E})\) is taken with respect to the empirical distribution
\(\hat{P}_n\) defined by the observed data points
\(x_1, x_2, \ldots, x_n\).

Now, what happens as we collect more and more data, i.e., as \(n\)
approaches infinity? Our ``data-driven'' distribution \(\hat{P}_n\)
starts to look exactly like the ``true'' distribution \(P\), therefore:

\begin{align*}
\lim_{n \to \infty} \mathbb{E}_{x \sim \hat{P}_n}[\log Q(x | \theta)] &= \mathbb{E}_{x \sim P}[\log Q(x | \theta)] \\
&= \sum_{x} P(x) \log Q(x | \theta)
\end{align*}

We're almost there. Remember when we defined the cross-entropy in the
chapter \href{./information_theory/cross-entropy.ipynb}{cross-entropy
and KL divergence}? It was defined as:

\[
H(P, Q) = - \sum_x P(x) \log Q(x),
\]

which is exactly the negative of what we got before. We have shown so
far that maximizing the average log likelihood is equivalent to
minimizing the cross-entropy between the true distribution \(P\) and the
model distribution \(Q\). The caveat for this statement is that this
equivalence holds in the limit of infinite data. The very last step is
to connect this to the KL divergence, which we defined as:

\[
D_{KL}(P \| Q) = H(P, Q) - H(P).
\]

The term \(H(P)\) is the entropy of the true distribution \(P\), which
does not depend on the model parameters \(\theta\). Therefore,
minimizing the cross-entropy \(H(P, Q)\) is equivalent to minimizing the
KL divergence \(D_{KL}(P \| Q)\).

\section{implication}\label{implication}

The minimization of the KL divergence is ubiquitous in machine learning,
and it is often used as a loss function to train models. This connection
between MLE and KL divergence provides a theoretical foundation for many
machine learning algorithms.

\chapter{MLE and regularization}\label{mle-and-regularization}

\chapter{MLE and classification}\label{mle-and-classification}

\chapter{MLE and bayesian inference}\label{mle-and-bayesian-inference}

\part{miscellaneous}

\chapter{trend test}\label{trend-test}

How to determine if there is a trend (positive or negative) between two
variables?

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ linregress}
\ImportTok{import}\NormalTok{ scipy}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{"ticks"}\NormalTok{, font\_scale}\OperatorTok{=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{) }\CommentTok{\# for reproducibility}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{30}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ (}\FloatTok{0.2} \OperatorTok{*}\NormalTok{ x) }\OperatorTok{+} \DecValTok{10} \OperatorTok{+}\NormalTok{ np.random.normal(loc}\OperatorTok{=}\DecValTok{0}\NormalTok{, scale}\OperatorTok{=}\FloatTok{2.5}\NormalTok{, size}\OperatorTok{=}\DecValTok{30}\NormalTok{)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.scatter(x, y)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{, labelpad}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0, 0.5, 'Y')
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{misc/trend_test_files/figure-pdf/cell-3-output-2.png}}

\section{linear regression}\label{linear-regression}

One of the simplest ways to determine if there is a trend between two
variables is to use linear regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slope, intercept, \_, \_, \_ }\OperatorTok{=}\NormalTok{ linregress(x, y)}
\NormalTok{y\_hat }\OperatorTok{=}\NormalTok{ slope }\OperatorTok{*}\NormalTok{ x }\OperatorTok{+}\NormalTok{ intercept}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{ax.scatter(x, y, label}\OperatorTok{=}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.plot(x, y\_hat, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, label}\OperatorTok{=}\SpecialStringTok{f\textquotesingle{}model: y = }\SpecialCharTok{\{}\NormalTok{slope}\SpecialCharTok{:.2f\}}\SpecialStringTok{x + }\SpecialCharTok{\{}\NormalTok{intercept}\SpecialCharTok{:.2f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.}\BuiltInTok{set}\NormalTok{(xlabel}\OperatorTok{=}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{, rotation}\OperatorTok{=}\DecValTok{0}\NormalTok{, labelpad}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{ax.legend()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{misc/trend_test_files/figure-pdf/cell-4-output-1.png}}

Great, we found that there is a positive slope, its value is 0.13. It
this enough to say that there is a trend?

We need to determine if the slope is significantly different from zero.
For that we can use a t-test.

\textbf{Null Hypothesis:} The slope is equal to zero (no trend).\\
\textbf{Alternative Hypothesis:} The slope is not equal to zero (there
is a trend).

The t-statistic is calculated as:

\[
t = \frac{\text{slope} - 0}{SE_\text{slope}},
\]

where \(SE_\text{slope}\) is the standard error of the slope, and it is
given by:

\[
SE_\text{slope} = \frac{SD_\text{residuals}}{\sqrt{\sum (x_i - \bar{x})^2}}.
\]

where \(SD_\text{residuals}\) is the standard deviation of the
residuals, \(x_i\) are the individual \(x\) values, and \(\bar{x}\) is
the mean of the \(x\) values.

The standard deviation of the residuals is calculated as:

\[
SD_\text{residuals} = \sqrt{\frac{\sum (y_i - \hat{y}_i)^2}{n - 2}},
\]

where \(y_i\) are the observed y values, \(\hat{y}_i\) are the predicted
y values from the regression, and \(n\) is the number of data points.
The number of the degrees of freedom is \(n - 2\) because we are
estimating two parameters (the slope and the intercept) from the data.

Let's compute \(SE_\text{slope}\), the t-statistic, and the p-value for
our example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate the residuals}
\NormalTok{residuals }\OperatorTok{=}\NormalTok{ y }\OperatorTok{{-}}\NormalTok{ y\_hat}
\CommentTok{\# calculate the sum of squared residuals (SSR)}
\NormalTok{sum\_squared\_residuals }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(residuals}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\CommentTok{\# calculate the Residual Standard Error (s\_e)}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(x)}
\NormalTok{degrees\_of\_freedom }\OperatorTok{=}\NormalTok{ n }\OperatorTok{{-}} \DecValTok{2}
\NormalTok{residual\_std\_error }\OperatorTok{=}\NormalTok{ np.sqrt(sum\_squared\_residuals }\OperatorTok{/}\NormalTok{ degrees\_of\_freedom)}
\CommentTok{\# calculate the sum of squared deviations of x from its mean}
\NormalTok{x\_mean }\OperatorTok{=}\NormalTok{ np.mean(x)}
\NormalTok{sum\_squared\_x\_deviations }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((x }\OperatorTok{{-}}\NormalTok{ x\_mean)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}
\CommentTok{\# put it all together to get SE\_slope}
\CommentTok{\# SE\_slope = (typical error) / (spread of x)}
\NormalTok{SE\_slope }\OperatorTok{=}\NormalTok{ residual\_std\_error }\OperatorTok{/}\NormalTok{ np.sqrt(sum\_squared\_x\_deviations)}
\CommentTok{\# verify the result against the value directly from scipy}
\NormalTok{scipy\_slope, scipy\_intercept, scipy\_r, scipy\_p, scipy\_se }\OperatorTok{=}\NormalTok{ linregress(x, y)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"manually calculated SE\_slope:          }\SpecialCharTok{\{}\NormalTok{SE\_slope}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"SE\_slope from scipy.stats.linregress:  }\SpecialCharTok{\{}\NormalTok{scipy\_se}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
manually calculated SE_slope:          0.057695
SE_slope from scipy.stats.linregress:  0.057695
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_statistic }\OperatorTok{=}\NormalTok{ (slope}\OperatorTok{{-}}\DecValTok{0}\NormalTok{) }\OperatorTok{/}\NormalTok{ SE\_slope}
\NormalTok{p\_value }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ scipy.stats.t.cdf(np.}\BuiltInTok{abs}\NormalTok{(t\_statistic), df}\OperatorTok{=}\NormalTok{degrees\_of\_freedom))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"manually calculated p{-}value: }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}scipy p{-}value:               }\SpecialCharTok{\{}\NormalTok{scipy\_p}\SpecialCharTok{:.6f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
manually calculated p-value: 0.028344
scipy p-value:               0.028344
\end{verbatim}

If we choose a significance level \(\alpha=0.05\), the p-value we found
indicates that we can reject the null hypothesis and conclude that there
is a significant trend between x and y.

If instead of testing if the slope is different from zero, but rather if
it is greater than zero (i.e., a one-sided test), we would divide the
p-value by 2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_value }\OperatorTok{=}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ scipy.stats.t.cdf(np.}\BuiltInTok{abs}\NormalTok{(t\_statistic), df}\OperatorTok{=}\NormalTok{degrees\_of\_freedom))}
\NormalTok{scipy\_slope, scipy\_intercept, scipy\_r, scipy\_p, scipy\_se }\OperatorTok{=}\NormalTok{ linregress(x, y, alternative}\OperatorTok{=}\StringTok{\textquotesingle{}greater\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"manually calculated p{-}value: }\SpecialCharTok{\{}\NormalTok{p\_value}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}scipy p{-}value:               }\SpecialCharTok{\{}\NormalTok{scipy\_p}\SpecialCharTok{:.6f\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
manually calculated p-value: 0.014172
scipy p-value:               0.014172
\end{verbatim}

One last remark. What does the formula for the standard error of the
slope mean?

\begin{itemize}
\tightlist
\item
  inside the square root, we have a quantity dependent on y squared
  divided by a quantity dependent on x squared. Dimensionally this makes
  sense, because the standard error of the slope should have the same
  dimension as the slope \(\Delta y/\Delta x\).
\item
  the larger the variability of the residuals (i.e., the more scattered
  the data points are around the regression line), the larger the
  standard error of the slope, and thus the less precise our estimate of
  the slope is.
\item
  We can \href{https://stats.stackexchange.com/a/342672}{manipulate the
  formula} a little bit to get more intuition: \[
    SE_\text{slope} = \sqrt{\frac{1}{n-2}}\frac{SD_y}{SD_x}\sqrt{1-r^2},
  \] where \(SD_y\) and \(SD_x\) are the standard deviations of y and x,
  respectively, and \(r\) is the correlation coefficient between \(x\)
  and \(y\). From this formula we can see that:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    the standard error of the slope decreases with increasing sample
    size \(n\) (more data points lead to a more precise estimate of the
    slope);
  \item
    imagine all the data points in a rectangular box, and all the
    possible slopes that can be drawn within that box. If you change the
    dimensions of the box, you need to account for that, and that is the
    second term.
  \item
    The last term acounts for the spread of the points about the line.
  \end{enumerate}
\end{itemize}

\section{Mann-Kendall Trend Test}\label{mann-kendall-trend-test}

The method above assumed that the relationship between x and y is
linear, and that the residuals are normally distributed. If these
assumptions are not met, we can use a non-parametric test like the
Mann-Kendall trend test. The intuition behind this test works like a
voting system. You go through your data and compare every data point to
all points that come after it.

\begin{itemize}
\tightlist
\item
  if a later points is higher, you give a +1 vote
\item
  if a later point is lower, you give a -1 vote
\item
  if they are equal, you give a 0 vote
\end{itemize}

All these votes are summed up. A large positive sum indicates an
increasing trend, a large negative sum indicates a decreasing trend, and
a sum close to zero indicates no trend. We can then calculate a test
statistic \(Z\) based on the sum of votes, and use it to determine the
p-value. If the p-value is less than our chosen significance level
(e.g., 0.05), we can reject the null hypothesis of no trend. We can use
the package
\href{https://pypi.org/project/pymannkendall/}{\texttt{pymannkendall}}
to perform the Mann-Kendall trend test.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ pymannkendall }\ImportTok{import}\NormalTok{ original\_test}
\NormalTok{mk\_result }\OperatorTok{=}\NormalTok{ original\_test(y)}
\BuiltInTok{print}\NormalTok{(mk\_result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Mann_Kendall_Test(trend='increasing', h=True, p=0.04970274086760851, z=1.9625134103851736, Tau=0.25517241379310346, s=111.0, var_s=3141.6666666666665, slope=0.14036402565970577, intercept=11.836643578083883)
\end{verbatim}

The test concluded that there is an increasing trend, with a p-value of
0.0497.

\section{Spearman's Rank Correlation}\label{spearmans-rank-correlation}

This is another non-parametric test. It assesses how well the
relationship between two variables can be described using a monotonic
function. It does this by converting the data to ranks and then
calculating the Pearson correlation coefficient on the ranks. The
Spearman's rank correlation coefficient, denoted by \(\rho\) (rho),
ranges from -1 to 1, where:

\begin{itemize}
\tightlist
\item
  1 indicates a perfect positive \textbf{monotonic} relationship,
\item
  -1 indicates a perfect negative \textbf{monotonic} relationship,
\item
  0 indicates no monotonic relationship.
\end{itemize}

This test is robust to outliers and does not assume a linear
relationship between the variables.

We can use the \texttt{scipy.stats.spearmanr} function to calculate
Spearman's rank correlation coefficient and the associated p-value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spearman\_corr, spearman\_p }\OperatorTok{=}\NormalTok{ scipy.stats.spearmanr(x, y)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Spearman\textquotesingle{}s correlation: }\SpecialCharTok{\{}\NormalTok{spearman\_corr}\SpecialCharTok{:.6f\}}\SpecialStringTok{, p{-}value: }\SpecialCharTok{\{}\NormalTok{spearman\_p}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Spearman's correlation: 0.361958, p-value: 0.049356
\end{verbatim}

We found that there is a positive monotonic relationship between x and
y, with a p-value of 0.0494, indicating that the relationship is
statistically significant at the 0.05 significance level.

\section{Theil-Sen Estimator}\label{theil-sen-estimator}

The Theil-Sen estimator is a robust method for estimating the slope of a
linear trend. It is particularly useful when the data contains outliers
or is not normally distributed. The Theil-Sen estimator calculates the
slope as the median of all possible pairwise slopes between data points.
We can use the \texttt{scipy.stats.theilslopes} function to calculate
the Theil-Sen estimator and the associated confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ theilslopes}
\NormalTok{theil\_slope, theil\_intercept, theil\_lower, theil\_upper }\OperatorTok{=}\NormalTok{ theilslopes(y, x, }\FloatTok{0.95}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Theil{-}Sen slope: }\SpecialCharTok{\{}\NormalTok{theil\_slope}\SpecialCharTok{:.6f\}}\SpecialStringTok{, intercept: }\SpecialCharTok{\{}\NormalTok{theil\_intercept}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Theil-Sen slope: 0.140364, intercept: 11.836644
\end{verbatim}




\end{document}
