{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30c3c23",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"random forest\"\n",
    "execute:\n",
    "  # echo: false\n",
    "  freeze: auto  # re-render only when source changes\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-summary: \"Show the code\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df837f",
   "metadata": {},
   "source": [
    "The motivation behind random forests is to avoid the weird regions in the feature space that a single decision tree might create.\n",
    "In the [tutorial for classification with CART](/decision_trees/CART_classification.html#overfitting), we saw this:\n",
    "![](decision_boundaries_no_max_depth.png)\n",
    "\n",
    "The small blue regions inside the yellow region are highly undesirable.  The decision tree learned very well the data it was given, but it will probably not generalize well to new data points.\n",
    "Random forests solve this problem in three steps.\n",
    "\n",
    "## step 1: bootstrap sampling\n",
    "\n",
    "Instead of running the decision tree algorithm once on the entire training set, we run it multiple times on different bootstrap samples of the training set.\n",
    "We already learned about bootstrap sampling in the [empirical confidence interval](confidence_interval/empirical_confidence_interval.html) tutorial.\n",
    "In a nutshell, if we have a data set with $N$ data points, we create a bootstrap sample by sampling $N$ data points **with replacement** from the original data set.\n",
    "This means that some data points will appear multiple times in the bootstrap sample, while others will not appear at all.\n",
    "We can choose how many bootstrap samples we want to create.  The default used by `sklearn`'s `RandomForestRegressor` is 100 (this argument is called `n_estimators`).\n",
    "\n",
    "What fraction of the dataset will a given bootstrap sample contain, on average?\n",
    "The probability of choosing a specific data point in one draw is $1/N$.\n",
    "Therefore, the probability of **not** choosing that data point in one draw is $1 - 1/N$.\n",
    "If we draw $N$ times with replacement, the probability of never choosing that data point is\n",
    "\n",
    "$$\n",
    "\\left(1 - \\frac{1}{N}\\right)^N\n",
    "$$\n",
    "\n",
    "As $N$ becomes large...\n",
    "\n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\left(1 - \\frac{1}{N}\\right)^N = e^{-1} \\approx 0.37.\n",
    "$$\n",
    "\n",
    "::: {.column-margin}\n",
    "This follows from the definition of the exponential function:\n",
    "$$\n",
    "e^x = \\lim_{n \\to \\infty} \\left(1 + \\frac{x}{n}\\right)^n,\n",
    "$$\n",
    "just set $x = -1$.\n",
    ":::\n",
    "\n",
    "This means that, on average, a bootstrap sample will contain about 63% of the original data points (because about 37% of them will not be chosen at all).\n",
    "\n",
    "## step 2: random feature selection\n",
    "\n",
    "When training each decision tree on a bootstrap sample, we also randomly select a subset of the features to consider **for each split in the tree**.\n",
    "For example, if we have 10 features in total, we might randomly select 3 of them to consider for each split.\n",
    "This further increases the diversity among the trees in the forest, which helps to reduce overfitting.\n",
    "\n",
    "Why is this important? Imagine a dataset where feature number 1 is very strongly correlated with the target variable.\n",
    "In that case, most decision trees will likely use that feature for the top split, leading to similar trees and less diversity in the forest.\n",
    "\n",
    "As a rule of thumb, we typically use the square root or the logarithm (base 2) of the total number of features as the number of features to consider for each split.\n",
    "The argument in `sklearn`'s `RandomForestRegressor` that controls this is called `max_features`, and its default value is 1.0.\n",
    "This means that if we don't specify anything, it will use 100% of the features in each split, and we will not have \"feature decorrelation\".\n",
    "In the documentation, search for [max_features](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to find more details.\n",
    "\n",
    "## step 3: bagging\n",
    "\n",
    "Finally, to make a prediction for a new data point, we pass it through each of the decision trees in the forest and average their predictions (for regression) or take a majority vote (for classification).\n",
    "This process is called \"bagging\" (short for **b**ootstrap **agg**regat**ing**).\n",
    "By averaging the predictions of multiple trees, we can reduce the variance of the model and improve its generalization performance.\n",
    "\n",
    "## example: iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd26d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264d0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"import libraries\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3867daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Tree Score: 0.999\n",
      "Random Forest Score: 0.991\n"
     ]
    }
   ],
   "source": [
    "#| code-summary: \"load iris dataset and prepare data\"\n",
    "iris = load_iris()\n",
    "\n",
    "# 1. Prepare Data (3 Inputs, 1 Output)\n",
    "# columns: 0=SepalLen, 1=SepalWid, 2=PetalLen, 3=PetalWid\n",
    "X_full = iris.data[:, [0, 1, 2]]  # 3 features\n",
    "y_full = iris.data[:, 3]          # Target: Petal Width\n",
    "\n",
    "# 2. Train Models\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "tree_model.fit(X_full, y_full)\n",
    "rf_model.fit(X_full, y_full)\n",
    "\n",
    "# 3. Compare Errors (The numerical proof)\n",
    "print(f\"Single Tree Score: {tree_model.score(X_full, y_full):.3f}\")\n",
    "print(f\"Random Forest Score: {rf_model.score(X_full, y_full):.3f}\")\n",
    "\n",
    "# X = iris.data[:, [0, 1]]\n",
    "# y = iris.data[:,[2]].flatten()\n",
    "# iris.feature_names = ['height', 'weight', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04eb337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Tree Test Score:   0.845\n",
      "Random Forest Test Score: 0.931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Split the data (80% for training, 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Train on ONLY the training set\n",
    "tree_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Test on the unseen data\n",
    "print(f\"Single Tree Test Score:   {tree_model.score(X_test, y_test):.3f}\")\n",
    "print(f\"Random Forest Test Score: {rf_model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b1026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olympus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
